{
  "articles": [
    {
      "url": "https://medium.com/tailwinds-navigator/kubernetes-tip-how-statefulsets-behave-differently-than-deployments-when-node-fails-d29e36bca7d5",
      "title": "Kubernetes Tip: How Statefulsets Behave Differently Than Deployments When Node Fails?",
      "content": "<div><article><section class=\"ck cl cm cn aj co cp s\"></section><div><section class=\"cu cv cw cx cy\"><div class=\"n p\"><div class=\"ab ac ae af ag cz ai aj\"><p id=\"ec3a\" class=\"gg gh dc gi b gj gk gl gm gn go gp gq gr gs gt gu gv gw gx gy gz ha hb hc hd cu dz\">Kubernetes is a platform that one needs to morph to make it work. As part of personalizing effort, having a strategy to handle node failure cases becomes an important criterion. To implement such a blueprint, one needs to understand how different controllers behave when a node fails. In this blog post, we look at how Stateful Sets behave differently than Deployment Sets when node failure occurs.</p><h2 id=\"a75a\" class=\"he hf dc ej hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib dz\">Recap On How Kubernetes Treats Deployment Sets When Node Fails?.</h2><p id=\"5291\" class=\"gg gh dc gi b gj ic gl gm gn id gp gq gr ie gt gu gv if gx gy gz ig hb hc hd cu dz\">We have discussed this in detail in <a class=\"eu ih\" href=\"https://medium.com/tailwinds-navigator/kubernetes-tip-what-happens-to-pods-running-on-node-that-become-unreachable-3d409f734e5d\"><strong class=\"gi ii\">this</strong></a> blog post but the summary is, When a node fails, the Deployment controller terminates pods running that node and creates a new set of replicas to be scheduled on available nodes.</p><p id=\"42ca\" class=\"gg gh dc gi b gj gk gl gm gn go gp gq gr gs gt gu gv gw gx gy gz ha hb hc hd cu dz\">Here is the flow chart for how Deployment works when a node fails?</p><figure class=\"ik il im in io ip cm cn paragraph-image\"><img alt=\"Image for post\" class=\"t u v is aj\" src=\"https://miro.medium.com/max/1322/1*-OcgsgdBzLks14YeM2-HKw.jpeg\" width=\"661\" srcset=\"https://miro.medium.com/max/552/1*-OcgsgdBzLks14YeM2-HKw.jpeg 276w, https://miro.medium.com/max/1104/1*-OcgsgdBzLks14YeM2-HKw.jpeg 552w, https://miro.medium.com/max/1280/1*-OcgsgdBzLks14YeM2-HKw.jpeg 640w, https://miro.medium.com/max/1322/1*-OcgsgdBzLks14YeM2-HKw.jpeg 661w\" sizes=\"661px\"><figcaption class=\"jd je co cm cn jf jg ej b ek el fr\"><strong class=\"ej jh\">Deployment Set Behaviour When Node Fails.</strong></figcaption></figure><p id=\"9a62\" class=\"gg gh dc gi b gj gk gl gm gn go gp gq gr gs gt gu gv gw gx gy gz ha hb hc hd cu dz\">Let\u0019s understand the behavior of Statefulsets for node failures with an example.</p><h2 id=\"3213\" class=\"he hf dc ej hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib dz\">Example Cluster Having StatefulSets.</h2><p id=\"3c24\" class=\"gg gh dc gi b gj ic gl gm gn id gp gq gr ie gt gu gv if gx gy gz ig hb hc hd cu dz\">The example Kind cluster is created having one master node and 3 worker nodes. An Nginx Stateful Set is created having 2 replicas. These replicas run on different nodes; kind-worker &amp; kind-worker2. <strong class=\"gi ii\">Figure-1</strong> captures the state of the example kind cluster.</p><figure class=\"ik il im in io ip cm cn paragraph-image\"><img alt=\"Image for post\" class=\"t u v is aj\" src=\"https://miro.medium.com/max/3808/1*r6VP6Z27UqVIxv1nIwVgXw.png\" width=\"1904\" srcset=\"https://miro.medium.com/max/552/1*r6VP6Z27UqVIxv1nIwVgXw.png 276w, https://miro.medium.com/max/1104/1*r6VP6Z27UqVIxv1nIwVgXw.png 552w, https://miro.medium.com/max/1280/1*r6VP6Z27UqVIxv1nIwVgXw.png 640w, https://miro.medium.com/max/1400/1*r6VP6Z27UqVIxv1nIwVgXw.png 700w\" sizes=\"700px\"><figcaption class=\"jd je co cm cn jf jg ej b ek el fr\"><strong class=\"ej jh\">Figure-1: Example Cluster.</strong></figcaption></figure><h2 id=\"a27c\" class=\"he hf dc ej hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib dz\"><strong class=\"bg\">Create Node Failure Scenario.</strong></h2><p id=\"9215\" class=\"gg gh dc gi b gj ic gl gm gn id gp gq gr ie gt gu gv if gx gy gz ig hb hc hd cu dz\">A simple way is to create a node failure scenario is to delete the kind-worker2. <strong class=\"gi ii\">Figure-2</strong> provides the required steps.</p><figure class=\"ik il im in io ip cm cn paragraph-image\"><img alt=\"Image for post\" class=\"t u v is aj\" src=\"https://miro.medium.com/max/3808/1*NvrY7BBJLmRZVJPrnUd7qw.png\" width=\"1904\" srcset=\"https://miro.medium.com/max/552/1*NvrY7BBJLmRZVJPrnUd7qw.png 276w, https://miro.medium.com/max/1104/1*NvrY7BBJLmRZVJPrnUd7qw.png 552w, https://miro.medium.com/max/1280/1*NvrY7BBJLmRZVJPrnUd7qw.png 640w, https://miro.medium.com/max/1400/1*NvrY7BBJLmRZVJPrnUd7qw.png 700w\" sizes=\"700px\"><figcaption class=\"jd je co cm cn jf jg ej b ek el fr\"><strong class=\"ej jh\">Figure-2: Captures Steps To Create Node Failure.</strong></figcaption></figure><h2 id=\"20df\" class=\"he hf dc ej hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib dz\">How The Kubernetes System Behave?.</h2><p id=\"778d\" class=\"gg gh dc gi b gj ic gl gm gn id gp gq gr ie gt gu gv if gx gy gz ig hb hc hd cu dz\">The worker node(kind-worker2) is set to <strong class=\"gi ii\">NotReady</strong> state immediately but the pod continuous to run. The system waits for <strong class=\"gi ii\">pod-eviction-timeout</strong> interval (5 mins is the default value) before setting the pod to <strong class=\"gi ii\">Terminating</strong> state.</p><p id=\"d540\" class=\"gg gh dc gi b gj gk gl gm gn go gp gq gr gs gt gu gv gw gx gy gz ha hb hc hd cu dz\"><strong class=\"gi ii\">To our surprise</strong>, new pods are <strong class=\"gi ii\">not</strong> created for the Statefulsets while in a similar scenario news replicas were spun up for deployment sets. <strong class=\"gi ii\">Figure-3</strong> captures the state of the Kubernetes cluster after a node failure in case of Statefulsets.</p><figure class=\"ik il im in io ip cm cn paragraph-image\"><img alt=\"Image for post\" class=\"t u v is aj\" src=\"https://miro.medium.com/max/3784/1*WXuiPFGaARSdTOufe8hVOg.png\" width=\"1892\" srcset=\"https://miro.medium.com/max/552/1*WXuiPFGaARSdTOufe8hVOg.png 276w, https://miro.medium.com/max/1104/1*WXuiPFGaARSdTOufe8hVOg.png 552w, https://miro.medium.com/max/1280/1*WXuiPFGaARSdTOufe8hVOg.png 640w, https://miro.medium.com/max/1400/1*WXuiPFGaARSdTOufe8hVOg.png 700w\" sizes=\"700px\"><figcaption class=\"jd je co cm cn jf jg ej b ek el fr\"><strong class=\"ej jh\">Figure-3: State Of Nginx Stateful Set After Node Failure.</strong></figcaption></figure><h2 id=\"58d9\" class=\"he hf dc ej hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib dz\">Why A New Replica Is Not Spun Up?.</h2><p id=\"b15f\" class=\"gg gh dc gi b gj ic gl gm gn id gp gq gr ie gt gu gv if gx gy gz ig hb hc hd cu dz\">According to the <a href=\"https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/pod-safety.md\" class=\"eu ih\">Pod-Safety</a> document, for clustered software in the Kubernetes, the system provides a guarantee of running <strong class=\"gi ii\">at</strong> <strong class=\"gi ii\">most one pet</strong> at any point in time. This rule is applicable to Statefulsets as replica are treated as pets while this does not apply for Deployment as replica are treated like cattle.</p><p id=\"cf54\" class=\"gg gh dc gi b gj gk gl gm gn go gp gq gr gs gt gu gv gw gx gy gz ha hb hc hd cu dz\">The fundamental reason for having <strong class=\"gi ii\">at most one pet</strong> is because the Statefulsets are mostly used for clustered applications that have their own ways of electing master and slaves. In a node failure scenario, The master does not have enough information to ascertain if the node is actually failed or failure is due to a network partition. Therefore, the master refrains from taking any action thereby leading to more problems. The master takes a practical approach by having just one instance less but works in a reliable way.</p><p id=\"bc18\" class=\"gg gh dc gi b gj gk gl gm gn go gp gq gr gs gt gu gv gw gx gy gz ha hb hc hd cu dz\">How does one recover from this dangling state?</p><h2 id=\"ba1c\" class=\"he hf dc ej hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib dz\">Recommendation.</h2><p id=\"5725\" class=\"gg gh dc gi b gj ic gl gm gn id gp gq gr ie gt gu gv if gx gy gz ig hb hc hd cu dz\">There are a few ways one could handle this scenario. They are</p><p id=\"60d7\" class=\"gg gh dc gi b gj gk gl gm gn go gp gq gr gs gt gu gv gw gx gy gz ha hb hc hd cu dz\">Set <strong class=\"gi ii\">terminationGracePeriodSeconds</strong> to 0 on pods spec. This will ensure that statefulset pods will be deleted forcefully when the node rejoins the cluster. By doing so, Kubernetes master knows that pod safety guarantee is maintained, so will spin up a new replica. The downside is obviously that pod shutdown is not graceful.</p><p id=\"9110\" class=\"gg gh dc gi b gj gk gl gm gn go gp gq gr gs gt gu gv gw gx gy gz ha hb hc hd cu dz\">Have an automated way to detect node failures and delete those nodes forcefully if you know for sure the node is actually failed or removed. This will ensure statefulset pods to be respun on available nodes.</p><h2 id=\"19b7\" class=\"he hf dc ej hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib dz\">References.</h2><p id=\"baee\" class=\"gg gh dc gi b gj ic gl gm gn id gp gq gr ie gt gu gv if gx gy gz ig hb hc hd cu dz\">Pod Safety: <a href=\"https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/pod-safety.md\" class=\"eu ih\">https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/pod-safety.md</a></p><p id=\"cf13\" class=\"gg gh dc gi b gj gk gl gm gn go gp gq gr gs gt gu gv gw gx gy gz ha hb hc hd cu dz\">Statefulset Design Considerations: <a href=\"https://github.com/kubernetes/community/blob/master/contributors/design-proposals/apps/stateful-apps.md\" class=\"eu ih\">https://github.com/kubernetes/community/blob/master/contributors/design-proposals/apps/stateful-apps.md</a></p><p id=\"3fbc\" class=\"gg gh dc gi b gj gk gl gm gn go gp gq gr gs gt gu gv gw gx gy gz ha hb hc hd cu dz\">Discussions In Kubernetes Github On This Issue: <a href=\"https://github.com/kubernetes/kubernetes/issues/74947\" class=\"eu ih\">https://github.com/kubernetes/kubernetes/issues/74947</a> <a href=\"https://github.com/kubernetes/kubernetes/issues/54368\" class=\"eu ih\">https://github.com/kubernetes/kubernetes/issues/54368</a></p><p id=\"6240\" class=\"gg gh dc gi b gj gk gl gm gn go gp gq gr gs gt gu gv gw gx gy gz ha hb hc hd cu dz\"><strong class=\"gi ii\">P.S:</strong> Thanks To <a class=\"eu ih\" href=\"https://medium.com/@r.aaron.johnson\"><strong class=\"gi ii\">Aaron</strong></a> For Suggesting To Write This Post.</p></div></div></section></div></article></div>",
      "contentAsText": "Kubernetes is a platform that one needs to morph to make it work. As part of personalizing effort, having a strategy to handle node failure cases becomes an important criterion. To implement such a blueprint, one needs to understand how different controllers behave when a node fails. In this blog post, we look at how Stateful Sets behave differently than Deployment Sets when node failure occurs.Recap On How Kubernetes Treats Deployment Sets When Node Fails?.We have discussed this in detail in this blog post but the summary is, When a node fails, the Deployment controller terminates pods running that node and creates a new set of replicas to be scheduled on available nodes.Here is the flow chart for how Deployment works when a node fails?Deployment Set Behaviour When Node Fails.Let\u0019s understand the behavior of Statefulsets for node failures with an example.Example Cluster Having StatefulSets.The example Kind cluster is created having one master node and 3 worker nodes. An Nginx Stateful Set is created having 2 replicas. These replicas run on different nodes; kind-worker & kind-worker2. Figure-1 captures the state of the example kind cluster.Figure-1: Example Cluster.Create Node Failure Scenario.A simple way is to create a node failure scenario is to delete the kind-worker2. Figure-2 provides the required steps.Figure-2: Captures Steps To Create Node Failure.How The Kubernetes System Behave?.The worker node(kind-worker2) is set to NotReady state immediately but the pod continuous to run. The system waits for pod-eviction-timeout interval (5 mins is the default value) before setting the pod to Terminating state.To our surprise, new pods are not created for the Statefulsets while in a similar scenario news replicas were spun up for deployment sets. Figure-3 captures the state of the Kubernetes cluster after a node failure in case of Statefulsets.Figure-3: State Of Nginx Stateful Set After Node Failure.Why A New Replica Is Not Spun Up?.According to the Pod-Safety document, for clustered software in the Kubernetes, the system provides a guarantee of running at most one pet at any point in time. This rule is applicable to Statefulsets as replica are treated as pets while this does not apply for Deployment as replica are treated like cattle.The fundamental reason for having at most one pet is because the Statefulsets are mostly used for clustered applications that have their own ways of electing master and slaves. In a node failure scenario, The master does not have enough information to ascertain if the node is actually failed or failure is due to a network partition. Therefore, the master refrains from taking any action thereby leading to more problems. The master takes a practical approach by having just one instance less but works in a reliable way.How does one recover from this dangling state?Recommendation.There are a few ways one could handle this scenario. They areSet terminationGracePeriodSeconds to 0 on pods spec. This will ensure that statefulset pods will be deleted forcefully when the node rejoins the cluster. By doing so, Kubernetes master knows that pod safety guarantee is maintained, so will spin up a new replica. The downside is obviously that pod shutdown is not graceful.Have an automated way to detect node failures and delete those nodes forcefully if you know for sure the node is actually failed or removed. This will ensure statefulset pods to be respun on available nodes.References.Pod Safety: https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/pod-safety.mdStatefulset Design Considerations: https://github.com/kubernetes/community/blob/master/contributors/design-proposals/apps/stateful-apps.mdDiscussions In Kubernetes Github On This Issue: https://github.com/kubernetes/kubernetes/issues/74947 https://github.com/kubernetes/kubernetes/issues/54368P.S: Thanks To Aaron For Suggesting To Write This Post.",
      "publishedDate": "2020-08-26T03:24:02.629Z",
      "description": "Kubernetes is a platform that one needs to morph to make it work. As part of personalizing effort, having a strategy to handle node failure cases becomes an important criterion. To implement such a…",
      "ogDescription": "Kubernetes is a platform that one needs to morph to make it work. As part of personalizing effort, having a strategy to handle node…"
    },
    {
      "url": "https://github.com/timescale/tobs",
      "title": "timescale/tobs",
      "content": "<div><div><article class=\"markdown-body entry-content container-lg\">\n<p>Tobs is a tool that aims to make it as easy as possible to install a full observability\nstack into a Kubernetes cluster. Currently this stack includes:</p>\n\n<p>We plan to expand this stack over time and welcome contributions.</p>\n<p>Tobs provides a CLI tool to make deployment and operations easier. We also provide\nHelm charts that can be used directly or as sub-charts for other projects.</p>\n<p>See a demo of tobs in action by clicking the video below:</p>\n<p>\n<a href=\"https://www.youtube.com/watch?v=MSvBsXOI1ks\"> <img src=\"https://media.giphy.com/media/e8y7Lq7V5F0K9zQs20/giphy.gif\"> </a>\n</p>\n<h2><a id=\"user-content--quick-start\" class=\"anchor\" href=\"https://github.com/timescale/tobs#-quick-start\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a><g-emoji class=\"g-emoji\">=%</g-emoji> Quick start</h2>\n<p><strong>Dependencies</strong>: <a href=\"https://helm.sh/docs/intro/install/\">Helm</a></p>\n<h2><a id=\"user-content-using-the-tobs-cli-tool\" class=\"anchor\" href=\"https://github.com/timescale/tobs#using-the-tobs-cli-tool\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Using the tobs CLI tool</h2>\n<p>The <a href=\"https://github.com/timescale/tobs/blob/master/cli/README.md\">CLI tool</a> provides the most seamless experience for interacting with tobs.</p>\n<p>Getting started with the CLI tool is a two-step process: First you install the CLI tool locally, then you use the CLI tool to install the tobs stack into your Kubernetes cluster.</p>\n<h3><a id=\"user-content-installing-the-cli-tool\" class=\"anchor\" href=\"https://github.com/timescale/tobs#installing-the-cli-tool\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Installing the CLI tool</h3>\n<p>To download and install tobs, run the following in your terminal, then follow the on-screen instructions.</p>\n<div class=\"highlight highlight-source-shell\"><pre>curl --proto <span class=\"pl-s\"><span class=\"pl-pds\">&apos;</span>=https<span class=\"pl-pds\">&apos;</span></span> --tlsv1.2 -sSLf  https://tsdb.co/install-tobs-sh <span class=\"pl-k\">|</span>sh</pre></div>\n<p>Alternatively, you can download the CLI directly via <a href=\"https://github.com/timescale/tobs/blob/master/releases\">our releases page</a></p>\n<h3><a id=\"user-content-using-the-tobs-cli-tool-to-deploy-the-stack-into-your-kubernetes-cluster\" class=\"anchor\" href=\"https://github.com/timescale/tobs#using-the-tobs-cli-tool-to-deploy-the-stack-into-your-kubernetes-cluster\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Using the tobs CLI tool to deploy the stack into your Kubernetes cluster</h3>\n<p>After setting up tobs run the following to install the tobs helm charts into your Kubernetes cluster</p>\n\n<p>This will deploy all of the tobs components into your cluster and provide instructions as to next steps.</p>\n<h3><a id=\"user-content-getting-started-by-viewing-your-metrics-in-grafana\" class=\"anchor\" href=\"https://github.com/timescale/tobs#getting-started-by-viewing-your-metrics-in-grafana\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Getting started by viewing your metrics in Grafana</h3>\n<p>To see your Grafana dashboards after installation run</p>\n<div class=\"highlight highlight-source-shell\"><pre>tobs grafana change-password <span class=\"pl-k\">&lt;</span>new_password<span class=\"pl-k\">&gt;</span>\ntobs grafana port-forward</pre></div>\n<p>Then, point your browser to <a href=\"http://127.0.0.1:8080/\">http://127.0.0.1:8080/</a> and login with the <code>admin</code> username.</p>\n<p>More details about the CLI tool can be found <a href=\"https://github.com/timescale/tobs/blob/master/cli/README.md\">here</a></p>\n<h2><a id=\"user-content-configuring-the-stack\" class=\"anchor\" href=\"https://github.com/timescale/tobs#configuring-the-stack\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Configuring the stack</h2>\n<p>All configuration for all components happens through the helm values.yml file.\nYou can view the self-documenting <a href=\"https://github.com/timescale/tobs/blob/master/chart/values.yaml\">default values.yaml</a> in the repo.\nWe also have additional documentation about individual configuration settings in our\n<a href=\"https://github.com/timescale/tobs/blob/master/chart/README.md#configuring-helm-chart\">Helm chart docs</a>.</p>\n<p>To modify the settings, first create a values.yaml file:</p>\n<div class=\"highlight highlight-source-shell\"><pre>tobs helm show-values <span class=\"pl-k\">&gt;</span> values.yaml</pre></div>\n<p>Then modify the values.yaml file using your favorite editor.\nFinally, deploy with the new settings using:</p>\n<div class=\"highlight highlight-source-shell\"><pre>tobs install -f values.yaml</pre></div>\n<h2><a id=\"user-content-alternative-deployment-methods\" class=\"anchor\" href=\"https://github.com/timescale/tobs#alternative-deployment-methods\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a><g-emoji class=\"g-emoji\">=&#xFFFD;</g-emoji>Alternative deployment methods</h2>\n\n<p>Users sometimes want to use our Helm charts as sub-charts for other project or integrate them into their infrastructure without using our CLI tool. This is a supported use-case and instructions on using the Helm charts can be found <a href=\"https://github.com/timescale/tobs/blob/master/chart/README.md\">here</a>.</p>\n<h2><a id=\"user-content-\u000f-contributing\" class=\"anchor\" href=\"https://github.com/timescale/tobs#\u000f-contributing\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a><g-emoji class=\"g-emoji\">\u000f\u000f</g-emoji> Contributing</h2>\n<p>We welcome contributions to tobs, which is\nlicensed and released under the open-source Apache License, Version 2.  The\nsame <a href=\"https://github.com/timescale/timescaledb/blob/master/CONTRIBUTING.md\">Contributor&apos;s\nAgreement</a>\napplies as in TimescaleDB; please sign the <a href=\"https://cla-assistant.io/timescale/tobs\">Contributor License\nAgreement</a> (CLA) if\nyou&apos;re a new contributor.</p>\n</article></div></div><hr><h4>Page 2</h4><body class=\"logged-out env-production page-responsive\"> <include-fragment class=\"js-notification-shelf-include-fragment\"></include-fragment> <div class=\"application-main \"> </div> <p id=\"ajax-error-message\" class=\"ajax-error-message\"> <svg class=\"octicon octicon-alert\" width=\"16\" height=\"16\"><path/></svg> You can&#x2019;t perform that action at this time. </p> <p class=\"js-stale-session-flash\"> <svg class=\"octicon octicon-alert\" width=\"16\" height=\"16\"><path/></svg> <span class=\"js-stale-session-flash-signed-in\">You signed in with another tab or window. <a href>Reload</a> to refresh your session.</span> <span class=\"js-stale-session-flash-signed-out\">You signed out in another tab or window. <a href>Reload</a> to refresh your session.</span> </p> <template id=\"site-details-dialog\"> <details class=\"details-reset details-overlay details-overlay-dark lh-default text-gray-dark hx_rsm\"> <summary></summary> <details-dialog class=\"Box Box--overlay d-flex flex-column anim-fade-in fast hx_rsm-dialog hx_rsm-modal\"> </details-dialog> </details>\n</template> <div class=\"js-cookie-consent-banner\"> <div class=\"hx_cookie-banner p-2 p-sm-3 p-md-4\"> <div class=\"Box hx_cookie-banner-box box-shadow-medium mx-auto\"> <div class=\"Box-body border-0 py-0 px-3 px-md-4\"> <div class=\"js-main-cookie-banner hx_cookie-banner-main\"> <div class=\"d-md-flex flex-items-center py-3\"> <p class=\"f5 flex-1 mb-3 mb-md-0\"> We use <span class=\"text-bold\">optional</span> third-party analytics cookies to understand how you use GitHub.com so we can build better products. <span class=\"btn-link js-cookie-consent-learn-more\">Learn more</span>. </p> </div> </div> <div class=\"js-cookie-details hx_cookie-banner-details\"> <div class=\"d-md-flex flex-items-center py-3\"> <p class=\"f5 flex-1 mb-2 mb-md-0\"> We use <span class=\"text-bold\">optional</span> third-party analytics cookies to understand how you use GitHub.com so we can build better products. <br> You can always update your selection by clicking <span class=\"text-bold\">Cookie Preferences</span> at the bottom of the page. For more information, see our <a href=\"https://docs.github.com/en/free-pro-team@latest/github/site-policy/github-privacy-statement\">Privacy Statement</a>. </p> </div> <div class=\"d-md-flex flex-items-center py-3 border-top\"> <div class=\"f5 flex-1 mb-2 mb-md-0\"> <p class=\"f6 mb-md-0\">We use essential cookies to perform essential website functions, e.g. they&apos;re used to log you in. <a href=\"https://docs.github.com/en/github/site-policy/github-subprocessors-and-cookies\">Learn more</a> </p> </div> <h5 class=\"text-blue\">Always active</h5> </div> <div class=\"d-md-flex flex-items-center py-3 border-top\"> <div class=\"f5 flex-1 mb-2 mb-md-0\"> <p class=\"f6 mb-md-0\">We use analytics cookies to understand how you use our websites so we can make them better, e.g. they&apos;re used to gather information about the pages you visit and how many clicks you need to accomplish a task. <a href=\"https://docs.github.com/en/github/site-policy/github-subprocessors-and-cookies\">Learn more</a> </p> </div> </div> </div>\n</div></div> </div>\n</div> </body>",
      "contentAsText": "\nTobs is a tool that aims to make it as easy as possible to install a full observability\nstack into a Kubernetes cluster. Currently this stack includes:\n\nWe plan to expand this stack over time and welcome contributions.\nTobs provides a CLI tool to make deployment and operations easier. We also provide\nHelm charts that can be used directly or as sub-charts for other projects.\nSee a demo of tobs in action by clicking the video below:\n\n  \n\n=% Quick start\nDependencies: Helm\nUsing the tobs CLI tool\nThe CLI tool provides the most seamless experience for interacting with tobs.\nGetting started with the CLI tool is a two-step process: First you install the CLI tool locally, then you use the CLI tool to install the tobs stack into your Kubernetes cluster.\nInstalling the CLI tool\nTo download and install tobs, run the following in your terminal, then follow the on-screen instructions.\ncurl --proto '=https' --tlsv1.2 -sSLf  https://tsdb.co/install-tobs-sh |sh\nAlternatively, you can download the CLI directly via our releases page\nUsing the tobs CLI tool to deploy the stack into your Kubernetes cluster\nAfter setting up tobs run the following to install the tobs helm charts into your Kubernetes cluster\n\nThis will deploy all of the tobs components into your cluster and provide instructions as to next steps.\nGetting started by viewing your metrics in Grafana\nTo see your Grafana dashboards after installation run\ntobs grafana change-password <new_password>\ntobs grafana port-forward\nThen, point your browser to http://127.0.0.1:8080/ and login with the admin username.\nMore details about the CLI tool can be found here\nConfiguring the stack\nAll configuration for all components happens through the helm values.yml file.\nYou can view the self-documenting default values.yaml in the repo.\nWe also have additional documentation about individual configuration settings in our\nHelm chart docs.\nTo modify the settings, first create a values.yaml file:\ntobs helm show-values > values.yaml\nThen modify the values.yaml file using your favorite editor.\nFinally, deploy with the new settings using:\ntobs install -f values.yaml\n=�Alternative deployment methods\n\nUsers sometimes want to use our Helm charts as sub-charts for other project or integrate them into their infrastructure without using our CLI tool. This is a supported use-case and instructions on using the Helm charts can be found here.\n\u000f\u000f Contributing\nWe welcome contributions to tobs, which is\nlicensed and released under the open-source Apache License, Version 2.  The\nsame Contributor's\nAgreement\napplies as in TimescaleDB; please sign the Contributor License\nAgreement (CLA) if\nyou're a new contributor.\nPage 2      You can’t perform that action at this time.    You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session.          We use optional third-party analytics cookies to understand how you use GitHub.com so we can build better products. Learn more.       We use optional third-party analytics cookies to understand how you use GitHub.com so we can build better products.  You can always update your selection by clicking Cookie Preferences at the bottom of the page. For more information, see our Privacy Statement.     We use essential cookies to perform essential website functions, e.g. they're used to log you in. Learn more   Always active    We use analytics cookies to understand how you use our websites so we can make them better, e.g. they're used to gather information about the pages you visit and how many clicks you need to accomplish a task. Learn more    \n \n ",
      "description": "tobs - The Observability Stack for Kubernetes. Easy install of a full observability stack into a k8s cluster with a CLI tool or Helm charts. - timescale/tobs",
      "ogDescription": "tobs - The Observability Stack for Kubernetes. Easy install of a full observability stack into a k8s cluster with a CLI tool or Helm charts. - timescale/tobs"
    },
    {
      "url": "https://www.magalix.com/blog/kubernetes-cost-optimization-101",
      "title": "Kubernetes Cost Optimization 101",
      "content": "<div id=\"hs_cos_wrapper_post_body\" class=\"hs_cos_wrapper\"><p>Over the past two years at Magalix, we have focused on building our system, introducing new features, and scaling our infrastructure and microservices. During this time, we had a look at our Kubernetes clusters utilization and found it to be very low. We were paying for resources we didn\u0019t use, so we started a cost-saving practice to increase cluster utilization, use the resources we already had and pay less to run our cluster.</p> <p>In this article, I will discuss the top five techniques we used to better utilize our Kubernetes clusters on the cloud and eliminate wasted resources, thus saving money. In the end, we were able to cut our monthly bill by more than 50%!</p>\n<h2>1. Applying Workload Right-Sizing</h2>\n<p>Kubernetes manages and schedules pods are based on container resource specs:</p>\n<ul>\n<li><u><strong>Resource Requests</strong></u>: Kubernetes scheduler is used to place containers on the right node which has enough capacity</li>\n<li><u><strong>Resource Limits</strong></u>: Containers are NOT allowed to use more than their resource limit</li>\n</ul>\n<p>Resources requests and limits are container-scooped specs, while multi-container pods define separate resource specs for each container:</p>\n<pre><code class=\"language-yaml line-numbers\">apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: magalix\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 1\n            memory: 1Gi\n</code></pre>\n<p>Kubernetes schedules pods based on resource requests and other restrictions without impairing availability. The scheduler uses CPU and memory resource requests to schedule the workloads in the right nodes, control which pod works on which node and if multiple pods can schedule together on a single node.</p>\n<p>Every node type has its own allocatable CPU and memory capacities. Assigning high/unneeded CPU or memory resource requests can end up running underutilized pods on each node, which leads to underutilized nodes.</p>\n<p>In this section, we compared resource requests, limited against actual usage and changed the resource request to something closer to the actual utilization while adding a little safety margin.</p>\n<h2>2. Choosing the Right Worker Nodes</h2>\n<p>Every Kubernetes cluster has its own special workload utilization. Some clusters use memory more than CPU (e.g: database and caching workloads), while others use CPU more than memory (e.g: user-interactive and batch-processing workloads)</p>\n<p>Cloud providers such as GCP and AWS offer various node types that you can choose from.</p>\n<p>Choosing the wrong node size for your cluster can end up costing you. For instance, choosing high CPU-to-memory ratio nodes for workloads that use memory extensively can starve for memory easily and trigger auto node scale-up, wasting more CPUs that we don\u0019t need.</p>\n<p>Calculating the right ratio of CPU-to-memory isn\u0019t easy; you will need to monitor and know your workloads well.</p>\n<p>For example, GCP offers general purpose, compute-optimized, memory-optimized with various CPU and memory count and ratios:</p>\n<div><img src=\"https://www.magalix.com/hubfs/Google%20Drive%20Integration/Kubernetes%20Cost%20Optimization%20101-Sep-22-2020-05-16-57-23-AM.png\" alt=\"Kubernetes Cost Optimization 101\"></div> <p>Just keep in mind that 1 vCPU is way more expensive than 1GB memory. I have enough memory in the clusters I manage so I try to make sure that when there is a pending pod, this pod is pending on CPUs (which is expensive) so the autoscaler triggers a scale-up for the new node.</p>\n<p>To see the cost difference between CPU and memory, let us look at the GCP N2 machine price. GCP gives you the freedom to choose a custom machine type:</p>\n<pre><code class=\"language-yaml\"> (# vCPU x 1vCPU price) + (# GB memory x 1GB memory price)\n</code></pre>\n<p>It\u0019s clear here that the 1vCPU costs 7.44 times more than the cost of 1GB.</p>\n<div><img src=\"https://www.magalix.com/hubfs/Google%20Drive%20Integration/Kubernetes%20Cost%20Optimization%20101-2.png\" alt=\"Kubernetes Cost Optimization 101\"></div>\n<p>You can run multiple worker nodes with different node types and sizes, and control which workloads to run on which node pool using Kubernetes taints and toleration.</p> <h4><span>Already working in production with Kubernetes? Want to know more about kubernetes application patterns?</span></h4>\n<p><span>=G=G</span></p>\n<p><span><span class=\"hs-cta-wrapper\" id=\"hs-cta-wrapper-60591d93-9dc8-40ae-8685-02f6b03c25b8\"><span class=\"hs-cta-node hs-cta-60591d93-9dc8-40ae-8685-02f6b03c25b8\" id=\"hs-cta-60591d93-9dc8-40ae-8685-02f6b03c25b8\"><a href=\"https://cta-redirect.hubspot.com/cta/redirect/3487587/60591d93-9dc8-40ae-8685-02f6b03c25b8\"><img class=\"hs-cta-img\" id=\"hs-cta-img-60591d93-9dc8-40ae-8685-02f6b03c25b8\" src=\"https://no-cache.hubspot.com/cta/default/3487587/60591d93-9dc8-40ae-8685-02f6b03c25b8.png\" alt=\"Download Kubernetes Application Patterns E-Book\"></a></span></span></span></p> <h2>3. Autoscaling Workloads</h2>\n<p>Autoscaling is great because it helps to scale up/down your workloads and shut down nodes to save you money while you sleep.</p>\n<p>Many cases can benefit from autoscaling:</p>\n<ul>\n<li>Variable load web applications: A good example would be a web application which receives variable traffic through the day: traffic increases during certain hours of the day and decreases during the night.<p>Kubernetes comes with the Horizontal Pod Autoscaler (HPA) that can scale workload replicas based on their CPU and memory utilization ratio to the resource request. Kubernetes will keep monitoring the target resource in <code>scaleTargetRef</code><span> and will scale up/down the replica count to keep </span><code>targetCPUUtilizationPercentage</code><span> around 75%. </span></p><br>In this example, the deployment <code>frontend</code><span> will be scaled up to 20 replicas during the high load, and 4 replicas during the low load.</span>\n<pre><code class=\"language-yaml line-numbers\">apiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n&#xA0; name: frontend\n&#xA0; namespace: magalix\nspec:\n&#xA0; maxReplicas: 20\n&#xA0; minReplicas: 4\n&#xA0; scaleTargetRef:\n&#xA0; &#xA0; apiVersion: apps/v1\n&#xA0; &#xA0; kind: Deployment\n&#xA0; &#xA0; Name: frontend\n&#xA0; targetCPUUtilizationPercentage: 75\n</code></pre>\n</li>\n<li>Event-driven workers: background workers that need to be started in multiple replicas when there are messages in a Kafka topic or a message queue. It can be scaled to zero when there are no messages.<p>Compared to HPA, there is a more advanced Kubernetes Event-driven Autoscaling (<a href=\"https://github.com/kedacore/keda\">KEDA</a><span>) that can integrate with Prometheus/PosqreSQL/Kafka/Redis and many more to scale based on more advanced metrics from multiple data sources.</span></p><p>In this example, we installed this KEDA ScaledObject custom resource definition to scale the worker deployment <code>eventer</code><span> replicas. When the Kafka consumer lag changes, it can scale to 0 when there are no messages to consume and can scale up 1 replica for every 10,000 lagged messages up to 8 when the lag is more than 80,000:</span>\n</p><pre><code class=\"language-yaml line-numbers\">apiVersion: keda.k8s.io/v1alpha1\nkind: ScaledObject\nmetadata:\n&#xA0; labels:\n&#xA0; &#xA0; deploymentName: eventer\n&#xA0; name: eventer\n&#xA0; namespace: magalix\nspec:\n&#xA0; cooldownPeriod: 10\n&#xA0; maxReplicaCount: 8\n&#xA0; minReplicaCount: 0\n&#xA0; pollingInterval: 15\n&#xA0; scaleTargetRef:\n&#xA0; &#xA0; deploymentName: eventer\n&#xA0; triggers:\n&#xA0; - metadata:\n&#xA0; &#xA0; &#xA0; metricName: eventer\n&#xA0; &#xA0; &#xA0; query: sum(kafka_consumergroup_lag{consumergroup=&quot;eventer-group&quot;})\n&#xA0; &#xA0; &#xA0; serverAddress: http://prometheus-server.monitoring.svc.cluster.local\n&#xA0; &#xA0; &#xA0; threshold: &quot;10000&quot;\n&#xA0; &#xA0; type: prometheus\n</code></pre>\n</li>\n</ul>\n<h2>4. Autoscaling Worker Nodes</h2>\n<p>After scaling workloads, you will notice the number of running pods is low, but this won\u0019t save you money unless we configure auto-scaling the worker nodes</p>\n<p>Some Could providers provide node autoscaling out of the box on some node pools. <a href=\"https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler\">Cluster Autoscaler</a> can help you manage worker node autoscaling.</p>\n<p>GCP: Kubernetes Engine &#xFFFD; Cluster &#xFFFD; Node Pool</p>\n<div><img src=\"https://www.magalix.com/hubfs/Google%20Drive%20Integration/Kubernetes%20Cost%20Optimization%20101-1.png\" alt=\"Kubernetes Cost Optimization 101\"></div>\n<p>AWS: EKS &#xFFFD; Cluster &#xFFFD; Node Group</p>\n<div><img src=\"https://www.magalix.com/hubfs/Google%20Drive%20Integration/Kubernetes%20Cost%20Optimization%20101-Sep-22-2020-05-16-57-06-AM.png\" alt=\"Kubernetes Cost Optimization 101\"></div>\n<p>Azure: Kubernetes services &#xFFFD; Node pools &#xFFFD; Scale &#xFFFD; Automatic</p>\n<div><img src=\"https://www.magalix.com/hubfs/Google%20Drive%20Integration/Kubernetes%20Cost%20Optimization%20101-4.png\" alt=\"Kubernetes Cost Optimization 101\"></div>\n<p>The result of scaling workload and worker nodes together can end with the node count trends:</p>\n<div><img src=\"https://www.magalix.com/hubfs/Google%20Drive%20Integration/Kubernetes%20Cost%20Optimization%20101.png\" alt=\"Kubernetes Cost Optimization 101\"></div>\n<h2>5. Purchasing Commitment/Saving Plans</h2>\n<p>Running a Kubernetes service is relatively cheap. What\u0019s most expensive is the worker nodes compute cost.</p>\n<ul>\n<li><strong>GCP</strong> offers \u001cCommitment Plans\u001d on a certain number of vCPUs, memory, GPUs, and local SSDs for 1 or 3 years. This can save up to 57% of the compute cost</li>\n<li><strong>AWS</strong> offers \u001cCompute Saving Plans\u001d to commit to using a certain amount of money on compute every month, as well as \u001cReserved Instances\u001d to commit to using a certain type of machine, both for 1 or 3 years with a possible savings of up to 60%</li>\n<li><strong>Azure </strong>offers \u001cAzure Reserved VM Instances\u001d such as AWS with a possible savings of up to 60%</li>\n</ul>\n<h2>Conclusion</h2>\n<p>As we read in this article, there are multiple factors and considerations when trying to reduce your cluster cost. Going through the whole process can give you huge savings. We have managed to reduce our cluster daily cost by 56%- and you can do the same!</p>\n<div><img src=\"https://www.magalix.com/hubfs/Google%20Drive%20Integration/Kubernetes%20Cost%20Optimization%20101-3.png\" alt=\"Kubernetes Cost Optimization 101\"></div> <div> <p><span>=G=G</span></p>\n<p><span><span class=\"hs-cta-wrapper\" id=\"hs-cta-wrapper-2c65165a-51ca-4979-aa1b-2526964ce343\"><span class=\"hs-cta-node hs-cta-2c65165a-51ca-4979-aa1b-2526964ce343\" id=\"hs-cta-2c65165a-51ca-4979-aa1b-2526964ce343\"><a href=\"https://cta-redirect.hubspot.com/cta/redirect/3487587/2c65165a-51ca-4979-aa1b-2526964ce343\"><img class=\"hs-cta-img\" id=\"hs-cta-img-2c65165a-51ca-4979-aa1b-2526964ce343\" src=\"https://no-cache.hubspot.com/cta/default/3487587/2c65165a-51ca-4979-aa1b-2526964ce343.png\" alt=\"Learn More\"></a></span></span></span></p> </div></div>",
      "contentAsText": "Over the past two years at Magalix, we have focused on building our system, introducing new features, and scaling our infrastructure and microservices. During this time, we had a look at our Kubernetes clusters utilization and found it to be very low. We were paying for resources we didn\u0019t use, so we started a cost-saving practice to increase cluster utilization, use the resources we already had and pay less to run our cluster. In this article, I will discuss the top five techniques we used to better utilize our Kubernetes clusters on the cloud and eliminate wasted resources, thus saving money. In the end, we were able to cut our monthly bill by more than 50%!\n1. Applying Workload Right-Sizing\nKubernetes manages and schedules pods are based on container resource specs:\n\nResource Requests: Kubernetes scheduler is used to place containers on the right node which has enough capacity\nResource Limits: Containers are NOT allowed to use more than their resource limit\n\nResources requests and limits are container-scooped specs, while multi-container pods define separate resource specs for each container:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: magalix\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 1\n            memory: 1Gi\n\nKubernetes schedules pods based on resource requests and other restrictions without impairing availability. The scheduler uses CPU and memory resource requests to schedule the workloads in the right nodes, control which pod works on which node and if multiple pods can schedule together on a single node.\nEvery node type has its own allocatable CPU and memory capacities. Assigning high/unneeded CPU or memory resource requests can end up running underutilized pods on each node, which leads to underutilized nodes.\nIn this section, we compared resource requests, limited against actual usage and changed the resource request to something closer to the actual utilization while adding a little safety margin.\n2. Choosing the Right Worker Nodes\nEvery Kubernetes cluster has its own special workload utilization. Some clusters use memory more than CPU (e.g: database and caching workloads), while others use CPU more than memory (e.g: user-interactive and batch-processing workloads)\nCloud providers such as GCP and AWS offer various node types that you can choose from.\nChoosing the wrong node size for your cluster can end up costing you. For instance, choosing high CPU-to-memory ratio nodes for workloads that use memory extensively can starve for memory easily and trigger auto node scale-up, wasting more CPUs that we don\u0019t need.\nCalculating the right ratio of CPU-to-memory isn\u0019t easy; you will need to monitor and know your workloads well.\nFor example, GCP offers general purpose, compute-optimized, memory-optimized with various CPU and memory count and ratios:\n Just keep in mind that 1 vCPU is way more expensive than 1GB memory. I have enough memory in the clusters I manage so I try to make sure that when there is a pending pod, this pod is pending on CPUs (which is expensive) so the autoscaler triggers a scale-up for the new node.\nTo see the cost difference between CPU and memory, let us look at the GCP N2 machine price. GCP gives you the freedom to choose a custom machine type:\n (# vCPU x 1vCPU price) + (# GB memory x 1GB memory price)\n\nIt\u0019s clear here that the 1vCPU costs 7.44 times more than the cost of 1GB.\n\nYou can run multiple worker nodes with different node types and sizes, and control which workloads to run on which node pool using Kubernetes taints and toleration. Already working in production with Kubernetes? Want to know more about kubernetes application patterns?\n=G=G\n 3. Autoscaling Workloads\nAutoscaling is great because it helps to scale up/down your workloads and shut down nodes to save you money while you sleep.\nMany cases can benefit from autoscaling:\n\nVariable load web applications: A good example would be a web application which receives variable traffic through the day: traffic increases during certain hours of the day and decreases during the night.Kubernetes comes with the Horizontal Pod Autoscaler (HPA) that can scale workload replicas based on their CPU and memory utilization ratio to the resource request. Kubernetes will keep monitoring the target resource in scaleTargetRef and will scale up/down the replica count to keep targetCPUUtilizationPercentage around 75%. In this example, the deployment frontend will be scaled up to 20 replicas during the high load, and 4 replicas during the low load.\napiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: frontend\n  namespace: magalix\nspec:\n  maxReplicas: 20\n  minReplicas: 4\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    Name: frontend\n  targetCPUUtilizationPercentage: 75\n\n\nEvent-driven workers: background workers that need to be started in multiple replicas when there are messages in a Kafka topic or a message queue. It can be scaled to zero when there are no messages.Compared to HPA, there is a more advanced Kubernetes Event-driven Autoscaling (KEDA) that can integrate with Prometheus/PosqreSQL/Kafka/Redis and many more to scale based on more advanced metrics from multiple data sources.In this example, we installed this KEDA ScaledObject custom resource definition to scale the worker deployment eventer replicas. When the Kafka consumer lag changes, it can scale to 0 when there are no messages to consume and can scale up 1 replica for every 10,000 lagged messages up to 8 when the lag is more than 80,000:\napiVersion: keda.k8s.io/v1alpha1\nkind: ScaledObject\nmetadata:\n  labels:\n    deploymentName: eventer\n  name: eventer\n  namespace: magalix\nspec:\n  cooldownPeriod: 10\n  maxReplicaCount: 8\n  minReplicaCount: 0\n  pollingInterval: 15\n  scaleTargetRef:\n    deploymentName: eventer\n  triggers:\n  - metadata:\n      metricName: eventer\n      query: sum(kafka_consumergroup_lag{consumergroup=\"eventer-group\"})\n      serverAddress: http://prometheus-server.monitoring.svc.cluster.local\n      threshold: \"10000\"\n    type: prometheus\n\n\n\n4. Autoscaling Worker Nodes\nAfter scaling workloads, you will notice the number of running pods is low, but this won\u0019t save you money unless we configure auto-scaling the worker nodes\nSome Could providers provide node autoscaling out of the box on some node pools. Cluster Autoscaler can help you manage worker node autoscaling.\nGCP: Kubernetes Engine � Cluster � Node Pool\n\nAWS: EKS � Cluster � Node Group\n\nAzure: Kubernetes services � Node pools � Scale � Automatic\n\nThe result of scaling workload and worker nodes together can end with the node count trends:\n\n5. Purchasing Commitment/Saving Plans\nRunning a Kubernetes service is relatively cheap. What\u0019s most expensive is the worker nodes compute cost.\n\nGCP offers \u001cCommitment Plans\u001d on a certain number of vCPUs, memory, GPUs, and local SSDs for 1 or 3 years. This can save up to 57% of the compute cost\nAWS offers \u001cCompute Saving Plans\u001d to commit to using a certain amount of money on compute every month, as well as \u001cReserved Instances\u001d to commit to using a certain type of machine, both for 1 or 3 years with a possible savings of up to 60%\nAzure offers \u001cAzure Reserved VM Instances\u001d such as AWS with a possible savings of up to 60%\n\nConclusion\nAs we read in this article, there are multiple factors and considerations when trying to reduce your cluster cost. Going through the whole process can give you huge savings. We have managed to reduce our cluster daily cost by 56%- and you can do the same!\n  =G=G\n ",
      "description": "there are multiple factors and considerations when trying to reduce your cluster cost. Going through the whole process can give you huge savings.",
      "ogDescription": "there are multiple factors and considerations when trying to reduce your cluster cost. Going through the whole process can give you huge savings."
    },
    {
      "url": "https://github.com/policy-hub/policy-hub-cli",
      "title": "policy-hub/policy-hub-cli",
      "content": "<div><div><article class=\"markdown-body entry-content container-lg\">\n<p>This is the home of the PolicyHub CLI, a CLI tool that makes Rego policies searchable.</p>\n<h2><a id=\"user-content-goals\" class=\"anchor\" href=\"https://github.com/policy-hub/policy-hub-cli#goals\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Goals</h2>\n<p>Policies are everywhere. Compliance policies, security policies, policies that define organisational best practices. The Open Policy Agent project provided a single policy language, Rego, that can be used to automate policy enforcement. However currently there is no existing mechanism that allows you to search for specific Rego policies.</p>\n<p>For example you might be looking for a set of policies that validate Kubernetes security best practices as a starting point for your organisations Kubernetes policies. Or you might be looking for a set of Microservice Authorization policies. Right now you have to hope that your google search points you in the right direction.</p>\n<p>The PolicyHub CLI aims to make policies searchable. We provide a standard format for policy creators to share their policies. Users of the CLI can search our registry for specific tags or descriptions, hopefully finding the policy they where looking for.</p>\n<h2><a id=\"user-content-searching-policies\" class=\"anchor\" href=\"https://github.com/policy-hub/policy-hub-cli#searching-policies\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Searching policies</h2>\n<p>To search our registry, you can use the <code>search</code> command:</p>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-k\">&gt;</span> policy-hub search k8s\n\n+---------------------------+---------------------------------+--------------------------------+\n<span class=\"pl-k\">|</span>           NAME            <span class=\"pl-k\">|</span>           MAINTAINERS           <span class=\"pl-k\">|</span>             LABELS             <span class=\"pl-k\">|</span>\n+---------------------------+---------------------------------+--------------------------------+\n<span class=\"pl-k\">|</span> deprek8ion                <span class=\"pl-k\">|</span> https://github.com/swade1987    <span class=\"pl-k\">|</span> k8s, kubernetes, gatekeeper    <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span> contrib.k8s_node_selector <span class=\"pl-k\">|</span> https://github.com/tsandall     <span class=\"pl-k\">|</span> kubernetes, k8s, node_selector <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span> redhat-cop.rego-policies  <span class=\"pl-k\">|</span> https://github.com/garethahealy <span class=\"pl-k\">|</span> k8s, kubernetes, gatekeeper    <span class=\"pl-k\">|</span>\n<span class=\"pl-k\">|</span> konstraint                <span class=\"pl-k\">|</span> https://github.com/garethahealy <span class=\"pl-k\">|</span> k8s, kubernetes, gatekeeper    <span class=\"pl-k\">|</span>\n+---------------------------+---------------------------------+--------------------------------+</pre></div>\n<h2><a id=\"user-content-downloading-policies\" class=\"anchor\" href=\"https://github.com/policy-hub/policy-hub-cli#downloading-policies\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Downloading policies</h2>\n<p>To download a policy, use the <code>pull</code> command:</p>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-k\">&gt;</span> policy-hub pull konstraint</pre></div>\n<h2><a id=\"user-content-make-your-policies-discoverable\" class=\"anchor\" href=\"https://github.com/policy-hub/policy-hub-cli#make-your-policies-discoverable\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Make your policies discoverable</h2>\n<p>Do you have policies that the community could benefit from too?\nMake them searchable for users of policy-hub by adding metadata for them\nhere (<a href=\"https://github.com/policy-hub/policy-hub-cli/blob/main/metadata/registries.yml\">https://github.com/policy-hub/policy-hub-cli/blob/main/metadata/registries.yml</a>)\nand submitting a PR</p>\n<h2><a id=\"user-content-contributing\" class=\"anchor\" href=\"https://github.com/policy-hub/policy-hub-cli#contributing\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Contributing</h2>\n<p>Join us make policies more searchable!</p>\n<ul>\n<li>We accept contributions to our registry.</li>\n<li>Use <a href=\"https://github.com/policy-hub/policy-hub-cli/issues\">GitHub Issues</a> to file bugs or propose new features.</li>\n<li>Create a <a href=\"https://github.com/policy-hub/policy-hub-cli/pulls\">Pull Request</a> and contribute to the project.</li>\n</ul>\n</article></div></div><hr><h4>Page 2</h4><body class=\"logged-out env-production page-responsive\"> <include-fragment class=\"js-notification-shelf-include-fragment\"></include-fragment> <div class=\"application-main \"> </div> <p id=\"ajax-error-message\" class=\"ajax-error-message\"> <svg class=\"octicon octicon-alert\" width=\"16\" height=\"16\"><path/></svg> You can&#x2019;t perform that action at this time. </p> <p class=\"js-stale-session-flash\"> <svg class=\"octicon octicon-alert\" width=\"16\" height=\"16\"><path/></svg> <span class=\"js-stale-session-flash-signed-in\">You signed in with another tab or window. <a href>Reload</a> to refresh your session.</span> <span class=\"js-stale-session-flash-signed-out\">You signed out in another tab or window. <a href>Reload</a> to refresh your session.</span> </p> <template id=\"site-details-dialog\"> <details class=\"details-reset details-overlay details-overlay-dark lh-default text-gray-dark hx_rsm\"> <summary></summary> <details-dialog class=\"Box Box--overlay d-flex flex-column anim-fade-in fast hx_rsm-dialog hx_rsm-modal\"> </details-dialog> </details>\n</template> <div class=\"js-cookie-consent-banner\"> <div class=\"hx_cookie-banner p-2 p-sm-3 p-md-4\"> <div class=\"Box hx_cookie-banner-box box-shadow-medium mx-auto\"> <div class=\"Box-body border-0 py-0 px-3 px-md-4\"> <div class=\"js-main-cookie-banner hx_cookie-banner-main\"> <div class=\"d-md-flex flex-items-center py-3\"> <p class=\"f5 flex-1 mb-3 mb-md-0\"> We use <span class=\"text-bold\">optional</span> third-party analytics cookies to understand how you use GitHub.com so we can build better products. <span class=\"btn-link js-cookie-consent-learn-more\">Learn more</span>. </p> </div> </div> <div class=\"js-cookie-details hx_cookie-banner-details\"> <div class=\"d-md-flex flex-items-center py-3\"> <p class=\"f5 flex-1 mb-2 mb-md-0\"> We use <span class=\"text-bold\">optional</span> third-party analytics cookies to understand how you use GitHub.com so we can build better products. <br> You can always update your selection by clicking <span class=\"text-bold\">Cookie Preferences</span> at the bottom of the page. For more information, see our <a href=\"https://docs.github.com/en/free-pro-team@latest/github/site-policy/github-privacy-statement\">Privacy Statement</a>. </p> </div> <div class=\"d-md-flex flex-items-center py-3 border-top\"> <div class=\"f5 flex-1 mb-2 mb-md-0\"> <p class=\"f6 mb-md-0\">We use essential cookies to perform essential website functions, e.g. they&apos;re used to log you in. <a href=\"https://docs.github.com/en/github/site-policy/github-subprocessors-and-cookies\">Learn more</a> </p> </div> <h5 class=\"text-blue\">Always active</h5> </div> <div class=\"d-md-flex flex-items-center py-3 border-top\"> <div class=\"f5 flex-1 mb-2 mb-md-0\"> <p class=\"f6 mb-md-0\">We use analytics cookies to understand how you use our websites so we can make them better, e.g. they&apos;re used to gather information about the pages you visit and how many clicks you need to accomplish a task. <a href=\"https://docs.github.com/en/github/site-policy/github-subprocessors-and-cookies\">Learn more</a> </p> </div> </div> </div>\n</div></div> </div>\n</div> </body>",
      "contentAsText": "\nThis is the home of the PolicyHub CLI, a CLI tool that makes Rego policies searchable.\nGoals\nPolicies are everywhere. Compliance policies, security policies, policies that define organisational best practices. The Open Policy Agent project provided a single policy language, Rego, that can be used to automate policy enforcement. However currently there is no existing mechanism that allows you to search for specific Rego policies.\nFor example you might be looking for a set of policies that validate Kubernetes security best practices as a starting point for your organisations Kubernetes policies. Or you might be looking for a set of Microservice Authorization policies. Right now you have to hope that your google search points you in the right direction.\nThe PolicyHub CLI aims to make policies searchable. We provide a standard format for policy creators to share their policies. Users of the CLI can search our registry for specific tags or descriptions, hopefully finding the policy they where looking for.\nSearching policies\nTo search our registry, you can use the search command:\n> policy-hub search k8s\n\n+---------------------------+---------------------------------+--------------------------------+\n|           NAME            |           MAINTAINERS           |             LABELS             |\n+---------------------------+---------------------------------+--------------------------------+\n| deprek8ion                | https://github.com/swade1987    | k8s, kubernetes, gatekeeper    |\n| contrib.k8s_node_selector | https://github.com/tsandall     | kubernetes, k8s, node_selector |\n| redhat-cop.rego-policies  | https://github.com/garethahealy | k8s, kubernetes, gatekeeper    |\n| konstraint                | https://github.com/garethahealy | k8s, kubernetes, gatekeeper    |\n+---------------------------+---------------------------------+--------------------------------+\nDownloading policies\nTo download a policy, use the pull command:\n> policy-hub pull konstraint\nMake your policies discoverable\nDo you have policies that the community could benefit from too?\nMake them searchable for users of policy-hub by adding metadata for them\nhere (https://github.com/policy-hub/policy-hub-cli/blob/main/metadata/registries.yml)\nand submitting a PR\nContributing\nJoin us make policies more searchable!\n\nWe accept contributions to our registry.\nUse GitHub Issues to file bugs or propose new features.\nCreate a Pull Request and contribute to the project.\n\nPage 2      You can’t perform that action at this time.    You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session.          We use optional third-party analytics cookies to understand how you use GitHub.com so we can build better products. Learn more.       We use optional third-party analytics cookies to understand how you use GitHub.com so we can build better products.  You can always update your selection by clicking Cookie Preferences at the bottom of the page. For more information, see our Privacy Statement.     We use essential cookies to perform essential website functions, e.g. they're used to log you in. Learn more   Always active    We use analytics cookies to understand how you use our websites so we can make them better, e.g. they're used to gather information about the pages you visit and how many clicks you need to accomplish a task. Learn more    \n \n ",
      "description": "CLI for searching Rego policies. Contribute to policy-hub/policy-hub-cli development by creating an account on GitHub.",
      "ogDescription": "CLI for searching Rego policies. Contribute to policy-hub/policy-hub-cli development by creating an account on GitHub."
    },
    {
      "url": "https://nillsf.com/index.php/2020/11/17/running-windows-containers-on-the-azure-kubernetes-service-aks/",
      "title": "Running Windows containers on the Azure Kubernetes Service (AKS)",
      "content": "<div class=\"entry-content\"> <p>Containers and Kubernetes have traditionally been the area of Linux-based workloads. However, things have changed. Windows has supported Docker containers <a href=\"https://www.docker.com/blog/dockerforws2016/\">for a while now</a>, and since <a href=\"https://kubernetes.io/blog/2019/03/25/kubernetes-1-14-release-announcement/\">Kubernetes 1.14</a>, Windows support has been generally available in Kubernetes as well.</p> <p>In this blog post, we&#x2019;ll explore how you can add Windows nodes to a Kubernetes cluster running on Azure. After we&#x2019;ve set up the cluster, we&#x2019;ll have a look at how actual Windows containers can be created on the cluster.</p> <h2>Setting up the cluster</h2> <p>To run Windows containers on AKS, we&#x2019;ll need the following:</p> <ul><li>An AKS cluster</li><li>(at least) 1 Linux Nodepool. This is used for running system components such as CoreDNS.</li><li>A Windows Nodepool</li></ul> <p>Let&#x2019;s create all of this using the Azure CLI:</p> <pre class=\"wp-block-code\"><code># Create a resource group\naz group create -n win-aks -l westus2\n\n# Create the cluster, with the default linux nodepool\naz aks create -g win-aks -n win-aks \\\n  --node-count 2 --ssh-key-value ~/.ssh/id_rsa.pub \\\n  --windows-admin-username nilfranadmin \\\n  --windows-admin-password superSecret123! \\\n  --network-plugin azure\n\n# Create a second nodepool using Windows\n az aks nodepool add -g win-aks \\\n  --cluster-name win-aks \\\n  --os-type Windows --name winnp \\\n  --node-count 2</code></pre> <p>The second command will take some time to run (about 15 minutes), but after a while, we will be able to go ahead and schedule Windows containers. While you&#x2019;re waiting for the node pool to be added, let&#x2019;s explore how we need to tell Kubernetes to schedule a Windows workload on Windows nodes.</p> <h2>A little info about labels and nodeSelectors</h2> <p>Once you have a cluster with both Linux and Windows nodes, you should be able to run <code>kubectl get nodes -o wide</code> and see you now have nodes with a Windows operating system:</p> <figure class=\"wp-block-image size-large\"><img src=\"/wp-content/uploads/2020/11/image-1024x97.png\" alt class=\"wp-image-1436\" srcset=\"https://nillsf.com/wp-content/uploads/2020/11/image-1024x97.png 1024w, https://nillsf.com/wp-content/uploads/2020/11/image-300x28.png 300w, https://nillsf.com/wp-content/uploads/2020/11/image-768x73.png 768w, https://nillsf.com/wp-content/uploads/2020/11/image-1536x145.png 1536w, https://nillsf.com/wp-content/uploads/2020/11/image.png 1809w\" sizes=\"(max-width: 1024px) 100vw, 1024px\"><figcaption>You should have nodes with a Windows OS</figcaption></figure> <p>What&#x2019;s more, these nodes are also labeled with their OS information. To see those labels, run a <code>kubectl describe node &lt;windows-node-name&gt;</code>:</p> <figure class=\"wp-block-image size-large\"><img src=\"/wp-content/uploads/2020/11/image-1-1024x541.png\" alt class=\"wp-image-1437\" srcset=\"https://nillsf.com/wp-content/uploads/2020/11/image-1-1024x541.png 1024w, https://nillsf.com/wp-content/uploads/2020/11/image-1-300x158.png 300w, https://nillsf.com/wp-content/uploads/2020/11/image-1-768x406.png 768w, https://nillsf.com/wp-content/uploads/2020/11/image-1.png 1333w\" sizes=\"(max-width: 1024px) 100vw, 1024px\"><figcaption>Labels on Windows nodes</figcaption></figure> <p>To schedule pods on a Windows node (or a Linux node for that matter) you&#x2019;ll have to set a <code>nodeSelector </code>in the pod definition. In the node selector, you define which labels on the nodes need to be met to schedule pods on certain nodes. In case of the operating system, we&#x2019;ll set the <code>kubernetes.io/os</code> label to Windows in the <code>nodeSelector</code>.</p> <p>What&#x2019;s important to note here: if you run in a mixed cluster (with both Linux and Windows nodes and workloads), you need to include the <code>nodeSelector </code>for both Windows and Linux workloads. Otherwise, Kubernetes might schedule Linux pods on Windows nodes (or vice-versa), and that will lead to issues.</p> <p>Let&#x2019;s have a look at how to do that:</p> <h2>How to schedule pods on Windows nodes</h2> <p>As mentioned in the previous section, to schedule a pod on a Windows node, you need to include a <code>nodeSelector </code>in your workload definition. An example of that below (<a href=\"https://github.com/NillsF/blog/tree/master/aks-windows\">code is also available on GitHub</a>):</p> <pre class=\"wp-block-code\"><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: win-webserver\n  name: win-webserver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: win-webserver\n  template:\n    metadata:\n      labels:\n        app: win-webserver\n      name: win-webserver\n    spec:\n     containers:\n      - name: windowswebserver\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n        image: mcr.microsoft.com/windows/servercore/iis:windowsservercore-ltsc2019\n     nodeSelector:\n      kubernetes.io/os: windows</code></pre> <p>This will create a deployment, containing 2 IIS web servers. To verify that things work well, let&#x2019;s also include a service to route traffic to these IIS servers. Note how the service definition isn&#x2019;t any different for Windows vs Linux workloads.</p> <pre class=\"wp-block-code\"><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: win-webserver\n  labels:\n    app: win-webserver\nspec:\n  selector:\n    app: win-webserver\n  ports:\n  - port: 80\n    targetPort: 80\n  type: LoadBalancer</code></pre> <p>We can deploy both using: <code>kubectl create -f .</code> .</p> <p>It will take a while for the pods to spin up since Windows images are generally a bit bigger than Linux images. But after a couple of minutes, you should see your Windows pods running, which you can confirm using <code>kubectl get pods -o wide</code>:</p> <figure class=\"wp-block-image size-large\"><img src=\"/wp-content/uploads/2020/11/image-2-1024x86.png\" alt class=\"wp-image-1438\" srcset=\"https://nillsf.com/wp-content/uploads/2020/11/image-2-1024x86.png 1024w, https://nillsf.com/wp-content/uploads/2020/11/image-2-300x25.png 300w, https://nillsf.com/wp-content/uploads/2020/11/image-2-768x65.png 768w, https://nillsf.com/wp-content/uploads/2020/11/image-2.png 1405w\" sizes=\"(max-width: 1024px) 100vw, 1024px\"><figcaption>Getting the windows server pods.</figcaption></figure> <p>And we can now also browse to the service. To get its public IP, use <code>kubectl get svc</code>:</p> <figure class=\"wp-block-image size-large\"><img src=\"/wp-content/uploads/2020/11/image-3-1024x131.png\" alt class=\"wp-image-1439\" srcset=\"https://nillsf.com/wp-content/uploads/2020/11/image-3-1024x131.png 1024w, https://nillsf.com/wp-content/uploads/2020/11/image-3-300x38.png 300w, https://nillsf.com/wp-content/uploads/2020/11/image-3-768x98.png 768w, https://nillsf.com/wp-content/uploads/2020/11/image-3.png 1134w\" sizes=\"(max-width: 1024px) 100vw, 1024px\"><figcaption>Getting the service&#x2019;s public IP address</figcaption></figure> <p>And if we browse to that IP, you can see a glorious IIS web server running on Kubernetes:</p> <figure class=\"wp-block-image size-large\"><img src=\"/wp-content/uploads/2020/11/image-4-1024x678.png\" alt class=\"wp-image-1440\" srcset=\"https://nillsf.com/wp-content/uploads/2020/11/image-4-1024x678.png 1024w, https://nillsf.com/wp-content/uploads/2020/11/image-4-300x199.png 300w, https://nillsf.com/wp-content/uploads/2020/11/image-4-768x509.png 768w, https://nillsf.com/wp-content/uploads/2020/11/image-4.png 1469w\" sizes=\"(max-width: 1024px) 100vw, 1024px\"><figcaption>A glorious IIS web server running on AKS.</figcaption></figure> <p>And that&#x2019;s how you run Windows containers on AKS.</p> <h2>Conclusion</h2> <p>In this blog post, we looked into how to run Windows containers on the Azure Kubernetes Service (AKS). We created a new AKS cluster, and we added a Windows nodepool. After that, we scheduled an actual workload on the nodes in that pool. To do this, we used the <code>nodeSelector </code>in the pod definition.</p> <p>If you&#x2019;re interested to take this one step further and also run Windows pods on ACI instances using the virtual kubelet, check <a href=\"https://blog.nillsf.com/index.php/2020/10/26/creating-windows-azure-container-instances-using-the-virtual-kubelet-from-the-azure-kubernetes-service/\">out this blog post</a>.</p> </div>",
      "contentAsText": " Containers and Kubernetes have traditionally been the area of Linux-based workloads. However, things have changed. Windows has supported Docker containers for a while now, and since Kubernetes 1.14, Windows support has been generally available in Kubernetes as well. In this blog post, we’ll explore how you can add Windows nodes to a Kubernetes cluster running on Azure. After we’ve set up the cluster, we’ll have a look at how actual Windows containers can be created on the cluster. Setting up the cluster To run Windows containers on AKS, we’ll need the following: An AKS cluster(at least) 1 Linux Nodepool. This is used for running system components such as CoreDNS.A Windows Nodepool Let’s create all of this using the Azure CLI: # Create a resource group\naz group create -n win-aks -l westus2\n\n# Create the cluster, with the default linux nodepool\naz aks create -g win-aks -n win-aks \\\n  --node-count 2 --ssh-key-value ~/.ssh/id_rsa.pub \\\n  --windows-admin-username nilfranadmin \\\n  --windows-admin-password superSecret123! \\\n  --network-plugin azure\n\n# Create a second nodepool using Windows\n az aks nodepool add -g win-aks \\\n  --cluster-name win-aks \\\n  --os-type Windows --name winnp \\\n  --node-count 2 The second command will take some time to run (about 15 minutes), but after a while, we will be able to go ahead and schedule Windows containers. While you’re waiting for the node pool to be added, let’s explore how we need to tell Kubernetes to schedule a Windows workload on Windows nodes. A little info about labels and nodeSelectors Once you have a cluster with both Linux and Windows nodes, you should be able to run kubectl get nodes -o wide and see you now have nodes with a Windows operating system: You should have nodes with a Windows OS What’s more, these nodes are also labeled with their OS information. To see those labels, run a kubectl describe node <windows-node-name>: Labels on Windows nodes To schedule pods on a Windows node (or a Linux node for that matter) you’ll have to set a nodeSelector in the pod definition. In the node selector, you define which labels on the nodes need to be met to schedule pods on certain nodes. In case of the operating system, we’ll set the kubernetes.io/os label to Windows in the nodeSelector. What’s important to note here: if you run in a mixed cluster (with both Linux and Windows nodes and workloads), you need to include the nodeSelector for both Windows and Linux workloads. Otherwise, Kubernetes might schedule Linux pods on Windows nodes (or vice-versa), and that will lead to issues. Let’s have a look at how to do that: How to schedule pods on Windows nodes As mentioned in the previous section, to schedule a pod on a Windows node, you need to include a nodeSelector in your workload definition. An example of that below (code is also available on GitHub): apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: win-webserver\n  name: win-webserver\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: win-webserver\n  template:\n    metadata:\n      labels:\n        app: win-webserver\n      name: win-webserver\n    spec:\n     containers:\n      - name: windowswebserver\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n        image: mcr.microsoft.com/windows/servercore/iis:windowsservercore-ltsc2019\n     nodeSelector:\n      kubernetes.io/os: windows This will create a deployment, containing 2 IIS web servers. To verify that things work well, let’s also include a service to route traffic to these IIS servers. Note how the service definition isn’t any different for Windows vs Linux workloads. apiVersion: v1\nkind: Service\nmetadata:\n  name: win-webserver\n  labels:\n    app: win-webserver\nspec:\n  selector:\n    app: win-webserver\n  ports:\n  - port: 80\n    targetPort: 80\n  type: LoadBalancer We can deploy both using: kubectl create -f . . It will take a while for the pods to spin up since Windows images are generally a bit bigger than Linux images. But after a couple of minutes, you should see your Windows pods running, which you can confirm using kubectl get pods -o wide: Getting the windows server pods. And we can now also browse to the service. To get its public IP, use kubectl get svc: Getting the service’s public IP address And if we browse to that IP, you can see a glorious IIS web server running on Kubernetes: A glorious IIS web server running on AKS. And that’s how you run Windows containers on AKS. Conclusion In this blog post, we looked into how to run Windows containers on the Azure Kubernetes Service (AKS). We created a new AKS cluster, and we added a Windows nodepool. After that, we scheduled an actual workload on the nodes in that pool. To do this, we used the nodeSelector in the pod definition. If you’re interested to take this one step further and also run Windows pods on ACI instances using the virtual kubelet, check out this blog post. ",
      "publishedDate": "2020-11-16T16:00:00.000Z"
    },
    {
      "url": "https://getenroute.io/blog/leverage-open-source-oss-derive-insights-grafana-prometheus-tsdb-kubernetes-standalone-api-gateway/",
      "title": "TSDB, Prometheus, Grafana in Kubernetes: Tracing a variable across the OSS Monitoring stack",
      "content": "<div class=\"contant\"><p>Metrics are a critical aspect of any system to understand its health and operational state. Design of any system requires collection, storage and reporting of metrics to provide a pulse of the system. We explore how tsdb, prometheus and grafana form that stack for open source metrics reporting in Kubernetes.</p><p>Data is stored over a series of time intervals and needs an efficient database to store and retrieve this data.</p><p><span class=\"text-color letter-spacing\"><a href=\"http://opentsdb.net/overview.html\">OpenTSDB Time Series Database</a></span> is one such time series database that can serve that need.</p><p>While data is stored in a time series database, an standard system to scrape such metrics and store it in the database has emerged in form of <a class=\"text-color\" href=\"https://prometheus.io\">Prometheus </a>. When a data source exports metrics in <a class=\"text-color\" href=\"https://prometheus.io/docs/instrumenting/exposition_formats/\">prometheus exposition format</a>, it can be scraped by prometheus. While time series database stores metrics, Prometheus collects the metrics and pushes them in the database.</p><p>Any database also needs an efficient and programmer friendy way to query information, eg: SQL for popular transactional databases like Postgres and MySQL. Prometheus defines a rich query language in form of PromQL to query data from this time series database.</p><p>Any form of reporting solution isn&#x2019;t complete without a graphical component to plot data in graphs, bar charts, pie charts, time series and other mechanisms to visualize data. Grafana serves this need where it can take a data source (like Prometheus) and provides the programmability and flexibility to display data in a form that is useful to the user.</p><p>We broadly cover how the different systems work in tandem, what makes them stick together, some detail of each sub-system and tracing a variable from end-to-end to gain a clear understanding.</p><p>Enroute is built using Envoy proxy and both sub-systems provide a prometheus endpoint to scrape/export the operational metrics in the system.</p><h3 id=\"discovery-storage-and-querying-of-metrics-data\">Discovery, Storage and Querying of Metrics Data</h3><p>This section describes how prometheus uses service discovery to scrape data and store it in TSDB. We then describe how Grafana uses PromQL to query this data.</p><h5 id=\"finding-instances-to-scrape-using-service-discovery\">Finding Instances to Scrape using Service Discovery</h5><p>Prometheus needs a port and path to scrape data from. How does it find that? This problem is addressed by Prometheus service discovery.</p><p>You can set the instance port and path to scrape in a prometheus config file. With dynamic environments like kubernetes where containers, endpoints, pods, services, ip-addresses are transient, a static configuration won&#x2019;t work. A more dynamic approach is using service discovery and providing instructions about how to work with discovered services. This is where prometheus service discovery can be configured to discover instances (or job which is a collection of instances).</p><p>Prometheus service discovery supports several environments like Kubernetes, Azure, Digital Ocean and several other options. A complete set of service discovery options can be found on <a class=\"text-color\" href=\"https://prometheus.io/docs/prometheus/latest/configuration/configuration/\">prometheus configuration reference </a>. The <code>&lt;*_sd_config&gt;</code> are all different environments in which prometheus has built-in support and can be configured for service discovery.</p><p>Service discovery returns a list of instances to scrape metrics from. The discovery process may also discover additional metadata about the discovered instances. This additional metatdata may be used by the user to ignore, filter, customize or to<a class=\"text-color\" href=\"https://github.com/prometheus/prometheus/tree/master/discovery\"> add attributes to the collected data</a>. Prometheus uses a <a class=\"text-color\" href=\"https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config\">relablelling</a> mechanism to achieve that.</p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/9e278a0e4469b28dd26f14efcf4181d65b6fdf20/905e4/img/prometheus-discovery.png\" alt=\"prometheus grafana discovery\" width=\"900\"></figure><p>In the example above from <a class=\"text-color\" href=\"https://github.com/prometheus/prometheus/blob/v2.22.0-rc.0/documentation/examples/prometheus-kubernetes.yml\">prometheus github </a>, the configuration file for prometheus specifies kubernetes service discovery using <code>kubernetes_sd_config</code> directive. Note how relabelling can be used to ignore certain instances and replace variable names.</p><p>Prometheus also shows the list of targets that it has discovered on the <code>/targets</code> endpoint</p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/7a2213180bf93362518d534cf44fbb0d3c481b12/0b4bc/img/prometheus-discovery-targets.png\" alt=\"prometheus grafana discovery targets\" width=\"650\"></figure><h5 id=\"reporting-of-variables\">Reporting of variables</h5><p>The first requirement for the system to work is how variable state or metric is reported. When a system that has metric to report, it is broadly classified as the type of data which is either a counter that monotonically increases or a gauge that can increase or decrease over time.</p><p>Such a distinction at the source of where the metric is generated is required to facilitate the metric stack to correctly query, store and retrieve data.</p><p>Prometheus has several libraries in different languages that help facilitate defining the variable type, operating on it and exporting it.</p><h5 id=\"scraping-and-storage-of-variables\">Scraping and storage of variables</h5><p>OpenTSDB supports a multi-dimensonal data model where a data point can be identified using a name and key/value pairs. As an example, let us consider the data model with an example.</p><p>When a request traverses through Envoy, it is proxied and connects to an upstream to serve the request. One such variable for upstream stats is <code>envoy_cluster_upstream_rq</code>.</p><p>The figure below shows <a class=\"text-color\" href=\"https://prometheus.io/docs/instrumenting/exposition_formats/\">prometheus exposition format</a> that is used as an export format for the metric flows over the wire:</p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/ce9d0ca14ff136db29d7d013c838fc0c7de90507/b613f/img/prometheus-scrape.png\" alt=\"prometheus grafana scrape\" width=\"1000\"></figure><p>We can <code>GET</code> the information about this variable in Envoy from <code>/stats/prometheus</code> url of <a class=\"text-color\" href=\"https://www.envoyproxy.io/docs/envoy/latest/operations/admin#get--stats-prometheus\">envoy stats</a> endpoint:</p><div class=\"highlight\"><pre><code class=\"language-shell\"><span> 1</span>$ curl -s localhost:9001/stats/prometheus <span>|</span> grep envoy_cluster_upstream_rq\n<span><span> 2</span><span># TYPE envoy_cluster_upstream_rq counter</span>\n</span><span> 3</span>envoy_cluster_upstream_rq<span>{</span><span>envoy_response_code</span><span>=</span><span>&quot;200&quot;</span>,envoy_cluster_name<span>=</span><span>&quot;enroutedemo_externalauth_443&quot;</span><span>}</span> <span>1</span>\n<span> 4</span>envoy_cluster_upstream_rq<span>{</span><span>envoy_response_code</span><span>=</span><span>&quot;200&quot;</span>,envoy_cluster_name<span>=</span><span>&quot;enroutedemo_hello-enroute_9091&quot;</span><span>}</span> <span>11</span>\n<span> 5</span>envoy_cluster_upstream_rq<span>{</span><span>envoy_response_code</span><span>=</span><span>&quot;429&quot;</span>,envoy_cluster_name<span>=</span><span>&quot;enroutedemo_hello-enroute_9091&quot;</span><span>}</span> <span>1</span>\n<span> 6</span>envoy_cluster_upstream_rq<span>{</span><span>envoy_response_code</span><span>=</span><span>&quot;200&quot;</span>,envoy_cluster_name<span>=</span><span>&quot;enroutedemo_lambdacluster_443&quot;</span><span>}</span> <span>12</span>\n<span> 7</span>envoy_cluster_upstream_rq<span>{</span><span>envoy_response_code</span><span>=</span><span>&quot;200&quot;</span>,envoy_cluster_name<span>=</span><span>&quot;saaras-enroute_enroute_8001&quot;</span><span>}</span> <span>13</span>\n<span> 8</span>envoy_cluster_upstream_rq<span>{</span><span>envoy_response_code</span><span>=</span><span>&quot;503&quot;</span>,envoy_cluster_name<span>=</span><span>&quot;saaras-enroute_enroute_8001&quot;</span><span>}</span> <span>3</span>\n<span> 9</span>envoy_cluster_upstream_rq<span>{</span><span>envoy_response_code</span><span>=</span><span>&quot;200&quot;</span>,envoy_cluster_name<span>=</span><span>&quot;saaras-enroute_enroute_8003&quot;</span><span>}</span> <span>12</span>\n<span>10</span>envoy_cluster_upstream_rq<span>{</span><span>envoy_response_code</span><span>=</span><span>&quot;200&quot;</span>,envoy_cluster_name<span>=</span><span>&quot;saaras-enroute_enroute_8004&quot;</span><span>}</span> <span>212</span>\n<span>11</span>envoy_cluster_upstream_rq<span>{</span><span>envoy_response_code</span><span>=</span><span>&quot;200&quot;</span>,envoy_cluster_name<span>=</span><span>&quot;saaras-enroute_service-stats_9001&quot;</span><span>}</span> <span>6237</span></code></pre></div><p>Note the <code>TYPE</code> that tells prometheus about the type of variable. <code>counter</code> is one type of variable supported by prometheus.</p><p>This variable <code>envoy_cluster_upstream_rq</code> is stored in <a class=\"text-color\" href=\"http://opentsdb.net/overview.html\">OpenTSDB </a>, which in addition to the value of the variable stores a timestamp and key/value pairs. These key/value pairs facilitate querying of data. The following are all valid queries -</p><ul><li>get a total count of upstream requests where response code is 200.</li><li>get value of all upstream requests for cluster hello-enroute_9091 where reponse code is 429 (rate-limited)</li></ul><h5 id=\"querying-of-variables\">Querying of variables</h5><p>Next we look at how data can be queried once it is stored in TSDB. Querying is how a tool like Grafana pulls data from Prometheus/TSDB</p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/bf35e188bb291182e10be04b5ed7374527109a2f/9dc8b/img/prometheus-q.png\" alt=\"prometheus grafana query\" width=\"400\"></figure><p>Note that data variables hold not only time-series value but also an associated key/value. These attributes can be used for querying (eg: <code>envoy_response_code</code> key). This querying is what makes it powerful. There are several ways to look at the same data. While envoy instance from which data was scraped may report a couple of dimensions (or additional key/value tags), additional tags may be added by Prometheus. For instance Prometheus adds kubernetes metadata like service (eg: <code>service</code> key), namespace (eg: <code>kubernetes_namespace</code> key), pod-name, endpoint info etc. when it scrapes and stores information from an instance. All this additional tags can be used to query the data.</p><p>The query language to support rich set of use-cases is critical, and PromQL provides that flexibility. We quickly take a look at how to use PromQL to form queries. More details about PromQL can be found on the prometheus <a class=\"text-color\" href=\"https://prometheus.io/docs/prometheus/latest/querying/basics/\">PromQL</a></p><p>Going back to the original variable <code>envoy_cluster_upstream_rq</code>, it counts the number of upstream requests for clusters. Here are several ways to get this counter:</p><h6 id=\"query-span-stylecolorgreenfor-service-enroutespan\">Query <span>for service enroute</span></h6><div class=\"highlight\"><pre><code class=\"language-shell\">envoy_cluster_upstream_rq<span>{</span>\n<span>\t<span>service</span><span>=</span><span>&quot;enroute&quot;</span><span>}</span></span></code></pre></div><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/d38928b8dd713ec60dfe52a310e04e48c20292cf/4184b/img/prometheus-query-service.png\" alt=\"prometheus grafana query services tag\" width=\"850\"></figure><h6 id=\"query-span-stylecolorgreenfor-service-enroute-and-response-code-200span\">Query <span>for service enroute AND response code 200</span></h6><div class=\"highlight\"><pre><code class=\"language-shell\">envoy_cluster_upstream_rq<span>{</span>\n<span>\t<span>service</span><span>=</span><span>&quot;enroute&quot;</span>,\n</span><span>\t<span>envoy_response_code</span><span>=</span><span>&quot;200&quot;</span><span>}</span></span></code></pre></div><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/6e41e14c2c8b142c41a64962c938c34133228510/8557e/img/prometheus-query-service-200.png\" alt=\"prometheus grafana query service return code 200\" width=\"850\"></figure><h6 id=\"query-span-stylecolorgreenfor-service-enroute-and-response-code-200-and-namespace-enroutedemospan\">Query <span>for service enroute AND response code 200 and namespace enroutedemo</span></h6><div class=\"highlight\"><pre><code class=\"language-shell\">envoy_cluster_upstream_rq<span>{</span>\n<span>\t<span>service</span><span>=</span><span>&quot;enroute&quot;</span>,\n</span><span>\t<span>envoy_response_code</span><span>=</span><span>&quot;200&quot;</span>,\n</span><span>\t<span>kubernetes_namespace</span><span>=</span><span>&quot;enroutedemo&quot;</span>\n</span><span>}</span></code></pre></div><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/a2c35468a38bc8855190ddbc6319dae9a22c0fce/7071e/img/prometheus-query-service-200-enroutedemo.png\" alt=\"prometheus grafana query enroute service return code 200\" width=\"850\"></figure><p>Note that the above value is time series data. The above is a fairly simple example. While querying a time series the offset, duration, subquery, aggregation, function and other qualifiers can be used to extract more fine-grained data. For example, you can dump the [30m] rate of this time-series:</p><div class=\"highlight\"><pre><code class=\"language-shell\">rate<span>(</span>envoy_cluster_upstream_rq<span>{</span> <span>service</span><span>=</span><span>&quot;enroute&quot;</span>, <span>envoy_response_code</span><span>=</span><span>&quot;200&quot;</span>, <span>kubernetes_namespace</span><span>=</span><span>&quot;enroutedemo&quot;</span>\n<span> <span>}[</span>30m<span>])</span></span></code></pre></div><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/95c0786fdc1213877dfdff06b3f386e333cf6264/68ad5/img/prometheus-query-service-200-enroutedemo-rate.png\" alt=\"prometheus grafana query rate function\" width=\"850\"></figure><p>More detailed examples can be found on <a class=\"text-color\" href=\"https://prometheus.io/docs/prometheus/latest/querying/basics/\">prometheus documentation reference</a></p><h3 id=\"types-of-metrics\">Types of Metrics</h3><p>Prometheus system offers a few core metric types that are supported by the system.</p><ul><li>Counter: monotonically increasing value, eg: request count, connection count etc.</li><li>Gauge: can increase/decrease in value, eg: upstream count, certificate count, cluster count</li><li>Histogram: Bucketed counters</li><li>Summary: Is just like histogram but also calculates quantiles.</li></ul><p>A detailed explaination of these variable types can be found on <a class=\"text-color\" href=\"https://prometheus.io/docs/concepts/metric_types/\">prometheus section on variable types</a></p><h3 id=\"statistics-in-envoy-and-enroute\">Statistics in Envoy and Enroute</h3><p>Enroute is a lightweight shim that provides a cache for Envoy config. It exports metrics that are relevant to the internals of the cache it maintains to serve Envoy. Additionally, it has has metrics about other subsystems that work in tandem to provide API security.</p><p>A lot of interesting state and metrics data is provided by Envoy. A proxy like Envoy has several sub-systems that exports a lot data. We won&#x2019;t look at all the metrics but high level components and the data they export. The <a class=\"text-color\" href=\"https://www.envoyproxy.io/docs/envoy/latest/\">Envoy documentation reference</a> provides a complete list of stats exported.</p><table class=\"table table-hover\"><thead><tr><th>Component</th><th>Stats</th></tr></thead><tbody><tr><td>Downstream Envoy Internal Context</td><td><div class=\"highlight\"><pre><code class=\"language-shell\">envoy_http_downstream_cx_active\nenvoy_http_downstream_cx_http1_active\nenvoy_http_downstream_cx_http2_active\nenvoy_http_downstream_cx_http3_active\nenvoy_http_downstream_cx_protocol_error\nenvoy_http_downstream_cx_ssl_active</code></pre></div></td></tr><tr><td>Downstream Requests</td><td><div class=\"highlight\"><pre><code class=\"language-shell\">envoy_http_downstream_rq_active\nenvoy_http_downstream_http1_total\nenvoy_http_downstream_http2_total\nenvoy_http_downstream_http3_total\nenvoy_http_downstream_rq_rx_reset\nenvoy_http_downstream_rq_total\nenvoy_http_downstream_tx_reset\nenvoy_http_downstream_rq_xx</code></pre></div></td></tr><tr><td>SSL</td><td><div class=\"highlight\"><pre><code class=\"language-shell\">envoy_listener_ssl_connection_error\nenvoy_listener_ssl_handshake\nenvoy_listener_ssl_session_reused\n\nenvoy_cluster_ssl_ciphers\nenvoy_cluster_ssl_connection_error\nenvoy_cluster_ssl_handshake\nenvoy_cluster_ssl_session_reused</code></pre></div></td></tr><tr><td>Circuit Breakers</td><td><div class=\"highlight\"><pre><code class=\"language-shell\">envoy_cluster_circuit_breakers_default_cx_open\nenvoy_cluster_circuit_breakers_default_rq_open\nenvoy_cluster_circuit_breakers_high_cx_open\nenvoy_cluster_circuit_breakers_high_rq_open</code></pre></div></td></tr><tr><td>Upstream Envoy Internal Context</td><td><div class=\"highlight\"><pre><code class=\"language-shell\">envoy_cluster_upstream_cx_active\nenvoy_cluster_upstream_cx_connect_failed\nenvoy_cluster_upstream_cx_http1_total\nenvoy_cluster_upstream_cx_http2_total\nenvoy_cluster_upstream_cx_total</code></pre></div></td></tr><tr><td>Upstream Envoy Request</td><td><div class=\"highlight\"><pre><code class=\"language-shell\">envoy_cluster_upstream_rq_active\nenvoy_cluster_upstream_rq_retry\nenvoy_cluster_upstream_rq_total\nenvoy_cluster_upstream_rq_timeout\nenvoy_cluster_upstream_xx</code></pre></div></td></tr><tr><td>Envoy Access Log</td><td><div class=\"highlight\"><pre><code class=\"language-shell\">envoy_access_logs_grpc_access_log_logs_written\nenvoy_access_logs_grpc_access_log_logs_dropped</code></pre></div></td></tr><tr><td>Envoy OAuth and JWT and other interesting filters</td><td><div class=\"highlight\"><pre><code class=\"language-shell\">envoy_http_jwt_authn_allowed\nenvoy_http_jwt_authn_denied\nenvoy_http_oauth_unauthorized_rq\nenvoy_http_oauth_failure\nenvoy_http_oauth_success\nenvoy_http_aws_lambda_server_error\nenvoy_http_aws_lambda_upstream_rq\nenvoy_http_ext_authz_ok\nenvoy_http_ext_authz_denied\nenvoy_http_ext_authz_error\nenvoy_http_ext_authz_timeout\nmysql login_attempts\nmysql login_failure\nmysql queries_parsed\nkafka response.TYPE\n...</code></pre></div></td></tr></tbody></table><p>Grafana is an open source tool to build an observability dashboard. The primary abstractions used to build a UI in grafana is a Dashboard that is made up of several Panels. A panel embeds a query that pulls the data to be displayed and various styling and formatting options. An intermediate step between querying and visualizing the results of a query is transformation. As the name suggests, transformation provides a mechanism to work on the data before it is passed to visualiztion.</p><p>Grafana supports <a class=\"text-color\" href=\"https://grafana.com/docs/grafana/latest/datasources/\">several data sources</a> one of which is Prometheus.</p><p>In Grafana, you build graphs using Visualizations. Visualizations roughly map to displaying data types defined earlier in Prometheus eg: counter, gauge, histogram and summary. There also is flexibility in how they are displayed as graphs, bar charts as tables or heatmaps.</p><p>A more detailed explaination of <a class=\"text-color\" href=\"https://grafana.com/docs/grafana/latest/dashboards/\">Dashboard </a>, <a class=\"text-color\" href=\"https://grafana.com/docs/grafana/latest/panels/\">Panels </a>, Querying, <a class=\"text-color\" href=\"https://grafana.com/docs/grafana/latest/panels/transformations/\">Transformation</a> and Visualizations can be accessed from the Grafana website.</p><p>Here is an example Grafana Dashboard that is made up of panels -</p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/dbda53d69483c65f0e7ad682fdfdbd8e81aaaaf1/8b78b/img/enroute-grafana-front.jpg\" alt=\"prometheus grafana main stats flow\" width=\"1200\"></figure><p>The dashboard is made up of 12 panels across 3 rows and 4 columns. Each panel has a query to pull data from a data source. The panel also has a JSON representation in grafana. Here is one such representation for <strong>Downstream 2xx Responses</strong></p><div class=\"highlight\"><pre><code class=\"language-json\"><span>{</span>\n<span>...</span> <span>&quot;datasource&quot;</span><span>:</span> <span>&quot;prometheus&quot;</span><span>,</span>\n<span>...</span> <span>&quot;gridPos&quot;</span><span>:</span> <span>{</span> <span>&quot;h&quot;</span><span>:</span> <span>8</span><span>,</span> <span>&quot;w&quot;</span><span>:</span> <span>6</span><span>,</span> <span>&quot;x&quot;</span><span>:</span> <span>0</span><span>,</span> <span>&quot;y&quot;</span><span>:</span> <span>50</span> <span>},</span>\n<span>...</span> <span>&quot;targets&quot;</span><span>:</span> <span>[</span> <span>{</span>\n<span> <span>&quot;expr&quot;</span><span>:</span> <span>&quot;sum(rate(envoy_http_downstream_rq_xx{namespace=~\\&quot;$Namespace\\&quot;,service=~\\&quot;$Service\\&quot;,envoy_response_code_class=~\\&quot;2\\&quot;}[1m])) by (namespace,service)&quot;</span><span>,</span>\n</span> <span>&quot;format&quot;</span><span>:</span> <span>&quot;time_series&quot;</span><span>,</span> <span>&quot;intervalFactor&quot;</span><span>:</span> <span>2</span><span>,</span> <span>&quot;legendFormat&quot;</span><span>:</span> <span>&quot;{{namespace}}/{{service}}&quot;</span><span>,</span> <span>&quot;refId&quot;</span><span>:</span> <span>&quot;A&quot;</span> <span>}</span> <span>],</span>\n<span>...</span> <span>&quot;title&quot;</span><span>:</span> <span>&quot;Downstream 2xx Responses&quot;</span><span>,</span> <span>&quot;tooltip&quot;</span><span>:</span> <span>{</span> <span>&quot;shared&quot;</span><span>:</span> <span>true</span><span>,</span> <span>&quot;sort&quot;</span><span>:</span> <span>0</span><span>,</span> <span>&quot;value_type&quot;</span><span>:</span> <span>&quot;individual&quot;</span> <span>},</span> <span>&quot;type&quot;</span><span>:</span> <span>&quot;graph&quot;</span><span>,</span> <span>&quot;xaxis&quot;</span><span>:</span> <span>{</span>\n<span>...</span> <span>},</span> <span>&quot;yaxes&quot;</span><span>:</span> <span>[</span>\n<span>...</span> <span>]</span>\n<span>}</span></code></pre></div><p>The representation above has a lot of fields removed just to highlight the interesting ones to build an understanding of how a variable reported in enroute/envoy is displayed. The interesting parts are <code>datasource</code> and the <code>targets.expr</code> that captures the query. Note the PromQL syntax and how the cumulative value of 1m rate is plotted over time.</p><p>The query:</p><div class=\"highlight\"><pre><code class=\"language-json\"> <span>sum(</span> <span>rate(</span> <span>envoy_http_downstream_rq_xx</span><span>{</span> <span>namespace=~\\</span><span>&quot;$Namespace\\&quot;,\n</span><span> service=~\\&quot;$Service\\&quot;,\n</span><span> envoy_response_code_class=~\\&quot;2\\&quot;</span><span>}[</span><span>1</span><span>m</span><span>]</span><span>))</span> <span>by</span> <span>(namespace,service)</span></code></pre></div><p>fetches the value of <code>envoy_http_downstream_rq_xx</code> for a reponse code <code>2xx</code></p><p>Here is another example that plots a histogram of latency to upstream server after <a class=\"text-color\" href=\"http://localhost:1313/blog/can-your-api-gateway-tango-to-openapi-swagger-spec/\">importing swagger spec</a> using <code>enroutectl</code>.</p><figure><img src=\"https://d33wubrfki0l68.cloudfront.net/16f45cacf64186815cd049ec4c7654a7503c55a5/03daa/img/grafana-swagger-article.jpg\" width=\"1200\"></figure><p>There are use-cases where: either more variety of data types and metrics push is needed (InfluxDB), a unified solution for storage and graphing (Graphite), efficient indexed text based queries (ELK/Lucene), network service monitoring like Nagios. There are a wide variety of open source and paid tools depending on the needs and use-cases.</p><h3 id=\"open-source-metrics-withwithout-kubernetes\">Open Source Metrics with/without Kubernetes</h3><p>Enroute Universal API Gateway with its API both for Standalone and Kubernetes works with Prometheus and Grafana based open source telemetry. Both Enroute and Envoy can export operational data in Prometheus Exposition Format. Telemetry is critical for operations and using an open source format like Prometheus and Grafana ensures that programmable insights can be derived from data and the API gateway adheres to the overall logging architecture and choices.</p><p>When working with a complex system, any downtime is unacceptable. Understanding the root cause requires metrics to understand and troubleshoot. Logging and monitoring are critical to ensure the DevOps team has the necessary tool to keep the system running. Enroute Universal API Gateway automatically creates detailed dashboards for individual Envoy components to provide deep insights into working of several API Gateway sub-systems.</p></div>",
      "contentAsText": "Metrics are a critical aspect of any system to understand its health and operational state. Design of any system requires collection, storage and reporting of metrics to provide a pulse of the system. We explore how tsdb, prometheus and grafana form that stack for open source metrics reporting in Kubernetes.Data is stored over a series of time intervals and needs an efficient database to store and retrieve this data.OpenTSDB Time Series Database is one such time series database that can serve that need.While data is stored in a time series database, an standard system to scrape such metrics and store it in the database has emerged in form of Prometheus . When a data source exports metrics in prometheus exposition format, it can be scraped by prometheus. While time series database stores metrics, Prometheus collects the metrics and pushes them in the database.Any database also needs an efficient and programmer friendy way to query information, eg: SQL for popular transactional databases like Postgres and MySQL. Prometheus defines a rich query language in form of PromQL to query data from this time series database.Any form of reporting solution isn’t complete without a graphical component to plot data in graphs, bar charts, pie charts, time series and other mechanisms to visualize data. Grafana serves this need where it can take a data source (like Prometheus) and provides the programmability and flexibility to display data in a form that is useful to the user.We broadly cover how the different systems work in tandem, what makes them stick together, some detail of each sub-system and tracing a variable from end-to-end to gain a clear understanding.Enroute is built using Envoy proxy and both sub-systems provide a prometheus endpoint to scrape/export the operational metrics in the system.Discovery, Storage and Querying of Metrics DataThis section describes how prometheus uses service discovery to scrape data and store it in TSDB. We then describe how Grafana uses PromQL to query this data.Finding Instances to Scrape using Service DiscoveryPrometheus needs a port and path to scrape data from. How does it find that? This problem is addressed by Prometheus service discovery.You can set the instance port and path to scrape in a prometheus config file. With dynamic environments like kubernetes where containers, endpoints, pods, services, ip-addresses are transient, a static configuration won’t work. A more dynamic approach is using service discovery and providing instructions about how to work with discovered services. This is where prometheus service discovery can be configured to discover instances (or job which is a collection of instances).Prometheus service discovery supports several environments like Kubernetes, Azure, Digital Ocean and several other options. A complete set of service discovery options can be found on prometheus configuration reference . The <*_sd_config> are all different environments in which prometheus has built-in support and can be configured for service discovery.Service discovery returns a list of instances to scrape metrics from. The discovery process may also discover additional metadata about the discovered instances. This additional metatdata may be used by the user to ignore, filter, customize or to add attributes to the collected data. Prometheus uses a relablelling mechanism to achieve that.In the example above from prometheus github , the configuration file for prometheus specifies kubernetes service discovery using kubernetes_sd_config directive. Note how relabelling can be used to ignore certain instances and replace variable names.Prometheus also shows the list of targets that it has discovered on the /targets endpointReporting of variablesThe first requirement for the system to work is how variable state or metric is reported. When a system that has metric to report, it is broadly classified as the type of data which is either a counter that monotonically increases or a gauge that can increase or decrease over time.Such a distinction at the source of where the metric is generated is required to facilitate the metric stack to correctly query, store and retrieve data.Prometheus has several libraries in different languages that help facilitate defining the variable type, operating on it and exporting it.Scraping and storage of variablesOpenTSDB supports a multi-dimensonal data model where a data point can be identified using a name and key/value pairs. As an example, let us consider the data model with an example.When a request traverses through Envoy, it is proxied and connects to an upstream to serve the request. One such variable for upstream stats is envoy_cluster_upstream_rq.The figure below shows prometheus exposition format that is used as an export format for the metric flows over the wire:We can GET the information about this variable in Envoy from /stats/prometheus url of envoy stats endpoint: 1$ curl -s localhost:9001/stats/prometheus | grep envoy_cluster_upstream_rq\n 2# TYPE envoy_cluster_upstream_rq counter\n 3envoy_cluster_upstream_rq{envoy_response_code=\"200\",envoy_cluster_name=\"enroutedemo_externalauth_443\"} 1\n 4envoy_cluster_upstream_rq{envoy_response_code=\"200\",envoy_cluster_name=\"enroutedemo_hello-enroute_9091\"} 11\n 5envoy_cluster_upstream_rq{envoy_response_code=\"429\",envoy_cluster_name=\"enroutedemo_hello-enroute_9091\"} 1\n 6envoy_cluster_upstream_rq{envoy_response_code=\"200\",envoy_cluster_name=\"enroutedemo_lambdacluster_443\"} 12\n 7envoy_cluster_upstream_rq{envoy_response_code=\"200\",envoy_cluster_name=\"saaras-enroute_enroute_8001\"} 13\n 8envoy_cluster_upstream_rq{envoy_response_code=\"503\",envoy_cluster_name=\"saaras-enroute_enroute_8001\"} 3\n 9envoy_cluster_upstream_rq{envoy_response_code=\"200\",envoy_cluster_name=\"saaras-enroute_enroute_8003\"} 12\n10envoy_cluster_upstream_rq{envoy_response_code=\"200\",envoy_cluster_name=\"saaras-enroute_enroute_8004\"} 212\n11envoy_cluster_upstream_rq{envoy_response_code=\"200\",envoy_cluster_name=\"saaras-enroute_service-stats_9001\"} 6237Note the TYPE that tells prometheus about the type of variable. counter is one type of variable supported by prometheus.This variable envoy_cluster_upstream_rq is stored in OpenTSDB , which in addition to the value of the variable stores a timestamp and key/value pairs. These key/value pairs facilitate querying of data. The following are all valid queries -get a total count of upstream requests where response code is 200.get value of all upstream requests for cluster hello-enroute_9091 where reponse code is 429 (rate-limited)Querying of variablesNext we look at how data can be queried once it is stored in TSDB. Querying is how a tool like Grafana pulls data from Prometheus/TSDBNote that data variables hold not only time-series value but also an associated key/value. These attributes can be used for querying (eg: envoy_response_code key). This querying is what makes it powerful. There are several ways to look at the same data. While envoy instance from which data was scraped may report a couple of dimensions (or additional key/value tags), additional tags may be added by Prometheus. For instance Prometheus adds kubernetes metadata like service (eg: service key), namespace (eg: kubernetes_namespace key), pod-name, endpoint info etc. when it scrapes and stores information from an instance. All this additional tags can be used to query the data.The query language to support rich set of use-cases is critical, and PromQL provides that flexibility. We quickly take a look at how to use PromQL to form queries. More details about PromQL can be found on the prometheus PromQLGoing back to the original variable envoy_cluster_upstream_rq, it counts the number of upstream requests for clusters. Here are several ways to get this counter:Query for service enrouteenvoy_cluster_upstream_rq{\n\tservice=\"enroute\"}Query for service enroute AND response code 200envoy_cluster_upstream_rq{\n\tservice=\"enroute\",\n\tenvoy_response_code=\"200\"}Query for service enroute AND response code 200 and namespace enroutedemoenvoy_cluster_upstream_rq{\n\tservice=\"enroute\",\n\tenvoy_response_code=\"200\",\n\tkubernetes_namespace=\"enroutedemo\"\n}Note that the above value is time series data. The above is a fairly simple example. While querying a time series the offset, duration, subquery, aggregation, function and other qualifiers can be used to extract more fine-grained data. For example, you can dump the [30m] rate of this time-series:rate(envoy_cluster_upstream_rq{ service=\"enroute\", envoy_response_code=\"200\", kubernetes_namespace=\"enroutedemo\"\n }[30m])More detailed examples can be found on prometheus documentation referenceTypes of MetricsPrometheus system offers a few core metric types that are supported by the system.Counter: monotonically increasing value, eg: request count, connection count etc.Gauge: can increase/decrease in value, eg: upstream count, certificate count, cluster countHistogram: Bucketed countersSummary: Is just like histogram but also calculates quantiles.A detailed explaination of these variable types can be found on prometheus section on variable typesStatistics in Envoy and EnrouteEnroute is a lightweight shim that provides a cache for Envoy config. It exports metrics that are relevant to the internals of the cache it maintains to serve Envoy. Additionally, it has has metrics about other subsystems that work in tandem to provide API security.A lot of interesting state and metrics data is provided by Envoy. A proxy like Envoy has several sub-systems that exports a lot data. We won’t look at all the metrics but high level components and the data they export. The Envoy documentation reference provides a complete list of stats exported.ComponentStatsDownstream Envoy Internal Contextenvoy_http_downstream_cx_active\nenvoy_http_downstream_cx_http1_active\nenvoy_http_downstream_cx_http2_active\nenvoy_http_downstream_cx_http3_active\nenvoy_http_downstream_cx_protocol_error\nenvoy_http_downstream_cx_ssl_activeDownstream Requestsenvoy_http_downstream_rq_active\nenvoy_http_downstream_http1_total\nenvoy_http_downstream_http2_total\nenvoy_http_downstream_http3_total\nenvoy_http_downstream_rq_rx_reset\nenvoy_http_downstream_rq_total\nenvoy_http_downstream_tx_reset\nenvoy_http_downstream_rq_xxSSLenvoy_listener_ssl_connection_error\nenvoy_listener_ssl_handshake\nenvoy_listener_ssl_session_reused\n\nenvoy_cluster_ssl_ciphers\nenvoy_cluster_ssl_connection_error\nenvoy_cluster_ssl_handshake\nenvoy_cluster_ssl_session_reusedCircuit Breakersenvoy_cluster_circuit_breakers_default_cx_open\nenvoy_cluster_circuit_breakers_default_rq_open\nenvoy_cluster_circuit_breakers_high_cx_open\nenvoy_cluster_circuit_breakers_high_rq_openUpstream Envoy Internal Contextenvoy_cluster_upstream_cx_active\nenvoy_cluster_upstream_cx_connect_failed\nenvoy_cluster_upstream_cx_http1_total\nenvoy_cluster_upstream_cx_http2_total\nenvoy_cluster_upstream_cx_totalUpstream Envoy Requestenvoy_cluster_upstream_rq_active\nenvoy_cluster_upstream_rq_retry\nenvoy_cluster_upstream_rq_total\nenvoy_cluster_upstream_rq_timeout\nenvoy_cluster_upstream_xxEnvoy Access Logenvoy_access_logs_grpc_access_log_logs_written\nenvoy_access_logs_grpc_access_log_logs_droppedEnvoy OAuth and JWT and other interesting filtersenvoy_http_jwt_authn_allowed\nenvoy_http_jwt_authn_denied\nenvoy_http_oauth_unauthorized_rq\nenvoy_http_oauth_failure\nenvoy_http_oauth_success\nenvoy_http_aws_lambda_server_error\nenvoy_http_aws_lambda_upstream_rq\nenvoy_http_ext_authz_ok\nenvoy_http_ext_authz_denied\nenvoy_http_ext_authz_error\nenvoy_http_ext_authz_timeout\nmysql login_attempts\nmysql login_failure\nmysql queries_parsed\nkafka response.TYPE\n...Grafana is an open source tool to build an observability dashboard. The primary abstractions used to build a UI in grafana is a Dashboard that is made up of several Panels. A panel embeds a query that pulls the data to be displayed and various styling and formatting options. An intermediate step between querying and visualizing the results of a query is transformation. As the name suggests, transformation provides a mechanism to work on the data before it is passed to visualiztion.Grafana supports several data sources one of which is Prometheus.In Grafana, you build graphs using Visualizations. Visualizations roughly map to displaying data types defined earlier in Prometheus eg: counter, gauge, histogram and summary. There also is flexibility in how they are displayed as graphs, bar charts as tables or heatmaps.A more detailed explaination of Dashboard , Panels , Querying, Transformation and Visualizations can be accessed from the Grafana website.Here is an example Grafana Dashboard that is made up of panels -The dashboard is made up of 12 panels across 3 rows and 4 columns. Each panel has a query to pull data from a data source. The panel also has a JSON representation in grafana. Here is one such representation for Downstream 2xx Responses{\n... \"datasource\": \"prometheus\",\n... \"gridPos\": { \"h\": 8, \"w\": 6, \"x\": 0, \"y\": 50 },\n... \"targets\": [ {\n \"expr\": \"sum(rate(envoy_http_downstream_rq_xx{namespace=~\\\"$Namespace\\\",service=~\\\"$Service\\\",envoy_response_code_class=~\\\"2\\\"}[1m])) by (namespace,service)\",\n \"format\": \"time_series\", \"intervalFactor\": 2, \"legendFormat\": \"{{namespace}}/{{service}}\", \"refId\": \"A\" } ],\n... \"title\": \"Downstream 2xx Responses\", \"tooltip\": { \"shared\": true, \"sort\": 0, \"value_type\": \"individual\" }, \"type\": \"graph\", \"xaxis\": {\n... }, \"yaxes\": [\n... ]\n}The representation above has a lot of fields removed just to highlight the interesting ones to build an understanding of how a variable reported in enroute/envoy is displayed. The interesting parts are datasource and the targets.expr that captures the query. Note the PromQL syntax and how the cumulative value of 1m rate is plotted over time.The query: sum( rate( envoy_http_downstream_rq_xx{ namespace=~\\\"$Namespace\\\",\n service=~\\\"$Service\\\",\n envoy_response_code_class=~\\\"2\\\"}[1m])) by (namespace,service)fetches the value of envoy_http_downstream_rq_xx for a reponse code 2xxHere is another example that plots a histogram of latency to upstream server after importing swagger spec using enroutectl.There are use-cases where: either more variety of data types and metrics push is needed (InfluxDB), a unified solution for storage and graphing (Graphite), efficient indexed text based queries (ELK/Lucene), network service monitoring like Nagios. There are a wide variety of open source and paid tools depending on the needs and use-cases.Open Source Metrics with/without KubernetesEnroute Universal API Gateway with its API both for Standalone and Kubernetes works with Prometheus and Grafana based open source telemetry. Both Enroute and Envoy can export operational data in Prometheus Exposition Format. Telemetry is critical for operations and using an open source format like Prometheus and Grafana ensures that programmable insights can be derived from data and the API gateway adheres to the overall logging architecture and choices.When working with a complex system, any downtime is unacceptable. Understanding the root cause requires metrics to understand and troubleshoot. Logging and monitoring are critical to ensure the DevOps team has the necessary tool to keep the system running. Enroute Universal API Gateway automatically creates detailed dashboards for individual Envoy components to provide deep insights into working of several API Gateway sub-systems.",
      "description": "Prometheus and Grafana with Enroute Universal API Gateway"
    },
    {
      "url": "https://humanitec.com/blog/your-helm-zoo-will-kill-you",
      "title": "Your Helm Zoo will kill you",
      "content": "<div href class=\"article__content w-richtext\"><p>This article is controversial. It aggressively questions helm-charts and current dev workflow designs, and I\u0019m well aware that not everyone will like this. Let me be clear before we dive in: this is an enterprise view. It\u0019s a view that is relevant to team sizes of 20 developers onwards. If you\u0019re a smaller dev shop that builds a few apps, this doesn\u0019t apply to you and you should just keep things as is. But for those of you that are working at scale or that are about to scale: watch out. Your helm-chart zoo will kill you. Maybe not tomorrow but almost definitely next year.</p><h2>Working change by change with kubectl</h2><p>At first they created kubectl-kangaroo and everyone could do everything the way they wanted. However, the challenge with just using kubectl is that you are working change by change. That\u0019s fast but makes it impossible to track what has actually changed in your cluster. One super clever person went ahead and managed everything in Kubernetes manifests and then versioned them in Git. Dope my friend, dope. </p><figure class=\"w-richtext-align-center w-richtext-figure-type-image\"><div><img src=\"https://assets.website-files.com/5c73bbfe3312822f153dd310/5fabfa08721bd4b36084afd9_kubectl-Kangaroo-humanitec.jpg\" alt></div><figcaption>A kubectl-Kangaroo that looked so sweet when you first came across it punches your DevOps team in the face.</figcaption></figure><h2>The Helm-Hedgehogs crawl your way&#xFFFD;</h2><p>But then, the amount of changes grew and someone realized that most of the manifest isn\u0019t really changing. \u001cCool,\u001d said the community, \u001cLet\u0019s create helm-charts-hedgehogs\u001d. Now everyone thought \u001cYeah, I can now write only bits of the manifest and take the configs from the template\u001d. That made writing charts faster, you could still version them in Git and execute changes against the K8s API.<br></p><p>But then, the team and apps grew. Versions were piling up, and applications added. This time there was a clear trace of what happened to Kubernetes Resources. And then the prickliness started.</p><figure class=\"w-richtext-align-center w-richtext-figure-type-image\"><div><img src=\"https://assets.website-files.com/5c73bbfe3312822f153dd310/5fabfa7bd60accce2f3a70da_helm-hedgehogs-humanitec.jpg\" alt></div><figcaption>Hedgehog babies look cute, innocent and a lot like their mother. That\u0019s misleading. They will develop their own very unique nasty character as they grow up.</figcaption></figure><h2>Legacy is like the shaggy fur</h2><p>It\u0019s lightweight at first. It takes half an hour to browse through an unstructured chart to understand what got changed where. Then, yet another onboarding session so people understand how to handle charts. A couple of hours per DevOps to roll back or find a specific version. Numerous hours to just align the changes to the infrastructure and the resources out of the cluster (databases etc) with the ones in the cluster. But legacy is like a difficult to untangle, shaggy fur.</p><figure class=\"w-richtext-align-center w-richtext-figure-type-image\"><div><img src=\"https://assets.website-files.com/5c73bbfe3312822f153dd310/5fabfb2da484fc921dfa20a6_legacy-wool-pig.jpg\" alt></div><figcaption>Legacy is like a dirty wool-pig. Better don\u0019t touch it!<br></figcaption></figure><p>A task that takes 30 minutes today might take 30 hours tomorrow. As you are rolling out more apps and more services, as colleagues leave with precious documentation in their heads, hours become weeks. The itching is becoming painful. Really, really painful. At first you will increase operational overhead until your DevOps colleagues get frustrated being the help-desks for overwhelmed app developers. Then you will try some potential solutions. Adding the <a href=\"https://kustomize.io/\">Kustomize</a>-Kingfisher sounds great. But you will discover that it\u0019s yet another patch for the disease and not a cure.</p><figure class=\"w-richtext-align-center w-richtext-figure-type-image\"><div><img src=\"https://assets.website-files.com/5c73bbfe3312822f153dd310/5fabfba6ab5d84fa3021cbc3_kustomize-kingfisher.jpg\" alt></div><figcaption>A Kustomize-Kingfisher in action. You can literally see the performance and ease. But wait half a year and the majestic feeling will cease.</figcaption></figure><p>At this point, folks usually start rearranging the way they store helm charts and versions in Git. They\u0019ll have long argumentations whether it\u0019s smarter to save them separately or with the application. Then, someone attends yet another meetup and you call it GitOps! Yessss, Gitops. That will do the job for another quarter until you will find that it has fundamental flaws as well, I could spend ages talking about those but,<a href=\"https://blog.container-solutions.com/gitops-the-bad-and-the-ugly?utm_source=twitter_np&amp;utm_medium=text&amp;utm_campaign=blog\"> this is a great article around what they are that speaks for itself</a>.</p><figure class=\"w-richtext-align-center w-richtext-figure-type-image\"><div><img src=\"https://assets.website-files.com/5c73bbfe3312822f153dd310/5fabfc0dd1addda8f553726d_chameleon-garden-1386295.jpeg\" alt=\"Chameleon Garden\"></div><figcaption>GitOps-Chameleons look like magic. Everytime you deploy them something changes. When it works: magic. If it doesn\u0019t: magic. Only this time it takes you hours sweating while the business team screams at you because your service is down and you cannot find the correct version to roll-back to.</figcaption></figure><h2>Helm has fundamental design flaws&#xFFFD;</h2><p>Long story short, I can tell you where the problem lies, Helm-charts and the entire ecosystem have a fundamental design flaw: they are developed for setups only such as where there is a comparably small number of experts working closely together, writing both applications and charts. Everyone owns their charts, which they know in and out. They develop a very specific style of writing them and will know exactly what to change if necessary. The thing is: that\u0019s not the mainstream use-case, especially not in the enterprise.<br></p><p>In enterprise you will have an ever changing group of people working on several applications across teams. You will have many more onboarding situations, you will have the division of labor and you will always have the application developer who just wants to focus on React and Typescript and not about Helm-Charts. In these setups, the approach is a recipe for a disaster. One that comes in the form of a slow and uncomfortable disease that will eventually halt delivery almost completely. I talked to the CTO of a 500 people dev team that is completely devastated because they are hardly delivering features at all.<br></p><h2>Let\u0019s zoom in on the details&#xFFFD;</h2><h3>Design flaw #1: Too many was to get to Rome</h3><p>Scripts and Helm Charts in particular, allow too many ways to get to Rome. If you want to update a cluster, there are just too many ways an individual contributor can write the syntax (I would argue 1000+). That makes it really hard to standardize and really hard to maintain, update and read.&#xFFFD;</p><h4>Solution for design flaw #1:&#xFFFD;</h4><p>Treat manifests as a protocol and standardize the way you apply and track changes to the baseline configurations of this protocol through an API. Never let anyone except the DevOps maintainers change the template directly but ONLY through the API. Keep the amount of different templates to an absolute minimum.&#xFFFD;</p><p>Let the API also execute the manifest to the in-cluster resources through kubectl.</p><h3>Design flaw #2: No proper way to manage out-of-cluster resources</h3><p>The scripts we detailed above deal with the in-cluster stuff. But what about all the out-of-cluster resources? Which database is your app supposed to consume in what state? What DNS in what setup? You might have that scripted in IaC but how do you make the connection between in-cluster and out-of-cluster resource? This is a graphical representation of this problem:</p><figure id=\"w-node-9b6ab754f17b-8f972d5f\" class=\"w-richtext-align-fullwidth w-richtext-figure-type-image\"><div><img src=\"https://assets.website-files.com/5c73bbfe3312822f153dd310/5fabfc3be23cc56018c8a5fb_L9NplSEA97XPxjzJCFA5ZQ_MgWNN3XXNF97AhC8oXBXTdBSvzXYL2EumNQYYp4fvyDctU-rLVSxv5MhA60a61AHp_DcIG9P6Pc2ZEMUmN6IGB9koNSUFmrMk-AhoDner1asQCKw0.png\" alt></div></figure><p>But what\u0019s the cure? Fortunately the cure is surprisingly simple:<br></p><h4>Solution for design flaw #2</h4><p>The cure for design flaw #2 is slightly harder to implement than #1. The key here is to have a central record referencing the correct state of the in-cluster resources as well as the out-of-cluster resources. We\u0019ve built this for us and call it a deployment set. It combines the information of the state of everything the application needs to run at deployment time. It will tell the Kubernetes API what to set up. It also will call the correct scripts to reinstate the desired state of the resources (internally we call these scripts resource drivers).<br></p><h4>Don\u0019t these solutions sound awfully abstract?<br></h4><p>Let\u0019s look at that in a graphical representation. It would look like this:&#xFFFD;</p><figure id=\"w-node-b5d2faebc708-8f972d5f\" class=\"w-richtext-align-fullwidth w-richtext-figure-type-image\"><div><img src=\"https://assets.website-files.com/5c73bbfe3312822f153dd310/5fac0466ad123b731b468ec7_Untitled%20drawing%20(1).jpg\" alt></div></figure><p>As you can see there are two \u001coperators\u001d now. DevOps teams set the baseline charts (for the K8s resources to cure disease 1) and configure the drivers (necessary for all out-of-cluster resources). Application developers use a simple GUI, CLI or API to specify deployment sets (I want this image with this database, in this cluster and with this DNS setting etc.). The API combines all information and executes the state.<br></p><p>What you will get using this approach: Clear and easy tracking of all changes at scale for all resource types</p><ul><li>Standardized and comparable charts that take a fraction of the time to update, maintain, and roll-back.&#xFFFD;</li><li>Your developers don\u0019t need to learn kubectl, helm or any other tool but only specify the minimum necessary.&#xFFFD;</li><li>Replication of environments and developer self-serving works like charm&#xFFFD;</li><li>Tracking and analytics on top of the API at scale&#xFFFD;</li></ul><p>And the best thing: You will only need a fraction of the total overhead previously required.<br></p><h2>A problem becomes mainstream</h2><p>Kubernetes is comparably new to the market so it will be a while until this problem goes mainstream. We get many requests from teams who see this coming on the horizon in the near future, much faster than anticipated. It will hurt. A lot. But there is a cure if you follow the above outlined design patterns. Good luck on this journey!</p></div>",
      "contentAsText": "This article is controversial. It aggressively questions helm-charts and current dev workflow designs, and I\u0019m well aware that not everyone will like this. Let me be clear before we dive in: this is an enterprise view. It\u0019s a view that is relevant to team sizes of 20 developers onwards. If you\u0019re a smaller dev shop that builds a few apps, this doesn\u0019t apply to you and you should just keep things as is. But for those of you that are working at scale or that are about to scale: watch out. Your helm-chart zoo will kill you. Maybe not tomorrow but almost definitely next year.Working change by change with kubectlAt first they created kubectl-kangaroo and everyone could do everything the way they wanted. However, the challenge with just using kubectl is that you are working change by change. That\u0019s fast but makes it impossible to track what has actually changed in your cluster. One super clever person went ahead and managed everything in Kubernetes manifests and then versioned them in Git. Dope my friend, dope. A kubectl-Kangaroo that looked so sweet when you first came across it punches your DevOps team in the face.The Helm-Hedgehogs crawl your way�But then, the amount of changes grew and someone realized that most of the manifest isn\u0019t really changing. \u001cCool,\u001d said the community, \u001cLet\u0019s create helm-charts-hedgehogs\u001d. Now everyone thought \u001cYeah, I can now write only bits of the manifest and take the configs from the template\u001d. That made writing charts faster, you could still version them in Git and execute changes against the K8s API.But then, the team and apps grew. Versions were piling up, and applications added. This time there was a clear trace of what happened to Kubernetes Resources. And then the prickliness started.Hedgehog babies look cute, innocent and a lot like their mother. That\u0019s misleading. They will develop their own very unique nasty character as they grow up.Legacy is like the shaggy furIt\u0019s lightweight at first. It takes half an hour to browse through an unstructured chart to understand what got changed where. Then, yet another onboarding session so people understand how to handle charts. A couple of hours per DevOps to roll back or find a specific version. Numerous hours to just align the changes to the infrastructure and the resources out of the cluster (databases etc) with the ones in the cluster. But legacy is like a difficult to untangle, shaggy fur.Legacy is like a dirty wool-pig. Better don\u0019t touch it!A task that takes 30 minutes today might take 30 hours tomorrow. As you are rolling out more apps and more services, as colleagues leave with precious documentation in their heads, hours become weeks. The itching is becoming painful. Really, really painful. At first you will increase operational overhead until your DevOps colleagues get frustrated being the help-desks for overwhelmed app developers. Then you will try some potential solutions. Adding the Kustomize-Kingfisher sounds great. But you will discover that it\u0019s yet another patch for the disease and not a cure.A Kustomize-Kingfisher in action. You can literally see the performance and ease. But wait half a year and the majestic feeling will cease.At this point, folks usually start rearranging the way they store helm charts and versions in Git. They\u0019ll have long argumentations whether it\u0019s smarter to save them separately or with the application. Then, someone attends yet another meetup and you call it GitOps! Yessss, Gitops. That will do the job for another quarter until you will find that it has fundamental flaws as well, I could spend ages talking about those but, this is a great article around what they are that speaks for itself.GitOps-Chameleons look like magic. Everytime you deploy them something changes. When it works: magic. If it doesn\u0019t: magic. Only this time it takes you hours sweating while the business team screams at you because your service is down and you cannot find the correct version to roll-back to.Helm has fundamental design flaws�Long story short, I can tell you where the problem lies, Helm-charts and the entire ecosystem have a fundamental design flaw: they are developed for setups only such as where there is a comparably small number of experts working closely together, writing both applications and charts. Everyone owns their charts, which they know in and out. They develop a very specific style of writing them and will know exactly what to change if necessary. The thing is: that\u0019s not the mainstream use-case, especially not in the enterprise.In enterprise you will have an ever changing group of people working on several applications across teams. You will have many more onboarding situations, you will have the division of labor and you will always have the application developer who just wants to focus on React and Typescript and not about Helm-Charts. In these setups, the approach is a recipe for a disaster. One that comes in the form of a slow and uncomfortable disease that will eventually halt delivery almost completely. I talked to the CTO of a 500 people dev team that is completely devastated because they are hardly delivering features at all.Let\u0019s zoom in on the details�Design flaw #1: Too many was to get to RomeScripts and Helm Charts in particular, allow too many ways to get to Rome. If you want to update a cluster, there are just too many ways an individual contributor can write the syntax (I would argue 1000+). That makes it really hard to standardize and really hard to maintain, update and read.�Solution for design flaw #1:�Treat manifests as a protocol and standardize the way you apply and track changes to the baseline configurations of this protocol through an API. Never let anyone except the DevOps maintainers change the template directly but ONLY through the API. Keep the amount of different templates to an absolute minimum.�Let the API also execute the manifest to the in-cluster resources through kubectl.Design flaw #2: No proper way to manage out-of-cluster resourcesThe scripts we detailed above deal with the in-cluster stuff. But what about all the out-of-cluster resources? Which database is your app supposed to consume in what state? What DNS in what setup? You might have that scripted in IaC but how do you make the connection between in-cluster and out-of-cluster resource? This is a graphical representation of this problem:But what\u0019s the cure? Fortunately the cure is surprisingly simple:Solution for design flaw #2The cure for design flaw #2 is slightly harder to implement than #1. The key here is to have a central record referencing the correct state of the in-cluster resources as well as the out-of-cluster resources. We\u0019ve built this for us and call it a deployment set. It combines the information of the state of everything the application needs to run at deployment time. It will tell the Kubernetes API what to set up. It also will call the correct scripts to reinstate the desired state of the resources (internally we call these scripts resource drivers).Don\u0019t these solutions sound awfully abstract?Let\u0019s look at that in a graphical representation. It would look like this:�As you can see there are two \u001coperators\u001d now. DevOps teams set the baseline charts (for the K8s resources to cure disease 1) and configure the drivers (necessary for all out-of-cluster resources). Application developers use a simple GUI, CLI or API to specify deployment sets (I want this image with this database, in this cluster and with this DNS setting etc.). The API combines all information and executes the state.What you will get using this approach: Clear and easy tracking of all changes at scale for all resource typesStandardized and comparable charts that take a fraction of the time to update, maintain, and roll-back.�Your developers don\u0019t need to learn kubectl, helm or any other tool but only specify the minimum necessary.�Replication of environments and developer self-serving works like charm�Tracking and analytics on top of the API at scale�And the best thing: You will only need a fraction of the total overhead previously required.A problem becomes mainstreamKubernetes is comparably new to the market so it will be a while until this problem goes mainstream. We get many requests from teams who see this coming on the horizon in the near future, much faster than anticipated. It will hurt. A lot. But there is a cure if you follow the above outlined design patterns. Good luck on this journey!",
      "description": "There is a fundamental design flaw with Helm, leading to dramatic long-term consequences and you have to solve this now.",
      "ogDescription": "There is a fundamental design flaw with Helm, leading to dramatic long-term consequences and you have to solve this now."
    },
    {
      "url": "http://blog.kubecost.com/blog/complete-picture-when-monitoring-kubernetes-costs/",
      "title": "How to get the complete picture when monitoring Kubernetes costs",
      "content": "<div><article class=\"page\"> <meta> <meta> <meta> <p class=\"page__inner-wrap\"> <header> <p class=\"page__meta\"> <span class=\"page__meta-readtime\"> <i class=\"far fa-clock\"></i> 1 minute read </span> </p> </header> <section class=\"page__content\"> <p><a href=\"http://kubecost.com\">Kubecost</a> now provides more than one thousand teams with visibility into their Kubernetes cost, efficiency, and resource consumption. Yet many teams, including our own, still spend considerable sums on cloud resources that aren\u0019t visible from within a cluster. For that reason, we\u0019re excited to announce a set of new features to help answer the question: besides Kubernetes resources, where else are we spending our money? This new functionality lets teams see all Kubernetes and out of cluster spend on cloud services in a single unified view.</p> <p><img src=\"/assets/images/unified-cost-screenshot.gif\" alt=\"unified kubernetes cost monitoring\"> &#xFFFD;<em>A unified view of all Kubernetes assets and external AWS cloud services spend.</em></p> <p>\nThis feature is available for free today, is built on open source, and supports AWS and GCP. It joins real-time Kubernetes cost data with cloud billing data and allows teams to: </p></section></p> <ul> <li>Without any tagging, see costs broken down by Kuberenetes and other cloud services.</li> <li>View unified costs by team, department, project, etc by joining costs incurred in a Kubernetes cluster with costs incurred outside, e.g. tagged RDS instances or S3 buckets.</li> <li>Drill down to individual Assets in a Kubernetes cluster, including nodes, disks, load balancers and more. These assets and their costs can be tied to owners with Kubecost Allocation APIs.</li> <li>Drill down to all out-of-cluster cloud resources, e.g. Lambda, DynamoDB, or storage buckets, to determine detailed cost drivers</li> <li>Supports multi-cluster, multi-account, and multi-provider views. Feature available on AWS + GCP today.</li> <li>Tightly integrate on-prem Kubernetes clusters with cloud services for hybrid environments</li>\n</ul> <h2 id=\"how-to-set-up\">How to Set Up</h2> <p>After completing these steps, you will be able to view your unified costs in the <a href=\"https://github.com/kubecost/docs/blob/master/assets.md\">Kubecost Assets UI</a> as well as the backing JSON API.</p> <p>As always with Kubecost, you own and control your data and never have to share any information with remote servers.</p> <h2 id=\"about-us\">About Us</h2> <p>Kubecost provides cost visibility and cost optimization solutions built specifically for teams running Kubernetes. To learn more, we\u0019ll be demoing and answering questions at <a href=\"https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/\">Kubecon North America</a> so drop by our virtual booth in Startup Hall C. You can also email us at team@kubecost.com or join our <a href=\"https://join.slack.com/t/kubecost/shared_invite/enQtNTA2MjQ1NDUyODE5LWFjYzIzNWE4MDkzMmUyZGU4NjkwMzMyMjIyM2E0NGNmYjExZjBiNjk1YzY5ZDI0ZTNhZDg4NjlkMGRkYzFlZTU\">Slack community</a> at any time!</p> <footer class=\"page__meta\"> <p class=\"page__taxonomy\"> <strong><i class=\"fas fa-fw fa-tags\"></i> Tags: </strong> <span> <a href=\"/tags/#cost-monitoring\" class=\"page__taxonomy-item\">cost monitoring</a><span class=\"sep\">, </span> <a href=\"/tags/#kubecost\" class=\"page__taxonomy-item\">Kubecost</a> </span> </p> <p class=\"page__taxonomy\"> <strong><i class=\"fas fa-fw fa-folder-open\"></i> Categories: </strong> <span> <a href=\"/categories/#blog\" class=\"page__taxonomy-item\">blog</a> </span> </p> <p class=\"page__date\"><strong><i class=\"fas fa-fw fa-calendar-alt\"></i> Updated:</strong> <time>November 16, 2020</time></p> </footer> <section class=\"page__share\"> <a href=\"https://twitter.com/intent/tweet?text=How+to+get+the+complete+picture+when+monitoring+Kubernetes+costs%20http%3A%2F%2Fblog.kubecost.com%2Fblog%2Fcomplete-picture-when-monitoring-kubernetes-costs%2F\" class=\"btn btn--twitter\"><i class=\"fab fa-fw fa-twitter\"></i><span> Twitter</span></a> <a href=\"https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Fblog.kubecost.com%2Fblog%2Fcomplete-picture-when-monitoring-kubernetes-costs%2F\" class=\"btn btn--facebook\"><i class=\"fab fa-fw fa-facebook\"></i><span> Facebook</span></a> <a href=\"https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3A%2F%2Fblog.kubecost.com%2Fblog%2Fcomplete-picture-when-monitoring-kubernetes-costs%2F\" class=\"btn btn--linkedin\"><i class=\"fab fa-fw fa-linkedin\"></i><span> LinkedIn</span></a>\n</section> </article><div class=\"page__related\"> <div class=\"grid__wrapper\"> <p class=\"grid__item\"> <article class=\"archive__item\"> <p class=\"page__meta\"> <span class=\"page__meta-readtime\"> <i class=\"far fa-clock\"></i> 4 minute read </span> </p> <p class=\"archive__item-excerpt\">One of the most impactful ways to reduce spend on Kubernetes infrastructure is to make sure your clusters are optimally sized for the workloads they run. Wo...</p> </article>\n</p> <p class=\"grid__item\"> <article class=\"archive__item\"> <p class=\"page__meta\"> <span class=\"page__meta-readtime\"> <i class=\"far fa-clock\"></i> 1 minute read </span> </p> <p class=\"archive__item-excerpt\">Late last year, we had an unpredicted experience while on-boarding a gaming company to the Kubecost platform. A principal engineer on their team was investi...</p> </article>\n</p> <p class=\"grid__item\"> <article class=\"archive__item\"> <p class=\"page__meta\"> <span class=\"page__meta-readtime\"> <i class=\"far fa-clock\"></i> 7 minute read </span> </p> <p class=\"archive__item-excerpt\">Setting Kubernetes requests and limits effectively has a major impact on application performance, stability, and cost. And yet working with many teams over ...</p> </article>\n</p> <p class=\"grid__item\"> <article class=\"archive__item\"> <p class=\"page__meta\"> <span class=\"page__meta-readtime\"> <i class=\"far fa-clock\"></i> 5 minute read </span> </p> <p class=\"archive__item-excerpt\">At Kubecost, we help teams monitor and manage Kubernetes spend. Teams commonly implement their first Kubernetes chargeback or showback solution with our soft...</p> </article>\n</p> </div> </div></div>",
      "contentAsText": "         1 minute read     Kubecost now provides more than one thousand teams with visibility into their Kubernetes cost, efficiency, and resource consumption. Yet many teams, including our own, still spend considerable sums on cloud resources that aren\u0019t visible from within a cluster. For that reason, we\u0019re excited to announce a set of new features to help answer the question: besides Kubernetes resources, where else are we spending our money? This new functionality lets teams see all Kubernetes and out of cluster spend on cloud services in a single unified view.  �A unified view of all Kubernetes assets and external AWS cloud services spend. \nThis feature is available for free today, is built on open source, and supports AWS and GCP. It joins real-time Kubernetes cost data with cloud billing data and allows teams to:   Without any tagging, see costs broken down by Kuberenetes and other cloud services. View unified costs by team, department, project, etc by joining costs incurred in a Kubernetes cluster with costs incurred outside, e.g. tagged RDS instances or S3 buckets. Drill down to individual Assets in a Kubernetes cluster, including nodes, disks, load balancers and more. These assets and their costs can be tied to owners with Kubecost Allocation APIs. Drill down to all out-of-cluster cloud resources, e.g. Lambda, DynamoDB, or storage buckets, to determine detailed cost drivers Supports multi-cluster, multi-account, and multi-provider views. Feature available on AWS + GCP today. Tightly integrate on-prem Kubernetes clusters with cloud services for hybrid environments\n How to Set Up After completing these steps, you will be able to view your unified costs in the Kubecost Assets UI as well as the backing JSON API. As always with Kubecost, you own and control your data and never have to share any information with remote servers. About Us Kubecost provides cost visibility and cost optimization solutions built specifically for teams running Kubernetes. To learn more, we\u0019ll be demoing and answering questions at Kubecon North America so drop by our virtual booth in Startup Hall C. You can also email us at team@kubecost.com or join our Slack community at any time!    Tags:   cost monitoring,  Kubecost     Categories:   blog    Updated: November 16, 2020    Twitter  Facebook  LinkedIn\n        4 minute read   One of the most impactful ways to reduce spend on Kubernetes infrastructure is to make sure your clusters are optimally sized for the workloads they run. Wo... \n      1 minute read   Late last year, we had an unpredicted experience while on-boarding a gaming company to the Kubecost platform. A principal engineer on their team was investi... \n      7 minute read   Setting Kubernetes requests and limits effectively has a major impact on application performance, stability, and cost. And yet working with many teams over ... \n      5 minute read   At Kubecost, we help teams monitor and manage Kubernetes spend. Teams commonly implement their first Kubernetes chargeback or showback solution with our soft... \n  ",
      "publishedDate": "2020-11-16T15:34:30.000Z",
      "description": "Kubecost now provides more than one thousand teams with visibility into their Kubernetes cost, efficiency, and resource consumption.  Yet many teams, including our own, still spend considerable sums on cloud resources that aren’t visible from within a cluster.  For that reason, we’re excited to announce a set of new features to help answer the question: besides Kubernetes resources, where else are we spending our money?  This new functionality lets teams see all Kubernetes and out of cluster spend on cloud services in a single unified view.",
      "ogDescription": "Kubecost now provides more than one thousand teams with visibility into their Kubernetes cost, efficiency, and resource consumption.  Yet many teams, including our own, still spend considerable sums on cloud resources that aren’t visible from within a cluster.  For that reason, we’re excited to announce a set of new features to help answer the question: besides Kubernetes resources, where else are we spending our money?  This new functionality lets teams see all Kubernetes and out of cluster spend on cloud services in a single unified view."
    },
    {
      "url": "https://trstringer.com/run-kubernetes-pods-on-vm-types/",
      "title": "Run Kubernetes Pods on Specific VM Types in AKS",
      "content": "<div class=\"post-content\"><p>When dealing with software that you run in Kubernetes, it is a common requirement to have your applications running on certain types of underlying infrastructure (virtual machines). Perhaps your software has a high memory requirement, or maybe needs GPU optimization. At any rate, you might find yourself saying <em>\u001cI need these pods to be running on this type of virtual machine\u001d</em>.</p><p>With Azure Kubernetes Service (AKS) this is possible through the use of <strong>node pools</strong>. Node pools are the compute representation of the underlying agent nodes for a Kubernetes cluster.</p><h2 id=\"what-is-a-node-pool\">What is a node pool?</h2><p>A node pool is a collection of VMs (usually backed by a Virtual Machine Scale Set (VMSS)) that can participate in a Kubernetes cluster.</p><p><img src=\"/images/node_pools_scheduling.png\" alt=\"Node pools diagram\"></p><p>In the above diagram you can see that we have three node pools. The first is the system node pool, which contains the system-related pods (e.g. <code class=\"language-plaintext highlighter-rouge\">kube-system</code>). This system node pool is created by default when you create your AKS cluster. It will run the system pods, but <em>also</em> run the user pods if it is the only node pool in the cluster. It is the only \u001crequired\u001d node pool.</p><p>You may want to add compute and have different options for your agent nodes, so you can create additional user node pools. All VMs in the same node pool share the same configuration (VM size, labels, disk size, etc.).</p><p>If you have some software that should be running on a specific type of compute, you can create a node pool to host that software. We\u0019ll see how to do that and how to wire it all up.</p><h2 id=\"create-the-aks-cluster-with-user-node-pools\">Create the AKS cluster with user node pools</h2><p>To create the AKS cluster, we\u0019ll start out with the origin (only the single node pool):</p><div class=\"language-plaintext highlighter-rouge\"><p class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n4\n5\n6\n7\n</pre></td><td class=\"rouge-code\"><pre>$ az group create \\\n    --location eastus \\\n    --name rg\n$ az aks create \\\n    --resource-group rg \\\n    --name aks \\\n    --node-count 2\n</pre></td></tr></tbody></table></code></p></div><p>After this cluster is created, we can see that we have our two agent nodes (system) ready:</p><div class=\"language-plaintext highlighter-rouge\"><p class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n4\n</pre></td><td class=\"rouge-code\"><pre>$ kubectl get nodes\nNAME                                STATUS   ROLES   AGE   VERSION\naks-nodepool1-36584864-vmss000000   Ready    agent   69s   v1.17.13\naks-nodepool1-36584864-vmss000001   Ready    agent   73s   v1.17.13\n</pre></td></tr></tbody></table></code></p></div><p>A helpful way to see the node pool breakdown is to run <code class=\"language-plaintext highlighter-rouge\">az aks nodepool list</code>:</p><div class=\"language-plaintext highlighter-rouge\"><p class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n4\n</pre></td><td class=\"rouge-code\"><pre>$ az aks nodepool list --resource-group rg --cluster-name aks -o table\nName       OsType    VmSize           Count    MaxPods    ProvisioningState    Mode\n---------  --------  ---------------  -------  ---------  -------------------  ------\nnodepool1  Linux     Standard_DS2_v2  2        110        Succeeded            System\n</pre></td></tr></tbody></table></code></p></div><p>This shows that we only have our single system node pool with two VMs in it.</p><p>If we wanted to create two additional node pools, one that has VMs that are smaller and one with VMs that are larger, we can do the following:</p><div class=\"language-plaintext highlighter-rouge\"><p class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n</pre></td><td class=\"rouge-code\"><pre>$ az aks nodepool add \\\n    --resource-group rg \\\n    --cluster-name aks \\\n    --name nodepool2 \\\n    --node-count 3 \\\n    --node-vm-size Standard_DS2_v2 \\\n    --labels &quot;vmsize=small&quot;\n\n$ az aks nodepool add \\\n    --resource-group rg \\\n    --cluster-name aks \\\n    --name nodepool3 \\\n    --node-count 3 \\\n    --node-vm-size Standard_DS5_v2 \\\n    --labels &quot;vmsize=large&quot;\n</pre></td></tr></tbody></table></code></p></div><p>The important thing to note here is the <code class=\"language-plaintext highlighter-rouge\">--labels</code> parameter, which will put a label on each of these nodes in the node pool. It is through this label that we\u0019ll be able to specify which nodes our pods should land on.</p><p>Now we can see that we have these additional agent nodes:</p><div class=\"language-plaintext highlighter-rouge\"><p class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n</pre></td><td class=\"rouge-code\"><pre>$ kubectl get nodes --label-columns vmsize\nNAME                                STATUS   ROLES   AGE     VERSION    VMSIZE\naks-nodepool1-36584864-vmss000000   Ready    agent   7m5s    v1.17.13\naks-nodepool1-36584864-vmss000001   Ready    agent   7m9s    v1.17.13\naks-nodepool2-36584864-vmss000000   Ready    agent   3m13s   v1.17.13   small\naks-nodepool2-36584864-vmss000001   Ready    agent   3m9s    v1.17.13   small\naks-nodepool2-36584864-vmss000002   Ready    agent   3m18s   v1.17.13   small\naks-nodepool3-36584864-vmss000000   Ready    agent   2m48s   v1.17.13   large\naks-nodepool3-36584864-vmss000001   Ready    agent   2m35s   v1.17.13   large\naks-nodepool3-36584864-vmss000002   Ready    agent   2m48s   v1.17.13   large\n</pre></td></tr></tbody></table></code></p></div><p>And the additional node pools:</p><div class=\"language-plaintext highlighter-rouge\"><p class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n4\n5\n6\n</pre></td><td class=\"rouge-code\"><pre>$ az aks nodepool list --resource-group rg --cluster-name aks -o table\nName       OsType    VmSize           Count    MaxPods    ProvisioningState    Mode\n---------  --------  ---------------  -------  ---------  -------------------  ------\nnodepool1  Linux     Standard_DS2_v2  2        110        Succeeded            System\nnodepool2  Linux     Standard_DS2_v2  3        110        Succeeded            User\nnodepool3  Linux     Standard_DS5_v2  3        110        Succeeded            User\n</pre></td></tr></tbody></table></code></p></div><h2 id=\"create-pods-on-specific-node-pools\">Create pods on specific node pools</h2><p>When we created our node pools, we had the option to specify <code class=\"language-plaintext highlighter-rouge\">--labels</code>. This translates to node labels which can be used in the <a href=\"https://v1-18.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#podspec-v1-core\">PodSpec</a>. If we were creating a deployment that should have pods scheduled only on large VMs, it could look like this:</p><div class=\"language-yaml highlighter-rouge\"><p class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n</pre></td><td class=\"rouge-code\"><pre><span class=\"na\">kind</span><span class=\"pi\">:</span> <span class=\"s\">Deployment</span>\n<span class=\"na\">apiVersion</span><span class=\"pi\">:</span> <span class=\"s\">apps/v1</span>\n<span class=\"na\">metadata</span><span class=\"pi\">:</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">test-app1</span>\n<span class=\"na\">spec</span><span class=\"pi\">:</span> <span class=\"na\">replicas</span><span class=\"pi\">:</span> <span class=\"m\">16</span> <span class=\"na\">selector</span><span class=\"pi\">:</span> <span class=\"na\">matchLabels</span><span class=\"pi\">:</span> <span class=\"na\">app</span><span class=\"pi\">:</span> <span class=\"s\">app1</span> <span class=\"na\">template</span><span class=\"pi\">:</span> <span class=\"na\">metadata</span><span class=\"pi\">:</span> <span class=\"na\">labels</span><span class=\"pi\">:</span> <span class=\"na\">app</span><span class=\"pi\">:</span> <span class=\"s\">app1</span> <span class=\"na\">spec</span><span class=\"pi\">:</span> <span class=\"na\">containers</span><span class=\"pi\">:</span> <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">app1</span> <span class=\"na\">image</span><span class=\"pi\">:</span> <span class=\"s\">debian:latest</span> <span class=\"na\">command</span><span class=\"pi\">:</span> <span class=\"pi\">[</span><span class=\"s2\">&quot;</span><span class=\"s\">/bin/bash&quot;</span><span class=\"pi\">]</span> <span class=\"na\">args</span><span class=\"pi\">:</span> <span class=\"pi\">[</span><span class=\"s2\">&quot;</span><span class=\"s\">-c&quot;</span><span class=\"pi\">,</span> <span class=\"s2\">&quot;</span><span class=\"s\">while</span><span class=\"nv\"> </span><span class=\"s\">true;</span><span class=\"nv\"> </span><span class=\"s\">do</span><span class=\"nv\"> </span><span class=\"s\">echo</span><span class=\"nv\"> </span><span class=\"s\">hello</span><span class=\"nv\"> </span><span class=\"s\">world;</span><span class=\"nv\"> </span><span class=\"s\">sleep</span><span class=\"nv\"> </span><span class=\"s\">10;</span><span class=\"nv\"> </span><span class=\"s\">done&quot;</span><span class=\"pi\">]</span> <span class=\"na\">nodeSelector</span><span class=\"pi\">:</span> <span class=\"na\">vmsize</span><span class=\"pi\">:</span> <span class=\"s\">large</span>\n</pre></td></tr></tbody></table></code></p></div><p>As you can see, we set our <code class=\"language-plaintext highlighter-rouge\">nodeSelector</code> to match the label selector of the node pool, <code class=\"language-plaintext highlighter-rouge\">vmsize=large</code>.</p><p>And now with these pods running, we can validate which nodes they are running on:</p><div class=\"language-plaintext highlighter-rouge\"><p class=\"highlight\"><code><table class=\"rouge-table\"><tbody><tr><td class=\"rouge-gutter gl\"><pre class=\"lineno\">1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n</pre></td><td class=\"rouge-code\"><pre>$ kubectl get pods -o wide\nNAME                         READY   STATUS    RESTARTS   AGE   IP           NODE                                NOMINATED NODE   READINESS GATES\ntest-app1-555d8bc87b-2gj9j   1/1     Running   0          23s   10.244.5.3   aks-nodepool3-36584864-vmss000000   &lt;none&gt;           &lt;none&gt;\ntest-app1-555d8bc87b-46p4m   1/1     Running   0          23s   10.244.7.4   aks-nodepool3-36584864-vmss000001   &lt;none&gt;           &lt;none&gt;\ntest-app1-555d8bc87b-6hvzw   1/1     Running   0          23s   10.244.7.3   aks-nodepool3-36584864-vmss000001   &lt;none&gt;           &lt;none&gt;\ntest-app1-555d8bc87b-6vj9b   1/1     Running   0          23s   10.244.6.5   aks-nodepool3-36584864-vmss000002   &lt;none&gt;           &lt;none&gt;\ntest-app1-555d8bc87b-9zkpt   1/1     Running   0          23s   10.244.5.2   aks-nodepool3-36584864-vmss000000   &lt;none&gt;           &lt;none&gt;\ntest-app1-555d8bc87b-b6bqm   1/1     Running   0          23s   10.244.5.6   aks-nodepool3-36584864-vmss000000   &lt;none&gt;           &lt;none&gt;\ntest-app1-555d8bc87b-grv77   1/1     Running   0          23s   10.244.7.2   aks-nodepool3-36584864-vmss000001   &lt;none&gt;           &lt;none&gt;\ntest-app1-555d8bc87b-hgjwz   1/1     Running   0          23s   10.244.6.6   aks-nodepool3-36584864-vmss000002   &lt;none&gt;           &lt;none&gt;\ntest-app1-555d8bc87b-m27cp   1/1     Running   0          23s   10.244.7.5   aks-nodepool3-36584864-vmss000001   &lt;none&gt;           &lt;none&gt;\ntest-app1-555d8bc87b-m4bck   1/1     Running   0          23s   10.244.6.2   aks-nodepool3-36584864-vmss000002   &lt;none&gt;           &lt;none&gt;\ntest-app1-555d8bc87b-msztk   1/1     Running   0          23s   10.244.5.5   aks-nodepool3-36584864-vmss000000   &lt;none&gt;           &lt;none&gt;\ntest-app1-555d8bc87b-mvz9k   1/1     Running   0          23s   10.244.5.4   aks-nodepool3-36584864-vmss000000   &lt;none&gt;           &lt;none&gt;\ntest-app1-555d8bc87b-mwm5v   1/1     Running   0          23s   10.244.6.3   aks-nodepool3-36584864-vmss000002   &lt;none&gt;           &lt;none&gt;\ntest-app1-555d8bc87b-q9p7b   1/1     Running   0          23s   10.244.6.4   aks-nodepool3-36584864-vmss000002   &lt;none&gt;           &lt;none&gt;\ntest-app1-555d8bc87b-wsbjk   1/1     Running   0          23s   10.244.6.7   aks-nodepool3-36584864-vmss000002   &lt;none&gt;           &lt;none&gt;\ntest-app1-555d8bc87b-xtdnr   1/1     Running   0          23s   10.244.7.6   aks-nodepool3-36584864-vmss000001   &lt;none&gt;           &lt;none&gt;\n</pre></td></tr></tbody></table></code></p></div><p>The NODE column shows that, as expected, these pods are running only on nodes in <code class=\"language-plaintext highlighter-rouge\">nodepool3</code>, which matches the <code class=\"language-plaintext highlighter-rouge\">Standard_DS5_v2</code> VMs.</p><p>By designing your node pools to fit your software requirements, with AKS you can easily and effectively target certain types of VMs for your workloads to run on. This gives you the control you need to successfully deliver your software on your Kubernetes cluster!</p></div>",
      "contentAsText": "When dealing with software that you run in Kubernetes, it is a common requirement to have your applications running on certain types of underlying infrastructure (virtual machines). Perhaps your software has a high memory requirement, or maybe needs GPU optimization. At any rate, you might find yourself saying \u001cI need these pods to be running on this type of virtual machine\u001d.With Azure Kubernetes Service (AKS) this is possible through the use of node pools. Node pools are the compute representation of the underlying agent nodes for a Kubernetes cluster.What is a node pool?A node pool is a collection of VMs (usually backed by a Virtual Machine Scale Set (VMSS)) that can participate in a Kubernetes cluster.In the above diagram you can see that we have three node pools. The first is the system node pool, which contains the system-related pods (e.g. kube-system). This system node pool is created by default when you create your AKS cluster. It will run the system pods, but also run the user pods if it is the only node pool in the cluster. It is the only \u001crequired\u001d node pool.You may want to add compute and have different options for your agent nodes, so you can create additional user node pools. All VMs in the same node pool share the same configuration (VM size, labels, disk size, etc.).If you have some software that should be running on a specific type of compute, you can create a node pool to host that software. We\u0019ll see how to do that and how to wire it all up.Create the AKS cluster with user node poolsTo create the AKS cluster, we\u0019ll start out with the origin (only the single node pool):1\n2\n3\n4\n5\n6\n7\n$ az group create \\\n    --location eastus \\\n    --name rg\n$ az aks create \\\n    --resource-group rg \\\n    --name aks \\\n    --node-count 2\nAfter this cluster is created, we can see that we have our two agent nodes (system) ready:1\n2\n3\n4\n$ kubectl get nodes\nNAME                                STATUS   ROLES   AGE   VERSION\naks-nodepool1-36584864-vmss000000   Ready    agent   69s   v1.17.13\naks-nodepool1-36584864-vmss000001   Ready    agent   73s   v1.17.13\nA helpful way to see the node pool breakdown is to run az aks nodepool list:1\n2\n3\n4\n$ az aks nodepool list --resource-group rg --cluster-name aks -o table\nName       OsType    VmSize           Count    MaxPods    ProvisioningState    Mode\n---------  --------  ---------------  -------  ---------  -------------------  ------\nnodepool1  Linux     Standard_DS2_v2  2        110        Succeeded            System\nThis shows that we only have our single system node pool with two VMs in it.If we wanted to create two additional node pools, one that has VMs that are smaller and one with VMs that are larger, we can do the following:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n$ az aks nodepool add \\\n    --resource-group rg \\\n    --cluster-name aks \\\n    --name nodepool2 \\\n    --node-count 3 \\\n    --node-vm-size Standard_DS2_v2 \\\n    --labels \"vmsize=small\"\n\n$ az aks nodepool add \\\n    --resource-group rg \\\n    --cluster-name aks \\\n    --name nodepool3 \\\n    --node-count 3 \\\n    --node-vm-size Standard_DS5_v2 \\\n    --labels \"vmsize=large\"\nThe important thing to note here is the --labels parameter, which will put a label on each of these nodes in the node pool. It is through this label that we\u0019ll be able to specify which nodes our pods should land on.Now we can see that we have these additional agent nodes:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n$ kubectl get nodes --label-columns vmsize\nNAME                                STATUS   ROLES   AGE     VERSION    VMSIZE\naks-nodepool1-36584864-vmss000000   Ready    agent   7m5s    v1.17.13\naks-nodepool1-36584864-vmss000001   Ready    agent   7m9s    v1.17.13\naks-nodepool2-36584864-vmss000000   Ready    agent   3m13s   v1.17.13   small\naks-nodepool2-36584864-vmss000001   Ready    agent   3m9s    v1.17.13   small\naks-nodepool2-36584864-vmss000002   Ready    agent   3m18s   v1.17.13   small\naks-nodepool3-36584864-vmss000000   Ready    agent   2m48s   v1.17.13   large\naks-nodepool3-36584864-vmss000001   Ready    agent   2m35s   v1.17.13   large\naks-nodepool3-36584864-vmss000002   Ready    agent   2m48s   v1.17.13   large\nAnd the additional node pools:1\n2\n3\n4\n5\n6\n$ az aks nodepool list --resource-group rg --cluster-name aks -o table\nName       OsType    VmSize           Count    MaxPods    ProvisioningState    Mode\n---------  --------  ---------------  -------  ---------  -------------------  ------\nnodepool1  Linux     Standard_DS2_v2  2        110        Succeeded            System\nnodepool2  Linux     Standard_DS2_v2  3        110        Succeeded            User\nnodepool3  Linux     Standard_DS5_v2  3        110        Succeeded            User\nCreate pods on specific node poolsWhen we created our node pools, we had the option to specify --labels. This translates to node labels which can be used in the PodSpec. If we were creating a deployment that should have pods scheduled only on large VMs, it could look like this:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\nkind: Deployment\napiVersion: apps/v1\nmetadata: name: test-app1\nspec: replicas: 16 selector: matchLabels: app: app1 template: metadata: labels: app: app1 spec: containers: - name: app1 image: debian:latest command: [\"/bin/bash\"] args: [\"-c\", \"while true; do echo hello world; sleep 10; done\"] nodeSelector: vmsize: large\nAs you can see, we set our nodeSelector to match the label selector of the node pool, vmsize=large.And now with these pods running, we can validate which nodes they are running on:1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n$ kubectl get pods -o wide\nNAME                         READY   STATUS    RESTARTS   AGE   IP           NODE                                NOMINATED NODE   READINESS GATES\ntest-app1-555d8bc87b-2gj9j   1/1     Running   0          23s   10.244.5.3   aks-nodepool3-36584864-vmss000000   <none>           <none>\ntest-app1-555d8bc87b-46p4m   1/1     Running   0          23s   10.244.7.4   aks-nodepool3-36584864-vmss000001   <none>           <none>\ntest-app1-555d8bc87b-6hvzw   1/1     Running   0          23s   10.244.7.3   aks-nodepool3-36584864-vmss000001   <none>           <none>\ntest-app1-555d8bc87b-6vj9b   1/1     Running   0          23s   10.244.6.5   aks-nodepool3-36584864-vmss000002   <none>           <none>\ntest-app1-555d8bc87b-9zkpt   1/1     Running   0          23s   10.244.5.2   aks-nodepool3-36584864-vmss000000   <none>           <none>\ntest-app1-555d8bc87b-b6bqm   1/1     Running   0          23s   10.244.5.6   aks-nodepool3-36584864-vmss000000   <none>           <none>\ntest-app1-555d8bc87b-grv77   1/1     Running   0          23s   10.244.7.2   aks-nodepool3-36584864-vmss000001   <none>           <none>\ntest-app1-555d8bc87b-hgjwz   1/1     Running   0          23s   10.244.6.6   aks-nodepool3-36584864-vmss000002   <none>           <none>\ntest-app1-555d8bc87b-m27cp   1/1     Running   0          23s   10.244.7.5   aks-nodepool3-36584864-vmss000001   <none>           <none>\ntest-app1-555d8bc87b-m4bck   1/1     Running   0          23s   10.244.6.2   aks-nodepool3-36584864-vmss000002   <none>           <none>\ntest-app1-555d8bc87b-msztk   1/1     Running   0          23s   10.244.5.5   aks-nodepool3-36584864-vmss000000   <none>           <none>\ntest-app1-555d8bc87b-mvz9k   1/1     Running   0          23s   10.244.5.4   aks-nodepool3-36584864-vmss000000   <none>           <none>\ntest-app1-555d8bc87b-mwm5v   1/1     Running   0          23s   10.244.6.3   aks-nodepool3-36584864-vmss000002   <none>           <none>\ntest-app1-555d8bc87b-q9p7b   1/1     Running   0          23s   10.244.6.4   aks-nodepool3-36584864-vmss000002   <none>           <none>\ntest-app1-555d8bc87b-wsbjk   1/1     Running   0          23s   10.244.6.7   aks-nodepool3-36584864-vmss000002   <none>           <none>\ntest-app1-555d8bc87b-xtdnr   1/1     Running   0          23s   10.244.7.6   aks-nodepool3-36584864-vmss000001   <none>           <none>\nThe NODE column shows that, as expected, these pods are running only on nodes in nodepool3, which matches the Standard_DS5_v2 VMs.By designing your node pools to fit your software requirements, with AKS you can easily and effectively target certain types of VMs for your workloads to run on. This gives you the control you need to successfully deliver your software on your Kubernetes cluster!",
      "publishedDate": "2020-11-15T16:00:00.000Z",
      "description": "When dealing with software that you run in Kubernetes, it is a common requirement to have your applications running on certain types of underlying infrastructure (virtual machines). Perhaps your software has a high memory requirement, or maybe needs GPU optimization. At any rate, you might find yourself saying “I need these pods to be running on this type of virtual machine”.",
      "ogDescription": "When dealing with software that you run in Kubernetes, it is a common requirement to have your applications running on certain types of underlying infrastructure (virtual machines). Perhaps your software has a high memory requirement, or maybe needs GPU optimization. At any rate, you might find yourself saying “I need these pods to be running on this type of virtual machine”."
    },
    {
      "url": "https://inlets.dev/blog/2020/11/16/inlets-the-ipv6-proxy.html",
      "title": "Inlets as an IPv6 proxy",
      "content": "<div><div class=\"blog-max-w prose prose-lg text-gray-500 mx-auto\"> <p>Many users have no IPv6 stack available to them where they run their services, whether at home or in the datacenter. Learn how you can use inlets to serve traffic to IPv6 users</p> <h2 id=\"introduction\">Introduction</h2> <p><img src=\"/images/2020-11-ipv6-proxy/top.jpg\" alt=\"Datacenter Networking\"></p> <p>This weekend an <a href=\"https://inlets.dev/\">inlets PRO customer</a> reached out and asked for a new feature in inlets:</p> <blockquote> <p>Please can you add IPv6 support?</p>\n</blockquote> <p>When I asked why, they told me that they were replacing their bespoke solution using Wireguard with inlets PRO. Everything worked as expected, but there was one missing piece. They were running an IPv6 proxy on the public VM so that users could access tunnelled services over IPv6.</p> <p>After a little digging it transpired that because inlets and inlets PRO are written in <a href=\"https://golang.org\">Go</a>, that IPv6 was already part of the package and already working. No patches were required, so I wanted to put this post together and show you how you can start exposing traffic to users over IPv6.</p> <p><img src=\"/images/2020-11-ipv6-proxy/conceptual.png\" alt=\"inlets as an IPv6 Proxy\"></p> <blockquote> <p>Conceptual diagram: an inlets exit-server running on a public EC2 instance, using its IPv6 networking stack.</p>\n</blockquote> <h3 id=\"building-your-own-ipv6-proxy\">Building your own IPv6 proxy</h3> <p>You can use inlets or inlets PRO for this setup. The key is that your exit-server must be on an IaaS or VPS which has IPv6 enabled.</p> <blockquote> <p><a href=\"https://metal.equinix.com\">Equinix Metal (Packet)</a> is the easiest option available and has IPv6 fully configured in the host OS out of the box.</p>\n</blockquote> <p>All versions of inlets come as static Go binaries, but it&#x2019;s often easier to use some automation to install them on a cloud host, than to provision the host manually. <a href=\"https://github.com/inlets/inletsctl\">inletsctl</a> sets up a host, generates an auth token and installs a systemd unit file.</p> <blockquote> <p>Did you know? inlets can also be used with Kubernetes and both the client and server components can be used as Pods. The easiest way to get started is with the <a href=\"https://github.com/inlets/inlets-operator\">inlets-operator</a>.</p>\n</blockquote> <p>Install inletsctl and download inlets to your client:</p> <div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>curl <span class=\"nt\">-SLs</span> https://inletsctl.inlets.dev | sh\n<span class=\"nb\">mv </span>inletsctl /usr/local/bin/ <span class=\"nb\">sudo </span>inletsctl download\n\ninlets version\n</code></pre></div></div> <blockquote> <p>The user told me that DigitalOcean also has an IPv6 capability, but you need to click to enable it in the dashboard, then within your Operating System. See also: <a href=\"https://www.digitalocean.com/docs/networking/ipv6/how-to/enable/\">How to Enable IPv6 on Droplets</a></p>\n</blockquote> <p>Go to your Equinix Metal dashboard and create an API Key, call it &#x201C;inletsctl&#x201D; and give it read/write permissions. Save it to a file: <code class=\"language-plaintext highlighter-rouge\">$HOME/api-token-packet.txt</code></p> <p>Let&#x2019;s run a server process for our IPv6 clients to connect to. Now if you&#x2019;re using inlets OSS, then you need to use a HTTP server, but if you&#x2019;re using inlets PRO you can use a TCP service like RDP or SSH.</p> <p>Download <a href=\"https://github.com/openfaas/of-watchdog\">the OpenFaaS watchdog</a> which is described as a &#x201C;Reverse proxy for HTTP microservices and STDIO&#x201D;</p> <p>Simply put, it&#x2019;s an HTTP server which acts a little like cgi-bin. It will run a local command-line process to execute whenever a request comes in. You can also use it with streaming workloads like ffmpeg or HTTP servers like Express.js.</p> <div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>curl <span class=\"nt\">-o</span> of-watchdog <span class=\"nt\">-SL</span> https://github.com/openfaas/of-watchdog/releases/download/0.8.1/of-watchdog-darwin\n<span class=\"nb\">chmod</span> +x of-watchdog\n</code></pre></div></div> <p>You can also download for Windows and Linux here: <a href=\"https://github.com/openfaas/of-watchdog/releases/tag/0.8.1\">watchdog binaries</a></p> <p>Run <code class=\"language-plaintext highlighter-rouge\">of-watchdog</code> so that every HTTP request that comes in triggers the <code class=\"language-plaintext highlighter-rouge\">cal</code> command from bash.</p> <div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">mode</span><span class=\"o\">=</span>streaming <span class=\"nv\">fprocess</span><span class=\"o\">=</span>cal <span class=\"nv\">port</span><span class=\"o\">=</span>8080 ./of-watchdog\n</code></pre></div></div> <p>Test out the server on your local computer:</p> <div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>curl <span class=\"nt\">-s</span> http://localhost:8080\n\n   November 2020      \nSu Mo Tu We Th Fr Sa  \n 1  2  3  4  5  6  7  \n 8  9 10 11 12 13 14  \n15 16 17 18 19 20 21  \n22 23 24 25 26 27 28  \n29 30\n</code></pre></div></div> <p>Now create an exit-server with inlets and point it at your SimpleHTTPServer:</p> <div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\"># Set from your dashboard (under &quot;Project settings&quot;)</span>\n<span class=\"nb\">export </span><span class=\"nv\">PROJECT_ID</span><span class=\"o\">=</span><span class=\"s2\">&quot;66ae0069-7d03-4db5-9af1-6b14036e380a&quot;</span> inletsctl create <span class=\"nt\">--provider</span> packet <span class=\"se\">\\</span> <span class=\"nt\">--access-token-file</span> <span class=\"nv\">$HOME</span>/api-token-packet.txt <span class=\"se\">\\</span> <span class=\"nt\">--project-id</span> <span class=\"nv\">$PROJECT_ID</span>\n</code></pre></div></div> <p>This command will create a <code class=\"language-plaintext highlighter-rouge\">t1.small.x86</code> instance in the <em>ams1</em> region. You can override the region with <code class=\"language-plaintext highlighter-rouge\">--region</code>.</p> <p>You will see the IPv4 address printed out on the console along with connection info:</p> <div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Using provider: packet\nRequesting host: confident-hamilton3 <span class=\"k\">in </span>ams1, from packet\nHost: 90078982-4490-4430-9915-54e7630a6cd0, status: <span class=\"o\">[</span>1/500] Host: 90078982-4490-4430-9915-54e7630a6cd0, status: provisioning\n<span class=\"o\">[</span>2/500] Host: 90078982-4490-4430-9915-54e7630a6cd0, status: provisioning\n<span class=\"nt\">---</span>\n<span class=\"o\">[</span>40/500] Host: 90078982-4490-4430-9915-54e7630a6cd0, status: provisioning\n<span class=\"o\">[</span>41/500] Host: 90078982-4490-4430-9915-54e7630a6cd0, status: active\ninlets OSS <span class=\"o\">(</span>2.7.4<span class=\"o\">)</span> exit-server summary: IP: 147.75.33.3 Auth-token: da39a3ee5e6b4b0d3255bfef95601890afd80709KLOyx559efgzdITeAhgCHtaH74zysHZ Command: <span class=\"nb\">export </span><span class=\"nv\">UPSTREAM</span><span class=\"o\">=</span>http://127.0.0.1:8000 inlets client <span class=\"nt\">--remote</span> <span class=\"s2\">&quot;ws://147.75.33.3:8080&quot;</span> <span class=\"se\">\\</span> <span class=\"nt\">--token</span> <span class=\"s2\">&quot;da39a3ee5e6b4b0d3255bfef95601890afd80709KLOyx559efgzdITeAhgCHtaH74zysHZ&quot;</span> <span class=\"se\">\\</span> <span class=\"nt\">--upstream</span> <span class=\"nv\">$UPSTREAM</span> To Delete: inletsctl delete <span class=\"nt\">--provider</span> packet <span class=\"nt\">--id</span> <span class=\"s2\">&quot;90078982-4490-4430-9915-54e7630a6cd0&quot;</span>\n</code></pre></div></div> <p>To get the IPv6 address, log into your host over SSH, or open the Equinix Metal dashboard.</p> <p><img src=\"/images/2020-11-ipv6-proxy/packet-dashboard.png\" alt=\"curl test\"></p> <p>In my instance the address was: <code class=\"language-plaintext highlighter-rouge\">2604:1380:2000:b700::5</code></p> <h3 id=\"trying-it-out\">Trying it out</h3> <p>Connect your inlets client:</p> <div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">export </span><span class=\"nv\">UPSTREAM</span><span class=\"o\">=</span>http://127.0.0.1:8080\ninlets client <span class=\"nt\">--remote</span> <span class=\"s2\">&quot;ws://147.75.33.3:8080&quot;</span> <span class=\"se\">\\</span> <span class=\"nt\">--token</span> <span class=\"s2\">&quot;CnLgAxPkOw594ZbZ0nEfsbclpQUov9ZBFKLOyx559efgzdITeAhgCHtaH74zysHZ&quot;</span> <span class=\"se\">\\</span> <span class=\"nt\">--upstream</span> <span class=\"nv\">$UPSTREAM</span>\n</code></pre></div></div> <p>You can use <code class=\"language-plaintext highlighter-rouge\">curl</code> to test the IPv6 endpoint, however if your local network does not support IPv6, you&#x2019;ll need to create another host and run the command there. Most clouds and IaaS platforms support IPv6 out of the box.</p> <p>Note the brackets around the address:</p> <div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>curl <span class=\"nt\">-v</span> http://[2604:1380:2000:b700::5]:80/\n</code></pre></div></div> <p><img src=\"/images/2020-11-ipv6-proxy/curl-test.png\" alt=\"curl test\"></p> <p>See how to set up TLS for the IPv6 proxy using Caddy and this guide: <a href=\"https://blog.alexellis.io/expose-grafana-dashboards/\">Expose your private Grafana dashboards with TLS</a>.</p> <p>Try stopping the of-watchdog process and changing the &#x201C;fprocess&#x201D; command to something else like <code class=\"language-plaintext highlighter-rouge\">wc</code>:</p> <div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>curl <span class=\"nt\">-s</span> http://[2604:1380:2000:b700::5]:80/ <span class=\"se\">\\</span> <span class=\"nt\">--data-binary</span> <span class=\"s2\">&quot;How many characters is this</span><span class=\"se\">\\?</span><span class=\"s2\">&quot;</span>\n       0       5      29\n</code></pre></div></div> <p>Try running <code class=\"language-plaintext highlighter-rouge\">node</code> and a simple program to gather any input and print it back to the user:</p> <p>Save <code class=\"language-plaintext highlighter-rouge\">app.js</code>:</p> <div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"dl\">&quot;</span><span class=\"s2\">use strict</span><span class=\"dl\">&quot;</span> <span class=\"kd\">let</span> <span class=\"nx\">getStdin</span> <span class=\"o\">=</span> <span class=\"nx\">require</span><span class=\"p\">(</span><span class=\"dl\">&apos;</span><span class=\"s1\">get-stdin</span><span class=\"dl\">&apos;</span><span class=\"p\">);</span> <span class=\"kd\">let</span> <span class=\"nx\">handle</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"nx\">req</span><span class=\"p\">)</span> <span class=\"o\">=&gt;</span> <span class=\"p\">{</span> <span class=\"nx\">console</span><span class=\"p\">.</span><span class=\"nx\">log</span><span class=\"p\">(</span><span class=\"s2\">`Input was: &quot;</span><span class=\"p\">${</span><span class=\"nx\">req</span><span class=\"p\">}</span><span class=\"s2\">&quot;`</span><span class=\"p\">);</span>\n<span class=\"p\">};</span> <span class=\"nx\">getStdin</span><span class=\"p\">().</span><span class=\"nx\">then</span><span class=\"p\">(</span><span class=\"nx\">val</span> <span class=\"o\">=&gt;</span> <span class=\"p\">{</span> <span class=\"nx\">handle</span><span class=\"p\">(</span><span class=\"nx\">val</span><span class=\"p\">);</span>\n<span class=\"p\">}).</span><span class=\"k\">catch</span><span class=\"p\">(</span><span class=\"nx\">e</span> <span class=\"o\">=&gt;</span> <span class=\"p\">{</span> <span class=\"nx\">console</span><span class=\"p\">.</span><span class=\"nx\">error</span><span class=\"p\">(</span><span class=\"nx\">e</span><span class=\"p\">.</span><span class=\"nx\">stack</span><span class=\"p\">);</span>\n<span class=\"p\">});</span>\n</code></pre></div></div> <p>Install dependenices</p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>npm init <span class=\"nt\">-y</span>\nnpm i get-stdin <span class=\"nt\">--save</span> <span class=\"nv\">mode</span><span class=\"o\">=</span>streaming <span class=\"nv\">fprocess</span><span class=\"o\">=</span><span class=\"nb\">wc </span><span class=\"nv\">port</span><span class=\"o\">=</span>8080 ./of-watchdog\n</code></pre></div></div> <p>Invoke it:</p> <div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>curl <span class=\"nt\">-s</span> http://[2604:1380:2000:b700::5]:80/ <span class=\"nt\">--data-binary</span> <span class=\"s2\">&quot;There are many proxies, but this one is mine&quot;</span> Input was: <span class=\"s2\">&quot;There are many proxies, but this one is mine&quot;</span>\n</code></pre></div></div> <p>The of-watchdog is just one component of OpenFaaS, and is designed to be used as part of a whole solution including metrics, auto-scaling, dashboards, and much more. If you&#x2019;re interested in Functions as a Service, <a href=\"https://www.openfaas.com/\">checkout the OpenFaaS project homepage</a> to see what else it can offer.</p> <h2 id=\"wrapping-up\">Wrapping up</h2> <p>In this tutorial we exposed a local API endpoint on a network with an IPv4 networking stack to public clients to access over IPv6. The IPv6 rollout is coming, but is slow and in the meantime solutions like inlets OSS and inlets PRO provide a way for us to serve traffic clients and to patch the gaps.</p> <blockquote> <p>inlets can be used to create a self-hosted tunnels, but what you&#x2019;re tunneling can also be kept private by binding to a local ethernet adapter. You can then access your services from the remote LAN, instead of over the Internet. A recent example of that is the post by Johan Siebens where he runs MySQL on-premises on a private network and WordPress on a cloud Kubernetes cluster. <a href=\"https://inlets.dev/blog/2020/11/06/hybrid-cloud-with-inlets.html\">Read more here</a></p>\n</blockquote> <ul> <li><a href=\"https://twitter.com/inletsdev/\">Follow inlets on Twitter</a></li> <li><a href=\"https://inlets.dev/\">Learn more about inlets PRO features</a></li>\n</ul> </div></div>",
      "contentAsText": " Many users have no IPv6 stack available to them where they run their services, whether at home or in the datacenter. Learn how you can use inlets to serve traffic to IPv6 users Introduction  This weekend an inlets PRO customer reached out and asked for a new feature in inlets:  Please can you add IPv6 support?\n When I asked why, they told me that they were replacing their bespoke solution using Wireguard with inlets PRO. Everything worked as expected, but there was one missing piece. They were running an IPv6 proxy on the public VM so that users could access tunnelled services over IPv6. After a little digging it transpired that because inlets and inlets PRO are written in Go, that IPv6 was already part of the package and already working. No patches were required, so I wanted to put this post together and show you how you can start exposing traffic to users over IPv6.   Conceptual diagram: an inlets exit-server running on a public EC2 instance, using its IPv6 networking stack.\n Building your own IPv6 proxy You can use inlets or inlets PRO for this setup. The key is that your exit-server must be on an IaaS or VPS which has IPv6 enabled.  Equinix Metal (Packet) is the easiest option available and has IPv6 fully configured in the host OS out of the box.\n All versions of inlets come as static Go binaries, but it’s often easier to use some automation to install them on a cloud host, than to provision the host manually. inletsctl sets up a host, generates an auth token and installs a systemd unit file.  Did you know? inlets can also be used with Kubernetes and both the client and server components can be used as Pods. The easiest way to get started is with the inlets-operator.\n Install inletsctl and download inlets to your client: curl -SLs https://inletsctl.inlets.dev | sh\nmv inletsctl /usr/local/bin/ sudo inletsctl download\n\ninlets version\n  The user told me that DigitalOcean also has an IPv6 capability, but you need to click to enable it in the dashboard, then within your Operating System. See also: How to Enable IPv6 on Droplets\n Go to your Equinix Metal dashboard and create an API Key, call it “inletsctl” and give it read/write permissions. Save it to a file: $HOME/api-token-packet.txt Let’s run a server process for our IPv6 clients to connect to. Now if you’re using inlets OSS, then you need to use a HTTP server, but if you’re using inlets PRO you can use a TCP service like RDP or SSH. Download the OpenFaaS watchdog which is described as a “Reverse proxy for HTTP microservices and STDIO” Simply put, it’s an HTTP server which acts a little like cgi-bin. It will run a local command-line process to execute whenever a request comes in. You can also use it with streaming workloads like ffmpeg or HTTP servers like Express.js. curl -o of-watchdog -SL https://github.com/openfaas/of-watchdog/releases/download/0.8.1/of-watchdog-darwin\nchmod +x of-watchdog\n You can also download for Windows and Linux here: watchdog binaries Run of-watchdog so that every HTTP request that comes in triggers the cal command from bash. mode=streaming fprocess=cal port=8080 ./of-watchdog\n Test out the server on your local computer: curl -s http://localhost:8080\n\n   November 2020      \nSu Mo Tu We Th Fr Sa  \n 1  2  3  4  5  6  7  \n 8  9 10 11 12 13 14  \n15 16 17 18 19 20 21  \n22 23 24 25 26 27 28  \n29 30\n Now create an exit-server with inlets and point it at your SimpleHTTPServer: # Set from your dashboard (under \"Project settings\")\nexport PROJECT_ID=\"66ae0069-7d03-4db5-9af1-6b14036e380a\" inletsctl create --provider packet \\ --access-token-file $HOME/api-token-packet.txt \\ --project-id $PROJECT_ID\n This command will create a t1.small.x86 instance in the ams1 region. You can override the region with --region. You will see the IPv4 address printed out on the console along with connection info: Using provider: packet\nRequesting host: confident-hamilton3 in ams1, from packet\nHost: 90078982-4490-4430-9915-54e7630a6cd0, status: [1/500] Host: 90078982-4490-4430-9915-54e7630a6cd0, status: provisioning\n[2/500] Host: 90078982-4490-4430-9915-54e7630a6cd0, status: provisioning\n---\n[40/500] Host: 90078982-4490-4430-9915-54e7630a6cd0, status: provisioning\n[41/500] Host: 90078982-4490-4430-9915-54e7630a6cd0, status: active\ninlets OSS (2.7.4) exit-server summary: IP: 147.75.33.3 Auth-token: da39a3ee5e6b4b0d3255bfef95601890afd80709KLOyx559efgzdITeAhgCHtaH74zysHZ Command: export UPSTREAM=http://127.0.0.1:8000 inlets client --remote \"ws://147.75.33.3:8080\" \\ --token \"da39a3ee5e6b4b0d3255bfef95601890afd80709KLOyx559efgzdITeAhgCHtaH74zysHZ\" \\ --upstream $UPSTREAM To Delete: inletsctl delete --provider packet --id \"90078982-4490-4430-9915-54e7630a6cd0\"\n To get the IPv6 address, log into your host over SSH, or open the Equinix Metal dashboard.  In my instance the address was: 2604:1380:2000:b700::5 Trying it out Connect your inlets client: export UPSTREAM=http://127.0.0.1:8080\ninlets client --remote \"ws://147.75.33.3:8080\" \\ --token \"CnLgAxPkOw594ZbZ0nEfsbclpQUov9ZBFKLOyx559efgzdITeAhgCHtaH74zysHZ\" \\ --upstream $UPSTREAM\n You can use curl to test the IPv6 endpoint, however if your local network does not support IPv6, you’ll need to create another host and run the command there. Most clouds and IaaS platforms support IPv6 out of the box. Note the brackets around the address: curl -v http://[2604:1380:2000:b700::5]:80/\n  See how to set up TLS for the IPv6 proxy using Caddy and this guide: Expose your private Grafana dashboards with TLS. Try stopping the of-watchdog process and changing the “fprocess” command to something else like wc: curl -s http://[2604:1380:2000:b700::5]:80/ \\ --data-binary \"How many characters is this\\?\"\n       0       5      29\n Try running node and a simple program to gather any input and print it back to the user: Save app.js: \"use strict\" let getStdin = require('get-stdin'); let handle = (req) => { console.log(`Input was: \"${req}\"`);\n}; getStdin().then(val => { handle(val);\n}).catch(e => { console.error(e.stack);\n});\n Install dependenices\nnpm init -y\nnpm i get-stdin --save mode=streaming fprocess=wc port=8080 ./of-watchdog\n Invoke it: curl -s http://[2604:1380:2000:b700::5]:80/ --data-binary \"There are many proxies, but this one is mine\" Input was: \"There are many proxies, but this one is mine\"\n The of-watchdog is just one component of OpenFaaS, and is designed to be used as part of a whole solution including metrics, auto-scaling, dashboards, and much more. If you’re interested in Functions as a Service, checkout the OpenFaaS project homepage to see what else it can offer. Wrapping up In this tutorial we exposed a local API endpoint on a network with an IPv4 networking stack to public clients to access over IPv6. The IPv6 rollout is coming, but is slow and in the meantime solutions like inlets OSS and inlets PRO provide a way for us to serve traffic clients and to patch the gaps.  inlets can be used to create a self-hosted tunnels, but what you’re tunneling can also be kept private by binding to a local ethernet adapter. You can then access your services from the remote LAN, instead of over the Internet. A recent example of that is the post by Johan Siebens where he runs MySQL on-premises on a private network and WordPress on a cloud Kubernetes cluster. Read more here\n  Follow inlets on Twitter Learn more about inlets PRO features\n ",
      "publishedDate": "2020-11-15T16:00:00.000Z",
      "description": "Many users have no IPv6 stack available to them where they run their services, whether at home or in the datacenter. Learn how you can use inlets to serve traffic to IPv6 users\n",
      "ogDescription": "Many users have no IPv6 stack available to them where they run their services, whether at home or in the datacenter. Learn how you can use inlets to serve traffic to IPv6 users\n"
    },
    {
      "url": "https://github.com/bbl/secretize",
      "title": "bbl/secretize",
      "content": "<div><div><article class=\"markdown-body entry-content container-lg\"><p>\n  <a href=\"https://github.com/bbl/secretize/blob/main/.assets/logo.png\"><img width=\"409\" src=\"https://github.com/bbl/secretize/raw/main/.assets/logo.png\"></a>\n  <br>\n<i> Secretize is a kustomize plugin that helps generating kubernetes secrets from various sources.  <br>\nIt&apos;s like a swiss army knife, but for kubernetes secrets. </i> \n  <br>\n  <br>\n  <a href=\"https://camo.githubusercontent.com/f5b25eefe570533223917d016faa16c8f8706cc2521f33400ecfda3dd6ccc06b/68747470733a2f2f676f7265706f7274636172642e636f6d2f62616467652f6769746875622e636f6d2f62626c2f736563726574697a65\"><img src=\"https://camo.githubusercontent.com/f5b25eefe570533223917d016faa16c8f8706cc2521f33400ecfda3dd6ccc06b/68747470733a2f2f676f7265706f7274636172642e636f6d2f62616467652f6769746875622e636f6d2f62626c2f736563726574697a65\"></a>\n   <a href=\"https://github.com/bbl/secretize/workflows/CI/badge.svg\"><img src=\"https://github.com/bbl/secretize/workflows/CI/badge.svg\"></a>\n   <a href=\"https://codecov.io/gh/bbl/secretize\">\n     <img src=\"https://camo.githubusercontent.com/32e38a1bb49b3887e5e966b8239aa3921535aaa108d741136d384cd63a1c8ad3/68747470733a2f2f636f6465636f762e696f2f67682f62626c2f736563726574697a652f6272616e63682f6d61696e2f67726170682f62616467652e737667\">\n   </a>\n</p>\n\n<h2><a id=\"user-content-sources\" class=\"anchor\" href=\"https://github.com/bbl/secretize#sources\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Sources</h2>\n<p>Secretize is able to generate secrets using the following providers:</p>\n\n<p>It is possible to use multiple providers at once.</p>\n<h2><a id=\"user-content-installation\" class=\"anchor\" href=\"https://github.com/bbl/secretize#installation\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Installation</h2>\n<p>Install secretize to your <code>$XDG_CONFIG_HOME/kustomize/plugin</code> folder:</p>\n<ol>\n<li>Export the <code>XDG_CONFIG_HOME</code> variable if it&apos;s not already set:</li>\n</ol>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-k\">export</span> XDG_CONFIG_HOME=<span class=\"pl-k\">~</span>/.config</pre></div>\n<ol>\n<li>Download the release binary into the kustomize plugin folder:</li>\n</ol>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-k\">export</span> SECRETIZE_DIR=<span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span><span class=\"pl-smi\">$XDG_CONFIG_HOME</span>/kustomize/plugin/secretize/v1/secretgenerator<span class=\"pl-pds\">&quot;</span></span>\nmkdir -p <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span><span class=\"pl-smi\">$SECRETIZE_DIR</span><span class=\"pl-pds\">&quot;</span></span>\ncurl -L https://github.com/bbl/secretize/releases/download/v0.0.1/secretize-v0.0.1-linux-amd64.tar.gz  <span class=\"pl-k\">|</span> tar -xz -C <span class=\"pl-smi\">$SECRETIZE_DIR</span></pre></div>\n<h2><a id=\"user-content-usage\" class=\"anchor\" href=\"https://github.com/bbl/secretize#usage\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Usage</h2>\n<p>All providers can generate two types of secrets: <code>literals</code> and <code>kv</code> (Key-Value secrets).<br>\nLiteral secrets simply generate a single string output, while KV secrets will output with a dictionary of the key-value pairs.</p>\n<p>The full configuration API could be found in the <a href=\"https://github.com/bbl/secretize/blob/main/examples/secret-generator.yaml\">examples/secret-generator.yaml</a> file.</p>\n<h3><a id=\"user-content-aws-secrets-manager\" class=\"anchor\" href=\"https://github.com/bbl/secretize#aws-secrets-manager\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>AWS Secrets Manager</h3>\n<p>Fetching literal secrets is as simple, as using a default kustomize <code>secretGenerator</code> plugin:</p>\n<div class=\"highlight highlight-source-yaml\"><pre><span class=\"pl-ent\">apiVersion</span>: <span class=\"pl-s\">secretize/v1</span>\n<span class=\"pl-ent\">kind</span>: <span class=\"pl-s\">SecretGenerator</span>\n<span class=\"pl-ent\">metadata</span>:\n  <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">aws-sm-secrets</span>\n<span class=\"pl-ent\">sources</span>:\n    - <span class=\"pl-ent\">provider</span>: <span class=\"pl-s\">aws-sm</span>\n      <span class=\"pl-ent\">literals</span>: \n        - <span class=\"pl-s\">mySecret</span>\n        - <span class=\"pl-s\">newName=mySecret </span></pre></div>\n<p>The above config would query AWS Secrets Manager provider to get the <code>mySecret</code> string value. As a result, the following manifest will be generated:</p>\n<div class=\"highlight highlight-source-yaml\"><pre><span class=\"pl-ent\">apiVersion</span>: <span class=\"pl-c1\">v1</span>\n<span class=\"pl-ent\">kind</span>: <span class=\"pl-s\">Secret</span>\n<span class=\"pl-ent\">metadata</span>:\n  <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">aws-sm-secrets</span>\n<span class=\"pl-ent\">data</span>:\n  <span class=\"pl-ent\">mySecret</span>: <span class=\"pl-s\">c2VjcmV0X3ZhbHVlXzE= </span><span class=\"pl-c\"><span class=\"pl-c\">#</span> a sample base64 encoded data </span>\n  <span class=\"pl-ent\">newName</span>: <span class=\"pl-s\">c2VjcmV0X3ZhbHVlXzE=</span></pre></div>\n<p>Now let&apos;s assume that value of <code>mySecret</code> is a json string:</p>\n<div class=\"highlight highlight-source-json\"><pre>{\n  <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>secret_key_1<span class=\"pl-pds\">&quot;</span></span>:<span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>secret_value_1<span class=\"pl-pds\">&quot;</span></span>, \n  <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>secret_key_2<span class=\"pl-pds\">&quot;</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>secret_value_2<span class=\"pl-pds\">&quot;</span></span>\n}</pre></div>\n<p>The generator config can be slightly modified, to generate a <code>kv</code> secret:</p>\n<div class=\"highlight highlight-source-yaml\"><pre><span class=\"pl-ent\">apiVersion</span>: <span class=\"pl-s\">secretize/v1</span>\n<span class=\"pl-ent\">kind</span>: <span class=\"pl-s\">SecretGenerator</span>\n<span class=\"pl-ent\">metadata</span>:\n  <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">aws-sm-secrets</span>\n<span class=\"pl-ent\">sources</span>:\n    - <span class=\"pl-ent\">provider</span>: <span class=\"pl-s\">aws-sm</span>\n      <span class=\"pl-ent\">kv</span>: \n        - <span class=\"pl-s\">mySecret</span></pre></div>\n<p>As a result, the following secret is generated:</p>\n<div class=\"highlight highlight-source-yaml\"><pre><span class=\"pl-ent\">apiVersion</span>: <span class=\"pl-c1\">v1</span>\n<span class=\"pl-ent\">kind</span>: <span class=\"pl-s\">Secret</span>\n<span class=\"pl-ent\">metadata</span>:\n  <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">aws-sm-secrets</span>\n<span class=\"pl-ent\">data</span>:\n  <span class=\"pl-ent\">secret_key_1</span>: <span class=\"pl-s\">c2VjcmV0X3ZhbHVlXzE=</span>\n  <span class=\"pl-ent\">secret_key_2</span>: <span class=\"pl-s\">c2VjcmV0X3ZhbHVlXzI=</span></pre></div>\n<h3><a id=\"user-content-azure-vault\" class=\"anchor\" href=\"https://github.com/bbl/secretize#azure-vault\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Azure Vault</h3>\n<p>Azure Vault configuration is pretty similar to the above examples. However, there&apos;s additional <code>params</code> field, which is used to specify the Vault Name:</p>\n<div class=\"highlight highlight-source-yaml\"><pre><span class=\"pl-ent\">apiVersion</span>: <span class=\"pl-s\">secretize/v1</span>\n<span class=\"pl-ent\">kind</span>: <span class=\"pl-s\">SecretGenerator</span>\n<span class=\"pl-ent\">metadata</span>:\n  <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">aws-sm-secrets</span>\n<span class=\"pl-ent\">sources</span>:\n  - <span class=\"pl-ent\">provider</span>: <span class=\"pl-s\">azure-vault</span>\n    <span class=\"pl-ent\">params</span>:\n      <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">vault-name</span>\n    <span class=\"pl-ent\">kv</span>:\n      - <span class=\"pl-s\">kv-secrets </span><span class=\"pl-c\"><span class=\"pl-c\">#</span> will treat this as JSON, the same way as in the AWS example</span>\n    <span class=\"pl-ent\">literals</span>:\n      - <span class=\"pl-s\">literal-secret-1</span>\n      - <span class=\"pl-s\">new_name=literal-secret-1</span></pre></div>\n<h3><a id=\"user-content-hashicorp-vault\" class=\"anchor\" href=\"https://github.com/bbl/secretize#hashicorp-vault\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Hashicorp Vault</h3>\n<p>Some providers only support key-value output, e.g. Hashicorp Vault and K8S Secret.\nFor instance, the <code>mySecret</code> in Hashicorp Vault might look like the following:</p>\n<div class=\"highlight highlight-source-shell\"><pre>vault kv get secret/mySecret\n====== Data ======\nKey           Value\n---           -----\nsecret_key_1  secret_value_1\nsecret_key_2  secret_value_2</pre></div>\n<p>Querying provider&apos;s <code>kv</code> secrets will generate the corresponding key-value data:</p>\n<div class=\"highlight highlight-source-yaml\"><pre><span class=\"pl-ent\">apiVersion</span>: <span class=\"pl-s\">secretize/v1</span>\n<span class=\"pl-ent\">kind</span>: <span class=\"pl-s\">SecretGenerator</span>\n<span class=\"pl-ent\">metadata</span>:\n  <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">hashicorp-vault-secrets</span>\n<span class=\"pl-ent\">sources</span>:\n    - <span class=\"pl-ent\">provider</span>: <span class=\"pl-s\">hashicorp-vault</span>\n      <span class=\"pl-ent\">kv</span>: \n        - <span class=\"pl-s\">secret/data/mySecret </span><span class=\"pl-c\"><span class=\"pl-c\">#</span> you need to specify the full path in hashicorp vault provider</span></pre></div>\n<div class=\"highlight highlight-source-yaml\"><pre><span class=\"pl-ent\">apiVersion</span>: <span class=\"pl-c1\">v1</span>\n<span class=\"pl-ent\">kind</span>: <span class=\"pl-s\">Secret</span>\n<span class=\"pl-ent\">metadata</span>:\n  <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">hashicorp-vault-secrets</span>\n<span class=\"pl-ent\">data</span>:\n  <span class=\"pl-ent\">secret_key_1</span>: <span class=\"pl-s\">c2VjcmV0X3ZhbHVlXzE=</span>\n  <span class=\"pl-ent\">secret_key_2</span>: <span class=\"pl-s\">c2VjcmV0X3ZhbHVlXzI=</span></pre></div>\n<p>However you&apos;re able to query a certain literal in the key-value output using the following syntax: <code>secret-name:key</code>, e.g.:</p>\n<div class=\"highlight highlight-source-yaml\"><pre><span class=\"pl-ent\">apiVersion</span>: <span class=\"pl-s\">secretize/v1</span>\n<span class=\"pl-ent\">kind</span>: <span class=\"pl-s\">SecretGenerator</span>\n<span class=\"pl-ent\">metadata</span>:\n  <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">hashicorp-vault-secrets</span>\n<span class=\"pl-ent\">sources</span>:\n    - <span class=\"pl-ent\">provider</span>: <span class=\"pl-s\">hashicorp-vault</span>\n      <span class=\"pl-ent\">literals</span>:\n          - <span class=\"pl-s\">secret/data/mySecret-1:secret_key_1</span></pre></div>\n<p>As a result, the following manifest will be generated:</p>\n<div class=\"highlight highlight-source-yaml\"><pre><span class=\"pl-ent\">apiVersion</span>: <span class=\"pl-c1\">v1</span>\n<span class=\"pl-ent\">kind</span>: <span class=\"pl-s\">Secret</span>\n<span class=\"pl-ent\">metadata</span>:\n  <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">hashicorp-vault-secrets</span>\n<span class=\"pl-ent\">data</span>:\n  <span class=\"pl-ent\">secret_key_1</span>: <span class=\"pl-s\">c2VjcmV0X3ZhbHVlXzE=</span></pre></div>\n<h2><a id=\"user-content-kubernetes-secret\" class=\"anchor\" href=\"https://github.com/bbl/secretize#kubernetes-secret\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Kubernetes Secret</h2>\n<p>Kubernetes secret provider is similar to the Hashicorp Vault. Additionally, this provider expects the <code>params</code> field with the <code>namespace</code> specification.<br>\nYou&apos;re able to get the entire secret data using the <code>kv</code> query, or get a particular key using the <code>literals</code> query with the <code>:</code> delimiter syntax:</p>\n<div class=\"highlight highlight-source-yaml\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> The original secret in a default namespace</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span></span>\n<span class=\"pl-ent\">apiVersion</span>: <span class=\"pl-c1\">v1</span>\n<span class=\"pl-ent\">kind</span>: <span class=\"pl-s\">Secret</span>\n<span class=\"pl-ent\">metadata</span>:\n  <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">original-secret</span>\n  <span class=\"pl-ent\">namespace</span>: <span class=\"pl-s\">default</span>\n<span class=\"pl-ent\">data</span>:\n  <span class=\"pl-ent\">secret_key_1</span>: <span class=\"pl-s\">c2VjcmV0X3ZhbHVlXzE=</span>\n  <span class=\"pl-ent\">secret_key_2</span>: <span class=\"pl-s\">c2VjcmV0X3ZhbHVlXzI=</span>\n---\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Secret generator configuration</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span></span>\n<span class=\"pl-ent\">apiVersion</span>: <span class=\"pl-s\">secretize/v1</span>\n<span class=\"pl-ent\">kind</span>: <span class=\"pl-s\">SecretGenerator</span>\n<span class=\"pl-ent\">metadata</span>:\n  <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">kubernetes-secrets</span>\n<span class=\"pl-ent\">sources</span>:\n    - <span class=\"pl-ent\">provider</span>: <span class=\"pl-s\">k8s-secret</span>\n      <span class=\"pl-ent\">params</span>:\n        <span class=\"pl-ent\">namespace</span>: <span class=\"pl-s\">default</span>\n      <span class=\"pl-ent\">kv</span>:\n        - <span class=\"pl-s\">original-secret</span>\n      <span class=\"pl-ent\">literals</span>:\n        - <span class=\"pl-s\">new_name=original-secret:secret_key_1</span>\n---\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Generated secret</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span></span>\n<span class=\"pl-ent\">apiVersion</span>: <span class=\"pl-c1\">v1</span>\n<span class=\"pl-ent\">kind</span>: <span class=\"pl-s\">Secret</span>\n<span class=\"pl-ent\">metadata</span>:\n  <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">kubernetes-secrets</span>\n<span class=\"pl-ent\">data</span>:\n  <span class=\"pl-ent\">secret_key_1</span>: <span class=\"pl-s\">c2VjcmV0X3ZhbHVlXzE=</span>\n  <span class=\"pl-ent\">secret_key_2</span>: <span class=\"pl-s\">c2VjcmV0X3ZhbHVlXzI=</span>\n  <span class=\"pl-ent\">new_name</span>: <span class=\"pl-s\">c2VjcmV0X3ZhbHVlXzE=</span>\n</pre></div>\n<h2><a id=\"user-content-env\" class=\"anchor\" href=\"https://github.com/bbl/secretize#env\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Env</h2>\n<p>The environment variables plugin is similar to the AWS and Azure plugins. The <code>literals</code> would simply fetch corresponding environment variables, while <code>kv</code> would treat each variable as JSON and try to parse it:</p>\n<div class=\"highlight highlight-source-yaml\"><pre><span class=\"pl-ent\">apiVersion</span>: <span class=\"pl-s\">secretize/v1</span>\n<span class=\"pl-ent\">kind</span>: <span class=\"pl-s\">SecretGenerator</span>\n<span class=\"pl-ent\">metadata</span>:\n  <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">env-secrets</span>\n<span class=\"pl-ent\">sources</span>:\n    - <span class=\"pl-ent\">provider</span>: <span class=\"pl-s\">env</span>\n      <span class=\"pl-ent\">kv</span>:\n        - <span class=\"pl-s\">MY_KV_SECRET</span>\n      <span class=\"pl-ent\">literals</span>: \n        - <span class=\"pl-s\">MY_LITERAL_SECRET</span></pre></div>\n<p>Secretize will fetch the corresponding environment variables during the <code>kustomize build</code> command:</p>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-k\">export</span> MY_KV_SECRET=<span class=\"pl-s\"><span class=\"pl-pds\">&apos;</span>{&quot;secret_key_1&quot;:&quot;secret_value_1&quot;, &quot;secret_key_2&quot;: &quot;secret_value_2&quot;}<span class=\"pl-pds\">&apos;</span></span>\n<span class=\"pl-k\">export</span> MY_LITERAL_SECRET=super_secret\n\nkustomize build</pre></div>\n<p>The following secret is generated:</p>\n<div class=\"highlight highlight-source-yaml\"><pre><span class=\"pl-ent\">apiVersion</span>: <span class=\"pl-c1\">v1</span>\n<span class=\"pl-ent\">kind</span>: <span class=\"pl-s\">Secret</span>\n<span class=\"pl-ent\">metadata</span>:\n  <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">env-kv-secrets</span>\n<span class=\"pl-ent\">data</span>:\n  <span class=\"pl-ent\">MY_LITERAL_SECRET</span>: <span class=\"pl-s\">c3VwZXJfc2VjcmV0</span>\n  <span class=\"pl-ent\">secret_key_1</span>: <span class=\"pl-s\">c2VjcmV0X3ZhbHVlXzE=</span>\n  <span class=\"pl-ent\">secret_key_2</span>: <span class=\"pl-s\">c2VjcmV0X3ZhbHVlXzI=</span></pre></div>\n</article></div></div>",
      "contentAsText": "\n  \n  \n Secretize is a kustomize plugin that helps generating kubernetes secrets from various sources.  \nIt's like a swiss army knife, but for kubernetes secrets.  \n  \n  \n  \n   \n   \n     \n   \n\n\nSources\nSecretize is able to generate secrets using the following providers:\n\nIt is possible to use multiple providers at once.\nInstallation\nInstall secretize to your $XDG_CONFIG_HOME/kustomize/plugin folder:\n\nExport the XDG_CONFIG_HOME variable if it's not already set:\n\nexport XDG_CONFIG_HOME=~/.config\n\nDownload the release binary into the kustomize plugin folder:\n\nexport SECRETIZE_DIR=\"$XDG_CONFIG_HOME/kustomize/plugin/secretize/v1/secretgenerator\"\nmkdir -p \"$SECRETIZE_DIR\"\ncurl -L https://github.com/bbl/secretize/releases/download/v0.0.1/secretize-v0.0.1-linux-amd64.tar.gz  | tar -xz -C $SECRETIZE_DIR\nUsage\nAll providers can generate two types of secrets: literals and kv (Key-Value secrets).\nLiteral secrets simply generate a single string output, while KV secrets will output with a dictionary of the key-value pairs.\nThe full configuration API could be found in the examples/secret-generator.yaml file.\nAWS Secrets Manager\nFetching literal secrets is as simple, as using a default kustomize secretGenerator plugin:\napiVersion: secretize/v1\nkind: SecretGenerator\nmetadata:\n  name: aws-sm-secrets\nsources:\n    - provider: aws-sm\n      literals: \n        - mySecret\n        - newName=mySecret \nThe above config would query AWS Secrets Manager provider to get the mySecret string value. As a result, the following manifest will be generated:\napiVersion: v1\nkind: Secret\nmetadata:\n  name: aws-sm-secrets\ndata:\n  mySecret: c2VjcmV0X3ZhbHVlXzE= # a sample base64 encoded data \n  newName: c2VjcmV0X3ZhbHVlXzE=\nNow let's assume that value of mySecret is a json string:\n{\n  \"secret_key_1\":\"secret_value_1\", \n  \"secret_key_2\": \"secret_value_2\"\n}\nThe generator config can be slightly modified, to generate a kv secret:\napiVersion: secretize/v1\nkind: SecretGenerator\nmetadata:\n  name: aws-sm-secrets\nsources:\n    - provider: aws-sm\n      kv: \n        - mySecret\nAs a result, the following secret is generated:\napiVersion: v1\nkind: Secret\nmetadata:\n  name: aws-sm-secrets\ndata:\n  secret_key_1: c2VjcmV0X3ZhbHVlXzE=\n  secret_key_2: c2VjcmV0X3ZhbHVlXzI=\nAzure Vault\nAzure Vault configuration is pretty similar to the above examples. However, there's additional params field, which is used to specify the Vault Name:\napiVersion: secretize/v1\nkind: SecretGenerator\nmetadata:\n  name: aws-sm-secrets\nsources:\n  - provider: azure-vault\n    params:\n      name: vault-name\n    kv:\n      - kv-secrets # will treat this as JSON, the same way as in the AWS example\n    literals:\n      - literal-secret-1\n      - new_name=literal-secret-1\nHashicorp Vault\nSome providers only support key-value output, e.g. Hashicorp Vault and K8S Secret.\nFor instance, the mySecret in Hashicorp Vault might look like the following:\nvault kv get secret/mySecret\n====== Data ======\nKey           Value\n---           -----\nsecret_key_1  secret_value_1\nsecret_key_2  secret_value_2\nQuerying provider's kv secrets will generate the corresponding key-value data:\napiVersion: secretize/v1\nkind: SecretGenerator\nmetadata:\n  name: hashicorp-vault-secrets\nsources:\n    - provider: hashicorp-vault\n      kv: \n        - secret/data/mySecret # you need to specify the full path in hashicorp vault provider\napiVersion: v1\nkind: Secret\nmetadata:\n  name: hashicorp-vault-secrets\ndata:\n  secret_key_1: c2VjcmV0X3ZhbHVlXzE=\n  secret_key_2: c2VjcmV0X3ZhbHVlXzI=\nHowever you're able to query a certain literal in the key-value output using the following syntax: secret-name:key, e.g.:\napiVersion: secretize/v1\nkind: SecretGenerator\nmetadata:\n  name: hashicorp-vault-secrets\nsources:\n    - provider: hashicorp-vault\n      literals:\n          - secret/data/mySecret-1:secret_key_1\nAs a result, the following manifest will be generated:\napiVersion: v1\nkind: Secret\nmetadata:\n  name: hashicorp-vault-secrets\ndata:\n  secret_key_1: c2VjcmV0X3ZhbHVlXzE=\nKubernetes Secret\nKubernetes secret provider is similar to the Hashicorp Vault. Additionally, this provider expects the params field with the namespace specification.\nYou're able to get the entire secret data using the kv query, or get a particular key using the literals query with the : delimiter syntax:\n# The original secret in a default namespace\n#\napiVersion: v1\nkind: Secret\nmetadata:\n  name: original-secret\n  namespace: default\ndata:\n  secret_key_1: c2VjcmV0X3ZhbHVlXzE=\n  secret_key_2: c2VjcmV0X3ZhbHVlXzI=\n---\n# Secret generator configuration\n#\napiVersion: secretize/v1\nkind: SecretGenerator\nmetadata:\n  name: kubernetes-secrets\nsources:\n    - provider: k8s-secret\n      params:\n        namespace: default\n      kv:\n        - original-secret\n      literals:\n        - new_name=original-secret:secret_key_1\n---\n# Generated secret\n#\napiVersion: v1\nkind: Secret\nmetadata:\n  name: kubernetes-secrets\ndata:\n  secret_key_1: c2VjcmV0X3ZhbHVlXzE=\n  secret_key_2: c2VjcmV0X3ZhbHVlXzI=\n  new_name: c2VjcmV0X3ZhbHVlXzE=\n\nEnv\nThe environment variables plugin is similar to the AWS and Azure plugins. The literals would simply fetch corresponding environment variables, while kv would treat each variable as JSON and try to parse it:\napiVersion: secretize/v1\nkind: SecretGenerator\nmetadata:\n  name: env-secrets\nsources:\n    - provider: env\n      kv:\n        - MY_KV_SECRET\n      literals: \n        - MY_LITERAL_SECRET\nSecretize will fetch the corresponding environment variables during the kustomize build command:\nexport MY_KV_SECRET='{\"secret_key_1\":\"secret_value_1\", \"secret_key_2\": \"secret_value_2\"}'\nexport MY_LITERAL_SECRET=super_secret\n\nkustomize build\nThe following secret is generated:\napiVersion: v1\nkind: Secret\nmetadata:\n  name: env-kv-secrets\ndata:\n  MY_LITERAL_SECRET: c3VwZXJfc2VjcmV0\n  secret_key_1: c2VjcmV0X3ZhbHVlXzE=\n  secret_key_2: c2VjcmV0X3ZhbHVlXzI=\n",
      "description": "Kubernetes secrets generator. Contribute to bbl/secretize development by creating an account on GitHub.",
      "ogDescription": "Kubernetes secrets generator. Contribute to bbl/secretize development by creating an account on GitHub."
    },
    {
      "url": "https://medium.com/in-the-weeds/service-to-service-authentication-on-kubernetes-94dcb8216cdc",
      "title": "Service to Service Authentication on Kubernetes",
      "content": "<div><article><section class=\"cx cy cz da aj db dc s\"></section><div><section class=\"dh di dj dk dl\"><div class=\"n p\"><div class=\"ab ac ae af ag dm ai aj\"><p id=\"0f3a\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">Service to service authentication is hard. Or more accurately, service to service authentication done properly and securely is hard. And if you want to support human users with SSO at the same time, well that\u0019s extremely hard.</p><p id=\"a762\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">Luckily, Kubernetes provides functionality to make the process easier: <a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection\" class=\"cm hm\">projected service account tokens</a>. Taking some lessons from AWS\u0019s <a href=\"https://aws.amazon.com/blogs/opensource/introducing-fine-grained-iam-roles-service-accounts/\" class=\"cm hm\">IAM Roles for Service Accounts</a> (IRSA) architecture, you can morph these projected tokens to be <a href=\"https://auth0.com/docs/protocols/openid-connect-protocol\" class=\"cm hm\">OpenID Connect</a> (OIDC) ready. OIDC compatibility in place, you are set to use them adjacent to your human users with an OIDC-compatible authentication proxy like <a href=\"https://github.com/oauth2-proxy/oauth2-proxy\" class=\"cm hm\">OAuth2-Proxy</a>.</p><h2 id=\"4c5a\" class=\"hn ho dp cg hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ik em\">Why Bother?</h2><p id=\"3985\" class=\"go gp dp gq b gr il gt gu gv im gx gy gz in hb hc hd io hf hg hh ip hj hk hl dh em\">This may seem daunting at first glance, but implementing this across your application platform yields huge benefits.</p><p id=\"5061\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">From your developers\u0019 point of view:</p><ul class><li id=\"54b9\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl iq ir is em\">Auto-rotating, easily accessible ID Tokens</li><li id=\"aebb\" class=\"go gp dp gq b gr it gt gu gv iu gx gy gz iv hb hc hd iw hf hg hh ix hj hk hl iq ir is em\">OIDC-compatible \u0014 ready for internal authentication and federated external authentication (e.g. to <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_create_oidc.html\" class=\"cm hm\">AWS</a>)</li><li id=\"931d\" class=\"go gp dp gq b gr it gt gu gv iu gx gy gz iv hb hc hd iw hf hg hh ix hj hk hl iq ir is em\">No need to worry about private signing key management and storage</li><li id=\"915d\" class=\"go gp dp gq b gr it gt gu gv iu gx gy gz iv hb hc hd iw hf hg hh ix hj hk hl iq ir is em\">Support for human users via SSO as well</li></ul><p id=\"bbb1\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">And if you take the time to implement this throughout your application platform, all these features are available to your developers seamlessly. They can focus on business logic instead of trying to implement authentication themselves (&amp;which will likely have bugs and security vulnerabilities).</p><h2 id=\"18eb\" class=\"hn ho dp cg hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ik em\">The Final Architecture Goal</h2><figure class=\"iz ja jb jc jd je cz da paragraph-image\"><img alt=\"A diagram of a client app authenticating with a bearer token to a server app with OAuth2-Proxy in front of it.\" class=\"t u v jm aj\" src=\"https://miro.medium.com/max/1458/1*nawQgqi2cE9cSKC_wcTQZA.png\" width=\"729\" srcset=\"https://miro.medium.com/max/552/1*nawQgqi2cE9cSKC_wcTQZA.png 276w, https://miro.medium.com/max/1104/1*nawQgqi2cE9cSKC_wcTQZA.png 552w, https://miro.medium.com/max/1280/1*nawQgqi2cE9cSKC_wcTQZA.png 640w, https://miro.medium.com/max/1400/1*nawQgqi2cE9cSKC_wcTQZA.png 700w\" sizes=\"700px\"><figcaption class=\"jw jx db cz da jy jz cg b ew ci fz\">The Final Architecture Goal</figcaption></figure><h2 id=\"ea59\" class=\"ka\">Identity on Kubernetes</h2><p id=\"52a5\" class=\"go gp dp gq b gr il gt gu gv im gx gy gz in hb hc hd io hf hg hh ip hj hk hl dh em\">On Kubernetes, the <a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\" class=\"cm hm\">Service Account</a> resource is the way to provide an identity to workloads running in your Pods.</p><p id=\"2838\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">Clusters provide Pods access to their identity via JSON Web Tokens (JWTs). They use a service account key to sign a JWT that contains all the needed Service Account details in the JWT\u0019s claims. Historically, this was an unchanging \u001cforever\u001d token with no expiration and no audience, and it was stored as a Kubernetes Secret. This design had <a href=\"https://thenewstack.io/no-more-forever-tokens-changes-in-identity-management-for-kubernetes/\" class=\"cm hm\">limitations and security concerns</a> that the advent of projected tokens in Kubernetes v1.12 alleviated.</p><h2 id=\"cdfa\" class=\"hn ho dp cg hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ik em\">Service Account Token Volume Projection</h2><p id=\"fdb5\" class=\"go gp dp gq b gr il gt gu gv im gx gy gz in hb hc hd io hf hg hh ip hj hk hl dh em\"><a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection\" class=\"cm hm\">Service Account token volume projection</a> gives you a way to overcome the limitations of the Secret-held \u001cforever\u001d tokens.</p><p id=\"c87c\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">By using a projected volume, Kubernetes has the ability to provide dynamically rotating tokens to your Pods. This allows a proper expiration on your tokens. Kubernetes automatically handles creating a new token with a fresh expiration when your existing mounted token gets to 80% of its lifespan.</p><p id=\"215b\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">You aren\u0019t limited to one projected volume either. You can mount multiple projected tokens \u0014 allowing you to specify the distinct audience (<code class=\"jq kp kq kr ks b\">aud</code>) claims you need for each token. Not only does the audience claim provide security benefits, it will be a key aspect of your OIDC conversion \u0014 acting as your OIDC client ID.</p><p id=\"edba\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">Here is a sample Pod spec showing the volume projection and mount:</p><figure class=\"iz ja jb jc jd je\"></figure><p id=\"915f\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">Any container running in that Pod can get that token to use via HTTP bearer authentication by reading it from <code class=\"jq kp kq kr ks b\">/var/run/secrets/tokens/sample-token</code>.</p><p id=\"5a78\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">Digging into that token\u0019s contents and Base64 decoding the payload, you would see a payload like this:</p><figure class=\"iz ja jb jc jd je\"></figure><p id=\"4040\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">Notice your Service Account name and its Namespace are part of your token\u0019s subject (<code class=\"jq kp kq kr ks b\">sub</code>) claim. Receiving applications should use this to authorize the Service Account identity after authenticating the token.</p><p id=\"b42d\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">We\u0019ll touch on the audience (<code class=\"jq kp kq kr ks b\">aud</code>) and issuer (<code class=\"jq kp kq kr ks b\">iss</code>) claims later. They are important for the OIDC compatibility of these tokens.</p><p id=\"51a8\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">Everything under the complex <code class=\"jq kp kq kr ks b\">kubernetes.io</code> claim isn\u0019t needed for OIDC purposes. But if you want it, that claim provides nice additional details for the receiving application. You just have to code up how to extract it yourself.</p><h2 id=\"8b18\" class=\"ka\">Federated Identity with OIDC</h2><p id=\"8152\" class=\"go gp dp gq b gr il gt gu gv im gx gy gz in hb hc hd io hf hg hh ip hj hk hl dh em\">OpenID Connect (OIDC) is an authentication protocol that helps verify a user\u0019s (or machine\u0019s) identity. It adds an identity layer on top of the OAuth2 protocol (hence we will use <a href=\"https://github.com/oauth2-proxy/oauth2-proxy\" class=\"cm hm\">OAuth2-Proxy</a> in the final stage of this guide). There\u0019s a lot of nitty-gritty details to the specification, you can brush up on them <a href=\"https://gravitational.com/blog/how-oidc-authentication-works/\" class=\"cm hm\">here</a> if you want.</p><p id=\"048c\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">For this design, the OIDC discovery documents are the key aspect of the OIDC protocol you will need \u0014 particularly the JSON Web Key Sets (JWKS). Those store the public keys needed to verify token signatures.</p><p id=\"3021\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">Inside an OIDC ID Token, there is an issuer (<code class=\"jq kp kq kr ks b\">iss</code>) claim in the payload that should have a publicly accessible URL. With OIDC, you can find the OpenID configuration at the <code class=\"jq kp kq kr ks b\">/.well-known/openid-configuration</code> path under it.</p><p id=\"4c19\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">As a sample, you can peek at Greenhouse\u0019s OneLogin OIDC identity provider <a href=\"https://greenhouse.onelogin.com/oidc/2/.well-known/openid-configuration\" class=\"cm hm\">here</a>.</p><p id=\"dc96\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">There\u0019s a lot going on in that JSON, but amidst all that configuration noise the most important setting for this guide is the <code class=\"jq kp kq kr ks b\">jwks_uri</code> which contains your JWKS file. As mentioned before, this houses all your public keys needed to verify OIDC ID Token signatures.</p><p id=\"47c9\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">In OIDC, your public keys being publicly accessible in a consistent manner based on the OIDC discovery documents is the key to identity federation. This allows external entities to verify the authenticity of your ID Tokens and use them for authorization.</p><h2 id=\"ee67\" class=\"ka\">Kubernetes Tokens and OIDC</h2><p id=\"013b\" class=\"go gp dp gq b gr il gt gu gv im gx gy gz in hb hc hd io hf hg hh ip hj hk hl dh em\">Here at Greenhouse, we got introduced to the potential of using Kubernetes projected tokens as OIDC ID Tokens by <a href=\"https://aws.amazon.com/blogs/opensource/introducing-fine-grained-iam-roles-service-accounts/\" class=\"cm hm\">AWS\u0019s IRSA design</a>. There\u0019s a LOT of details in that introduction, but if you have the mental stamina to read through it you\u0019ll see a Pod spec that looks very similar to our examples above.</p><p id=\"b13f\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">Not only did AWS socialize the architecture, they provided the source code for tools that help ease the deployment on your Kubernetes clusters. For instance, their <a href=\"https://github.com/aws/amazon-eks-pod-identity-webhook\" class=\"cm hm\">EKS Pod Identity Webhook</a> project shows how to project tokens as needed into any Pod with merely an annotation using a <a href=\"https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook\" class=\"cm hm\">Mutating Admission Webhook</a>.</p><p id=\"1152\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">Most importantly, they didn\u0019t reserve the toolkit just for their EKS offering. AWS published how to use these techniques in a <a href=\"https://github.com/aws/amazon-eks-pod-identity-webhook/blob/master/SELF_HOSTED_SETUP.md\" class=\"cm hm\">self-hosted Kubernetes environment</a> \u0014 most critically the steps for how to turn projected tokens into valid OIDC ID Tokens.</p><h2 id=\"6fbb\" class=\"hn ho dp cg hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ik em\">OIDC for Kubernetes</h2><p id=\"4c16\" class=\"go gp dp gq b gr il gt gu gv im gx gy gz in hb hc hd io hf hg hh ip hj hk hl dh em\">As we touched on before, the key OIDC elements you need to make your projected tokens look like bare-bones OIDC ID Tokens is the <code class=\"jq kp kq kr ks b\">openid-configuration</code> discovery document and a JWKS file containing the public keys needed to verify a token\u0019s signature.</p><p id=\"7d2b\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">Following <a href=\"https://github.com/aws/amazon-eks-pod-identity-webhook/blob/master/SELF_HOSTED_SETUP.md#create-the-oidc-discovery-and-keys-documents\" class=\"cm hm\">AWS\u0019s guide</a>, you can create a minimal <code class=\"jq kp kq kr ks b\">openid-configuration</code> file for service to service authentication that looks like this:</p><figure class=\"iz ja jb jc jd je\"></figure><p id=\"dadb\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">The next more tricky part is generating a public JWKS from your service account keys found on your Kubernetes cluster\u0019s <code class=\"jq kp kq kr ks b\">kube-apiserver</code>. AWS also published the <a href=\"https://github.com/aws/amazon-eks-pod-identity-webhook/blob/master/hack/self-hosted/main.go\" class=\"cm hm\">Go source code</a> for this \u0014 just pass in your public key and it will spit out the JWKS version of it you need that matches your private signing key.</p><p id=\"e0be\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">Lastly, you need to host these public documents under the <code class=\"jq kp kq kr ks b\">/.well-known</code> path at your projected tokens\u0019 issuer URL. When you set up projected tokens on your cluster, you enable them by setting these <a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection\" class=\"cm hm\">kube-apiserver options</a>, so be sure to align with the <code class=\"jq kp kq kr ks b\">-service-account-issuer</code> flag.</p><h2 id=\"e8a8\" class=\"ka\">Authenticating with OAuth2-Proxy</h2><p id=\"6477\" class=\"go gp dp gq b gr il gt gu gv im gx gy gz in hb hc hd io hf hg hh ip hj hk hl dh em\">Now that all your Pods have an identity tied to their Service Account that they can pass around via OIDC ID Tokens, you need a way to authenticate those tokens at your receiving services.</p><p id=\"a0ab\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">Unless you want to manually code up support into your applications (which many <a href=\"https://github.com/coreos/go-oidc\" class=\"cm hm\">libraries</a> exist to do this if you choose), an authentication proxy is the best route to easily deliver this functionality to your developers. <a href=\"https://github.com/oauth2-proxy/oauth2-proxy\" class=\"cm hm\">OAuth2-Proxy</a> stands out in the open-source auth proxy space for its ability to handle our makeshift machine OIDC identities and human users simultaneously.</p><h2 id=\"20d0\" class=\"hn ho dp cg hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ik em\">Machine Users Only</h2><p id=\"920c\" class=\"go gp dp gq b gr il gt gu gv im gx gy gz in hb hc hd io hf hg hh ip hj hk hl dh em\">OAuth2-Proxy provides a bunch of potential OAuth2 <a href=\"https://oauth2-proxy.github.io/oauth2-proxy/docs/configuration/oauth_provider\" class=\"cm hm\">providers</a> to plug in. But if you only want to support machine users directly, the <a href=\"https://oauth2-proxy.github.io/oauth2-proxy/docs/configuration/oauth_provider#openid-connect-provider\" class=\"cm hm\">OIDC provider</a> is the one you need.</p><p id=\"0880\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">Here are the minimum <a href=\"https://oauth2-proxy.github.io/oauth2-proxy/docs/configuration/overview\" class=\"cm hm\">command-line flags</a> you need to make the OIDC provider work with your Kubernetes projected tokens that you will be sending via bearer headers:</p><figure class=\"iz ja jb jc jd je\"></figure><p id=\"ae65\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">Some notes on these flags (since some of the settings are a little confusing&amp;):</p><ul class><li id=\"5553\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl iq ir is em\"><code class=\"jq kp kq kr ks b\">oidc-issuer-url</code> must match the issuer (<code class=\"jq kp kq kr ks b\">iss</code>) claim in your tokens. This is where you are hosting the public OIDC discovery documents you configured.</li><li id=\"7af1\" class=\"go gp dp gq b gr it gt gu gv iu gx gy gz iv hb hc hd iw hf hg hh ix hj hk hl iq ir is em\"><code class=\"jq kp kq kr ks b\">client-id</code> needs to match the audience (<code class=\"jq kp kq kr ks b\">aud</code>) claim in your tokens. You set the audience in the Pod spec in the projected volume details.</li><li id=\"5a9e\" class=\"go gp dp gq b gr it gt gu gv iu gx gy gz iv hb hc hd iw hf hg hh ix hj hk hl iq ir is em\"><code class=\"jq kp kq kr ks b\">cookie-secret</code> and <code class=\"jq kp kq kr ks b\">client-secret</code> don\u0019t matter for machine users. But they have to be set for OAuth2-Proxy to start up. Just set them with junk data.</li><li id=\"0107\" class=\"go gp dp gq b gr it gt gu gv iu gx gy gz iv hb hc hd iw hf hg hh ix hj hk hl iq ir is em\"><code class=\"jq kp kq kr ks b\">skip-jwt-bearer-tokens</code> is what allows OAuth2-Proxy to verify ID Tokens in a bearer header directly. Otherwise it would look for a session cookie for authorization purposes.</li><li id=\"d227\" class=\"go gp dp gq b gr it gt gu gv iu gx gy gz iv hb hc hd iw hf hg hh ix hj hk hl iq ir is em\"><code class=\"jq kp kq kr ks b\">email-domains</code> must be <code class=\"jq kp kq kr ks b\">*</code> for Kubernetes machine users support. If you glance above at the decoded contents of a projected token payload, you\u0019ll notice there\u0019s no <code class=\"jq kp kq kr ks b\">email</code> claim. Hence the <code class=\"jq kp kq kr ks b\">*</code> is mandatory.</li></ul><h2 id=\"ddff\" class=\"hn ho dp cg hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ik em\">With Human SSO Users Too!</h2><p id=\"fec8\" class=\"go gp dp gq b gr il gt gu gv im gx gy gz in hb hc hd io hf hg hh ip hj hk hl dh em\">If this is a service to service authentication guide, why should you care about supporting SSO as well?</p><p id=\"725a\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">Inevitably, you will have an application on your platform that wants to expose an API endpoint to services and human users alike. So you should plan ahead and architect a solution that is flexible enough to support both use cases.</p><p id=\"0085\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">OAuth2-Proxy allows this split paradigm. You can reserve the bearer header for service authentication while leaving traditional requests to be handled by your SSO provider for your human users.</p><p id=\"e47e\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">If the <code class=\"jq kp kq kr ks b\">skip-jwt-bearer-tokens</code> flag is set enabling bearer token authentication, OAuth2-Proxy supports the <code class=\"jq kp kq kr ks b\">extra-jwt-issuers</code> flag as well.</p><p id=\"5378\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\"><code class=\"jq kp kq kr ks b\">extra-jwt-issuers</code> lets you provide a list of <code class=\"jq kp kq kr ks b\">issuer=audience</code> pairs to attempt to verify any bearer ID Tokens that come in. The proxy will automatically look at the issuer URL for an OIDC <code class=\"jq kp kq kr ks b\">/.well-known/openid-configuration</code> or <code class=\"jq kp kq kr ks b\">/.well-known/jwks.json</code>. It will then use those public keys to validate the signature of any bearer tokens that come in (assuming their audience claim matches the audience you configured as well).</p><p id=\"5222\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">The important aspect of the <code class=\"jq kp kq kr ks b\">extra-jwt-issuers</code> flag is it works on bearer tokens adjacent to ANY provider you configure for your standard human users.</p><p id=\"e3b8\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">So your normal users without bearer headers can hit the proxy and get the behavior you expect from an SSO proxy. If they have a valid session cookie, they pass through no problem. If not, they get redirected to your configured OAuth2 identity provider to sign in. Upon success, they get a session cookie from OAuth2-Proxy and are golden to talk to your backend applications.</p><h2 id=\"f2f2\" class=\"ka\">Putting it all Together</h2><p id=\"9d79\" class=\"go gp dp gq b gr il gt gu gv im gx gy gz in hb hc hd io hf hg hh ip hj hk hl dh em\">Between Kubernetes projected tokens, OIDC federation and OAuth2-Proxy as an authentication proxy sidecar, there\u0019s a lot of complex ideas intermingling here.</p><p id=\"74ed\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">If you decide to adopt all or part of this architecture, you\u0019ll need to figure out the best way to deploy it in your environment. Luckily you have options:</p><p id=\"3b7f\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">You\u0019ll also need to decide how to deploy OAuth2-Proxy if you want that layer as well. Adding a sidecar container directly in your Pods or adding <a href=\"https://kubernetes.github.io/ingress-nginx/examples/auth/oauth-external-auth/\" class=\"cm hm\">Kubernetes Ingress annotations</a> are both popular architectures.</p><p id=\"f3cc\" class=\"go gp dp gq b gr gs gt gu gv gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl dh em\">Admittedly, none of this is trivial. But now that it\u0019s implemented at Greenhouse, we\u0019ve found the final product is worth the investment. So give it a whirl!</p></div></div></section></div></article></div>",
      "contentAsText": "Service to service authentication is hard. Or more accurately, service to service authentication done properly and securely is hard. And if you want to support human users with SSO at the same time, well that\u0019s extremely hard.Luckily, Kubernetes provides functionality to make the process easier: projected service account tokens. Taking some lessons from AWS\u0019s IAM Roles for Service Accounts (IRSA) architecture, you can morph these projected tokens to be OpenID Connect (OIDC) ready. OIDC compatibility in place, you are set to use them adjacent to your human users with an OIDC-compatible authentication proxy like OAuth2-Proxy.Why Bother?This may seem daunting at first glance, but implementing this across your application platform yields huge benefits.From your developers\u0019 point of view:Auto-rotating, easily accessible ID TokensOIDC-compatible \u0014 ready for internal authentication and federated external authentication (e.g. to AWS)No need to worry about private signing key management and storageSupport for human users via SSO as wellAnd if you take the time to implement this throughout your application platform, all these features are available to your developers seamlessly. They can focus on business logic instead of trying to implement authentication themselves (&which will likely have bugs and security vulnerabilities).The Final Architecture GoalThe Final Architecture GoalIdentity on KubernetesOn Kubernetes, the Service Account resource is the way to provide an identity to workloads running in your Pods.Clusters provide Pods access to their identity via JSON Web Tokens (JWTs). They use a service account key to sign a JWT that contains all the needed Service Account details in the JWT\u0019s claims. Historically, this was an unchanging \u001cforever\u001d token with no expiration and no audience, and it was stored as a Kubernetes Secret. This design had limitations and security concerns that the advent of projected tokens in Kubernetes v1.12 alleviated.Service Account Token Volume ProjectionService Account token volume projection gives you a way to overcome the limitations of the Secret-held \u001cforever\u001d tokens.By using a projected volume, Kubernetes has the ability to provide dynamically rotating tokens to your Pods. This allows a proper expiration on your tokens. Kubernetes automatically handles creating a new token with a fresh expiration when your existing mounted token gets to 80% of its lifespan.You aren\u0019t limited to one projected volume either. You can mount multiple projected tokens \u0014 allowing you to specify the distinct audience (aud) claims you need for each token. Not only does the audience claim provide security benefits, it will be a key aspect of your OIDC conversion \u0014 acting as your OIDC client ID.Here is a sample Pod spec showing the volume projection and mount:Any container running in that Pod can get that token to use via HTTP bearer authentication by reading it from /var/run/secrets/tokens/sample-token.Digging into that token\u0019s contents and Base64 decoding the payload, you would see a payload like this:Notice your Service Account name and its Namespace are part of your token\u0019s subject (sub) claim. Receiving applications should use this to authorize the Service Account identity after authenticating the token.We\u0019ll touch on the audience (aud) and issuer (iss) claims later. They are important for the OIDC compatibility of these tokens.Everything under the complex kubernetes.io claim isn\u0019t needed for OIDC purposes. But if you want it, that claim provides nice additional details for the receiving application. You just have to code up how to extract it yourself.Federated Identity with OIDCOpenID Connect (OIDC) is an authentication protocol that helps verify a user\u0019s (or machine\u0019s) identity. It adds an identity layer on top of the OAuth2 protocol (hence we will use OAuth2-Proxy in the final stage of this guide). There\u0019s a lot of nitty-gritty details to the specification, you can brush up on them here if you want.For this design, the OIDC discovery documents are the key aspect of the OIDC protocol you will need \u0014 particularly the JSON Web Key Sets (JWKS). Those store the public keys needed to verify token signatures.Inside an OIDC ID Token, there is an issuer (iss) claim in the payload that should have a publicly accessible URL. With OIDC, you can find the OpenID configuration at the /.well-known/openid-configuration path under it.As a sample, you can peek at Greenhouse\u0019s OneLogin OIDC identity provider here.There\u0019s a lot going on in that JSON, but amidst all that configuration noise the most important setting for this guide is the jwks_uri which contains your JWKS file. As mentioned before, this houses all your public keys needed to verify OIDC ID Token signatures.In OIDC, your public keys being publicly accessible in a consistent manner based on the OIDC discovery documents is the key to identity federation. This allows external entities to verify the authenticity of your ID Tokens and use them for authorization.Kubernetes Tokens and OIDCHere at Greenhouse, we got introduced to the potential of using Kubernetes projected tokens as OIDC ID Tokens by AWS\u0019s IRSA design. There\u0019s a LOT of details in that introduction, but if you have the mental stamina to read through it you\u0019ll see a Pod spec that looks very similar to our examples above.Not only did AWS socialize the architecture, they provided the source code for tools that help ease the deployment on your Kubernetes clusters. For instance, their EKS Pod Identity Webhook project shows how to project tokens as needed into any Pod with merely an annotation using a Mutating Admission Webhook.Most importantly, they didn\u0019t reserve the toolkit just for their EKS offering. AWS published how to use these techniques in a self-hosted Kubernetes environment \u0014 most critically the steps for how to turn projected tokens into valid OIDC ID Tokens.OIDC for KubernetesAs we touched on before, the key OIDC elements you need to make your projected tokens look like bare-bones OIDC ID Tokens is the openid-configuration discovery document and a JWKS file containing the public keys needed to verify a token\u0019s signature.Following AWS\u0019s guide, you can create a minimal openid-configuration file for service to service authentication that looks like this:The next more tricky part is generating a public JWKS from your service account keys found on your Kubernetes cluster\u0019s kube-apiserver. AWS also published the Go source code for this \u0014 just pass in your public key and it will spit out the JWKS version of it you need that matches your private signing key.Lastly, you need to host these public documents under the /.well-known path at your projected tokens\u0019 issuer URL. When you set up projected tokens on your cluster, you enable them by setting these kube-apiserver options, so be sure to align with the -service-account-issuer flag.Authenticating with OAuth2-ProxyNow that all your Pods have an identity tied to their Service Account that they can pass around via OIDC ID Tokens, you need a way to authenticate those tokens at your receiving services.Unless you want to manually code up support into your applications (which many libraries exist to do this if you choose), an authentication proxy is the best route to easily deliver this functionality to your developers. OAuth2-Proxy stands out in the open-source auth proxy space for its ability to handle our makeshift machine OIDC identities and human users simultaneously.Machine Users OnlyOAuth2-Proxy provides a bunch of potential OAuth2 providers to plug in. But if you only want to support machine users directly, the OIDC provider is the one you need.Here are the minimum command-line flags you need to make the OIDC provider work with your Kubernetes projected tokens that you will be sending via bearer headers:Some notes on these flags (since some of the settings are a little confusing&):oidc-issuer-url must match the issuer (iss) claim in your tokens. This is where you are hosting the public OIDC discovery documents you configured.client-id needs to match the audience (aud) claim in your tokens. You set the audience in the Pod spec in the projected volume details.cookie-secret and client-secret don\u0019t matter for machine users. But they have to be set for OAuth2-Proxy to start up. Just set them with junk data.skip-jwt-bearer-tokens is what allows OAuth2-Proxy to verify ID Tokens in a bearer header directly. Otherwise it would look for a session cookie for authorization purposes.email-domains must be * for Kubernetes machine users support. If you glance above at the decoded contents of a projected token payload, you\u0019ll notice there\u0019s no email claim. Hence the * is mandatory.With Human SSO Users Too!If this is a service to service authentication guide, why should you care about supporting SSO as well?Inevitably, you will have an application on your platform that wants to expose an API endpoint to services and human users alike. So you should plan ahead and architect a solution that is flexible enough to support both use cases.OAuth2-Proxy allows this split paradigm. You can reserve the bearer header for service authentication while leaving traditional requests to be handled by your SSO provider for your human users.If the skip-jwt-bearer-tokens flag is set enabling bearer token authentication, OAuth2-Proxy supports the extra-jwt-issuers flag as well.extra-jwt-issuers lets you provide a list of issuer=audience pairs to attempt to verify any bearer ID Tokens that come in. The proxy will automatically look at the issuer URL for an OIDC /.well-known/openid-configuration or /.well-known/jwks.json. It will then use those public keys to validate the signature of any bearer tokens that come in (assuming their audience claim matches the audience you configured as well).The important aspect of the extra-jwt-issuers flag is it works on bearer tokens adjacent to ANY provider you configure for your standard human users.So your normal users without bearer headers can hit the proxy and get the behavior you expect from an SSO proxy. If they have a valid session cookie, they pass through no problem. If not, they get redirected to your configured OAuth2 identity provider to sign in. Upon success, they get a session cookie from OAuth2-Proxy and are golden to talk to your backend applications.Putting it all TogetherBetween Kubernetes projected tokens, OIDC federation and OAuth2-Proxy as an authentication proxy sidecar, there\u0019s a lot of complex ideas intermingling here.If you decide to adopt all or part of this architecture, you\u0019ll need to figure out the best way to deploy it in your environment. Luckily you have options:You\u0019ll also need to decide how to deploy OAuth2-Proxy if you want that layer as well. Adding a sidecar container directly in your Pods or adding Kubernetes Ingress annotations are both popular architectures.Admittedly, none of this is trivial. But now that it\u0019s implemented at Greenhouse, we\u0019ve found the final product is worth the investment. So give it a whirl!",
      "publishedDate": "2020-11-18T21:21:28.386Z",
      "description": "Service to service authentication is hard. Or more accurately, service to service authentication done properly and securely is hard. And if you want to support human users with SSO at the same time…",
      "ogDescription": "Using Service Account Token Volume Projection and OAuth2-Proxy"
    },
    {
      "url": "https://thenewstack.io/open-policy-agent-for-the-enterprise-styras-declarative-authorization-service/",
      "title": "Open Policy Agent for the Enterprise: Styra’s Declarative Authorization Service",
      "content": "<div class=\"post-content\"> <p class=\"attribution\"><a href=\"https://www.honeycomb.io/\" class=\"ext-link\">Honeycomb</a> sponsored The New Stack\u0019s coverage of Kubecon+CloudNativeCon North America 2020.</p>\n<p>Long, long before we were coding policy enforcement into our clouds, we tried to code it into our programs. Most of the answers we created were hard-coded, difficult to maintain, and nigh unto impossible to update. But, in 2016, <a href=\"https://www.openpolicyagent.org/\" class=\"ext-link\">Open Policy Agent</a> (OPA, pronounced \u001coh-pa\u001d) for cloud native environments was created, and policy enforcement in code became much more practical. Now, its developers, under their company, <a href=\"https://www.styra.com/\" class=\"ext-link\">Styra</a>, have announced a <a href=\"https://www.styra.com/pricing\" class=\"ext-link\">new three-tier product offering for Styra Declarative Authorization Service (DAS)</a>.</p>\n<p>Before diving into DAS, though, let&#x2019;s make sure we&#x2019;re all on the same page with OPA and policies in general.</p>\n<p>OPA is an open source, general-purpose policy engine that unifies policy enforcement across the stack. You write these policies in its high-level declarative language <a href=\"https://www.openpolicyagent.org/docs/latest/policy-language/\" class=\"ext-link\">Rego</a>, which, in turn, is based on the old Prolog-based <a href=\"https://docs.datomic.com/on-prem/query.html\" class=\"ext-link\">Datalog</a> query language. With Rego, you can specify policy as code and create simple APIs to offload policy decision-making from your software. You can then use OPA to enforce policies in microservices, Kubernetes, CI/CD pipelines, API gateways, and more.</p>\n<p>And, what&#x2019;s a policy engine you ask? <a href=\"https://www.linkedin.com/in/torin-sandall-1967387/\" class=\"ext-link\">Torin Sandall</a>, Styra Software Engineer and OPA Technical Lead, explained, &#x201C;policy means different things to different people in different contexts. In the context of software systems, <a href=\"https://blog.openpolicyagent.org/what-is-policy-part-one-enforcement-bad8ea8eb35c\" class=\"ext-link\">policies are the rules that govern how the system behaves</a>.&#x201D;</p>\n<p>So, for instance, Is this user allowed to change the config of that service? Is this VM allowed to accept TCP connections from that VM? Which host should this container be deployed on? And, so on. Using Rego policies, the OPA policy engine takes &#x201C;<a href=\"https://blog.openpolicyagent.org/what-is-policy-part-two-policy-engines-7ee1d972386b\" class=\"ext-link\">policy and data as input and produces answers to policy questions as output</a>.&#x201D;</p>\n<p>This approach has been very successful. OPA has been used for creating <a href=\"https://kubernetes.io/\" class=\"ext-link\">Kubernetes</a> access policies; setting up <a href=\"https://blog.openpolicyagent.org/what-is-policy-part-two-policy-engines-7ee1d972386b\" class=\"ext-link\">cloud security policies</a>; Netflix uses OPA to control internal API resources access; <a href=\"https://www.chef.io/\" class=\"ext-link\">Chef</a> uses it to provide Identity and Access Management (IAM) capabilities in its end-user products.</p>\n<p>OPA is also a <a href=\"https://www.cncf.io/\" class=\"ext-link\">Cloud Native Computing Foundation (CNCF)</a> incubator project. There it averages a rather amazing one million downloads a week.</p>\n<p>Of course, just as in the past, you could enforce policies. But, as <a href=\"https://www.linkedin.com/in/mohamedfahmed\" class=\"ext-link\">Mohamed Ahmed</a>, <a href=\"https://www.magalix.com/\" class=\"ext-link\">Magalix</a> CEO, observed that while in Kubernetes, &#x201C;You can definitely use <a href=\"https://www.magalix.com/blog/kubernetes-rbac-101\" class=\"ext-link\">RBAC</a> and Pod security policies to impose fine-grained control over the cluster.&#xFFFD; But again, this will only apply to the cluster. Kubernetes RBAC is of no use except in a Kubernetes cluster. That\u0019s where OPA comes into play. <a href=\"https://www.magalix.com/blog/introducing-policy-as-code-the-open-policy-agent-opa\" class=\"ext-link\">OPA was introduced to create a unified method of enforcing security policy</a> in the stack.&#x201D;</p>\n<p>All that&#x2019;s great, if you want to learn how to use OPA and write in Rego. If you&#x2019;d rather spend your time working on your project rather than on learning how to automate policies, you need Styra&#x2019;s DAS.</p>\n<p>DAS is available in two new editions, DAS Free and DAS Pro editions, along with the pre-existing DAS Enterprise. With these, you get a budget-friendly and fast option to deploy OPA at scale for Kubernetes. With any of the trio, you can now deploy DAS in just minutes and have access to more than 100 built-in policies. These new offerings enable a self-service experience and eliminate the need for learning and custom coding OPA policies for Kubernetes admission control.</p>\n<p>If you, like me, prefer to see code examples, the policies alone are worth the price of admission.</p>\n<p>While it&#x2019;s not quite turnkey \u0014 every company has their own policies \u0014 it&#x2019;s close. DAS provides a single control plane for authorization both within applications and for the infrastructure they run upon. With it, you get easy-to-deploy security, compliance and operational guardrails for both Kubernetes and microservices to help customers mitigate risk, reduce errors and accelerate software development.</p> <p>Styra&#x2019;s not the only one singing its approach praises. According to the Gartner report, <a href=\"https://www.gartner.com/en/documents/3986057/market-guide-for-compliance-automation-tools-in-devops\" class=\"ext-link\">Market Guide for Compliance Automation Tools in DevOps</a>, \u001cAs organizations migrate workloads to the cloud or move from virtualized to containerized environments, I&amp;O leaders must evaluate existing tools that protect cloud and container-based infrastructure. These tools enable enforcing infrastructure compliance policies to minimize configuration-related risks. Opportunities exist for the orchestration of policies over distinct agile infrastructure environments. Specifically, the OPA open source initiative has started to emerge as a source for an ecosystem of startups building enterprise capabilities over OPA.&#x201D;</p>\n<p>You can see for yourself what&#x2019;s all the fuss is about with the new DAS Free. This is a completely free, self-service option for up to two clusters or 10 nodes to streamline the adoption process. For teams with larger production scale needs, DAS Pro offers a clear and transparent pricing model, for up to 50 nodes, to protect and manage Kubernetes clusters as they grow from initial testing/deployment to full production environments. Finally, DAS Enterprise gives teams unlimited OPA deployments and rules with around the clock support. Regardless of the version, all have access to the same management plane, policy libraries, impact analysis, monitoring, and decision logging.</p>\n<p>\u001cThese new editions will benefit any number of teams beginning their Kubernetes journey,\u001d said Tim Hinrichs, co-creator of OPA and Styra&#x2019;s CTO. \u001cIt will also help platform engineers new to OPA who want to deploy community best practices immediately without custom coding. Ultimately, this will help lessen the burden for anyone who needs to monitor, validate and test Kubernetes admission control with OPA.&#x201D;</p> <p class=\"attribution\">Featured image <a href=\"https://pixabay.com/fr/photos/feuilles-d-automne-automne-bois-4633854/\" class=\"ext-link\">via</a> Pixabay.</p> </div>",
      "contentAsText": " Honeycomb sponsored The New Stack\u0019s coverage of Kubecon+CloudNativeCon North America 2020.\nLong, long before we were coding policy enforcement into our clouds, we tried to code it into our programs. Most of the answers we created were hard-coded, difficult to maintain, and nigh unto impossible to update. But, in 2016, Open Policy Agent (OPA, pronounced \u001coh-pa\u001d) for cloud native environments was created, and policy enforcement in code became much more practical. Now, its developers, under their company, Styra, have announced a new three-tier product offering for Styra Declarative Authorization Service (DAS).\nBefore diving into DAS, though, let’s make sure we’re all on the same page with OPA and policies in general.\nOPA is an open source, general-purpose policy engine that unifies policy enforcement across the stack. You write these policies in its high-level declarative language Rego, which, in turn, is based on the old Prolog-based Datalog query language. With Rego, you can specify policy as code and create simple APIs to offload policy decision-making from your software. You can then use OPA to enforce policies in microservices, Kubernetes, CI/CD pipelines, API gateways, and more.\nAnd, what’s a policy engine you ask? Torin Sandall, Styra Software Engineer and OPA Technical Lead, explained, “policy means different things to different people in different contexts. In the context of software systems, policies are the rules that govern how the system behaves.”\nSo, for instance, Is this user allowed to change the config of that service? Is this VM allowed to accept TCP connections from that VM? Which host should this container be deployed on? And, so on. Using Rego policies, the OPA policy engine takes “policy and data as input and produces answers to policy questions as output.”\nThis approach has been very successful. OPA has been used for creating Kubernetes access policies; setting up cloud security policies; Netflix uses OPA to control internal API resources access; Chef uses it to provide Identity and Access Management (IAM) capabilities in its end-user products.\nOPA is also a Cloud Native Computing Foundation (CNCF) incubator project. There it averages a rather amazing one million downloads a week.\nOf course, just as in the past, you could enforce policies. But, as Mohamed Ahmed, Magalix CEO, observed that while in Kubernetes, “You can definitely use RBAC and Pod security policies to impose fine-grained control over the cluster.� But again, this will only apply to the cluster. Kubernetes RBAC is of no use except in a Kubernetes cluster. That\u0019s where OPA comes into play. OPA was introduced to create a unified method of enforcing security policy in the stack.”\nAll that’s great, if you want to learn how to use OPA and write in Rego. If you’d rather spend your time working on your project rather than on learning how to automate policies, you need Styra’s DAS.\nDAS is available in two new editions, DAS Free and DAS Pro editions, along with the pre-existing DAS Enterprise. With these, you get a budget-friendly and fast option to deploy OPA at scale for Kubernetes. With any of the trio, you can now deploy DAS in just minutes and have access to more than 100 built-in policies. These new offerings enable a self-service experience and eliminate the need for learning and custom coding OPA policies for Kubernetes admission control.\nIf you, like me, prefer to see code examples, the policies alone are worth the price of admission.\nWhile it’s not quite turnkey \u0014 every company has their own policies \u0014 it’s close. DAS provides a single control plane for authorization both within applications and for the infrastructure they run upon. With it, you get easy-to-deploy security, compliance and operational guardrails for both Kubernetes and microservices to help customers mitigate risk, reduce errors and accelerate software development. Styra’s not the only one singing its approach praises. According to the Gartner report, Market Guide for Compliance Automation Tools in DevOps, \u001cAs organizations migrate workloads to the cloud or move from virtualized to containerized environments, I&O leaders must evaluate existing tools that protect cloud and container-based infrastructure. These tools enable enforcing infrastructure compliance policies to minimize configuration-related risks. Opportunities exist for the orchestration of policies over distinct agile infrastructure environments. Specifically, the OPA open source initiative has started to emerge as a source for an ecosystem of startups building enterprise capabilities over OPA.”\nYou can see for yourself what’s all the fuss is about with the new DAS Free. This is a completely free, self-service option for up to two clusters or 10 nodes to streamline the adoption process. For teams with larger production scale needs, DAS Pro offers a clear and transparent pricing model, for up to 50 nodes, to protect and manage Kubernetes clusters as they grow from initial testing/deployment to full production environments. Finally, DAS Enterprise gives teams unlimited OPA deployments and rules with around the clock support. Regardless of the version, all have access to the same management plane, policy libraries, impact analysis, monitoring, and decision logging.\n\u001cThese new editions will benefit any number of teams beginning their Kubernetes journey,\u001d said Tim Hinrichs, co-creator of OPA and Styra’s CTO. \u001cIt will also help platform engineers new to OPA who want to deploy community best practices immediately without custom coding. Ultimately, this will help lessen the burden for anyone who needs to monitor, validate and test Kubernetes admission control with OPA.” Featured image via Pixabay. ",
      "publishedDate": "2020-11-18T17:00:05.000Z",
      "description": "Styra has announced a new three-tier product offering for Styra Declarative Authorization Service (DAS), built on the Open POlicy Agent (OPA)",
      "ogDescription": "Styra has announced a new three-tier product offering for Styra Declarative Authorization Service (DAS), built on the Open POlicy Agent (OPA)"
    },
    {
      "url": "https://www.capitalone.com/tech/solutions/container-orchestration/",
      "title": "Container Orchestration | Critical Stack by Capital One",
      "content": "<div class=\"grv-col--sm-4 grv-col--lg-6 checkered__content\"><p class=\"checkered__text\">Easily manage Kubernetes container orchestration. Critical Stack helps you create and deploy new services while enforcing common governance controls like container networking and Role Based Access Control.</p></div>",
      "contentAsText": "Easily manage Kubernetes container orchestration. Critical Stack helps you create and deploy new services while enforcing common governance controls like container networking and Role Based Access Control.",
      "description": "An enterprise container orchestration tool built for the most demanding regulatory environments. Eliminate configuration challenges with Critical Stack.",
      "ogDescription": "An enterprise container orchestration tool built for the most demanding regulatory environments. Eliminate configuration challenges with Critical Stack."
    },
    {
      "url": "https://www.p3r.one/how-to-become-a-certified-kubernetes-application-developer-in-2020/",
      "title": "How to become a Certified Kubernetes Application Developer in 2020 | p3r.",
      "content": "<div class=\"et_pb_module et_pb_post_content et_pb_post_content_0_tb_body\">\n<p>In this article, I will be describing my experience preparing for and taking the CKAD exam. A little bit about my background to relate with if you\u0019re also on the same boat as I was: I am primarily a Software Developer; I\u0019ve used mostly Python and Go. I have some experience in shell scripting and nearly negligible experience in DevOps.</p>\n<p>First, we\u0019ll spell out what the Certified Kubernetes Application Developer exam is. According to <a href=\"https://www.cncf.io/certification/ckad/\" class=\"rank-math-link\">the CNCF website</a>:</p>\n<blockquote class=\"wp-block-quote\"><p>\u001cThe Certified Kubernetes Application Developer exam certifies that users can design, build, configure, and expose cloud-native applications for Kubernetes.\u001d </p><cite><a href=\"https://www.cncf.io/certification/ckad/\" class=\"rank-math-link\">CNCF</a></cite></blockquote>\n<p>I recommend going through the CNCF website to understand the curriculum, exam tips, and other FAQs.</p>\n<p>We\u0019ll now dig into what it took me, an absolute beginner to DevOps, to prepare for the CKAD exam.</p>\n<p>There are a few fundamental prerequisites such as Linux know-how, YAML configuration, Docker, etc., that one must know before proceeding to the Kubernetes world. I find the <a href=\"https://kodekloud.com/p/learning-path\" class=\"rank-math-link\">learning path</a> recommended on the <a href=\"https://kodekloud.com/courses/\">KodeKloud</a> website to be especially helpful to plan the order of learning the prerequisites.</p>\n<h2>Certified Kubernetes Application Developer materials and courses</h2>\n<p class=\"wp-block-image\"><figure class=\"aligncenter size-full\"><img width=\"705\" src=\"https://www.p3r.one/wp-content/uploads/2020/11/ckad-course-image.png\" alt=\"Certified Kubernetes Application Developer\" class=\"wp-image-1034\" srcset=\"https://www.p3r.one/wp-content/uploads/2020/11/ckad-course-image.png 705w, https://www.p3r.one/wp-content/uploads/2020/11/ckad-course-image-480x270.png 480w\" sizes=\"(min-width: 0px) and (max-width: 480px) 480px, (min-width: 481px) 705px, 100vw\"><figcaption>Certified Kubernetes Application Developer at KodeKloud</figcaption></figure></p>\n<p>I did the following courses offered by KodeKloud (this is most definitely not a plug for KodeKloud, you may learn all of these from other sources if you wish) in the order listed below.</p>\n<p><strong>Linux </strong>\u0013 <strong>The Linux Basics Course</strong>. This course might not be necessary for everyone. If you\u0019re familiar with the Linux terminal, basic commands, and comfortable using the Linux terminal, then you\u0019re good to go.</p>\n<p><strong>Automation</strong> \u0013 <strong>Ansible for Beginners.</strong> It is essential to understand how the YAML configuration works and know the data structures used in a YAML file. This course explains all of this and provides exercises for hands-on practising, among other topics.</p>\n<p><strong>Docker \u0013 Docker for Absolute Beginners, Docker &#x2013; SWARM | SERVICES | STACKS</strong>. Before jumping directly to Kubernetes, I think it is essential to understand the fundamentals of containers and container-based systems. Learning about the Docker platform is a great way to start with it. Once you are familiar with the basics of Docker, proceed further to learn advanced topics such as Docker Swarm, Services, etc., These courses should provide you with a pretty strong foundation to being learning Kubernetes.</p>\n<p><strong>Kubernetes \u0013 Kubernetes for the Absolute Beginners, Certified Kubernetes Application Developer</strong>. These courses explain the core concepts in Kubernetes very well. The content of the CKAD course is tailor-made to fit the CKAD exam curriculum. Working through these courses, their practice materials, and mock exam should give one enough idea about the actual exam. I would also like to add that only these courses might not be sufficient to excel in the exam. Find more resources to help you practice.</p>\n<p>What I find noteworthy about these courses is that KodeKloud provides a virtual lab to practice what you have learnt right away. Personally, for me, this is a huge plus. If you have only been watching a bunch of videos without getting your hands dirty, then it is not going to be of much use.</p>\n<p>Other helpful resources to prepare for the exam:</p> <h2>Tips for pulling off the Certified Kubernetes Application Developer exam</h2>\n<p>There are a few key aspects that will help you with the exam.</p>\n<ol type=\"1\"><li><strong>Familiarity</strong> \u0013 Familiarize yourself with the exam curriculum, the kubectl command, exporting to a yaml definition file (instead of writing one from scratch), and editing it as needed.</li><li><strong>Practice</strong> &#x2013; The more you practice, the more familiar you are with the commands and shortcuts you learn along the way. Practising will also help you respond faster.</li><li><strong>Speed</strong> \u0013 Speed is particularly crucial to ace the exam. You have about 2 hours to answer 19 questions. And each of these questions may have sub-tasks. Again, at the cost of repeating myself, practice, practice, keep practising!</li><li><strong>Helpful resources during the exam</strong> &#x2013; You do not have to remember definitions of all the Kubernetes resources. You have access to Kubernetes documentation during the exam to look up any information you want. Besides documentation, you can also use the <strong>\u0018kubectl explain\u0019</strong> command to look up any resource. Learn how to use the <strong>\u0018kubectl explain\u0019 </strong>command to your advantage, as this will save you a lot of time.</li><li><strong>Time awareness </strong>\u0013 Be aware of the amount of time you spend on each question. Some might require only a couple of minutes, while others require more. Skim through all the questions quickly, make a mental estimate of the time you will have to spend on each of them.</li><li><strong>WiFi \u0013 </strong>Ensure that you have seamless internet connectivity to avoid disruptions during your exam.</li><li>Be physically and mentally prepared to sit through the exam for about 2 hours with no break. I know you could take breaks in between, but that will cost you time. So, be prepared.</li></ol>\n<p>Ensure that you\u0019re well-rested. Do not stress yourself about the exam as it will only hinder your performance. Even if you do not pass the exam on your first try, you can retake the exam once more. You will have gained insight into the exam expectations, which will surely help you prepare well for your next try. Keep calm and give your best =\n</p>\n</div>",
      "contentAsText": "\nIn this article, I will be describing my experience preparing for and taking the CKAD exam. A little bit about my background to relate with if you\u0019re also on the same boat as I was: I am primarily a Software Developer; I\u0019ve used mostly Python and Go. I have some experience in shell scripting and nearly negligible experience in DevOps.\nFirst, we\u0019ll spell out what the Certified Kubernetes Application Developer exam is. According to the CNCF website:\n\u001cThe Certified Kubernetes Application Developer exam certifies that users can design, build, configure, and expose cloud-native applications for Kubernetes.\u001d CNCF\nI recommend going through the CNCF website to understand the curriculum, exam tips, and other FAQs.\nWe\u0019ll now dig into what it took me, an absolute beginner to DevOps, to prepare for the CKAD exam.\nThere are a few fundamental prerequisites such as Linux know-how, YAML configuration, Docker, etc., that one must know before proceeding to the Kubernetes world. I find the learning path recommended on the KodeKloud website to be especially helpful to plan the order of learning the prerequisites.\nCertified Kubernetes Application Developer materials and courses\nCertified Kubernetes Application Developer at KodeKloud\nI did the following courses offered by KodeKloud (this is most definitely not a plug for KodeKloud, you may learn all of these from other sources if you wish) in the order listed below.\nLinux \u0013 The Linux Basics Course. This course might not be necessary for everyone. If you\u0019re familiar with the Linux terminal, basic commands, and comfortable using the Linux terminal, then you\u0019re good to go.\nAutomation \u0013 Ansible for Beginners. It is essential to understand how the YAML configuration works and know the data structures used in a YAML file. This course explains all of this and provides exercises for hands-on practising, among other topics.\nDocker \u0013 Docker for Absolute Beginners, Docker – SWARM | SERVICES | STACKS. Before jumping directly to Kubernetes, I think it is essential to understand the fundamentals of containers and container-based systems. Learning about the Docker platform is a great way to start with it. Once you are familiar with the basics of Docker, proceed further to learn advanced topics such as Docker Swarm, Services, etc., These courses should provide you with a pretty strong foundation to being learning Kubernetes.\nKubernetes \u0013 Kubernetes for the Absolute Beginners, Certified Kubernetes Application Developer. These courses explain the core concepts in Kubernetes very well. The content of the CKAD course is tailor-made to fit the CKAD exam curriculum. Working through these courses, their practice materials, and mock exam should give one enough idea about the actual exam. I would also like to add that only these courses might not be sufficient to excel in the exam. Find more resources to help you practice.\nWhat I find noteworthy about these courses is that KodeKloud provides a virtual lab to practice what you have learnt right away. Personally, for me, this is a huge plus. If you have only been watching a bunch of videos without getting your hands dirty, then it is not going to be of much use.\nOther helpful resources to prepare for the exam: Tips for pulling off the Certified Kubernetes Application Developer exam\nThere are a few key aspects that will help you with the exam.\nFamiliarity \u0013 Familiarize yourself with the exam curriculum, the kubectl command, exporting to a yaml definition file (instead of writing one from scratch), and editing it as needed.Practice – The more you practice, the more familiar you are with the commands and shortcuts you learn along the way. Practising will also help you respond faster.Speed \u0013 Speed is particularly crucial to ace the exam. You have about 2 hours to answer 19 questions. And each of these questions may have sub-tasks. Again, at the cost of repeating myself, practice, practice, keep practising!Helpful resources during the exam – You do not have to remember definitions of all the Kubernetes resources. You have access to Kubernetes documentation during the exam to look up any information you want. Besides documentation, you can also use the \u0018kubectl explain\u0019 command to look up any resource. Learn how to use the \u0018kubectl explain\u0019 command to your advantage, as this will save you a lot of time.Time awareness \u0013 Be aware of the amount of time you spend on each question. Some might require only a couple of minutes, while others require more. Skim through all the questions quickly, make a mental estimate of the time you will have to spend on each of them.WiFi \u0013 Ensure that you have seamless internet connectivity to avoid disruptions during your exam.Be physically and mentally prepared to sit through the exam for about 2 hours with no break. I know you could take breaks in between, but that will cost you time. So, be prepared.\nEnsure that you\u0019re well-rested. Do not stress yourself about the exam as it will only hinder your performance. Even if you do not pass the exam on your first try, you can retake the exam once more. You will have gained insight into the exam expectations, which will surely help you prepare well for your next try. Keep calm and give your best =\n\n",
      "description": "A developer's journey to mastering the CKAD exam without expensive training. Learn how to become a Certified Kubernetes Application Developer without prior expertise.",
      "ogDescription": "A developer's journey to mastering the CKAD exam without expensive training. Learn how to become a Certified Kubernetes Application Developer without prior expertise."
    },
    {
      "url": "https://sysdig.com/blog/kubernetes-native-network-security/",
      "title": "Kubernetes-native network security with Sysdig",
      "content": "<body><p>Microservices and Kubernetes have completely changed the way you reason about network security. Luckily, <strong>Kubernetes network security policies (KNP)</strong> are a native mechanism to address this issue at the correct level of abstraction. </p><p> Implementing a network policy is challenging, as<strong> developers and ops need to work together</strong> to define proper rules. However, the best approach is to adopt a zero trust framework for network security using Kubernetes native controls. </p> <p> Learn how Sysdig is closing this gap with its latest<strong> Sysdig Network Policy feature</strong> that provides Kubernetes-native network visibility, allowing both teams to author the best possible network policy, without paying the price of learning yet another policy language. It also helps meet compliance requirements (NIST, PCI, etc) that require network segmentation. </p> <p> <iframe width=\"500\" height=\"281\" src=\"https://www.youtube.com/embed/lgIB4EItah4\" class></iframe> </p> <h2 id=\"context\">Context is king in Kubernetes</h2> <p> Kubernetes communications without <strong>metadata enrichment</strong> are scribbled. </p> <p> Before you can even start thinking about security and network policies, you first need to have <strong>deep visibility</strong> into how microservices are communicating with each other. </p> <p> In a Kubernetes-world, pods are short-lived, they jump between hosts, have ephemeral IP addresses, scale up and down. All that is awesome and gives you the flexibility and reactiveness that you love. But it also means that if you look at the physical communication layer at L3/L4 (just IPs and ports, it looks like this: </p> <p> <a href=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-01-cluster-network.png\"><img src=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-01-cluster-network.png\" alt width=\"960\" class=\"img-lightbox aligncenter size-full wp-image-31614 u-no-shadow\" srcset=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-01-cluster-network.png 960w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-01-cluster-network-350x130.png 350w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-01-cluster-network-768x286.png 768w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-01-cluster-network-175x65.png 175w, https://sysdig.com/ 1w\" sizes=\"(max-width: 960px) 100vw, 960px\"></a> </p> <p> In other words, you have all the connection information, but it is not correctly aggregated and segmented. As you can expect, trying to configure classic firewall rules is not going to make it. Yes, you can start grouping the different containers according to attributes like image name, image tags, container names&#x2026; But this is laborious, error prone and remember all that information is dynamic and constantly changing, it will always be an uphill battle. </p> <p> Why reinvent the wheel when you already have all the metadata you need to reason about network communications and security at the Kubernetes level? The Kubernetes API contains all the up-to-date information about namespaces, services, deployments, etc. It also provides you a tool to create policies based on the labels assigned to those entities (<a href=\"https://kubernetes.io/docs/concepts/services-networking/network-policies/\">KNPs</a>). </p> <p> <a href=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-02-cluster-network-segmentation.png\"><img src=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-02-cluster-network-segmentation.png\" alt width=\"960\" class=\"img-lightbox aligncenter size-full wp-image-31615 u-no-shadow\" srcset=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-02-cluster-network-segmentation.png 960w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-02-cluster-network-segmentation-350x163.png 350w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-02-cluster-network-segmentation-768x358.png 768w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-02-cluster-network-segmentation-175x82.png 175w, https://sysdig.com/ 1w\" sizes=\"(max-width: 960px) 100vw, 960px\"></a> </p> <h2 id=\"creating-network-policy\">Creating a Kubernetes network policy</h2> <p>Your developers team met with your ops team to create the ultimate network policy for one of your apps. </p> <p> After one hour of meeting, this is how developers defined the app: </p> <p> <a href=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-03-app-diagram.png\"><img src=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-03-app-diagram.png\" alt width=\"960\" class=\"img-lightbox aligncenter size-full wp-image-31616 u-no-shadow\" srcset=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-03-app-diagram.png 960w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-03-app-diagram-350x113.png 350w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-03-app-diagram-768x249.png 768w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-03-app-diagram-175x57.png 175w, https://sysdig.com/ 1w\" sizes=\"(max-width: 960px) 100vw, 960px\"></a> </p> <ul><li>The &#x201C;example-java-app&#x201D; queries two mongoDB databases, one local to the cluster another external, it also needs a redis cache. </li><li>It receives requests from an &#x201C;example-java-client&#x201D; app. </li></ul><p> And this is the network policy they came up with. </p> <p>Let&#x2019;s start the policy with the usual metadata. This is a <code>NetworkPolicy</code>: </p> <pre class=\"prettyprint\">apiVersion: networking.k8s.io/v1\r\nkind: NetworkPolicy\r\nmetadata:\r\n  name: generated-network-policy\r\n  namespace: example-java-app\r\n</pre> <p> Then the network rules. Starting with an ingress rule so our <code>example-java-app</code> can accept requests from the client app on port 8080: </p> <pre class=\"prettyprint\">spec:\r\n  ingress:\r\n    - from:\r\n        - namespaceSelector:\r\n            matchLabels:\r\n              app: raw\r\n              chart: raw-0.2.3\r\n              heritage: Helm\r\n              release: namespaces\r\n          podSelector:\r\n            matchLabels:\r\n              app.kubernetes.io/instance: example-java-app\r\n              app.kubernetes.io/name: example-java-app-jclient\r\n      ports:\r\n        - port: 8080\r\n          protocol: TCP\r\n</pre> <p> Then egress rules so our app can connect to our databases: mongodb, and redis. </p> <pre class=\"prettyprint\">  egress:\r\n    - to:\r\n        - namespaceSelector:\r\n            matchLabels:\r\n              app: raw\r\n              chart: raw-0.2.3\r\n              heritage: Helm\r\n              release: namespaces\r\n          podSelector:\r\n            matchLabels:\r\n              app.kubernetes.io/instance: example-java-app\r\n              app.kubernetes.io/name: example-java-app-mongo\r\n      ports:\r\n        - port: 27017\r\n          protocol: TCP\r\n    - to:\r\n        - namespaceSelector:\r\n            matchLabels:\r\n              app: raw\r\n              chart: raw-0.2.3\r\n              heritage: Helm\r\n              release: namespaces\r\n          podSelector:\r\n            matchLabels:\r\n              app.kubernetes.io/instance: example-java-app\r\n              app.kubernetes.io/name: example-java-app-redis\r\n      ports:\r\n        - port: 6379\r\n          protocol: TCP\r\n</pre> <p>You finish by specifying this will apply to the pod named <code>example-java-app-javaapp</code>: </p> <pre class=\"prettyprint\">  podSelector:\r\n    matchLabels:\r\n      app.kubernetes.io/instance: example-java-app\r\n      app.kubernetes.io/name: example-java-app-javaapp\r\n  policyTypes:\r\n    - Ingress\r\n    - Egress\r\n</pre> <p> Wow, this is a long YAML for such a simple app. A network policy for a regular app can expand up to thousands of lines. </p> <p> And this is the first caveat of Kubernetes network policies, you need to learn yet another domain specific language to create them. Not every developer will&#xA0; be able to or want to,&#xA0; and the ops team doesn&#x2019;t know enough about your apps to create them. They need to work together. </p> <p> <strong>Let&#x2019;s deploy this network policy in production!</strong> </p> <p> After applying the policy, the app stops working &#x1F631; </p> <p> <a href=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-04-this-is-fine.gif\"><img src=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-04-this-is-fine.gif\" alt width=\"480\" class=\"img-lightbox aligncenter size-full wp-image-31617 u-no-shadow\"></a> </p> <p> And this is another caveat of the process of creating Kubernetes network policies. Troubleshooting needs several stakeholders involved, which slows down the process. It is really easy to forget about some services or common KNP pitfalls when creating a network policy by hand. </p> <p> <strong>What did you miss?</strong> </p> <ul><li>You forgot a connection to an external service, ip <code>192.229.173.207</code>. </li><li>There was a deprecated connection to a Cassandra database, not fully removed. </li><li>You forgot to allow DNS, so although services could talk to each other, they cannot resolve their service names to IP addresses. </li></ul><p> Of course, the teams didn&#x2019;t find them all at once. It was a trial and error process that spanned a few hours. You fixed it by completely removing the Cassandra dependencies, and adding these egress rules: </p> <p> Rule to allow traffic to an external IP: </p> <pre class=\"prettyprint\">[...]\r\nspec:\r\n[...]\r\n  egress:\r\n  [...]\r\n    - to:\r\n        - ipBlock:\r\n            cidr: 192.229.173.207/32\r\n            except: []\r\n[...]\r\n</pre> <p> Rule to allow traffic for DNS resolution: </p> <pre class=\"prettyprint\">[...]\r\nspec:\r\n[...]\r\n  egress:\r\n  [...]\r\n    - to:\r\n        - namespaceSelector: {}\r\n      ports:\r\n        - port: 53\r\n          protocol: UDP\r\n[...]\r\n</pre> <p> And finally, after applying this updated policy, everything is working as expected. And our team rest assured that our app network is secured. </p> <h2 id=\"challenge\">The challenge of creating Kubernetes network policies</h2> <p> Let&#x2019;s briefly stop to analyze why this process didn&#x2019;t make people happy. </p> <h3>Kubernetes network policies are not trivial</h3> <p> Ok, it&#x2019;s easy to start using them. KNPs come out of the box in Kubernetes, assuming you deployed a CNI that supports them, like Calico. But still, they have many implementation details and caveats: </p> <ul><li>They are <strong>allow-only</strong>, everything that is not explicitly allowed is forbidden. </li><li>They are <strong>based on the entities</strong> (pods, namespaces) labels, not on their names, which can be a little misleading at first. </li><li>You need to <strong>enable DNS traffic explicitly</strong>, at least inside the cluster. </li><li>Etc&#x2026; </li></ul><p> They are another policy language to learn and master. </p> <h3>The information needed is spread across teams</h3> <p> <strong>Developers</strong> typically have a very accurate picture of what the application should be doing at the functional level: Communicating with the database, pulling data from an external feed, etc. But are not the ones applying and managing the KNPs in production clusters. </p> <p> <strong>Ops teams</strong> live and breath Kubernetes, but they have a more limited visibility on how the application behaves internally, and the network connections it actually requires to function properly. </p> <p> The reality is that <strong>you need both stakeholders&#x2019; input</strong> to generate the best possible network policy for your microservice. </p> <p> KNPs&#x2019; learning curve and the back and forth of information between teams with different scopes have traditionally been a pain point. Some organizations have settled for a very broad policy (i.e. microservices can communicate with each other if they are in the same namespace), which is better than nothing and simple enough to be followed by everyone, but definitely not the most accurate enforcement. </p> <h2 id=\"sysdig\">Enter Sysdig Kubernetes network policies</h2> <p> What if you could just describe what your application is doing, in your own language, and then somebody will translate this for you to KNPs (or whatever other policy language you use in the future)? </p> <p> Let&#x2019;s see how Sysdig can help you create accurate network policies, without the pain. </p> <p> Today, Sysdig announces a new Kubernetes network policies feature, which delivers: </p> <ul><li><strong>Out-of-the-box visibility</strong> into all network traffic between apps and services, with a visual topology map to help you identify communications. </li><li><strong>Baseline network policy</strong> that you can directly refine and modify to match your desired declarative state. </li><li><strong>Automated KNPs generation</strong> based on the topology baseline + user defined adjustments. </li></ul><h3>Hands on!</h3> <p> Let&#x2019;s see how you can use the new Kubernetes network policies feature in Sysdig to create an accurate policy for our <code>example-java-app</code> in a few minutes. </p> <p> You can create a network policy by jumping into the <em>Sysdig interface</em> -&gt; <em>Policies</em> -&gt; <em>Network Security policies</em> </p> <p> <a href=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-05-topology-map-before.png\"><img src=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-05-topology-map-before.png\" alt width=\"1999\" class=\"img-lightbox aligncenter size-full wp-image-31618 u-drop-shadow\" srcset=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-05-topology-map-before.png 1999w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-05-topology-map-before-350x183.png 350w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-05-topology-map-before-1170x610.png 1170w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-05-topology-map-before-768x401.png 768w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-05-topology-map-before-1536x801.png 1536w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-05-topology-map-before-175x91.png 175w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-05-topology-map-before-1930x1007.png 1930w, https://sysdig.com/ 1w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"></a> </p> <h3>Gathering information to create a Kubernetes network policy</h3> <p> First, select the set of pods that you want to create a policy for. </p> <p> Starting with by the cluster, and the namespace where the app lives: </p> <p> <a href=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-06-cluster-selector.png\"><img src=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-06-cluster-selector.png\" alt width=\"665\" class=\"img-lightbox aligncenter size-full wp-image-31619 u-drop-shadow\" srcset=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-06-cluster-selector.png 665w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-06-cluster-selector-350x56.png 350w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-06-cluster-selector-175x28.png 175w, https://sysdig.com/ 1w\" sizes=\"(max-width: 665px) 100vw, 665px\"></a> </p> <p> Then select the Pod Owner, or the group of pods, you want to apply the policy. You can group Pods by Service, Deployment, StatefulSet, DaemonSet, or Job. </p> <p> In this case let&#x2019;s look to our infrastructure from a <em>Deployment</em> perspective: </p> <p> <a href=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-07-pod-owner-selector.png\"><img src=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-07-pod-owner-selector.png\" alt width=\"297\" class=\"img-lightbox aligncenter size-full wp-image-31620 u-drop-shadow\" srcset=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-07-pod-owner-selector.png 297w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-07-pod-owner-selector-175x206.png 175w, https://sysdig.com/ 1w\" sizes=\"(max-width: 297px) 100vw, 297px\"></a> </p> <p> Three hours of data will be enough for Sysdig to understand the network connections from <code>example-java-app</code>. </p> <p> <a href=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-08-time-selector.png\"><img src=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-08-time-selector.png\" alt width=\"724\" class=\"img-lightbox aligncenter size-full wp-image-31621 u-drop-shadow\" srcset=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-08-time-selector.png 724w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-08-time-selector-350x33.png 350w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-08-time-selector-175x16.png 175w, https://sysdig.com/ 1w\" sizes=\"(max-width: 724px) 100vw, 724px\"></a> </p> <p> You <strong>don&#x2019;t need to wait</strong> for Sysdig to actually collect network data. As the Sysdig agent has been deployed in this cluster for a while, Sysdig can use existing data from the activity audit. </p> <p> Right after you select these parameters, Sysdig shows the observed network topology map for our app deployment: </p> <p> <a href=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-09-topology-map-detail.png\"><img src=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-09-topology-map-detail.png\" alt width=\"1912\" class=\"img-lightbox aligncenter size-full wp-image-31622 u-drop-shadow\" srcset=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-09-topology-map-detail.png 1912w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-09-topology-map-detail-350x129.png 350w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-09-topology-map-detail-1170x433.png 1170w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-09-topology-map-detail-768x284.png 768w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-09-topology-map-detail-1536x568.png 1536w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-09-topology-map-detail-175x65.png 175w, https://sysdig.com/ 1w\" sizes=\"(max-width: 1912px) 100vw, 1912px\"></a> </p> <p> There are some interesting keys here. </p> <p> This map shows all <strong>the current communications</strong> (connectors and ports). Including those that you missed in our initial approach, like the cassandra database, and the external ip. </p> <p> Note that the <strong>communications are shown at the Kubernetes metadata level</strong>. You can see the &#x201C;client&#x201D; application pushing requests to a service, then they are forwarded to deployment pods. </p> <p> It <strong>proposes a security policy</strong>, color-coded. Unresolved IPs, the entities that cannot be matched against a Kubernetes entity, are excluded by default and displayed in red. </p> <p> It is clean, simple and contains all the relevant information you need. </p> <h3>Tweaking a Kubernetes network policy</h3> <p> Now, as a developer with knowledge of the application, there are a few details you&#x2019;ll want to tweak: </p> <ul><li>I want to allow that specific external IP. </li><li>There are still connections to the cassandra service that you don&#x2019;t want to allow moving forward. </li></ul><p> Let&#x2019;s do so from the <em>Ingress</em> and <em>Egress</em> tabs on the interface. </p> <p> The Ingress / Egress tables expand and detail the information found in the Topology map: </p> <p> <a href=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-10-egress-table-before.png\"><img src=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-10-egress-table-before.png\" alt width=\"1999\" class=\"img-lightbox aligncenter size-full wp-image-31623 u-drop-shadow\" srcset=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-10-egress-table-before.png 1999w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-10-egress-table-before-350x151.png 350w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-10-egress-table-before-1170x505.png 1170w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-10-egress-table-before-768x331.png 768w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-10-egress-table-before-1536x662.png 1536w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-10-egress-table-before-175x75.png 175w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-10-egress-table-before-1930x832.png 1930w, https://sysdig.com/ 1w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"></a> </p> <p> <a href=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-14-ingress-table.png\"><img src=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-14-ingress-table.png\" alt width=\"1999\" class=\"img-lightbox aligncenter size-full wp-image-31627 u-drop-shadow\" srcset=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-14-ingress-table.png 1999w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-14-ingress-table-350x102.png 350w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-14-ingress-table-1170x341.png 1170w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-14-ingress-table-768x224.png 768w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-14-ingress-table-1536x448.png 1536w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-14-ingress-table-175x51.png 175w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-14-ingress-table-1930x563.png 1930w, https://sysdig.com/ 1w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"></a> </p> <p> The <strong>extra information</strong> does help you identify the individual communications. For example: &#x201C;Which process is initiating the egress connection?&#x201D; </p> <p> They are <strong>actionable</strong>: </p> <ul><li>You can cherry-pick the communications that you want to allow from the table. </li><li>You can look at the unresolved IPs and decide if you want to allow them moving forward. </li></ul><p>Let&#x2019;s add an IP/mask combination to allow the external IP, as it belongs to a trusted service. </p> <p>Let&#x2019;s uncheck the row for the cassandra deployment, as you no longer need that communication. </p> <p> <a href=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-11-egress-table-after.png\"><img src=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-11-egress-table-after.png\" alt width=\"1999\" class=\"img-lightbox aligncenter size-full wp-image-31624 u-drop-shadow\" srcset=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-11-egress-table-after.png 1999w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-11-egress-table-after-350x163.png 350w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-11-egress-table-after-1170x544.png 1170w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-11-egress-table-after-768x357.png 768w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-11-egress-table-after-1536x714.png 1536w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-11-egress-table-after-175x81.png 175w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-11-egress-table-after-1930x897.png 1930w, https://sysdig.com/ 1w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"></a> </p> <p> Sysdig will automatically detect that our IP belongs to a network that is external to the cluster, and flags it as such. </p> <p> The network topology map will be automatically updated to reflect the changes: </p> <p> <a href=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-12-topology-map-after.png\"><img src=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-12-topology-map-after.png\" alt width=\"1965\" class=\"img-lightbox aligncenter size-full wp-image-31625 u-drop-shadow\" srcset=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-12-topology-map-after.png 1965w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-12-topology-map-after-350x143.png 350w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-12-topology-map-after-1170x477.png 1170w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-12-topology-map-after-768x313.png 768w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-12-topology-map-after-1536x626.png 1536w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-12-topology-map-after-175x71.png 175w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-12-topology-map-after-1930x787.png 1930w, https://sysdig.com/ 1w\" sizes=\"(max-width: 1965px) 100vw, 1965px\"></a> </p> <p> Notice the connection to the external ip is no longer red, but the connection to cassandra is. </p> <h3>Generating a Kubernetes network policy</h3> <p> Now that our policy looks exactly how you want it to look, you can generate the Kubernetes Network Policy YAML. </p> <p> It will be generated on the fly just clicking on the &#x201C;Generated Policy&#x201D; tab: </p> <p> <a href=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-13-generated-network-policy.png\"><img src=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-13-generated-network-policy.png\" alt width=\"1999\" class=\"img-lightbox aligncenter size-full wp-image-31626 u-drop-shadow\" srcset=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-13-generated-network-policy.png 1999w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-13-generated-network-policy-350x155.png 350w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-13-generated-network-policy-1170x518.png 1170w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-13-generated-network-policy-768x340.png 768w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-13-generated-network-policy-1536x680.png 1536w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-13-generated-network-policy-175x77.png 175w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/Kubernetes-native-network-security-13-generated-network-policy-1930x854.png 1930w, https://sysdig.com/ 1w\" sizes=\"(max-width: 1999px) 100vw, 1999px\"></a> </p> <p> Indicate here that the file goes on </p> <p> Now I can attach this policy artifact together with my application for the DevOps team. </p> <h3>Applying the Kubernetes network policy</h3> <p> As you can see in the workflow above, Sysdig is helping you generate the best policy by aggregating the observed network behavior and the user adjustments. </p> <p> But you are not actually <strong>enforcing</strong> this policy. </p> <p> Let&#x2019;s leave that to the Kubernetes control plane and CNI plugin. And this has some advantages: </p> <ul><li>It&#x2019;s a <strong>native approach</strong> that leverages out-of-the-box Kubernetes capabilities. </li><li>You<strong> avoid directly tampering</strong> with the network communications, host iptables, and the TCP/IP stack. </li><li>It&#x2019;s <strong>portable</strong>. You can apply this policy in all your identical applications / namespaces, it will work on most Kubernetes flavors like OpenShift and Rancher. </li></ul><h2 id=\"feature\">What is so exciting about this feature?</h2> <p> First, you <strong>didn&#x2019;t have to understand the low level detail</strong> of how KNPs work. If you are able to describe your application network behavior using a visual flow, Sysdig will do the translation for you. </p> <p> The topology map provided <strong>network visibility</strong> into all communication across a service / app / namespace / tag. So you won&#x2019;t forget to include any service. </p> <p> You also <strong>don&#x2019;t start from scratch</strong>. By baselining your network policy you only perform a 10% lift, which saves you time. Then Sysdig auto generates the YAML. </p> <p> Devops and security teams only need to sign off and check the policy. </p> <p> Finally, it leverages Kubernetes <strong>native</strong> controls, <strong>decoupling</strong> policy definition and enforcement. As such, it is not intrusive, and it doesn&#x2019;t imply a performance hit. Also, if you lose connection to Sysdig your policies are still on and your network communications will stay up. </p> <h2 id=\"conclusion\">Conclusion</h2> <p> To address the Kubernetes network security requirements, Sysdig Secure just introduced its <strong>Sysdig Network Policy</strong> feature. </p> <p> Using this feature you will: </p> <ul><li>Get automatic visibility for your microservices and their communications, using the Kubernetes metadata to abstract away all the physical-layer noise. </li><li>Apply least-privilege microsegmentation for your services </li><li>Automatically generate the KNPs that derives from your input, no previous KNP expertise required. </li></ul><p> You can check this feature today, <a href=\"https://sysdig.com/company/free-trial/\">start a free trial</a>! </p></body>",
      "contentAsText": "Microservices and Kubernetes have completely changed the way you reason about network security. Luckily, Kubernetes network security policies (KNP) are a native mechanism to address this issue at the correct level of abstraction.  Implementing a network policy is challenging, as developers and ops need to work together to define proper rules. However, the best approach is to adopt a zero trust framework for network security using Kubernetes native controls.   Learn how Sysdig is closing this gap with its latest Sysdig Network Policy feature that provides Kubernetes-native network visibility, allowing both teams to author the best possible network policy, without paying the price of learning yet another policy language. It also helps meet compliance requirements (NIST, PCI, etc) that require network segmentation.     Context is king in Kubernetes  Kubernetes communications without metadata enrichment are scribbled.   Before you can even start thinking about security and network policies, you first need to have deep visibility into how microservices are communicating with each other.   In a Kubernetes-world, pods are short-lived, they jump between hosts, have ephemeral IP addresses, scale up and down. All that is awesome and gives you the flexibility and reactiveness that you love. But it also means that if you look at the physical communication layer at L3/L4 (just IPs and ports, it looks like this:      In other words, you have all the connection information, but it is not correctly aggregated and segmented. As you can expect, trying to configure classic firewall rules is not going to make it. Yes, you can start grouping the different containers according to attributes like image name, image tags, container names… But this is laborious, error prone and remember all that information is dynamic and constantly changing, it will always be an uphill battle.   Why reinvent the wheel when you already have all the metadata you need to reason about network communications and security at the Kubernetes level? The Kubernetes API contains all the up-to-date information about namespaces, services, deployments, etc. It also provides you a tool to create policies based on the labels assigned to those entities (KNPs).     Creating a Kubernetes network policy Your developers team met with your ops team to create the ultimate network policy for one of your apps.   After one hour of meeting, this is how developers defined the app:     The “example-java-app” queries two mongoDB databases, one local to the cluster another external, it also needs a redis cache. It receives requests from an “example-java-client” app.  And this is the network policy they came up with.  Let’s start the policy with the usual metadata. This is a NetworkPolicy:  apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: generated-network-policy\n  namespace: example-java-app\n  Then the network rules. Starting with an ingress rule so our example-java-app can accept requests from the client app on port 8080:  spec:\n  ingress:\n    - from:\n        - namespaceSelector:\n            matchLabels:\n              app: raw\n              chart: raw-0.2.3\n              heritage: Helm\n              release: namespaces\n          podSelector:\n            matchLabels:\n              app.kubernetes.io/instance: example-java-app\n              app.kubernetes.io/name: example-java-app-jclient\n      ports:\n        - port: 8080\n          protocol: TCP\n  Then egress rules so our app can connect to our databases: mongodb, and redis.    egress:\n    - to:\n        - namespaceSelector:\n            matchLabels:\n              app: raw\n              chart: raw-0.2.3\n              heritage: Helm\n              release: namespaces\n          podSelector:\n            matchLabels:\n              app.kubernetes.io/instance: example-java-app\n              app.kubernetes.io/name: example-java-app-mongo\n      ports:\n        - port: 27017\n          protocol: TCP\n    - to:\n        - namespaceSelector:\n            matchLabels:\n              app: raw\n              chart: raw-0.2.3\n              heritage: Helm\n              release: namespaces\n          podSelector:\n            matchLabels:\n              app.kubernetes.io/instance: example-java-app\n              app.kubernetes.io/name: example-java-app-redis\n      ports:\n        - port: 6379\n          protocol: TCP\n You finish by specifying this will apply to the pod named example-java-app-javaapp:    podSelector:\n    matchLabels:\n      app.kubernetes.io/instance: example-java-app\n      app.kubernetes.io/name: example-java-app-javaapp\n  policyTypes:\n    - Ingress\n    - Egress\n  Wow, this is a long YAML for such a simple app. A network policy for a regular app can expand up to thousands of lines.   And this is the first caveat of Kubernetes network policies, you need to learn yet another domain specific language to create them. Not every developer will  be able to or want to,  and the ops team doesn’t know enough about your apps to create them. They need to work together.   Let’s deploy this network policy in production!   After applying the policy, the app stops working 😱      And this is another caveat of the process of creating Kubernetes network policies. Troubleshooting needs several stakeholders involved, which slows down the process. It is really easy to forget about some services or common KNP pitfalls when creating a network policy by hand.   What did you miss?  You forgot a connection to an external service, ip 192.229.173.207. There was a deprecated connection to a Cassandra database, not fully removed. You forgot to allow DNS, so although services could talk to each other, they cannot resolve their service names to IP addresses.  Of course, the teams didn’t find them all at once. It was a trial and error process that spanned a few hours. You fixed it by completely removing the Cassandra dependencies, and adding these egress rules:   Rule to allow traffic to an external IP:  [...]\nspec:\n[...]\n  egress:\n  [...]\n    - to:\n        - ipBlock:\n            cidr: 192.229.173.207/32\n            except: []\n[...]\n  Rule to allow traffic for DNS resolution:  [...]\nspec:\n[...]\n  egress:\n  [...]\n    - to:\n        - namespaceSelector: {}\n      ports:\n        - port: 53\n          protocol: UDP\n[...]\n  And finally, after applying this updated policy, everything is working as expected. And our team rest assured that our app network is secured.  The challenge of creating Kubernetes network policies  Let’s briefly stop to analyze why this process didn’t make people happy.  Kubernetes network policies are not trivial  Ok, it’s easy to start using them. KNPs come out of the box in Kubernetes, assuming you deployed a CNI that supports them, like Calico. But still, they have many implementation details and caveats:  They are allow-only, everything that is not explicitly allowed is forbidden. They are based on the entities (pods, namespaces) labels, not on their names, which can be a little misleading at first. You need to enable DNS traffic explicitly, at least inside the cluster. Etc…  They are another policy language to learn and master.  The information needed is spread across teams  Developers typically have a very accurate picture of what the application should be doing at the functional level: Communicating with the database, pulling data from an external feed, etc. But are not the ones applying and managing the KNPs in production clusters.   Ops teams live and breath Kubernetes, but they have a more limited visibility on how the application behaves internally, and the network connections it actually requires to function properly.   The reality is that you need both stakeholders’ input to generate the best possible network policy for your microservice.   KNPs’ learning curve and the back and forth of information between teams with different scopes have traditionally been a pain point. Some organizations have settled for a very broad policy (i.e. microservices can communicate with each other if they are in the same namespace), which is better than nothing and simple enough to be followed by everyone, but definitely not the most accurate enforcement.  Enter Sysdig Kubernetes network policies  What if you could just describe what your application is doing, in your own language, and then somebody will translate this for you to KNPs (or whatever other policy language you use in the future)?   Let’s see how Sysdig can help you create accurate network policies, without the pain.   Today, Sysdig announces a new Kubernetes network policies feature, which delivers:  Out-of-the-box visibility into all network traffic between apps and services, with a visual topology map to help you identify communications. Baseline network policy that you can directly refine and modify to match your desired declarative state. Automated KNPs generation based on the topology baseline + user defined adjustments. Hands on!  Let’s see how you can use the new Kubernetes network policies feature in Sysdig to create an accurate policy for our example-java-app in a few minutes.   You can create a network policy by jumping into the Sysdig interface -> Policies -> Network Security policies     Gathering information to create a Kubernetes network policy  First, select the set of pods that you want to create a policy for.   Starting with by the cluster, and the namespace where the app lives:      Then select the Pod Owner, or the group of pods, you want to apply the policy. You can group Pods by Service, Deployment, StatefulSet, DaemonSet, or Job.   In this case let’s look to our infrastructure from a Deployment perspective:      Three hours of data will be enough for Sysdig to understand the network connections from example-java-app.      You don’t need to wait for Sysdig to actually collect network data. As the Sysdig agent has been deployed in this cluster for a while, Sysdig can use existing data from the activity audit.   Right after you select these parameters, Sysdig shows the observed network topology map for our app deployment:      There are some interesting keys here.   This map shows all the current communications (connectors and ports). Including those that you missed in our initial approach, like the cassandra database, and the external ip.   Note that the communications are shown at the Kubernetes metadata level. You can see the “client” application pushing requests to a service, then they are forwarded to deployment pods.   It proposes a security policy, color-coded. Unresolved IPs, the entities that cannot be matched against a Kubernetes entity, are excluded by default and displayed in red.   It is clean, simple and contains all the relevant information you need.  Tweaking a Kubernetes network policy  Now, as a developer with knowledge of the application, there are a few details you’ll want to tweak:  I want to allow that specific external IP. There are still connections to the cassandra service that you don’t want to allow moving forward.  Let’s do so from the Ingress and Egress tabs on the interface.   The Ingress / Egress tables expand and detail the information found in the Topology map:         The extra information does help you identify the individual communications. For example: “Which process is initiating the egress connection?”   They are actionable:  You can cherry-pick the communications that you want to allow from the table. You can look at the unresolved IPs and decide if you want to allow them moving forward. Let’s add an IP/mask combination to allow the external IP, as it belongs to a trusted service.  Let’s uncheck the row for the cassandra deployment, as you no longer need that communication.      Sysdig will automatically detect that our IP belongs to a network that is external to the cluster, and flags it as such.   The network topology map will be automatically updated to reflect the changes:      Notice the connection to the external ip is no longer red, but the connection to cassandra is.  Generating a Kubernetes network policy  Now that our policy looks exactly how you want it to look, you can generate the Kubernetes Network Policy YAML.   It will be generated on the fly just clicking on the “Generated Policy” tab:      Indicate here that the file goes on   Now I can attach this policy artifact together with my application for the DevOps team.  Applying the Kubernetes network policy  As you can see in the workflow above, Sysdig is helping you generate the best policy by aggregating the observed network behavior and the user adjustments.   But you are not actually enforcing this policy.   Let’s leave that to the Kubernetes control plane and CNI plugin. And this has some advantages:  It’s a native approach that leverages out-of-the-box Kubernetes capabilities. You avoid directly tampering with the network communications, host iptables, and the TCP/IP stack. It’s portable. You can apply this policy in all your identical applications / namespaces, it will work on most Kubernetes flavors like OpenShift and Rancher. What is so exciting about this feature?  First, you didn’t have to understand the low level detail of how KNPs work. If you are able to describe your application network behavior using a visual flow, Sysdig will do the translation for you.   The topology map provided network visibility into all communication across a service / app / namespace / tag. So you won’t forget to include any service.   You also don’t start from scratch. By baselining your network policy you only perform a 10% lift, which saves you time. Then Sysdig auto generates the YAML.   Devops and security teams only need to sign off and check the policy.   Finally, it leverages Kubernetes native controls, decoupling policy definition and enforcement. As such, it is not intrusive, and it doesn’t imply a performance hit. Also, if you lose connection to Sysdig your policies are still on and your network communications will stay up.  Conclusion  To address the Kubernetes network security requirements, Sysdig Secure just introduced its Sysdig Network Policy feature.   Using this feature you will:  Get automatic visibility for your microservices and their communications, using the Kubernetes metadata to abstract away all the physical-layer noise. Apply least-privilege microsegmentation for your services Automatically generate the KNPs that derives from your input, no previous KNP expertise required.  You can check this feature today, start a free trial! ",
      "publishedDate": "2020-11-17T12:50:51.000Z",
      "description": "Learn how Sysdig is closing the visibility and security gap with its latest Network Policy feature for Kubernetes-native network security.",
      "ogDescription": "Learn how Sysdig is closing the visibility and security gap with its latest Network Policy feature for Kubernetes-native network security."
    },
    {
      "url": "https://github.com/walidshaari/Certified-Kubernetes-Security-Specialist",
      "title": "walidshaari/Certified-Kubernetes-Security-Specialist",
      "content": "<div><div><article class=\"markdown-body entry-content container-lg\"><p><a href=\"https://creativecommons.org/licenses/by-sa/4.0/\"><img src=\"https://licensebuttons.net/l/by-sa/4.0/80x15.png\" alt=\"License: CC BY-SA 4.0\"></a>\n<a href=\"http://makeapullrequest.com/\"><img src=\"https://camo.githubusercontent.com/0ff11ed110cfa69f703ef0dcca3cee6141c0a8ef465e8237221ae245de3deb3d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f5052732d77656c636f6d652d627269676874677265656e2e7376673f7374796c653d666c61742d737175617265\" alt=\"PRs Welcome\"></a></p>\n\n<p>Online curated resources that will help you prepare for taking the Kubernetes Certified Kubernetes Security Specialist <strong>CKS</strong> Certification exam.</p>\n<ul>\n<li>Please raise an issue, or make a pull request for fixes, new additions, or updates.</li>\n</ul>\n<p>I will try to restrict the cross references of resources primarly to <a href=\"https://kubernetes.io/\">kubernetes.io</a> as CNCF/Linux Foundation exam rules allows you search <strong>kubernetes.io/{docs|blog}</strong> and <a href=\"https://github.com/kubernetes\">kubernetes github repo</a> only. Youtube videos and other third party resources e.g. blogs will be provided as an optional complimentary material and any 3rd party material not allowed in the exam will be designated with <g-emoji class=\"g-emoji\">=&#xFFFD;</g-emoji> in the curriculum sections below.</p>\n<p>Ensure you have the right version of Kubernetes documentation selected (e.g. v1.19 as of 17th Nov GA announcement) especially for API objects and annotations, however for third party tools, you might find that you can still find references for them in old releases and blogs <a href=\"https://github.com/kubernetes/website/issues/24184\">e.g. falco install</a>.</p>\n<ul>\n<li>Icons/emoji legend\n<ul>\n<li><g-emoji class=\"g-emoji\">=&#xFFFD;</g-emoji>  Expand to see more content</li>\n<li><g-emoji class=\"g-emoji\">=\u0015</g-emoji>   Verify, not best resource yet</li>\n<li><g-emoji class=\"g-emoji\">=5</g-emoji> Good overall refence, can be used in the exam</li>\n<li><g-emoji class=\"g-emoji\">=&#xFFFD;</g-emoji> External third-party resource, can not be used during exam</li>\n<li><g-emoji class=\"g-emoji\">=&#xFFFD;</g-emoji>  ToDo, item that needs further checking(todo list for future research/commits)</li>\n</ul>\n</li>\n</ul>\n<h2><a id=\"user-content-exam-brief\" class=\"anchor\" href=\"https://github.com/walidshaari/Certified-Kubernetes-Security-Specialist#exam-brief\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Exam Brief</h2>\n<p>Offical exam objectives you review and understand in order to pass the test.</p>\n<ul>\n<li><a href=\"https://github.com/cncf/curriculum/blob/master/CKS_Curriculum_%20v1.19.pdf\">CNCF Exam Curriculum repository </a></li>\n</ul>\n<ul>\n<li>\n<p>Duration : two (2) hours</p>\n</li>\n<li>\n<p>Number of questions: 15-20 hands-on performance based tasks</p>\n</li>\n<li>\n<p>Passing score: 67%</p>\n</li>\n<li>\n<p>Certification validity: two (2) years</p>\n</li>\n<li>\n<p>Prerequisite: valid CKA</p>\n</li>\n<li>\n<p>Cost: $300 USD, One (1) year exam eligibility, with a free retake within the year.</p>\n<p><em>Linux Foundation offer several discounts around the year e.g. CyberMonday, Kubecon attendees among other special holidays/events</em></p>\n</li>\n</ul>\n<h2><a id=\"user-content-cks-repo-topics-overview\" class=\"anchor\" href=\"https://github.com/walidshaari/Certified-Kubernetes-Security-Specialist#cks-repo-topics-overview\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>CKS repo topics overview</h2>\n\n<h4><a id=\"user-content-extra-helpful-material\" class=\"anchor\" href=\"https://github.com/walidshaari/Certified-Kubernetes-Security-Specialist#extra-helpful-material\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Extra helpful material</h4>\n\n \n<p>\n  <a href=\"https://github.com/walidshaari/Certified-Kubernetes-Security-Specialist/blob/main/kubernetes-security-specialist-logo-300x285.png\"><img width=\"360\" src=\"https://github.com/walidshaari/Certified-Kubernetes-Security-Specialist/raw/main/kubernetes-security-specialist-logo-300x285.png\"></a>\n</p>\n<h3><a id=\"user-content-cluster-setup---10\" class=\"anchor\" href=\"https://github.com/walidshaari/Certified-Kubernetes-Security-Specialist#cluster-setup---10\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Cluster Setup - 10%</h3>\n<p><g-emoji class=\"g-emoji\">=5</g-emoji> <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/\">Securing a Cluster</a></p>\n\n<h3><a id=\"user-content-cluster-hardening---15\" class=\"anchor\" href=\"https://github.com/walidshaari/Certified-Kubernetes-Security-Specialist#cluster-hardening---15\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Cluster Hardening - 15%</h3>\n\n<h3><a id=\"user-content-system-hardening---15\" class=\"anchor\" href=\"https://github.com/walidshaari/Certified-Kubernetes-Security-Specialist#system-hardening---15\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>System Hardening - 15%</h3>\n\n<h3><a id=\"user-content-minimize-microservice-vulnerabilities---20\" class=\"anchor\" href=\"https://github.com/walidshaari/Certified-Kubernetes-Security-Specialist#minimize-microservice-vulnerabilities---20\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Minimize Microservice Vulnerabilities - 20%</h3>\n\n\n<h3><a id=\"user-content-supply-chain-security---20\" class=\"anchor\" href=\"https://github.com/walidshaari/Certified-Kubernetes-Security-Specialist#supply-chain-security---20\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Supply Chain Security - 20%</h3>\n\n<h3><a id=\"user-content-monitoring-logging-and-runtime-security---20\" class=\"anchor\" href=\"https://github.com/walidshaari/Certified-Kubernetes-Security-Specialist#monitoring-logging-and-runtime-security---20\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Monitoring, Logging and Runtime Security - 20%</h3>\n<ol>\n<li>Perform behavioural analytics of syscall process and file activities at the host and container level to detect malicious activities</li>\n</ol>\n<ul>\n<li><a href=\"https://v1-17.docs.kubernetes.io/docs/tasks/debug-application-cluster/falco/\">Old kubernetes.io URL: install falco on k8s 1.17</a></li>\n</ul>\n\n\n\n \n<h2><a id=\"user-content-extra-helpful-material-1\" class=\"anchor\" href=\"https://github.com/walidshaari/Certified-Kubernetes-Security-Specialist#extra-helpful-material-1\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Extra helpful material</h2>\n<h3><a id=\"user-content-slack\" class=\"anchor\" href=\"https://github.com/walidshaari/Certified-Kubernetes-Security-Specialist#slack\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Slack</h3>\n<ol>\n<li><a href=\"https://kubernetes.slack.com/\">Kubernetes Community - #cks-exam-prep</a></li>\n<li><a href=\"https://kubernauts-slack-join.herokuapp.com/\">Kubernauts Community #cks</a></li>\n</ol>\n<h3><a id=\"user-content-books\" class=\"anchor\" href=\"https://github.com/walidshaari/Certified-Kubernetes-Security-Specialist#books\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Books</h3>\n\n<h3><a id=\"user-content-youtube-videos\" class=\"anchor\" href=\"https://github.com/walidshaari/Certified-Kubernetes-Security-Specialist#youtube-videos\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Youtube Videos</h3>\n\n<ul>\n<li><a href=\"https://github.com/walidshaari/Certified-Kubernetes-Security-Specialist#webinars\">Webinars</a>\n<ul>\n<li><a href=\"https://www.aquasec.com/resources/virtual-container-security-channel/\">AquaSec webiners collection</a> - Webinars and videos presented by leading industry experts covering Microservices, Container &amp; Serverless security, Kubernetes, DevSecOps, and everything related to the most disruptive area in IT.</li>\n</ul>\n</li>\n</ul>\n<h3><a id=\"user-content-containers-and-kubernetes-security-training\" class=\"anchor\" href=\"https://github.com/walidshaari/Certified-Kubernetes-Security-Specialist#containers-and-kubernetes-security-training\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Containers and Kubernetes Security Training</h3>\n\n<h3><a id=\"user-content-extra-kubernetes-security-resources\" class=\"anchor\" href=\"https://github.com/walidshaari/Certified-Kubernetes-Security-Specialist#extra-kubernetes-security-resources\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Extra Kubernetes security resources</h3>\n\n<h4><a id=\"user-content-cves\" class=\"anchor\" href=\"https://github.com/walidshaari/Certified-Kubernetes-Security-Specialist#cves\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>CVEs</h4>\n\n<h4><a id=\"user-content-other-cks-related-repos\" class=\"anchor\" href=\"https://github.com/walidshaari/Certified-Kubernetes-Security-Specialist#other-cks-related-repos\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Other CKS related repos</h4>\n\n</article></div></div><hr><h4>Page 2</h4><body class=\"logged-out env-production page-responsive\"> <include-fragment class=\"js-notification-shelf-include-fragment\"></include-fragment> <div class=\"application-main \"> <p class> <main id=\"js-repo-pjax-container\"> <div class=\"bg-gray-light pt-3 hide-full-screen mb-5\"> <div class=\"d-flex mb-3 px-3 px-md-4 px-lg-5\"> <ul class=\"pagehead-actions flex-shrink-0 d-none d-md-inline\"> <li> <a class=\"tooltipped tooltipped-s btn btn-sm btn-with-count\" href=\"/login?return_to=%2Fwalidshaari%2FCertified-Kubernetes-Security-Specialist\"> <svg class=\"octicon octicon-eye\" height=\"16\" width=\"16\"><path/></svg> Watch\n</a> <a class=\"social-count\" href=\"/walidshaari/Certified-Kubernetes-Security-Specialist/watchers\"> 43 </a> </li> <li> <a class=\"btn btn-sm btn-with-count tooltipped tooltipped-s\" href=\"/login?return_to=%2Fwalidshaari%2FCertified-Kubernetes-Security-Specialist\"> <svg class=\"octicon octicon-star v-align-text-bottom\" height=\"16\" width=\"16\"><path/></svg> Star\n</a> <a class=\"social-count js-social-count\" href=\"/walidshaari/Certified-Kubernetes-Security-Specialist/stargazers\"> 292 </a> </li> <li> <a class=\"btn btn-sm btn-with-count tooltipped tooltipped-s\" href=\"/login?return_to=%2Fwalidshaari%2FCertified-Kubernetes-Security-Specialist\"> <svg class=\"octicon octicon-repo-forked\" width=\"16\" height=\"16\"><path/></svg> Fork\n</a> <a href=\"/walidshaari/Certified-Kubernetes-Security-Specialist/network/members\" class=\"social-count\"> 60 </a> </li>\n</ul> </div> </div> </main> </p> </div> <p id=\"ajax-error-message\" class=\"ajax-error-message\"> <svg class=\"octicon octicon-alert\" width=\"16\" height=\"16\"><path/></svg> You can&#x2019;t perform that action at this time. </p> <p class=\"js-stale-session-flash\"> <svg class=\"octicon octicon-alert\" width=\"16\" height=\"16\"><path/></svg> <span class=\"js-stale-session-flash-signed-in\">You signed in with another tab or window. <a href>Reload</a> to refresh your session.</span> <span class=\"js-stale-session-flash-signed-out\">You signed out in another tab or window. <a href>Reload</a> to refresh your session.</span> </p> <template id=\"site-details-dialog\"> <details class=\"details-reset details-overlay details-overlay-dark lh-default text-gray-dark hx_rsm\"> <summary></summary> <details-dialog class=\"Box Box--overlay d-flex flex-column anim-fade-in fast hx_rsm-dialog hx_rsm-modal\"> </details-dialog> </details>\n</template> <div class=\"js-cookie-consent-banner\"> <div class=\"hx_cookie-banner p-2 p-sm-3 p-md-4\"> <div class=\"Box hx_cookie-banner-box box-shadow-medium mx-auto\"> <div class=\"Box-body border-0 py-0 px-3 px-md-4\"> <div class=\"js-main-cookie-banner hx_cookie-banner-main\"> <div class=\"d-md-flex flex-items-center py-3\"> <p class=\"f5 flex-1 mb-3 mb-md-0\"> We use <span class=\"text-bold\">optional</span> third-party analytics cookies to understand how you use GitHub.com so we can build better products. <span class=\"btn-link js-cookie-consent-learn-more\">Learn more</span>. </p> </div> </div> <div class=\"js-cookie-details hx_cookie-banner-details\"> <div class=\"d-md-flex flex-items-center py-3\"> <p class=\"f5 flex-1 mb-2 mb-md-0\"> We use <span class=\"text-bold\">optional</span> third-party analytics cookies to understand how you use GitHub.com so we can build better products. <br> You can always update your selection by clicking <span class=\"text-bold\">Cookie Preferences</span> at the bottom of the page. For more information, see our <a href=\"https://docs.github.com/en/free-pro-team@latest/github/site-policy/github-privacy-statement\">Privacy Statement</a>. </p> </div> <div class=\"d-md-flex flex-items-center py-3 border-top\"> <div class=\"f5 flex-1 mb-2 mb-md-0\"> <p class=\"f6 mb-md-0\">We use essential cookies to perform essential website functions, e.g. they&apos;re used to log you in. <a href=\"https://docs.github.com/en/github/site-policy/github-subprocessors-and-cookies\">Learn more</a> </p> </div> <h5 class=\"text-blue\">Always active</h5> </div> <div class=\"d-md-flex flex-items-center py-3 border-top\"> <div class=\"f5 flex-1 mb-2 mb-md-0\"> <p class=\"f6 mb-md-0\">We use analytics cookies to understand how you use our websites so we can make them better, e.g. they&apos;re used to gather information about the pages you visit and how many clicks you need to accomplish a task. <a href=\"https://docs.github.com/en/github/site-policy/github-subprocessors-and-cookies\">Learn more</a> </p> </div> </div> </div>\n</div></div> </div>\n</div> </body>",
      "contentAsText": "\n\n\nOnline curated resources that will help you prepare for taking the Kubernetes Certified Kubernetes Security Specialist CKS Certification exam.\n\nPlease raise an issue, or make a pull request for fixes, new additions, or updates.\n\nI will try to restrict the cross references of resources primarly to kubernetes.io as CNCF/Linux Foundation exam rules allows you search kubernetes.io/{docs|blog} and kubernetes github repo only. Youtube videos and other third party resources e.g. blogs will be provided as an optional complimentary material and any 3rd party material not allowed in the exam will be designated with =� in the curriculum sections below.\nEnsure you have the right version of Kubernetes documentation selected (e.g. v1.19 as of 17th Nov GA announcement) especially for API objects and annotations, however for third party tools, you might find that you can still find references for them in old releases and blogs e.g. falco install.\n\nIcons/emoji legend\n\n=�  Expand to see more content\n=\u0015   Verify, not best resource yet\n=5 Good overall refence, can be used in the exam\n=� External third-party resource, can not be used during exam\n=�  ToDo, item that needs further checking(todo list for future research/commits)\n\n\n\nExam Brief\nOffical exam objectives you review and understand in order to pass the test.\n\nCNCF Exam Curriculum repository \n\n\n\nDuration : two (2) hours\n\n\nNumber of questions: 15-20 hands-on performance based tasks\n\n\nPassing score: 67%\n\n\nCertification validity: two (2) years\n\n\nPrerequisite: valid CKA\n\n\nCost: $300 USD, One (1) year exam eligibility, with a free retake within the year.\nLinux Foundation offer several discounts around the year e.g. CyberMonday, Kubecon attendees among other special holidays/events\n\n\nCKS repo topics overview\n\nExtra helpful material\n\n \n\n  \n\nCluster Setup - 10%\n=5 Securing a Cluster\n\nCluster Hardening - 15%\n\nSystem Hardening - 15%\n\nMinimize Microservice Vulnerabilities - 20%\n\n\nSupply Chain Security - 20%\n\nMonitoring, Logging and Runtime Security - 20%\n\nPerform behavioural analytics of syscall process and file activities at the host and container level to detect malicious activities\n\n\nOld kubernetes.io URL: install falco on k8s 1.17\n\n\n\n\n \nExtra helpful material\nSlack\n\nKubernetes Community - #cks-exam-prep\nKubernauts Community #cks\n\nBooks\n\nYoutube Videos\n\n\nWebinars\n\nAquaSec webiners collection - Webinars and videos presented by leading industry experts covering Microservices, Container & Serverless security, Kubernetes, DevSecOps, and everything related to the most disruptive area in IT.\n\n\n\nContainers and Kubernetes Security Training\n\nExtra Kubernetes security resources\n\nCVEs\n\nOther CKS related repos\n\nPage 2           Watch\n  43      Star\n  292      Fork\n  60  \n        You can’t perform that action at this time.    You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session.          We use optional third-party analytics cookies to understand how you use GitHub.com so we can build better products. Learn more.       We use optional third-party analytics cookies to understand how you use GitHub.com so we can build better products.  You can always update your selection by clicking Cookie Preferences at the bottom of the page. For more information, see our Privacy Statement.     We use essential cookies to perform essential website functions, e.g. they're used to log you in. Learn more   Always active    We use analytics cookies to understand how you use our websites so we can make them better, e.g. they're used to gather information about the pages you visit and how many clicks you need to accomplish a task. Learn more    \n \n ",
      "description": "Online resources that will help you prepare for taking the CNCF/Linux Foundation CKS 2020 \"Kubernetes Certified Security Specialist\" Certification exam. with time, This is not likely the comprehensive up to date list - please make a pull request if there something that should be added here. - walidshaari/Certified-Kubernetes-Security-Specialist",
      "ogDescription": "Online resources that will help you prepare for taking the CNCF/Linux Foundation CKS 2020 &quot;Kubernetes Certified Security Specialist&quot; Certification exam. with time, This is not likely the ..."
    },
    {
      "url": "https://github.com/sangam14/ContainerLabs",
      "title": "sangam14/ContainerLabs",
      "content": "<div></div><hr><h4>Page 2</h4><div></div>",
      "contentAsText": "Page 2",
      "description": "#1 Learning Platform Containerlabs - ContainerD| LXD | Docker | Kubernets | rancher | K3s | ..more  - sangam14/ContainerLabs",
      "ogDescription": "#1 Learning Platform Containerlabs - ContainerD| LXD | Docker | Kubernets | rancher | K3s | ..more  - sangam14/ContainerLabs"
    },
    {
      "url": "https://containerlabs.kubedaily.com/",
      "title": "ContainerLabs",
      "content": "<div><div id=\"main-content-wrap\" class=\"main-content-wrap\"><div id=\"main-content\" class=\"main-content\"><p><a href=\"https://discord.gg/rEvr7vq\" class=\"btn btn-green mr-4\">Join Containerlabs Community</a> <img src=\"https://hitcounter.pythonanywhere.com/count/tag.svg?url=http%3A%2F%2Fcontainerlabs.kubedaily.com%2F\" alt=\"Hits\"> <a href=\"https://ko-fi.com/K3K0E60M\"><img src=\"https://www.ko-fi.com/img/githubbutton_sm.svg\" alt=\"ko-fi\"></a></p><p><img src=\"https://raw.githubusercontent.com/sangam14/ContainerLabs/master/img/ContainerLabs-official.png\" alt=\"img\"></p><p class=\"label label-blue\">The Ultimate Workshop Track Specially Designed For You - Select Learning Paths</p><p><a href=\"http://containerlabs.kubedaily.com/Birth_of_Containerization/README.html\" class=\"btn btn-purple mr-2\">Birth of Containerization</a> <a href=\"http://containerlabs.kubedaily.com/LXC/\" class=\"btn btn-purple mr-2\">LXC</a><a href=\"http://containerlabs.kubedaily.com/Docker/Overview/\" class=\"btn btn-purple mr-2\">Docker Fundamental</a> <a href=\"https://containerlabs.kubedaily.com/Docker/Dev/\" class=\"btn btn-purple mr-2\">Docker For Developer</a> <a href=\"https://containerlabs.kubedaily.com/Kubernetes/beginner/README.html\" class=\"btn btn-purple mr-2\">Kubernetes 101</a> <a href=\"https://containerlabs.kubedaily.com/Kubernetes/\" class=\"btn btn-purple mr-2\">Kubernetes</a> <a href=\"https://containerlabs.kubedaily.com/rancher/Networking/\" class=\"btn btn-purple mr-2\">Rancher Networking</a> <a href=\"https://dev.to/t/litmuschaos\" class=\"btn btn-purple mr-2\">litmuschaos</a> <a href=\"https://containerlabs.kubedaily.com/Okteto/\" class=\"btn btn-purple mr-2\">Okteto</a> <a href class=\"btn btn-purple mr-2\">Lightweight Kubernetes - K3s</a><br> <a href=\"https://containerlabs.kubedaily.com/traefik/\" class=\"btn btn-purple mr-2\">Traefik</a></p><details><summary>LXD and LXC Tools </summary><div class=\"table-wrapper\"><table><tr><th> <a href=\"https://github.com/lxc/lxc\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=lxc&amp;repo=lxc&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/lxc/lxd\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=lxc&amp;repo=lxd&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/lxc/lxcfs\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=lxc&amp;repo=lxcfs&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/lxc/go-lxc\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=lxc&amp;repo=go-lxc&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/lxc/ruby-lxc\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=lxc&amp;repo=ruby-lxc&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/lxc/python3-lxc\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=lxc&amp;repo=python3-lxc&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/lxc/distrobuilder\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=lxc&amp;repo=distrobuilder&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/fgrehm/vagrant-lxc\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=fgrehm&amp;repo=vagrant-lxc&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/lxc-webpanel/LXC-Web-Panel\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=lxc-webpanel&amp;repo=LXC-Web-Panel&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/lxdock/lxdock\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=lxdock&amp;repo=lxdock&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/flesueur/mi-lxc\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=flesueur&amp;repo=mi-lxc&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/AdaptiveScale/lxdui\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=AdaptiveScale&amp;repo=lxdui&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=lxc&amp;repo=lxc&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/corneliusweig/kubernetes-lxd\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=corneliusweig&amp;repo=kubernetes-lxd&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=lxc&amp;repo=lxc&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/turtle0x1/LxdMosaic\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=turtle0x1&amp;repo=LxdMosaic&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/automaticserver/lxe\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=automaticserver&amp;repo=lxe&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/bravetools/bravetools\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=bravetools&amp;repo=bravetools&amp;theme=solarized-light\"> </a></th></tr></table></div></details> <details><summary> Docker Projects </summary><div class=\"table-wrapper\"><table><tr><th> <a href=\"https://github.com/moby/moby\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=moby&amp;repo=moby&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/docker/compose/\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=docker&amp;repo=compose&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/docker/machine\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=docker&amp;repo=machine&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/docker/distribution\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=docker&amp;repo=distribution&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/docker/classicswarm\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=docker&amp;repo=classicswarm&amp;theme=solarized-light\"> </a></th></tr></table></div></details> <details> <summary> Container Composition </summary><div class=\"table-wrapper\"><table><tr><th> <a href=\"https://github.com/icy/bocker\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=icy&amp;repo=bocker&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/p8952/bocker\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=p8952&amp;repo=bocker&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/box-builder/box\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=box-builder&amp;repo=box&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/byrnedo/capitan\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=byrnedo&amp;repo=capitan&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/funkwerk/compose_plantuml\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=funkwerk&amp;repo=compose_plantuml&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/magicmark/composerize\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=magicmark&amp;repo=composerize&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/polonskiy/crowdr\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=polonskiy&amp;repo=crowdr&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/abesto/docker-compose-graphviz\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=abesto&amp;repo=docker-compose-graphviz&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/sudo-bmitch/docker-config-update\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=sudo-bmitch&amp;repo=docker-config-update&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/Alexis-benoist/draw-compose\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=Alexis-benoist&amp;repo=draw-compose&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/cisco/elsy\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=cisco&amp;repo=elsy&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/cloud66-oss/habitus\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=cloud66-oss&amp;repo=habitus&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/kubernetes/kompose\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=kubernetes&amp;repo=kompose&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/toscanini/maestro\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=toscanini&amp;repo=maestro&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/ashmckenzie/percheron\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=ashmckenzie&amp;repo=percheron&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/containers/podman-compose\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=containers&amp;repo=podman-compose&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/ihucos/plash\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=ihucos&amp;repo=plash&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/CenturyLinkLabs/zodiac\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=CenturyLinkLabs&amp;repo=zodiac&amp;theme=solarized-light\"> </a></th></tr></table></div></details> <details><summary> Deployment and Infrastructure </summary><div class=\"table-wrapper\"><table><tr><th> <a href=\"https://github.com/newrelic/centurion\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=newrelic&amp;repo=centurion&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/brooklyncentral/clocker\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=brooklyncentral&amp;repo=clocker&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/hasura/gitkube\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=hasura&amp;repo=gitkube&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/ttiny/deploy\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=ttiny&amp;repo=deploy&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/grafeas/grafeas\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=grafeas&amp;repo=grafeas&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/longshoreman/longshoreman\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=longshoreman&amp;repo=longshoreman&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/DIPSAS/SwarmManagement\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=DIPSAS&amp;repo=SwarmManagement&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/werf/werf\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=werf&amp;repo=werf&amp;theme=solarized-light\"> </a></th></tr></table></div></details> <details><summary> Monitoring, Alerts, and Visualization</summary><div class=\"table-wrapper\"><table><tr><th> <a href=\"https://github.com/willfarrell/docker-autoheal\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=willfarrell&amp;repo=docker-autoheal&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/google/cadvisor\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=google&amp;repo=cadvisor&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/stefanprodan/dockprom\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=stefanprodan&amp;repo=dockprom&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/jeffwillette/docker-alertd\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=jeffwillette&amp;repo=docker-alertd&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/amir20/dozzle\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=amir20&amp;repo=dozzle&amp;theme=solarized-light\"> </a></th><th> <a href> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=google&amp;repo=cadvisor&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/nicolargo/glances\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=nicolargo&amp;repo=glances&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/vegasbrianc/docker-monitoring\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=vegasbrianc&amp;repo=docker-monitoring&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/gliderlabs/logspout\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=gliderlabs&amp;repo=logspout&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/decryptus/monit-docker\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=decryptus&amp;repo=monit-docker&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/NexClipper/NexClipper\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=NexClipper&amp;repo=NexClipper&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/uschtwill/docker_monitoring_logging_alerting\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=uschtwill&amp;repo=docker_monitoring_logging_alerting&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/gpulido/SwarmAlert\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=gpulido&amp;repo=SwarmAlert&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/monitoringartist/Zabbix-Docker-Monitoring\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=monitoringartist&amp;repo=Zabbix-Docker-Monitoring&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/thanos-io/thanos\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=thanos-io&amp;repo=thanos&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/prometheus/prometheus\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=prometheus&amp;repo=prometheus&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/grafana/grafana\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=grafana&amp;repo=grafana&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/johanhaleby/kubetail\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=johanhaleby&amp;repo=kubetail&amp;theme=solarized-light\"> </a> &lt;/tr&gt;</th></tr><tr><th> <a href=\"https://github.com/searchlight/searchlight\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=searchlight&amp;repo=searchlight&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/andrew-waters/linkerd2-mixin\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=andrew-waters&amp;repo=linkerd2-mixin&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/stevelacy/kuberhaus\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=stevelacy&amp;repo=kuberhaus&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/sukeesh/k8s-job-notify\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=sukeesh&amp;repo=k8s-job-notify&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\" https://github.com/infracloudio/botkube\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=infracloudio&amp;repo=botkube&amp;theme=solarized-light\"> </a></th><th> <a href=\" https://github.com/cortexproject/cortex\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=cortexproject&amp;repo=cortex&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/FairwindsOps/goldilocks\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=FairwindsOps&amp;repo=goldilocks&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/kiali/kiali\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=kiali&amp;repo=kiali&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/robscott/kube-capacity\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=robscott&amp;repo=kube-capacity&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/kubernetes/kube-state-metrics\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=kubernetes&amp;repo=kube-state-metrics&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/kubernetes-sigs/metrics-server\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=kubernetes-sigs&amp;repo=metrics-server&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/hjacobs/kube-ops-view\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=hjacobs&amp;repo=kube-ops-view&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/bitnami-labs/kubewatch\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=bitnami-labs&amp;repo=kubewatch&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/lensapp/lens\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=lensapp&amp;repo=lens&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/derailed/popeye\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=derailed&amp;repo=popeye&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/salesforce/sloop\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=salesforce&amp;repo=sloop&amp;theme=solarized-light\"> </a></th></tr></table></div></details> <details><summary> Networking </summary><div class=\"table-wrapper\"><table><tr><th> <a href=\"https://github.com/projectcalico/calicoctl\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=projectcalico&amp;repo=calicoctl&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/coreos/flannel/\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=coreos&amp;repo=flannel&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/Microsoft/Freeflow\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=Microsoft&amp;repo=Freeflow&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/nicolaka/netshoot\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=nicolaka&amp;repo=netshoot&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/jpetazzo/pipework\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=jpetazzo&amp;repo=pipework&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/weaveworks/weave\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=weaveworks&amp;repo=weave&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/projectcalico/calico\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=projectcalico&amp;repo=calico&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/jetstack/cert-manager\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=jetstack&amp;repo=cert-manager&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/coredns/coredns\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=coredns&amp;repo=coredns&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/kubernetes/ingress-nginx\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=kubernetes&amp;repo=ingress-nginx&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/coredns/coredns\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=coredns&amp;repo=coredns&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/kubernetes/ingress-nginx\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=kubernetes&amp;repo=ingress-nginx&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/Kong/kubernetes-ingress-controller\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=Kong&amp;repo=kubernetes-ingress-controller&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/eldadru/ksniff\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=eldadru&amp;repo=ksniff&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/iovisor/kubectl-trace\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=iovisor&amp;repo=kubectl-trace&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/nginxinc/kubernetes-ingress\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=nginxinc&amp;repo=kubernetes-ingress&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/aporeto-inc/trireme-kubernetes\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=aporeto-inc&amp;repo=trireme-kubernetes&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/aporeto-inc/kubepox\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=aporeto-inc&amp;repo=kubepox&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/redhat-nfvpe/kokotap\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=redhat-nfvpe&amp;repo=kokotap&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/submariner-io/submariner\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=submariner-io&amp;repo=submariner&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/monzo/egress-operator\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=monzo&amp;repo=egress-operator&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/txn2/kubefwd\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=txn2&amp;repo=kubefwd&amp;theme=solarized-light\"> </a></th></tr></table></div></details> <details><summary> Reverse Proxy </summary><div class=\"table-wrapper\"><table><tr><th> <a href=\"https://github.com/moonbuggy/docker-dnsmasq-updater\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=moonbuggy&amp;repo=docker-dnsmasq-updater&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/docker-flow/docker-flow-proxy\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=docker-flow&amp;repo=docker-flow-proxy&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/silarsis/docker-proxy\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=silarsis&amp;repo=docker-proxy&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/fabiolb/fabio\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=fabiolb&amp;repo=fabio&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/zchee/h2o-proxy\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=zchee&amp;repo=h2o-proxy&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/nginx-proxy/docker-letsencrypt-nginx-proxy-companion\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=nginx-proxy&amp;repo=docker-letsencrypt-nginx-proxy-companion&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/mattallty/muguet\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=mattallty&amp;repo=muguet&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/nginx-proxy/nginx-proxy\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=nginx-proxy&amp;repo=nginx-proxy&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/jc21/nginx-proxy-manager\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=jc21&amp;repo=nginx-proxy-manager&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/tpbowden/swarm-ingress-router\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=tpbowden&amp;repo=swarm-ingress-router&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"https://github.com/flavioaiello/swarm-router\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=flavioaiello&amp;repo=swarm-router&amp;theme=solarized-light\"> </a></th><th> <a href=\"https://github.com/traefik/traefik\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=traefik&amp;repo=traefik&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"github.com/envoyproxy/envoy\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=envoyproxy&amp;repo=envoy&amp;theme=solarized-light\"> </a></th><th> <a href=\"github.com/projectcontour/contour\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=projectcontour&amp;repo=contour&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"github.com/bfenetworks/bfe \"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=envoyproxy&amp;repo=envoy&amp;theme=solarized-light\"> </a></th><th> <a href=\"github.com/projectcontour/gimbal\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=projectcontour&amp;repo=gimbal&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"github.com/haproxy/haproxy\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=haproxy&amp;repo=haproxy&amp;theme=solarized-light\"> </a></th><th> <a href=\"github.com/inlets/inlets\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=inlets&amp;repo=inlets&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"github.com/mosn/mosn\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=mosn&amp;repo=mosn&amp;theme=solarized-light\"> </a></th><th> <a href=\"github.com/openresty/openresty\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=openresty&amp;repo=openresty&amp;theme=solarized-light\"> </a></th></tr><tr><th> <a href=\"github.com/zalando/skipper\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=zalando&amp;repo=skipper&amp;theme=solarized-light\"> </a></th><th> <a href=\"github.com/alibaba/tengine\"> <img src=\"https://github-readme-stats.vercel.app/api/pin/?username=alibaba&amp;repo=tengine&amp;theme=solarized-light\"> </a></th></tr></table></div></details><p><a href=\"https://twitter.com/KubeDaily\"><img src=\"https://img.shields.io/twitter/url/https/twitter.com/fold_left.svg?style=social&amp;label=Follow%20%40KubeDaily\" alt=\"Twitter URL\"></a></p><footer><div class=\"d-flex mt-2\"><p class=\"text-small text-grey-dk-000 mb-0\"> <a href=\"https://github.com/sangam14/ContainerLabs//tree/master/README.md\" id=\"edit-this-page\">Edit this page on GitHub.</a></p></div></footer></div></div></div>",
      "contentAsText": "Join Containerlabs Community  The Ultimate Workshop Track Specially Designed For You - Select Learning PathsBirth of Containerization LXCDocker Fundamental Docker For Developer Kubernetes 101 Kubernetes Rancher Networking litmuschaos Okteto Lightweight Kubernetes - K3s TraefikLXD and LXC Tools                                                         Docker Projects                   Container Composition                                                         Deployment and Infrastructure                           Monitoring, Alerts, and Visualization                                                       </tr>                                                  Networking                                                                     Reverse Proxy                                                                    Edit this page on GitHub.",
      "description": "Welcome To ContainerLabs",
      "ogDescription": "Welcome To ContainerLabs"
    },
    {
      "url": "https://sandeepbaldawa.medium.com/k8s-labels-selectors-9ad2fcf78a4e",
      "title": "K8s Labels & Selectors",
      "content": "<div><article><section class=\"dq dr ds dt w du bm s\"></section><div><section class=\"db eb ec cw ed\"><div class=\"n p\"><div class=\"ag ah ai aj ak ee am w\"><p id=\"5c0d\" class=\"gd ge eg gf b gg gh gi gj gk gl gm gn go gp gq gr gs gt gu gv gw db fc\">In this post, we will look at</p><ul class><li id=\"edcd\" class=\"gd ge eg gf b gg gx gh gi gj gy gk gl gm gz gn go gp ha gq gr gs hb gt gu gw hc hd he fc\">What Kubernetes(K8s) Labels and Selectors are</li><li id=\"eabe\" class=\"gd ge eg gf b gg hf gh gi gj hg gk gl gm hh gn go gp hi gq gr gs hj gt gu gw hc hd he fc\">Why do we need them</li><li id=\"d780\" class=\"gd ge eg gf b gg hf gh gi gj hg gk gl gm hh gn go gp hi gq gr gs hj gt gu gw hc hd he fc\">How to use them</li></ul><p id=\"e296\" class=\"gd ge eg gf b gg gx gh gi gj gy gk gl gm gz gn go gp ha gq gr gs hb gt gu gw db fc\">Let\u0019s try to understand the concept with an analogy. If you ever have looked at different types of homes, one can see townhomes, condos, Single Family Homes, apartments, etc.</p><figure class=\"hl hm hn ho hp hq ds dt paragraph-image\"><img alt=\"Image for post\" class=\"dv hx dw hy w\" src=\"https://miro.medium.com/max/1800/0*I4WMkyoEJZwIXpZu\" width=\"900\" srcset=\"https://miro.medium.com/max/552/0*I4WMkyoEJZwIXpZu 276w, https://miro.medium.com/max/1104/0*I4WMkyoEJZwIXpZu 552w, https://miro.medium.com/max/1280/0*I4WMkyoEJZwIXpZu 640w, https://miro.medium.com/max/1400/0*I4WMkyoEJZwIXpZu 700w\" sizes=\"700px\"><figcaption class=\"ij ik du ds dt il im as b at au bq\">Apartment vs Condo vs Single Family Home vs Townhomes</figcaption></figure><p id=\"b0e6\" class=\"gd ge eg gf b gg gx gh gi gj gy gk gl gm gz gn go gp ha gq gr gs hb gt gu gw db fc\">Let\u0019s say we would like to categorize these homes by</p><pre class=\"hl hm hn ho hp in io ip\"></pre><p id=\"3cb7\" class=\"gd ge eg gf b gg gx gh gi gj gy gk gl gm gz gn go gp ha gq gr gs hb gt gu gw db fc\">These are nothing but <strong class=\"gf co\">Labels. Selectors</strong> help filter out things like gives all homes which are of type SFH and greater than 500sqft in size. One can mix and match different types of selectors.</p><p id=\"f0f6\" class=\"gd ge eg gf b gg gx gh gi gj gy gk gl gm gz gn go gp ha gq gr gs hb gt gu gw db fc\">Overtime in K8s one an have 1000s of different objects and one would need ways to label &amp; select certain types of objects. <strong class=\"gf co\"><em class=\"jb\">Labels</em></strong> are key/value pairs that are attached to objects, such as pods.</p><p id=\"ab49\" class=\"gd ge eg gf b gg gx gh gi gj gy gk gl gm gz gn go gp ha gq gr gs hb gt gu gw db fc\">Example labels:</p><ul class><li id=\"abfe\" class=\"gd ge eg gf b gg gx gh gi gj gy gk gl gm gz gn go gp ha gq gr gs hb gt gu gw hc hd he fc\">\u001crelease\u001d : \u001cstable\u001d, \u001crelease\u001d : \u001ccanary\u001d</li><li id=\"0bc8\" class=\"gd ge eg gf b gg hf gh gi gj hg gk gl gm hh gn go gp hi gq gr gs hj gt gu gw hc hd he fc\">\u001cenvironment\u001d : \u001cdev\u001d, \u001cenvironment\u001d : \u001cqa\u001d, \u001cenvironment\u001d : \u001cproduction\u001d</li><li id=\"0972\" class=\"gd ge eg gf b gg hf gh gi gj hg gk gl gm hh gn go gp hi gq gr gs hj gt gu gw hc hd he fc\">\u001ctier\u001d: \u001cfrontend\u001d, \u001ctier\u001d: \u001cbackend\u001d, \u001ctier\u001d: \u001ccache\u001d</li><li id=\"09f5\" class=\"gd ge eg gf b gg hf gh gi gj hg gk gl gm hh gn go gp hi gq gr gs hj gt gu gw hc hd he fc\">\u001cpartition\u001d : \u001ccustomerA\u001d, \u001cpartition\u001d : \u001ccustomerB\u001d</li><li id=\"058b\" class=\"gd ge eg gf b gg hf gh gi gj hg gk gl gm hh gn go gp hi gq gr gs hj gt gu gw hc hd he fc\">\u001ctrack\u001d: \u001cdaily\u001d, \u001ctrack\u001d: \u001cweekly\u001d</li></ul><h2 id=\"2a64\" class=\"iq ir eg as jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx fc\">Why do we need these labels &amp; selectors?</h2><p id=\"7a3f\" class=\"gd ge eg gf b gg jy gh gi gj jz gk gl gm ka gn go gp kb gq gr gs kc gt gu gw db fc\">Even a small Kubernetes cluster may have hundreds of Containers, Pods, Services, and many other Kubernetes API objects.</p><p id=\"c804\" class=\"gd ge eg gf b gg gx gh gi gj gy gk gl gm gz gn go gp ha gq gr gs hb gt gu gw db fc\">It quickly becomes annoying to page through pages of kubectl output to find your object, <strong class=\"gf co\">labels</strong> address this issue perfectly.</p><p id=\"db36\" class=\"gd ge eg gf b gg gx gh gi gj gy gk gl gm gz gn go gp ha gq gr gs hb gt gu gw db fc\">The primary reasons you should use labels are:</p><ul class><li id=\"7f67\" class=\"gd ge eg gf b gg gx gh gi gj gy gk gl gm gz gn go gp ha gq gr gs hb gt gu gw hc hd he fc\">enables you to logically <strong class=\"gf co\">organize </strong>all your Kubernetes workloads in all your clusters,</li><li id=\"b6c5\" class=\"gd ge eg gf b gg hf gh gi gj hg gk gl gm hh gn go gp hi gq gr gs hj gt gu gw hc hd he fc\">enables you to very <strong class=\"gf co\">selectively filter </strong>kubectl outputs to just the objects you need.</li><li id=\"0596\" class=\"gd ge eg gf b gg hf gh gi gj hg gk gl gm hh gn go gp hi gq gr gs hj gt gu gw hc hd he fc\">enables you to <strong class=\"gf co\">understand the layers and hierarchies</strong> of all your API objects.</li></ul><h2 id=\"bb23\" class=\"iq ir eg as jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx fc\">How do we use the same?</h2><p id=\"47c9\" class=\"gd ge eg gf b gg jy gh gi gj jz gk gl gm ka gn go gp kb gq gr gs kc gt gu gw db fc\">Let\u0019s see we would like to create a Pod with a label. Using minikube for examples below, you could try this on any other K8 cluster.</p><pre class=\"hl hm hn ho hp in io ip\"></pre><p id=\"5c61\" class=\"gd ge eg gf b gg gx gh gi gj gy gk gl gm gz gn go gp ha gq gr gs hb gt gu gw db fc\">There are two ways we could do the same using the <strong class=\"gf co\">imperative</strong> way or the declarative way</p><pre class=\"hl hm hn ho hp in io ip\"></pre><p id=\"611f\" class=\"gd ge eg gf b gg gx gh gi gj gy gk gl gm gz gn go gp ha gq gr gs hb gt gu gw db fc\">Let\u0019s try to confirm the POD using the label</p><pre class=\"hl hm hn ho hp in io ip\"></pre><p id=\"1bf1\" class=\"gd ge eg gf b gg gx gh gi gj gy gk gl gm gz gn go gp ha gq gr gs hb gt gu gw db fc\">Let\u0019s try using the declarative way, `cat declarative.yml`</p><pre class=\"hl hm hn ho hp in io ip\"></pre><p id=\"13ea\" class=\"gd ge eg gf b gg gx gh gi gj gy gk gl gm gz gn go gp ha gq gr gs hb gt gu gw db fc\">Let\u0019s apply the YAML file, we see the POD with the given label is created. With <code class=\"id kd ke kf is b\">-l</code> the option here, we are selecting a specific label</p><pre class=\"hl hm hn ho hp in io ip\"></pre><p id=\"a8e2\" class=\"gd ge eg gf b gg gx gh gi gj gy gk gl gm gz gn go gp ha gq gr gs hb gt gu gw db fc\"><strong class=\"gf co\">Can we use multiple selectors?</strong></p><p id=\"0b7f\" class=\"gd ge eg gf b gg gx gh gi gj gy gk gl gm gz gn go gp ha gq gr gs hb gt gu gw db fc\">Of course, the example below uses selectors env, bu &amp; type</p><pre class=\"hl hm hn ho hp in io ip\"></pre><p id=\"be65\" class=\"gd ge eg gf b gg gx gh gi gj gy gk gl gm gz gn go gp ha gq gr gs hb gt gu gw db fc\"><strong class=\"gf co\">How to see all labels being used?</strong></p><pre class=\"hl hm hn ho hp in io ip\"></pre><p id=\"3d5e\" class=\"gd ge eg gf b gg gx gh gi gj gy gk gl gm gz gn go gp ha gq gr gs hb gt gu gw db fc\"><a href=\"https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/\" class=\"dg kg\">K8 documentation</a> for labels &amp; selectors has a lot more details if you would like to explore. That&apos;s all for now, till next time ciao!</p></div></div></section></div></article></div>",
      "contentAsText": "In this post, we will look atWhat Kubernetes(K8s) Labels and Selectors areWhy do we need themHow to use themLet\u0019s try to understand the concept with an analogy. If you ever have looked at different types of homes, one can see townhomes, condos, Single Family Homes, apartments, etc.Apartment vs Condo vs Single Family Home vs TownhomesLet\u0019s say we would like to categorize these homes byThese are nothing but Labels. Selectors help filter out things like gives all homes which are of type SFH and greater than 500sqft in size. One can mix and match different types of selectors.Overtime in K8s one an have 1000s of different objects and one would need ways to label & select certain types of objects. Labels are key/value pairs that are attached to objects, such as pods.Example labels:\u001crelease\u001d : \u001cstable\u001d, \u001crelease\u001d : \u001ccanary\u001d\u001cenvironment\u001d : \u001cdev\u001d, \u001cenvironment\u001d : \u001cqa\u001d, \u001cenvironment\u001d : \u001cproduction\u001d\u001ctier\u001d: \u001cfrontend\u001d, \u001ctier\u001d: \u001cbackend\u001d, \u001ctier\u001d: \u001ccache\u001d\u001cpartition\u001d : \u001ccustomerA\u001d, \u001cpartition\u001d : \u001ccustomerB\u001d\u001ctrack\u001d: \u001cdaily\u001d, \u001ctrack\u001d: \u001cweekly\u001dWhy do we need these labels & selectors?Even a small Kubernetes cluster may have hundreds of Containers, Pods, Services, and many other Kubernetes API objects.It quickly becomes annoying to page through pages of kubectl output to find your object, labels address this issue perfectly.The primary reasons you should use labels are:enables you to logically organize all your Kubernetes workloads in all your clusters,enables you to very selectively filter kubectl outputs to just the objects you need.enables you to understand the layers and hierarchies of all your API objects.How do we use the same?Let\u0019s see we would like to create a Pod with a label. Using minikube for examples below, you could try this on any other K8 cluster.There are two ways we could do the same using the imperative way or the declarative wayLet\u0019s try to confirm the POD using the labelLet\u0019s try using the declarative way, `cat declarative.yml`Let\u0019s apply the YAML file, we see the POD with the given label is created. With -l the option here, we are selecting a specific labelCan we use multiple selectors?Of course, the example below uses selectors env, bu & typeHow to see all labels being used?K8 documentation for labels & selectors has a lot more details if you would like to explore. That's all for now, till next time ciao!",
      "publishedDate": "2020-11-15T20:24:50.797Z",
      "description": "Let’s try to understand the concept with an analogy. If you ever have looked at different types of homes, one can see townhomes, condos, Single Family Homes, apartments, etc. These are nothing but…",
      "ogDescription": "In this post, we will look at"
    },
    {
      "url": "https://tech-blog.sonos.com/posts/kubernetes-infrastructure-at-sonos/",
      "title": "Building a Solid Foundation: Kubernetes Infrastructure at Sonos",
      "content": "<div class=\"css-2vq2qm\"><p class=\"css-1nm0jln\">Over the past few years, our cloud teams at Sonos have grown to support numerous services that serve our millions of customers throughout the world. Our teams have made significant strides in automating much of the work behind orchestrating, configuring and maintaining complex cloud deployments. Early in our cloud journey, Sonos embraced configuration as code and open source projects like Ansible to ensure consistency and reliability across our services.&#xFFFD; As time went on, our infrastructure teams started getting the same questions over and over:</p><ul type=\"bullet\" class=\"css-11p44u2\"><li class=\"css-1xjbt1k\"><p>How can we get code tested and deployed to production faster?</p></li><li class=\"css-1xjbt1k\"><p>How can we reduce boilerplate and duplication across teams?</p></li><li class=\"css-1xjbt1k\"><p>How can we continue to support DevOps best practices as teams evolve and change?</p></li></ul><p class=\"css-w48t98\">Luckily, over the past 5 years, there have been significant advancements in containerization and orchestration which can make answering these questions significantly easier.&#xFFFD; Although our team experimented with a number of platforms, we ended up choosing Kubernetes as a solution and were impressed with its ability to self-heal, scale, and provide a consistent abstraction layer our developers could more easily interact with.&#xFFFD; At this point, we easily could have said \u001cOkay, let&apos;s move everything to Kubernetes\u001d, but that would miss the additional complexity our Infrastructure teams would be taking on in supporting Kubernetes deployments. Although service teams would no longer have to support complex Ansible and cloud deployments, our team had some work ahead to better prepare for this journey.</p><p class=\"css-w48t98\">Running services in the cloud can be a complex endeavor, but let&apos;s dive into a few of the requirements our teams have.&#xFFFD; First of all, monitoring and logging is essential to successfully making sure applications are functioning properly.&#xFFFD; When problems arise, this tooling allows our teams to dive in quickly and diagnose the issues. Additionally, secret management and access to other dependencies such as databases, message queues and object stores are required to successfully stand up applications.&#xFFFD; Finally, we try to utilize autoscaling as much as we can for our services. We needed to ensure our Kubernetes autoscaling was robust and could react quickly to both predictable and unpredictable traffic levels to ensure good service reliability and cost management. We quickly realized our infrastructure teams would need to implement solutions that would take these and other burdens off our services teams, allowing them to focus less on \u001chow\u001d they run their applications and more on \u001cwhat\u001d their applications should be doing.</p><h2 class=\"css-1nekmyi\">Cluster Configuration</h2><p class=\"css-w48t98\">When teams at Sonos deploy their applications, we set best practices that enforce reproducible and reliable application deployment across environments. We quickly realized our infrastructure team would have to do the same thing for Kubernetes cluster deployments.&#xFFFD; We began diving into areas where automation and configuration would be required to help build resilient clusters, allowing our service teams to focus on their applications and our infrastructure teams to focus on solving broad problems for multiple teams at once:</p><ul type=\"bullet\" class=\"css-11p44u2\"><li class=\"css-1xjbt1k\"><p>Networking &amp; CIDR Management \u0013 To fit into existing infrastructure, we must connect to and manage many network ranges.</p></li><li class=\"css-1xjbt1k\"><p>DNS \u0013 Services utilize DNS for service discovery inside and outside of our clusters.</p></li><li class=\"css-1xjbt1k\"><p>User and Service account access \u0013 Teams and their automation need secure access to their namespaces and cluster tooling.</p></li><li class=\"css-1xjbt1k\"><p>Alpha &amp; Beta Features \u0013 Kubernetes developers often disable new APIs and features by default. We may want to enable these and allow teams to experiment with upcoming features.</p></li><li class=\"css-1xjbt1k\"><p>Cluster Monitoring, Alerting &amp; Logging \u0013 Clusters and their components will need to be monitored so our infrastructure teams can properly maintain them as we onboard service teams.</p></li><li class=\"css-1xjbt1k\"><p>Autoscaling \u0013 Our clusters should scale themselves up or down as necessary to ensure all workloads are scheduled on cost effective instances.</p></li><li class=\"css-1xjbt1k\"><p>Node Removal \u0013 Our clusters and monitoring tools should be able to remove instances or nodes that go bad for a variety of reasons without human intervention.</p></li></ul><p class=\"css-w48t98\">The next step our team followed was examining \u001chow\u001d we would deliver such a solution to Sonos.&#xFFFD; We spun up Kubernetes clusters with open source tools such as kOps and eksctl for our team to get familiar with the tooling.&#xFFFD; We knew we wanted options to be cloud agnostic and work with tooling that could easily be swapped out over time. Quickly, we realized that there were a few issues with building clusters and simply \u001cdeploying code to clusters.\u001d</p><p class=\"css-w48t98\">When building infrastructure, our teams try to follow the <a href=\"http://cloudscaling.com/blog/cloud-computing/the-history-of-pets-vs-cattle/\" class=\"css-s6bji1\">Pets vs Cattle</a> analogy of Randy Bias and Bill Baker, building highly available services and when unforeseen issues arise, we simply remove bad instances.&#xFFFD; As we imagined a year or two down the road, we realized our clusters needed to be built with this same theory in mind.&#xFFFD; We will have many clusters over time and should avoid having clusters become the source of truth.&#xFFFD; If an issue occurs and a cluster has so many problems it can\u0019t be recovered, we\u0019ll rip it down and replace it with a fresh clean cluster.&#xFFFD; This required us to start building tooling that would allow us to easily deploy almost identical infrastructure in each of our environments.</p><p class=\"css-w48t98\">Our first goal was to drive our cluster deployments using a combination of tools including kOps, terraform and helm that each manage different, tightly coupled layers of our cluster infrastructure.&#xFFFD; We began to orchestrate these tools together driven by values files that could be layered across environments to ensure consistency but would give us the ability to configure specific tools when necessary.&#xFFFD; We broke our configuration into three buckets: cloud account configuration, tier (dev vs prod environments) and cluster specific.</p><div class=\"css-o4nbm4\"><figure class=\"css-6im75o\"><img class=\"undefined transparent\"><img class=\"placeholder \" src=\"data:image/svg+xml,%3csvg xmlns=&apos;http://www.w3.org/2000/svg&apos; viewBox=&apos;0 0 300 146&apos;%3e%3cdefs/%3e%3cfilter id=&apos;a&apos;%3e%3cfeGaussianBlur stdDeviation=&apos;12&apos;/%3e%3c/filter%3e%3crect width=&apos;100%25&apos; height=&apos;100%25&apos; fill=&apos;%23343434&apos;/%3e%3cg filter=&apos;url(%23a)&apos;%3e%3cg fill-opacity=&apos;.5&apos; transform=&apos;translate(.6 .6) scale(1.17188)&apos;%3e%3ccircle r=&apos;1&apos; fill=&apos;%234a4c49&apos; transform=&apos;matrix(-26.1215 72.15985 -53.11773 -19.22835 .1 77.1)&apos;/%3e%3cpath fill=&apos;%239e9d9f&apos; d=&apos;M2 90h52v3H2z&apos;/%3e%3cpath fill=&apos;%23779477&apos; d=&apos;M3 50h60v3H3z&apos;/%3e%3cpath fill=&apos;%23a7a6a8&apos; d=&apos;M6 64h27v3H6z&apos;/%3e%3cpath fill=&apos;%237c746e&apos; d=&apos;M-13.5 120.3l-2.5 12L8.7 115l37.2 3.3z&apos;/%3e%3cellipse cx=&apos;12&apos; cy=&apos;38&apos; fill=&apos;%23959496&apos; rx=&apos;11&apos; ry=&apos;2&apos;/%3e%3cellipse cx=&apos;80&apos; cy=&apos;104&apos; fill=&apos;%235c5c5c&apos; rx=&apos;54&apos; ry=&apos;2&apos;/%3e%3cellipse cy=&apos;76&apos; fill=&apos;%23282729&apos; rx=&apos;5&apos; ry=&apos;77&apos;/%3e%3c/g%3e%3c/g%3e%3c/svg%3e\"></figure></div><p class=\"css-w48t98\">We hard code specific tool versions to ensure consistency across our environments.&#xFFFD; Our automation will utilize these versions of tools and configuration when spinning up our clusters. In this case, for our team\u0019s development tier, we also disable Velero, a Kubernetes backup tool, since these clusters never contain real data that needs to be retained. Below, you can see how we tie our clusters to our tiers and we can override any value necessary.&#xFFFD; This abstraction layer gives our team amazing control and allows us to quickly roll out new versions or configurations to development environments and later promote them to production.</p><div class=\"css-o4nbm4\"><figure class=\"css-bx0a3q\"><img class=\"undefined transparent\"><img class=\"placeholder \" src=\"data:image/svg+xml,%3csvg xmlns=&apos;http://www.w3.org/2000/svg&apos; viewBox=&apos;0 0 300 69&apos;%3e%3cdefs/%3e%3cfilter id=&apos;a&apos;%3e%3cfeGaussianBlur stdDeviation=&apos;12&apos;/%3e%3c/filter%3e%3crect width=&apos;100%25&apos; height=&apos;100%25&apos; fill=&apos;%23343434&apos;/%3e%3cg filter=&apos;url(%23a)&apos;%3e%3cg fill-opacity=&apos;.5&apos; transform=&apos;translate(.6 .6) scale(1.17188)&apos;%3e%3ccircle r=&apos;1&apos; fill=&apos;%23535651&apos; transform=&apos;matrix(2.34795 -18.01475 45.69062 5.95507 17.3 35.6)&apos;/%3e%3cpath fill=&apos;%23656366&apos; d=&apos;M6 73l1.3-17.1 30.3-11.3L6.5 30.2z&apos;/%3e%3cellipse cx=&apos;35&apos; cy=&apos;38&apos; fill=&apos;%236c8a6c&apos; rx=&apos;36&apos; ry=&apos;2&apos;/%3e%3cellipse cx=&apos;208&apos; cy=&apos;11&apos; fill=&apos;%23555&apos; rx=&apos;238&apos; ry=&apos;2&apos;/%3e%3cellipse cx=&apos;28&apos; cy=&apos;41&apos; fill=&apos;%23211f20&apos; rx=&apos;40&apos; ry=&apos;2&apos;/%3e%3cpath fill=&apos;%23a6a6a6&apos; d=&apos;M6 44h26v2H6z&apos;/%3e%3cpath fill=&apos;%23363637&apos; d=&apos;M0 20h162v17H0z&apos;/%3e%3cpath fill=&apos;%239f9e9f&apos; d=&apos;M1 31h20v2H1z&apos;/%3e%3c/g%3e%3c/g%3e%3c/svg%3e\"></figure></div><h2 class=\"css-1nekmyi\">Deploying &amp; Managing Clusters</h2><p class=\"css-w48t98\">Now that we had configured values and tools the way we wanted, we knew we needed to make it easy for our team to run both locally and in continuous integration.&#xFFFD; This would be critical to the stability of our clusters and we had to ensure our clusters were testable and easily manageable, just like our application service teams.&#xFFFD; We wrote shell scripts that would allow our team to securely authenticate with the proper cloud accounts, ensure tool versions defined in our values were utilized and begin working on a given cluster.&#xFFFD; By building all our tooling in an idempotent fashion, we can easily re-run our automation, allowing for fast iteration.&#xFFFD; Of course, we don\u0019t typically want our engineers wasting time manually provisioning and running these types of things locally, but building tooling in a fashion that embraces running the same locally as in CI allows our team to quickly debug when issues arise.</p><p class=\"css-w48t98\">Everything we\u0019ve talked about here is versioned and controlled in GitHub, which lead our teams to ask, \u001chow can we better utilize CI/CD tooling to drive cluster automation?\u001d&#xFFFD; We began to write a variety of tests that cover a range of infrastructure failure cases.&#xFFFD; For example, we created a simple test that will check our configuration values files to make sure they\u0019re in proper YAML formatting.&#xFFFD; This resolved a major pain point that would inexplicably cause various issues for our teams.&#xFFFD; We eventually created additional tests to verify every cluster configuration against a given change, letting us know if we accidentally broke the build before deployment.&#xFFFD;&#xFFFD;</p><p class=\"css-w48t98\">Here are a few other tests we developed on our cluster configuration:</p><ul type=\"bullet\" class=\"css-11p44u2\"><li class=\"css-1xjbt1k\"><p>Helm linter - We utilize helm to deploy monitoring and other configuration tooling to our clusters. We use this linter to make sure those configurations are formatted correctly.</p></li><li class=\"css-1xjbt1k\"><p>Terraform Validation - Before updating any infrastructure components, we run a validation to test the changes that are about to occur.</p></li><li class=\"css-1xjbt1k\"><p>Kind deployment and upgrade - We utilize <a href=\"https://kind.sigs.k8s.io/\" class=\"css-s6bji1\">kind</a> (Kubernetes in Docker) to spin up sample clusters quickly and upgrade them with any changes we\u0019re making.</p></li><li class=\"css-1xjbt1k\"><p>End to End Cluster - We will also spin up a full cluster in the cloud and check to make sure our configurations are working as planned on certain git branches.</p></li></ul><div class=\"css-o4nbm4\"><figure class=\"css-1d1ruyv\"><img class=\"undefined transparent\"><img class=\"placeholder \" src=\"data:image/svg+xml,%3csvg xmlns=&apos;http://www.w3.org/2000/svg&apos; viewBox=&apos;0 0 300 160&apos;%3e%3cdefs/%3e%3cfilter id=&apos;a&apos;%3e%3cfeGaussianBlur stdDeviation=&apos;12&apos;/%3e%3c/filter%3e%3crect width=&apos;100%25&apos; height=&apos;100%25&apos; fill=&apos;%23444&apos;/%3e%3cg filter=&apos;url(%23a)&apos;%3e%3cg fill-opacity=&apos;.5&apos; transform=&apos;translate(.6 .6) scale(1.17188)&apos;%3e%3ccircle cx=&apos;133&apos; cy=&apos;80&apos; r=&apos;159&apos; fill=&apos;white&apos;/%3e%3ccircle cx=&apos;189&apos; cy=&apos;79&apos; r=&apos;219&apos; fill=&apos;white&apos;/%3e%3ccircle cx=&apos;78&apos; cy=&apos;94&apos; r=&apos;203&apos; fill=&apos;%23f2f2f2&apos;/%3e%3cpath fill=&apos;%23c4c4c4&apos; d=&apos;M156 41h19v58h-19z&apos;/%3e%3cellipse cx=&apos;91&apos; cy=&apos;73&apos; fill=&apos;%23c8c8c8&apos; rx=&apos;10&apos; ry=&apos;32&apos;/%3e%3ccircle r=&apos;1&apos; fill=&apos;%23d6d6d6&apos; transform=&apos;matrix(-40.54048 -3.37192 1.48372 -17.83874 197.6 42)&apos;/%3e%3ccircle r=&apos;1&apos; fill=&apos;%23d5d5d5&apos; transform=&apos;rotate(55.2 -15.2 70.5) scale(19.41214 35.44815)&apos;/%3e%3cpath fill=&apos;%23959595&apos; d=&apos;M220 76h11v7h-11z&apos;/%3e%3c/g%3e%3c/g%3e%3c/svg%3e\"></figure></div><h2 class=\"css-1nekmyi\">Automation</h2><p class=\"css-w48t98\">By using Pull Requests to manage our Kubernetes Automation, we were able to allow our team to rapidly iterate, utilizing CI to test and manage code changes once members of our team reviewed and approved them.&#xFFFD; As these code changes piled up, our team began to experiment with <a href=\"https://github.com/kubernetes/test-infra/tree/master/prow\" class=\"css-s6bji1\">Prow</a>, a CI/CD system built by Kubernetes in order to speed up their own CI/CD workflows.&#xFFFD; Since Prow runs on Kubernetes, it gave our team additional practice managing our own workloads, scaling, and automation that would allow us to run many concurrent tests and automation tooling.&#xFFFD; We set up a Prow component called Tide to manage when our pull requests would merge to make sure even old code changes were always retested against the latest configuration and tests before merging.</p><p class=\"css-w48t98\">Once these code changes are merged, we have additional jobs that will automatically upgrade our team\u0019s development Kubernetes cluster with the latest configuration.&#xFFFD; If a problem arises, our team is alerted in Slack that we should take a look.</p><div class=\"css-o4nbm4\"><figure class=\"css-1tdoask\"><img class=\"undefined transparent\"><img class=\"placeholder \" src=\"data:image/svg+xml,%3csvg xmlns=&apos;http://www.w3.org/2000/svg&apos; viewBox=&apos;0 0 300 23&apos;%3e%3cdefs/%3e%3cfilter id=&apos;a&apos;%3e%3cfeGaussianBlur stdDeviation=&apos;12&apos;/%3e%3c/filter%3e%3crect width=&apos;100%25&apos; height=&apos;100%25&apos; fill=&apos;%23273e4a&apos;/%3e%3cg filter=&apos;url(%23a)&apos;%3e%3cg fill-opacity=&apos;.5&apos; transform=&apos;translate(.6 .6) scale(1.17188)&apos;%3e%3ccircle r=&apos;1&apos; fill=&apos;%23170a09&apos; transform=&apos;matrix(-10.05941 -41.90046 179.04238 -42.98427 131.7 17.8)&apos;/%3e%3cellipse cx=&apos;183&apos; cy=&apos;11&apos; fill=&apos;%23406376&apos; rx=&apos;206&apos; ry=&apos;2&apos;/%3e%3cpath fill=&apos;white&apos; d=&apos;M12.2 6l-6.7 4.5L3.8 8l6.7-4.5z&apos;/%3e%3cpath fill=&apos;%23120503&apos; d=&apos;M67 0l-82 4 27-19z&apos;/%3e%3cpath fill=&apos;%23a69996&apos; d=&apos;M17 4h20v2H17z&apos;/%3e%3ccircle cx=&apos;8&apos; cy=&apos;8&apos; r=&apos;4&apos; fill=&apos;%23d0d8fb&apos;/%3e%3cellipse cy=&apos;18&apos; fill=&apos;%23130706&apos; rx=&apos;6&apos; ry=&apos;18&apos;/%3e%3ccircle r=&apos;1&apos; fill=&apos;%2316171a&apos; transform=&apos;rotate(178.1 77.6 3.2) scale(103.83318 5.99039)&apos;/%3e%3c/g%3e%3c/g%3e%3c/svg%3e\"></figure></div><p class=\"css-w48t98\">Our next challenge was to build a deployment strategy that allowed us to gate specific versions of our automation code and ensure that we could selectively roll out changes to each cluster once we were confident in them.&#xFFFD; We realized that embracing the Unix philosophy of <a href=\"https://en.wikipedia.org/wiki/Unix_philosophy#Do_One_Thing_and_Do_It_Well\" class=\"css-s6bji1\">&quot;Do One Thing and Do It Well&quot;</a> to write multiple decoupled jobs to accomplish tasks were often more reliable than long-running, complex jobs.&#xFFFD; For instance, we wrote a job that would check for a successful Kubernetes cluster upgrade and update a cluster version number stored in our configuration repo.&#xFFFD; If the version needed updating, this job opens a Pull Request for our team to review and approve.&#xFFFD; Before merging and upgrading this pull request, we run additional tests to ensure the upgrade for a given cluster will be successful.&#xFFFD; If a problem arises, rolling back is as simple as reverting this pull request in GitHub.</p><div class=\"css-o4nbm4\"><figure class=\"css-1plfoh4\"><img class=\"undefined transparent\"><img class=\"placeholder \" src=\"data:image/svg+xml,%3csvg xmlns=&apos;http://www.w3.org/2000/svg&apos; viewBox=&apos;0 0 300 126&apos;%3e%3cdefs/%3e%3cfilter id=&apos;a&apos;%3e%3cfeGaussianBlur stdDeviation=&apos;12&apos;/%3e%3c/filter%3e%3crect width=&apos;100%25&apos; height=&apos;100%25&apos; fill=&apos;%233c5c74&apos;/%3e%3cg filter=&apos;url(%23a)&apos;%3e%3cg fill-opacity=&apos;.5&apos; transform=&apos;translate(.6 .6) scale(1.17188)&apos;%3e%3ccircle cx=&apos;123&apos; cy=&apos;27&apos; r=&apos;158&apos; fill=&apos;white&apos;/%3e%3ccircle cx=&apos;155&apos; cy=&apos;46&apos; r=&apos;175&apos; fill=&apos;white&apos;/%3e%3ccircle r=&apos;1&apos; fill=&apos;white&apos; transform=&apos;rotate(84.7 27.5 126.5) scale(96.8107 244.42342)&apos;/%3e%3ccircle r=&apos;1&apos; fill=&apos;white&apos; transform=&apos;matrix(-118.22142 10.9636 -6.92696 -74.69394 184.6 78)&apos;/%3e%3cpath fill=&apos;%23778300&apos; d=&apos;M194 86h40v5h-40z&apos;/%3e%3cellipse cx=&apos;113&apos; cy=&apos;4&apos; fill=&apos;%237b7a7a&apos; rx=&apos;82&apos; ry=&apos;2&apos;/%3e%3cpath fill=&apos;%23ed0709&apos; d=&apos;M63 77l-15 4 21 2z&apos;/%3e%3cpath fill=&apos;%23009800&apos; d=&apos;M6 10h17v6H6z&apos;/%3e%3c/g%3e%3c/g%3e%3c/svg%3e\"></figure></div><h2 class=\"css-1nekmyi\">Wrap up</h2><p class=\"css-w48t98\">Our teams have learned quite a bit about working with and deploying Kubernetes. As with many highly technical and complicated tools, we found it very useful to \u001ccrawl, walk, run,\u001d while applying good software development and deployment principles. Although Sonos is still in its early stages of utilizing Kubernetes, the additional work our team put into our deployment strategies has made it much simpler for us to iterate and spend more time working with service teams on solving broad deployment challenges.&#xFFFD; In the future, we hope to share additional updates on our Kubernetes deployments, tooling and migration.</p><h2 class=\"css-1nekmyi\">Future work</h2><ul type=\"bullet\" class=\"css-11p44u2\"><li class=\"css-1xjbt1k\"><p>Additional testing strategies \u0013 Kubernetes testing and reliability tooling is still relatively young with new projects released constantly.</p></li><li class=\"css-1xjbt1k\"><p>Automate component upgrades \u0013 We\u0019d like to create jobs that can automatically upgrade components and tooling when new versions are released.</p></li><li class=\"css-1xjbt1k\"><p>Open source some of our components.</p></li></ul></div>",
      "contentAsText": "Over the past few years, our cloud teams at Sonos have grown to support numerous services that serve our millions of customers throughout the world. Our teams have made significant strides in automating much of the work behind orchestrating, configuring and maintaining complex cloud deployments. Early in our cloud journey, Sonos embraced configuration as code and open source projects like Ansible to ensure consistency and reliability across our services.� As time went on, our infrastructure teams started getting the same questions over and over:How can we get code tested and deployed to production faster?How can we reduce boilerplate and duplication across teams?How can we continue to support DevOps best practices as teams evolve and change?Luckily, over the past 5 years, there have been significant advancements in containerization and orchestration which can make answering these questions significantly easier.� Although our team experimented with a number of platforms, we ended up choosing Kubernetes as a solution and were impressed with its ability to self-heal, scale, and provide a consistent abstraction layer our developers could more easily interact with.� At this point, we easily could have said \u001cOkay, let's move everything to Kubernetes\u001d, but that would miss the additional complexity our Infrastructure teams would be taking on in supporting Kubernetes deployments. Although service teams would no longer have to support complex Ansible and cloud deployments, our team had some work ahead to better prepare for this journey.Running services in the cloud can be a complex endeavor, but let's dive into a few of the requirements our teams have.� First of all, monitoring and logging is essential to successfully making sure applications are functioning properly.� When problems arise, this tooling allows our teams to dive in quickly and diagnose the issues. Additionally, secret management and access to other dependencies such as databases, message queues and object stores are required to successfully stand up applications.� Finally, we try to utilize autoscaling as much as we can for our services. We needed to ensure our Kubernetes autoscaling was robust and could react quickly to both predictable and unpredictable traffic levels to ensure good service reliability and cost management. We quickly realized our infrastructure teams would need to implement solutions that would take these and other burdens off our services teams, allowing them to focus less on \u001chow\u001d they run their applications and more on \u001cwhat\u001d their applications should be doing.Cluster ConfigurationWhen teams at Sonos deploy their applications, we set best practices that enforce reproducible and reliable application deployment across environments. We quickly realized our infrastructure team would have to do the same thing for Kubernetes cluster deployments.� We began diving into areas where automation and configuration would be required to help build resilient clusters, allowing our service teams to focus on their applications and our infrastructure teams to focus on solving broad problems for multiple teams at once:Networking & CIDR Management \u0013 To fit into existing infrastructure, we must connect to and manage many network ranges.DNS \u0013 Services utilize DNS for service discovery inside and outside of our clusters.User and Service account access \u0013 Teams and their automation need secure access to their namespaces and cluster tooling.Alpha & Beta Features \u0013 Kubernetes developers often disable new APIs and features by default. We may want to enable these and allow teams to experiment with upcoming features.Cluster Monitoring, Alerting & Logging \u0013 Clusters and their components will need to be monitored so our infrastructure teams can properly maintain them as we onboard service teams.Autoscaling \u0013 Our clusters should scale themselves up or down as necessary to ensure all workloads are scheduled on cost effective instances.Node Removal \u0013 Our clusters and monitoring tools should be able to remove instances or nodes that go bad for a variety of reasons without human intervention.The next step our team followed was examining \u001chow\u001d we would deliver such a solution to Sonos.� We spun up Kubernetes clusters with open source tools such as kOps and eksctl for our team to get familiar with the tooling.� We knew we wanted options to be cloud agnostic and work with tooling that could easily be swapped out over time. Quickly, we realized that there were a few issues with building clusters and simply \u001cdeploying code to clusters.\u001dWhen building infrastructure, our teams try to follow the Pets vs Cattle analogy of Randy Bias and Bill Baker, building highly available services and when unforeseen issues arise, we simply remove bad instances.� As we imagined a year or two down the road, we realized our clusters needed to be built with this same theory in mind.� We will have many clusters over time and should avoid having clusters become the source of truth.� If an issue occurs and a cluster has so many problems it can\u0019t be recovered, we\u0019ll rip it down and replace it with a fresh clean cluster.� This required us to start building tooling that would allow us to easily deploy almost identical infrastructure in each of our environments.Our first goal was to drive our cluster deployments using a combination of tools including kOps, terraform and helm that each manage different, tightly coupled layers of our cluster infrastructure.� We began to orchestrate these tools together driven by values files that could be layered across environments to ensure consistency but would give us the ability to configure specific tools when necessary.� We broke our configuration into three buckets: cloud account configuration, tier (dev vs prod environments) and cluster specific.We hard code specific tool versions to ensure consistency across our environments.� Our automation will utilize these versions of tools and configuration when spinning up our clusters. In this case, for our team\u0019s development tier, we also disable Velero, a Kubernetes backup tool, since these clusters never contain real data that needs to be retained. Below, you can see how we tie our clusters to our tiers and we can override any value necessary.� This abstraction layer gives our team amazing control and allows us to quickly roll out new versions or configurations to development environments and later promote them to production.Deploying & Managing ClustersNow that we had configured values and tools the way we wanted, we knew we needed to make it easy for our team to run both locally and in continuous integration.� This would be critical to the stability of our clusters and we had to ensure our clusters were testable and easily manageable, just like our application service teams.� We wrote shell scripts that would allow our team to securely authenticate with the proper cloud accounts, ensure tool versions defined in our values were utilized and begin working on a given cluster.� By building all our tooling in an idempotent fashion, we can easily re-run our automation, allowing for fast iteration.� Of course, we don\u0019t typically want our engineers wasting time manually provisioning and running these types of things locally, but building tooling in a fashion that embraces running the same locally as in CI allows our team to quickly debug when issues arise.Everything we\u0019ve talked about here is versioned and controlled in GitHub, which lead our teams to ask, \u001chow can we better utilize CI/CD tooling to drive cluster automation?\u001d� We began to write a variety of tests that cover a range of infrastructure failure cases.� For example, we created a simple test that will check our configuration values files to make sure they\u0019re in proper YAML formatting.� This resolved a major pain point that would inexplicably cause various issues for our teams.� We eventually created additional tests to verify every cluster configuration against a given change, letting us know if we accidentally broke the build before deployment.��Here are a few other tests we developed on our cluster configuration:Helm linter - We utilize helm to deploy monitoring and other configuration tooling to our clusters. We use this linter to make sure those configurations are formatted correctly.Terraform Validation - Before updating any infrastructure components, we run a validation to test the changes that are about to occur.Kind deployment and upgrade - We utilize kind (Kubernetes in Docker) to spin up sample clusters quickly and upgrade them with any changes we\u0019re making.End to End Cluster - We will also spin up a full cluster in the cloud and check to make sure our configurations are working as planned on certain git branches.AutomationBy using Pull Requests to manage our Kubernetes Automation, we were able to allow our team to rapidly iterate, utilizing CI to test and manage code changes once members of our team reviewed and approved them.� As these code changes piled up, our team began to experiment with Prow, a CI/CD system built by Kubernetes in order to speed up their own CI/CD workflows.� Since Prow runs on Kubernetes, it gave our team additional practice managing our own workloads, scaling, and automation that would allow us to run many concurrent tests and automation tooling.� We set up a Prow component called Tide to manage when our pull requests would merge to make sure even old code changes were always retested against the latest configuration and tests before merging.Once these code changes are merged, we have additional jobs that will automatically upgrade our team\u0019s development Kubernetes cluster with the latest configuration.� If a problem arises, our team is alerted in Slack that we should take a look.Our next challenge was to build a deployment strategy that allowed us to gate specific versions of our automation code and ensure that we could selectively roll out changes to each cluster once we were confident in them.� We realized that embracing the Unix philosophy of \"Do One Thing and Do It Well\" to write multiple decoupled jobs to accomplish tasks were often more reliable than long-running, complex jobs.� For instance, we wrote a job that would check for a successful Kubernetes cluster upgrade and update a cluster version number stored in our configuration repo.� If the version needed updating, this job opens a Pull Request for our team to review and approve.� Before merging and upgrading this pull request, we run additional tests to ensure the upgrade for a given cluster will be successful.� If a problem arises, rolling back is as simple as reverting this pull request in GitHub.Wrap upOur teams have learned quite a bit about working with and deploying Kubernetes. As with many highly technical and complicated tools, we found it very useful to \u001ccrawl, walk, run,\u001d while applying good software development and deployment principles. Although Sonos is still in its early stages of utilizing Kubernetes, the additional work our team put into our deployment strategies has made it much simpler for us to iterate and spend more time working with service teams on solving broad deployment challenges.� In the future, we hope to share additional updates on our Kubernetes deployments, tooling and migration.Future workAdditional testing strategies \u0013 Kubernetes testing and reliability tooling is still relatively young with new projects released constantly.Automate component upgrades \u0013 We\u0019d like to create jobs that can automatically upgrade components and tooling when new versions are released.Open source some of our components.",
      "description": "Read more about the unique combination of software and hardware that powers the Sonos ecosystem. Welcome to our technology blog.",
      "ogDescription": "Read more about the unique combination of software and hardware that powers the Sonos ecosystem. Welcome to our technology blog."
    },
    {
      "url": "https://www.alanmbarr.com/blog/internal-developer-platform/",
      "title": "Let\u0019s Start a Project",
      "content": "<div class=\"singleBlog__content\"><p>Your current software engineering-focused internal developer platform is mediocre and aging rapidly. You want to make a better one. You are going to spend 18-24 months making your new platform and train all 100+ software engineers on learning Kubernetes, kubectl, and Istio. Mission accomplished? Nope, you failed real hard over a long time and taught a bunch of people what should have been an <a href=\"https://twitter.com/kelseyhightower/status/1178822088481636353\">implementation detail</a>.</p><p>You need an actual &#x201C;internal developer platform&#x201D; and that could be composed of many things but first and foremost you need to start with enabling your developers to do their job quickly without hand-offs, tickets, or other teams causing bottlenecks. If you are not focused on self-service you will fail. Kubernetes is not magic fairy dust that will fix all your problems. You need to understand your users and what they are trying to achieve.</p><p>What&#x2019;s stopping you from building a kick-ass internal developer platform on top of Kubernetes? Inertia, a captive audience, a lack of vision, and potentially more. You might be lucky enough to be in an organization that does not think twice about using cloud services and you can cobble together what you need to get the job done. Is the solution affordable for your organization? Will it require significant maintenance over the long run? The benefit of Kubernetes is that you have a stable long-lasting API to provide this infrastructure substrate that you can control when you upgrade. When you use the cloud you are bound by the whims of public cloud providers.</p><p><a href=\"/blog/images/platforms_by_type.png\"><img src=\"/blog/images/platforms_by_type.png\" class=\"img-fluid\" alt=\"Platforms\"></a></p><p>Is this your first time building a platform? You are highly likely to fail unless you seek help. What is the point of the effort and what outcomes are you seeking? You need to get help and talk to someone, anyone, about what your plans are because you will be blindsided. You might be lucky 100% of leadership is aligned and trusts you to make all the right decisions but that is rare. Talk to someone like <a href=\"https://postlight.com/insights/here-is-postlight\">Postlight</a> if they cannot help you they will at least send you in the right direction.</p><p>Your problem might be as simple as, &#x201C;We have an internal team well-versed in CI and can get their code in containers we just need an easy place for them to deploy and iterate on their ideas&#x201D;. In that case, take a look at <a href=\"https://humanitec.com/\">Humanitec&#x2019;s</a> rapidly provisioned isolated Kubernetes environments in the public cloud. That is only the beginning you have lots of different pieces to connect, a small team, and not enough time to accomplish it all.</p><p>An &#x201C;Internal Developer Platform&#x201D; is meant to make the internal software engineers more effective at their jobs by providing tools that add leverage for them. Your team could be composed of <a href=\"https://softwareengineeringdaily.com/2020/02/13/setting-the-stage-for-platform-engineering/\">&#x201C;Platform Engineering&#x201D;</a> or it could be called something entirely different. According to an Adobe <a href=\"https://medium.com/adobetech/why-do-organizations-need-a-platform-team-910d79893e0a\">blog</a>, around 100 engineers is the inflection point when this type of platform is necessary.</p><h2 id=\"internal-platform-product-management-is-key-to-success\">Internal Platform Product Management is key to success</h2><p>Since stepping into my role as an internal platform product owner I struggled to know what the right terms to type into a search engine to find the help I was looking for. I stumbled upon material by <a href=\"https://docs.google.com/presentation/d/194AAf3csXCNN3HgNyOYiXFrWl44jU7rK88CltprnB9k/edit?usp=sharing\">Netflix</a>, <a href=\"https://lethain.com/product-management-infra-engineering/\">Will Larson</a>, and <a href=\"https://medium.com/@skamille/product-for-internal-platforms-9205c3a08142\">Camille Fournier</a> and all their writing is incredibly helpful but I imagine there are a lot of other people out their managing products like this without a clue in the world on where to start.</p><h2 id=\"start-here\">Start here</h2><ol><li>Get help, these are made all the time anyone&#x2019;s experience will help you</li><li>Cast a big vision and focus on the outcomes, not the specific technologies you are implementing</li><li>Listen to your stakeholders and do not implement exactly what they want</li></ol></div>",
      "contentAsText": "Your current software engineering-focused internal developer platform is mediocre and aging rapidly. You want to make a better one. You are going to spend 18-24 months making your new platform and train all 100+ software engineers on learning Kubernetes, kubectl, and Istio. Mission accomplished? Nope, you failed real hard over a long time and taught a bunch of people what should have been an implementation detail.You need an actual “internal developer platform” and that could be composed of many things but first and foremost you need to start with enabling your developers to do their job quickly without hand-offs, tickets, or other teams causing bottlenecks. If you are not focused on self-service you will fail. Kubernetes is not magic fairy dust that will fix all your problems. You need to understand your users and what they are trying to achieve.What’s stopping you from building a kick-ass internal developer platform on top of Kubernetes? Inertia, a captive audience, a lack of vision, and potentially more. You might be lucky enough to be in an organization that does not think twice about using cloud services and you can cobble together what you need to get the job done. Is the solution affordable for your organization? Will it require significant maintenance over the long run? The benefit of Kubernetes is that you have a stable long-lasting API to provide this infrastructure substrate that you can control when you upgrade. When you use the cloud you are bound by the whims of public cloud providers.Is this your first time building a platform? You are highly likely to fail unless you seek help. What is the point of the effort and what outcomes are you seeking? You need to get help and talk to someone, anyone, about what your plans are because you will be blindsided. You might be lucky 100% of leadership is aligned and trusts you to make all the right decisions but that is rare. Talk to someone like Postlight if they cannot help you they will at least send you in the right direction.Your problem might be as simple as, “We have an internal team well-versed in CI and can get their code in containers we just need an easy place for them to deploy and iterate on their ideas”. In that case, take a look at Humanitec’s rapidly provisioned isolated Kubernetes environments in the public cloud. That is only the beginning you have lots of different pieces to connect, a small team, and not enough time to accomplish it all.An “Internal Developer Platform” is meant to make the internal software engineers more effective at their jobs by providing tools that add leverage for them. Your team could be composed of “Platform Engineering” or it could be called something entirely different. According to an Adobe blog, around 100 engineers is the inflection point when this type of platform is necessary.Internal Platform Product Management is key to successSince stepping into my role as an internal platform product owner I struggled to know what the right terms to type into a search engine to find the help I was looking for. I stumbled upon material by Netflix, Will Larson, and Camille Fournier and all their writing is incredibly helpful but I imagine there are a lot of other people out their managing products like this without a clue in the world on where to start.Start hereGet help, these are made all the time anyone’s experience will help youCast a big vision and focus on the outcomes, not the specific technologies you are implementingListen to your stakeholders and do not implement exactly what they want"
    },
    {
      "url": "https://brennerm.github.io/posts/setting-up-eks-with-irsa-using-terraform.html",
      "title": "Setting up an EKS cluster with IAM/IRSA integration",
      "content": "<section class=\"content\"> <section class=\"byline\"> 6 min read - November 15, 2020 - <p class=\"taglist\">[ <a href=\"/tag/aws\"><code>aws</code></a> <a href=\"/tag/kubernetes\"><code>kubernetes</code></a> <a href=\"/tag/terraform\"><code>terraform</code></a> ]</p> </section> <p>AWS\u0019 IAM service is a powerful system to provide fine-grained control over AWS resources. Additionally it is integrated into several AWS services and EKS is no exception. Next to the cluster role, <a href=\"https://aws.amazon.com/blogs/opensource/introducing-fine-grained-iam-roles-service-accounts/\">AWS introduced 2019</a> the concept of IRSA, which stands for IAM Roles for Service Accounts.</p> <p>Together with Kubernetes\u0019 RBAC system it allows to assign IAM role capabilities to K8s computing resources like <em>Pods</em> and <em>Jobs</em>. A simple use case you can imagine is allowing a Pod to write to a S3 bucket. After reading through this blog post you will understand how to create an EKS cluster using Terraform with IRSA support and how to make use of it.</p> <h2 id=\"preparing-the-vpc\"> <a href=\"#preparing-the-vpc\">Preparing the VPC</a> </h2>\n<p>Before creating an EKS cluster you need to set up a VPC network that your data and control plane traffic can go through. It will also define whether your Kubernetes API endpoint will be accessible publicly or only from within a private network.</p> <p>Depending on your requirements the VPC configuration can be more or less complex. If you don\u0019t want to do anything too crazy I suggest to use <a href=\"https://github.com/terraform-aws-modules/terraform-aws-vpc\"><em>aws-vpc</em></a> module. It provides a nice abstraction layer for the AWS VPC Terraform resources that are being used under the hood. Below you can find an example VPC configuration that can act as a starting point for your EKS.</p> <div class=\"language-hcl highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">module</span> <span class=\"s2\">&quot;vpc&quot;</span> <span class=\"p\">{</span> <span class=\"nx\">source</span> <span class=\"p\">=</span> <span class=\"s2\">&quot;terraform-aws-modules/vpc/aws&quot;</span> <span class=\"nx\">name</span> <span class=\"p\">=</span> <span class=\"s2\">&quot;my-vpc&quot;</span> <span class=\"nx\">cidr</span> <span class=\"p\">=</span> <span class=\"s2\">&quot;10.0.0.0/16&quot;</span> <span class=\"nx\">azs</span> <span class=\"p\">=</span> <span class=\"p\">[</span><span class=\"s2\">&quot;eu-central-1a&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;eu-central-1b&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;eu-central-1c&quot;</span><span class=\"p\">]</span> <span class=\"nx\">public_subnets</span> <span class=\"p\">=</span> <span class=\"p\">[</span><span class=\"s2\">&quot;10.0.1.0/24&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;10.0.2.0/24&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;10.0.3.0/24&quot;</span><span class=\"p\">]</span> <span class=\"nx\">enable_dns_support</span> <span class=\"p\">=</span> <span class=\"kc\">true</span> <span class=\"nx\">enable_dns_hostnames</span> <span class=\"p\">=</span> <span class=\"kc\">true</span>\n<span class=\"p\">}</span>\n</code></pre></div></div> <p>Be aware that an EKS cluster needs at least two subnets in different availability zones. Enabling the DNS related flags is necessary to allow the worker nodes to find and register themselves at the API server.</p> <h2 id=\"creating-the-eks-cluster\"> <a href=\"#creating-the-eks-cluster\">Creating the EKS cluster</a> </h2>\n<p>Similar to the VPC, I recommend you to use the <a href=\"https://github.com/terraform-aws-modules/terraform-aws-eks\"><em>aws-eks</em></a> Terraform module if your EKS setup is not too far away from the ordinary.\nBelow you can find an example Terraform code snippet that uses the previously discussed VPC.</p>\n<div class=\"language-hcl highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">module</span> <span class=\"s2\">&quot;cluster&quot;</span> <span class=\"p\">{</span> <span class=\"nx\">source</span> <span class=\"p\">=</span> <span class=\"s2\">&quot;terraform-aws-modules/eks/aws&quot;</span> <span class=\"nx\">cluster_name</span> <span class=\"p\">=</span> <span class=\"s2\">&quot;cluster&quot;</span> <span class=\"nx\">cluster_version</span> <span class=\"p\">=</span> <span class=\"s2\">&quot;1.18&quot;</span> <span class=\"nx\">vpc_id</span> <span class=\"p\">=</span> <span class=\"nx\">var</span><span class=\"err\">.</span><span class=\"nx\">vpc_id</span> <span class=\"nx\">subnets</span> <span class=\"p\">=</span> <span class=\"nx\">var</span><span class=\"err\">.</span><span class=\"nx\">vpc_subnet_ids</span> <span class=\"nx\">enable_irsa</span> <span class=\"p\">=</span> <span class=\"kc\">true</span> <span class=\"nx\">worker_groups</span> <span class=\"p\">=</span> <span class=\"p\">[</span> <span class=\"p\">{</span> <span class=\"nx\">instance_type</span> <span class=\"p\">=</span> <span class=\"s2\">&quot;t3.medium&quot;</span> <span class=\"nx\">asg_max_size</span> <span class=\"p\">=</span> <span class=\"mi\">3</span> <span class=\"p\">}</span> <span class=\"p\">]</span>\n<span class=\"p\">}</span>\n</code></pre></div></div> <p>The <em>enable_irsa</em> flag will lead to the OIDC (OpenID Connect) provider being created. Additionally we will define a Terraform output that contains its ARN (Amazon Resource Name) which will be used in the next step.</p> <div class=\"language-hcl highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">output</span> <span class=\"s2\">&quot;oidc_provider_arn&quot;</span> <span class=\"p\">{</span> <span class=\"nx\">value</span> <span class=\"p\">=</span> <span class=\"nx\">module</span><span class=\"err\">.</span><span class=\"nx\">cluster</span><span class=\"err\">.</span><span class=\"nx\">oidc_provider_arn</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n<p>And that\u0019s all you need to continue with the next step.</p> <h2 id=\"creating-a-service-account-associated-with-an-iam-role\"> <a href=\"#creating-a-service-account-associated-with-an-iam-role\">Creating a service account associated with an IAM role</a> </h2>\n<p>In this example we are going to create a service account that has full access to all of your S3 buckets. This can be easily changed by adjusting the policies that you attach to your IAM role.</p> <p>To start with we need to define a few variables.</p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">ROLE_NAME</span><span class=\"o\">=</span>s3-writer <span class=\"c\"># the name of your IAM role</span>\n<span class=\"nv\">SERVICE_ACCOUNT_NAME</span><span class=\"o\">=</span>s3-writer <span class=\"c\"># the name of your service account name</span>\n<span class=\"nv\">SERVICE_ACCOUNT_NAMESPACE</span><span class=\"o\">=</span>default <span class=\"c\"># the namespace for your service account</span>\n<span class=\"nv\">PROVIDER_ARN</span><span class=\"o\">=</span><span class=\"si\">$(</span>terraform output <span class=\"nt\">-json</span> | jq <span class=\"nt\">-r</span> .oidc_provider_arn.value<span class=\"si\">)</span> <span class=\"c\"># the ARN of your OIDC provider</span>\n<span class=\"nv\">ISSUER_HOSTPATH</span><span class=\"o\">=</span><span class=\"si\">$(</span>aws eks describe-cluster <span class=\"nt\">--name</span> cluster <span class=\"nt\">--query</span> cluster.identity.oidc.issuer <span class=\"nt\">--output</span> text | <span class=\"nb\">cut</span> <span class=\"nt\">-f</span> 3- <span class=\"nt\">-d</span><span class=\"s1\">&apos;/&apos;</span><span class=\"si\">)</span> <span class=\"c\"># the host path of your OIDC issuer</span>\n</code></pre></div></div> <p>The most important step is defining the correct assume policy for your IAM role.</p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span><span class=\"nb\">cat</span> <span class=\"o\">&gt;</span> assume-policy.json <span class=\"o\">&lt;&lt;</span> <span class=\"no\">EOF</span><span class=\"sh\">\n{ &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Effect&quot;: &quot;Allow&quot;, &quot;Principal&quot;: { &quot;Federated&quot;: &quot;</span><span class=\"nv\">$PROVIDER_ARN</span><span class=\"sh\">&quot; }, &quot;Action&quot;: &quot;sts:AssumeRoleWithWebIdentity&quot;, &quot;Condition&quot;: { &quot;StringEquals&quot;: { &quot;</span><span class=\"k\">${</span><span class=\"nv\">ISSUER_HOSTPATH</span><span class=\"k\">}</span><span class=\"sh\">:sub&quot;: &quot;system:serviceaccount:</span><span class=\"k\">${</span><span class=\"nv\">SERVICE_ACCOUNT_NAMESPACE</span><span class=\"k\">}</span><span class=\"sh\">:</span><span class=\"k\">${</span><span class=\"nv\">SERVICE_ACCOUNT_NAME</span><span class=\"k\">}</span><span class=\"sh\">&quot; } } } ]\n}\n</span><span class=\"no\">EOF\n</span></code></pre></div></div>\n<p>This will allow the service account to switch to your role using the <em>AssumeRoleWithWebIdentity</em> command.</p> <p>Afterwards you can create the new role using the above assume policy and attach your desired policies to it.</p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>aws iam create-role <span class=\"nt\">--role-name</span> <span class=\"nv\">$ROLE_NAME</span> <span class=\"nt\">--assume-role-policy-document</span> file://assume-policy.json\n<span class=\"nv\">$ </span>aws iam update-assume-role-policy <span class=\"nt\">--role-name</span> <span class=\"nv\">$ROLE_NAME</span> <span class=\"nt\">--policy-document</span> file://assume-policy.json\n<span class=\"nv\">$ </span>aws iam attach-role-policy <span class=\"nt\">--role-name</span> <span class=\"nv\">$ROLE_NAME</span> <span class=\"nt\">--policy-arn</span> arn:aws:iam::aws:policy/AmazonS3FullAccess\n</code></pre></div></div> <p>The last step is to create the Kubernetes Service Account and annotate it with the role ARN.</p>\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>kubectl create sa <span class=\"nv\">$SERVICE_ACCOUNT_NAME</span>\n<span class=\"nv\">$ S3_ROLE_ARN</span><span class=\"o\">=</span><span class=\"si\">$(</span>aws iam get-role <span class=\"nt\">--role-name</span> <span class=\"nv\">$ROLE_NAME</span> <span class=\"nt\">--query</span> Role.Arn <span class=\"nt\">--output</span> text<span class=\"si\">)</span>\n<span class=\"nv\">$ </span>kubectl annotate sa <span class=\"nv\">$SERVICE_ACCOUNT_NAME</span> eks.amazonaws.com/role-arn<span class=\"o\">=</span><span class=\"nv\">$S3_ROLE_ARN</span>\n</code></pre></div></div> <h2 id=\"using-the-service-account-in-your-application\"> <a href=\"#using-the-service-account-in-your-application\">Using the service account in your application</a> </h2>\n<p>Assigning the newly created service account to your application only requires adding a single line to your <em>Deployment</em> or <em>Job</em> manifest.</p>\n<div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"na\">apiVersion</span><span class=\"pi\">:</span> <span class=\"s\">apps/v1</span>\n<span class=\"na\">kind</span><span class=\"pi\">:</span> <span class=\"s\">Deployment</span>\n<span class=\"na\">spec</span><span class=\"pi\">:</span> <span class=\"s\">...</span> <span class=\"s\">template</span><span class=\"pi\">:</span> <span class=\"s\">...</span> <span class=\"s\">spec</span><span class=\"pi\">:</span> <span class=\"na\">serviceAccountName</span><span class=\"pi\">:</span> <span class=\"s\">s3-writer</span> <span class=\"na\">containers</span><span class=\"pi\">:</span> <span class=\"pi\">-</span> <span class=\"na\">image</span><span class=\"pi\">:</span> <span class=\"s\">myapp:latest</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">myapp</span> <span class=\"s\">...</span>\n</code></pre></div></div> <p>This will result in the <a href=\"https://github.com/aws/amazon-eks-pod-identity-webhook/\">EKS Pod Identity Webhook</a> injecting some environment variables and a volume mount into each of your pods that contain the access credentials for your IAM role. If you use a recent version of the <a href=\"https://aws.amazon.com/tools/\">AWS SDK</a> these will be picked up automatically when creating a new session and you are good to go.</p> <p>As a conclusion you can find an overview of all components and processes being part of the IRSA concept below.</p>\n<figure class=\"image\"> <img src=\"https://d2908q01vomqb2.cloudfront.net/ca3512f4dfa95a03169c5a670a4c91a19b3077b4/2019/08/12/irp-eks-setup-1024x1015.png\" alt=\"Overview of IRSA components and processes\"> <figcaption> Overview of IRSA components and processes / <a href=\"https://aws.amazon.com/blogs/opensource/introducing-fine-grained-iam-roles-service-accounts/\">image source</a> </figcaption>\n</figure> <p>For further details check out <a href=\"https://aws.amazon.com/blogs/opensource/introducing-fine-grained-iam-roles-service-accounts/\">the official AWS blog post about IRSA</a>.</p> <section class=\"contact\"> If you have questions or want to give feedback feel free to contact me.<br> <a href=\"https://github.com/brennerm\"><i class=\"fa fa-2x fa-github\"></i></a> <a href=\"https://twitter.com/__brennerm\"><i class=\"fa fa-2x fa-twitter\"></i></a> <a href=\"mailto:xam.rennerb@gmail.com\" class=\"fa fa-2x fa-envelope-o\"></a> </section>\n</section>",
      "contentAsText": "  6 min read - November 15, 2020 - [ aws kubernetes terraform ]  AWS\u0019 IAM service is a powerful system to provide fine-grained control over AWS resources. Additionally it is integrated into several AWS services and EKS is no exception. Next to the cluster role, AWS introduced 2019 the concept of IRSA, which stands for IAM Roles for Service Accounts. Together with Kubernetes\u0019 RBAC system it allows to assign IAM role capabilities to K8s computing resources like Pods and Jobs. A simple use case you can imagine is allowing a Pod to write to a S3 bucket. After reading through this blog post you will understand how to create an EKS cluster using Terraform with IRSA support and how to make use of it.  Preparing the VPC \nBefore creating an EKS cluster you need to set up a VPC network that your data and control plane traffic can go through. It will also define whether your Kubernetes API endpoint will be accessible publicly or only from within a private network. Depending on your requirements the VPC configuration can be more or less complex. If you don\u0019t want to do anything too crazy I suggest to use aws-vpc module. It provides a nice abstraction layer for the AWS VPC Terraform resources that are being used under the hood. Below you can find an example VPC configuration that can act as a starting point for your EKS. module \"vpc\" { source = \"terraform-aws-modules/vpc/aws\" name = \"my-vpc\" cidr = \"10.0.0.0/16\" azs = [\"eu-central-1a\", \"eu-central-1b\", \"eu-central-1c\"] public_subnets = [\"10.0.1.0/24\", \"10.0.2.0/24\", \"10.0.3.0/24\"] enable_dns_support = true enable_dns_hostnames = true\n}\n Be aware that an EKS cluster needs at least two subnets in different availability zones. Enabling the DNS related flags is necessary to allow the worker nodes to find and register themselves at the API server.  Creating the EKS cluster \nSimilar to the VPC, I recommend you to use the aws-eks Terraform module if your EKS setup is not too far away from the ordinary.\nBelow you can find an example Terraform code snippet that uses the previously discussed VPC.\nmodule \"cluster\" { source = \"terraform-aws-modules/eks/aws\" cluster_name = \"cluster\" cluster_version = \"1.18\" vpc_id = var.vpc_id subnets = var.vpc_subnet_ids enable_irsa = true worker_groups = [ { instance_type = \"t3.medium\" asg_max_size = 3 } ]\n}\n The enable_irsa flag will lead to the OIDC (OpenID Connect) provider being created. Additionally we will define a Terraform output that contains its ARN (Amazon Resource Name) which will be used in the next step. output \"oidc_provider_arn\" { value = module.cluster.oidc_provider_arn\n}\n\nAnd that\u0019s all you need to continue with the next step.  Creating a service account associated with an IAM role \nIn this example we are going to create a service account that has full access to all of your S3 buckets. This can be easily changed by adjusting the policies that you attach to your IAM role. To start with we need to define a few variables.\nROLE_NAME=s3-writer # the name of your IAM role\nSERVICE_ACCOUNT_NAME=s3-writer # the name of your service account name\nSERVICE_ACCOUNT_NAMESPACE=default # the namespace for your service account\nPROVIDER_ARN=$(terraform output -json | jq -r .oidc_provider_arn.value) # the ARN of your OIDC provider\nISSUER_HOSTPATH=$(aws eks describe-cluster --name cluster --query cluster.identity.oidc.issuer --output text | cut -f 3- -d'/') # the host path of your OIDC issuer\n The most important step is defining the correct assume policy for your IAM role.\n$ cat > assume-policy.json << EOF\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"$PROVIDER_ARN\" }, \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"${ISSUER_HOSTPATH}:sub\": \"system:serviceaccount:${SERVICE_ACCOUNT_NAMESPACE}:${SERVICE_ACCOUNT_NAME}\" } } } ]\n}\nEOF\n\nThis will allow the service account to switch to your role using the AssumeRoleWithWebIdentity command. Afterwards you can create the new role using the above assume policy and attach your desired policies to it.\n$ aws iam create-role --role-name $ROLE_NAME --assume-role-policy-document file://assume-policy.json\n$ aws iam update-assume-role-policy --role-name $ROLE_NAME --policy-document file://assume-policy.json\n$ aws iam attach-role-policy --role-name $ROLE_NAME --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess\n The last step is to create the Kubernetes Service Account and annotate it with the role ARN.\n$ kubectl create sa $SERVICE_ACCOUNT_NAME\n$ S3_ROLE_ARN=$(aws iam get-role --role-name $ROLE_NAME --query Role.Arn --output text)\n$ kubectl annotate sa $SERVICE_ACCOUNT_NAME eks.amazonaws.com/role-arn=$S3_ROLE_ARN\n  Using the service account in your application \nAssigning the newly created service account to your application only requires adding a single line to your Deployment or Job manifest.\napiVersion: apps/v1\nkind: Deployment\nspec: ... template: ... spec: serviceAccountName: s3-writer containers: - image: myapp:latest name: myapp ...\n This will result in the EKS Pod Identity Webhook injecting some environment variables and a volume mount into each of your pods that contain the access credentials for your IAM role. If you use a recent version of the AWS SDK these will be picked up automatically when creating a new session and you are good to go. As a conclusion you can find an overview of all components and processes being part of the IRSA concept below.\n   Overview of IRSA components and processes / image source \n For further details check out the official AWS blog post about IRSA.  If you have questions or want to give feedback feel free to contact me.    \n",
      "description": "Setting up an EKS cluster with IAM/IRSA integration using Terraform and showing how to make use of it."
    },
    {
      "url": "https://medium.com/faun/local-kubernetes-for-linux-minikube-vs-microk8s-f096e8e869b2",
      "title": "Local Kubernetes for Linux\n\u0014\nMiniKube vs MicroK8s",
      "content": "<div><article><section class=\"cx cy cz da aj db dc s\"></section><div><section class=\"dh di dj dk dl\"><div class=\"n p\"><div class=\"ab ac ae af ag dm ai aj\"><figure class=\"gp gq gr gs gt gu cz da paragraph-image\"><img alt=\"Image for post\" class=\"t u v gx aj\" src=\"https://miro.medium.com/max/1340/0*GyMd_qSMWzHuBXJq.jpg\" width=\"670\" srcset=\"https://miro.medium.com/max/552/0*GyMd_qSMWzHuBXJq.jpg 276w, https://miro.medium.com/max/1104/0*GyMd_qSMWzHuBXJq.jpg 552w, https://miro.medium.com/max/1280/0*GyMd_qSMWzHuBXJq.jpg 640w, https://miro.medium.com/max/1340/0*GyMd_qSMWzHuBXJq.jpg 670w\" sizes=\"670px\"></figure><p id=\"6f56\" class=\"hi hj dp hk b hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if dh em\">In this article, we will focus on Linux. Minikube is still a contender here. Unfortunately, the Docker desktop is not available for Linux. Instead, we are going to look at MicroK8s, a Linux only solution for a lightweight local Kubernetes cluster.</p><p id=\"aa8d\" class=\"hi hj dp hk b hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if dh em\">We are evaluating these solutions and providing a short comparison based on ease of installation, deployment, and management.</p><p id=\"6057\" class=\"hi hj dp hk b hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if dh em\">To get in-Depth knowledge on Kubernetes you can enroll for a live demo on <br><a href=\"https://onlineitguru.com/kubernetes-training.html\" class=\"cm ig\"><strong class=\"hk ih\">Kubernetes online training</strong></a></p><h2 id=\"1e6c\" class=\"ii\">Minikube</h2><p id=\"e5a6\" class=\"hi hj dp hk b hl je hn ho hp jf hr hs ht jg hv hw hx jh hz ia ib ji id ie if dh em\">Minikube runs a single-node Kubernetes cluster inside a VM (e.g. Virtualbox ) in your local development environment. The result is a local Kubernetes endpoint that you can use with the kubectl client. Minikube supports most typical <a href=\"https://onlineitguru.com/blogger/what-is-kubernetes\" class=\"cm ig\"><strong class=\"hk ih\">Kubernetes</strong></a> features such as DNS, Dashboards, CNI, NodePorts, Config Maps, etc. It also supports multiple hypervisors, such as Virtualbox, kvm, etc.</p><h2 id=\"87a9\" class=\"ii\">Installation</h2><p id=\"50bb\" class=\"hi hj dp hk b hl je hn ho hp jf hr hs ht jg hv hw hx jh hz ia ib ji id ie if dh em\">In order to install Minikube to Linux, you can follow the steps described in the official documentation( https://github.com/kubernetes/minikube). In our evaluation, we used Ubuntu 18.04 LTS with VirtualBox support using the following commands:</p><p id=\"130a\" class=\"hi hj dp hk b hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if dh em\">Take your career to new heights of success with <a href=\"https://onlineitguru.com/kubernetes-training.html\" class=\"cm ig\"><strong class=\"hk ih\">Kubernetes Training</strong></a></p><figure class=\"gp gq gr gs gt gu cz da paragraph-image\"><img alt=\"Image for post\" class=\"t u v gx aj\" src=\"https://miro.medium.com/max/1586/1*VsuAaiQBdwi60GQesikXoQ.png\" width=\"793\" srcset=\"https://miro.medium.com/max/552/1*VsuAaiQBdwi60GQesikXoQ.png 276w, https://miro.medium.com/max/1104/1*VsuAaiQBdwi60GQesikXoQ.png 552w, https://miro.medium.com/max/1280/1*VsuAaiQBdwi60GQesikXoQ.png 640w, https://miro.medium.com/max/1400/1*VsuAaiQBdwi60GQesikXoQ.png 700w\" sizes=\"700px\"></figure><p id=\"df30\" class=\"hi hj dp hk b hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if dh em\">After installation of Minikube, the <code class=\"hc jp jq jr js b\">kubectl</code> tool needs to be installed in order to deploy and manage applications on Kubernetes. You can install <code class=\"hc jp jq jr js b\">kubectl</code> by adding a new APT repository using the following command:</p><figure class=\"gp gq gr gs gt gu cz da paragraph-image\"><img alt=\"Image for post\" class=\"t u v gx aj\" src=\"https://miro.medium.com/max/1684/1*5ntg5yMhdoX1o6XhKxedTQ.png\" width=\"842\" srcset=\"https://miro.medium.com/max/552/1*5ntg5yMhdoX1o6XhKxedTQ.png 276w, https://miro.medium.com/max/1104/1*5ntg5yMhdoX1o6XhKxedTQ.png 552w, https://miro.medium.com/max/1280/1*5ntg5yMhdoX1o6XhKxedTQ.png 640w, https://miro.medium.com/max/1400/1*5ntg5yMhdoX1o6XhKxedTQ.png 700w\" sizes=\"700px\"></figure><p id=\"584a\" class=\"hi hj dp hk b hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if dh em\">Finally, after a successful installation, you can start your minikube by issuing the command:</p><figure class=\"gp gq gr gs gt gu cz da paragraph-image\"><img alt=\"Image for post\" class=\"t u v gx aj\" src=\"https://miro.medium.com/max/1616/1*w8lfJdKgN8RB2c1lJ86uDw.png\" width=\"808\" srcset=\"https://miro.medium.com/max/552/1*w8lfJdKgN8RB2c1lJ86uDw.png 276w, https://miro.medium.com/max/1104/1*w8lfJdKgN8RB2c1lJ86uDw.png 552w, https://miro.medium.com/max/1280/1*w8lfJdKgN8RB2c1lJ86uDw.png 640w, https://miro.medium.com/max/1400/1*w8lfJdKgN8RB2c1lJ86uDw.png 700w\" sizes=\"700px\"></figure><h2 id=\"6c6b\" class=\"ii\">Microk8s</h2><p id=\"46ed\" class=\"hi hj dp hk b hl je hn ho hp jf hr hs ht jg hv hw hx jh hz ia ib ji id ie if dh em\">Microk8s is a new solution for running a lightweight Kubernetes local cluster. It was developed by the Kubernetes team at Canonical. It is designed to be a fast and lightweight upstream Kubernetes installation isolated from your local environment. This isolation is achieved by packaging all the binaries for Kubernetes, Docker.io, iptables, and CNI in a single snap package (available only in Ubuntu and compatible distributions).</p><p id=\"b730\" class=\"hi hj dp hk b hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if dh em\">By installing Microk8s using snap, you are able to create a \u001cclean\u001d deploy of the latest upstream Kubernetes on your local machine without any other overhead. The Snap tool is taking care of all needed operations and can upgrade all associated binaries to their latest versions. By default, Microk8s installs and runs the following services:</p><ul class><li id=\"fcd2\" class=\"hi hj dp hk b hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if jx jy jz em\">Api-server</li><li id=\"d1ef\" class=\"hi hj dp hk b hl ka hn ho hp kb hr hs ht kc hv hw hx kd hz ia ib ke id ie if jx jy jz em\">Controller-manager</li><li id=\"3593\" class=\"hi hj dp hk b hl ka hn ho hp kb hr hs ht kc hv hw hx kd hz ia ib ke id ie if jx jy jz em\">scheduler</li><li id=\"bc9a\" class=\"hi hj dp hk b hl ka hn ho hp kb hr hs ht kc hv hw hx kd hz ia ib ke id ie if jx jy jz em\">kubelet</li><li id=\"68cc\" class=\"hi hj dp hk b hl ka hn ho hp kb hr hs ht kc hv hw hx kd hz ia ib ke id ie if jx jy jz em\">cni</li></ul><p id=\"fe31\" class=\"hi hj dp hk b hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if dh em\">Additional services such as the <a class=\"cm ig\" href=\"https://medium.com/faun/kubernetes-dashboard-b7b9ecff5f15?source=---------12------------------\"><strong class=\"hk ih\">Kubernetes dashboard</strong></a> can be easily enabled/disabled using the <code class=\"hc jp jq jr js b\">microk8s.enable</code> and <code class=\"hc jp jq jr js b\">microk8s.disable</code> command. The list of available services are:</p><ol class><li id=\"c1e6\" class=\"hi hj dp hk b hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if kf jy jz em\">Dns</li><li id=\"5776\" class=\"hi hj dp hk b hl ka hn ho hp kb hr hs ht kc hv hw hx kd hz ia ib ke id ie if kf jy jz em\">Dashboard, including grafana and influxdb</li><li id=\"b41e\" class=\"hi hj dp hk b hl ka hn ho hp kb hr hs ht kc hv hw hx kd hz ia ib ke id ie if kf jy jz em\">Storage</li><li id=\"6d97\" class=\"hi hj dp hk b hl ka hn ho hp kb hr hs ht kc hv hw hx kd hz ia ib ke id ie if kf jy jz em\">Ingress, Istio</li><li id=\"d542\" class=\"hi hj dp hk b hl ka hn ho hp kb hr hs ht kc hv hw hx kd hz ia ib ke id ie if kf jy jz em\">Registry</li><li id=\"beaf\" class=\"hi hj dp hk b hl ka hn ho hp kb hr hs ht kc hv hw hx kd hz ia ib ke id ie if kf jy jz em\">Metrics Server</li></ol><h2 id=\"3a46\" class=\"ii\">Installation</h2><p id=\"b184\" class=\"hi hj dp hk b hl je hn ho hp jf hr hs ht jg hv hw hx jh hz ia ib ji id ie if dh em\">Microk8s can be installed as a single snap command, directly from the Snap store.</p><figure class=\"gp gq gr gs gt gu cz da paragraph-image\"><img alt=\"Image for post\" class=\"t u v gx aj\" src=\"https://miro.medium.com/max/1634/1*cWOyJwpen9UE4DIt9wCrRA.png\" width=\"817\" srcset=\"https://miro.medium.com/max/552/1*cWOyJwpen9UE4DIt9wCrRA.png 276w, https://miro.medium.com/max/1104/1*cWOyJwpen9UE4DIt9wCrRA.png 552w, https://miro.medium.com/max/1280/1*cWOyJwpen9UE4DIt9wCrRA.png 640w, https://miro.medium.com/max/1400/1*cWOyJwpen9UE4DIt9wCrRA.png 700w\" sizes=\"700px\"></figure><p id=\"b33e\" class=\"hi hj dp hk b hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if dh em\">This will install the <code class=\"hc jp jq jr js b\">microk8s</code> command and an api-server, controller-manager, scheduler, etcd, kubelet, cni, Kube-proxy, and Docker. To avoid any conflicts with an existing installation of Kubernetes, Microk8s adds a <code class=\"hc jp jq jr js b\">microk8s.kubectl</code> command, configured to exclusively access the new Microk8s install.</p><p id=\"b788\" class=\"hi hj dp hk b hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if dh em\">When following any generic Kubernetes instructions online, make sure to prefix <code class=\"hc jp jq jr js b\">kubectl</code> with Microk8s. To verify that installation was successful, you can use the following commands to retrieve available nodes and available services respectively:</p><figure class=\"gp gq gr gs gt gu cz da paragraph-image\"><img alt=\"Image for post\" class=\"t u v gx aj\" src=\"https://miro.medium.com/max/1622/1*ue7MkcrqpQCcTnJDeX72rg.png\" width=\"811\" srcset=\"https://miro.medium.com/max/552/1*ue7MkcrqpQCcTnJDeX72rg.png 276w, https://miro.medium.com/max/1104/1*ue7MkcrqpQCcTnJDeX72rg.png 552w, https://miro.medium.com/max/1280/1*ue7MkcrqpQCcTnJDeX72rg.png 640w, https://miro.medium.com/max/1400/1*ue7MkcrqpQCcTnJDeX72rg.png 700w\" sizes=\"700px\"></figure><h2 id=\"f116\" class=\"ii\">Management</h2><p id=\"8f7e\" class=\"hi hj dp hk b hl je hn ho hp jf hr hs ht jg hv hw hx jh hz ia ib ji id ie if dh em\">As mentioned above, Microk8s installs a barebones upstream Kubernetes. This means just the api-server, controller-manager, scheduler, kubelet, cni, and kube-proxy are installed and run. Additional services such as kube-dns and the dashboard can be run using the microk8s. enable command.</p><figure class=\"gp gq gr gs gt gu cz da paragraph-image\"><img alt=\"Image for post\" class=\"t u v gx aj\" src=\"https://miro.medium.com/max/1626/1*ZmtFrKHmThTSslMd4lMMQg.png\" width=\"813\" srcset=\"https://miro.medium.com/max/552/1*ZmtFrKHmThTSslMd4lMMQg.png 276w, https://miro.medium.com/max/1104/1*ZmtFrKHmThTSslMd4lMMQg.png 552w, https://miro.medium.com/max/1280/1*ZmtFrKHmThTSslMd4lMMQg.png 640w, https://miro.medium.com/max/1400/1*ZmtFrKHmThTSslMd4lMMQg.png 700w\" sizes=\"700px\"></figure><p id=\"2bc5\" class=\"hi hj dp hk b hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if dh em\">You can verify that all services are up and running with the following command:</p><figure class=\"gp gq gr gs gt gu cz da paragraph-image\"><img alt=\"Image for post\" class=\"t u v gx aj\" src=\"https://miro.medium.com/max/1642/1*Jt6YfV6HhD1fJXmLM-IfPA.png\" width=\"821\" srcset=\"https://miro.medium.com/max/552/1*Jt6YfV6HhD1fJXmLM-IfPA.png 276w, https://miro.medium.com/max/1104/1*Jt6YfV6HhD1fJXmLM-IfPA.png 552w, https://miro.medium.com/max/1280/1*Jt6YfV6HhD1fJXmLM-IfPA.png 640w, https://miro.medium.com/max/1400/1*Jt6YfV6HhD1fJXmLM-IfPA.png 700w\" sizes=\"700px\"></figure><p id=\"81c9\" class=\"hi hj dp hk b hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if dh em\">You can access any service by pointing the correct <code class=\"hc jp jq jr js b\">CLUSTER_IP</code> to your browser. For example, you can access the dashboard by using the following web address, https://10.152.183.77. See image below for the dashboard:</p><figure class=\"gp gq gr gs gt gu cz da paragraph-image\"><img alt=\"Image for post\" class=\"t u v gx aj\" src=\"https://miro.medium.com/max/1634/1*D-ow_ZZrp5OyTpOO4wyWKw.png\" width=\"817\" srcset=\"https://miro.medium.com/max/552/1*D-ow_ZZrp5OyTpOO4wyWKw.png 276w, https://miro.medium.com/max/1104/1*D-ow_ZZrp5OyTpOO4wyWKw.png 552w, https://miro.medium.com/max/1280/1*D-ow_ZZrp5OyTpOO4wyWKw.png 640w, https://miro.medium.com/max/1400/1*D-ow_ZZrp5OyTpOO4wyWKw.png 700w\" sizes=\"700px\"><figcaption class=\"kp kq db cz da kr ks cg b ew ci fz\">Kubernetes dashboard</figcaption></figure><p id=\"f386\" class=\"hi hj dp hk b hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if dh em\">At any time, you can pause and restart all Kubernetes services and installed containers without losing any of your configurations by issuing the following command. (Note that this will also disable all commands prefixed with Microk8s.)</p><figure class=\"gp gq gr gs gt gu cz da paragraph-image\"><img alt=\"Image for post\" class=\"t u v gx aj\" src=\"https://miro.medium.com/max/1632/1*iWutAalcztXvOnrfyoNfpQ.png\" width=\"816\" srcset=\"https://miro.medium.com/max/552/1*iWutAalcztXvOnrfyoNfpQ.png 276w, https://miro.medium.com/max/1104/1*iWutAalcztXvOnrfyoNfpQ.png 552w, https://miro.medium.com/max/1280/1*iWutAalcztXvOnrfyoNfpQ.png 640w, https://miro.medium.com/max/1400/1*iWutAalcztXvOnrfyoNfpQ.png 700w\" sizes=\"700px\"></figure><p id=\"227b\" class=\"hi hj dp hk b hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if dh em\">Removing Microk8s is very easy. You can do so by first disabling all Kubernetes services and then using the snap command to remove the complete installation and configuration files.</p><figure class=\"gp gq gr gs gt gu cz da paragraph-image\"><img alt=\"Image for post\" class=\"t u v gx aj\" src=\"https://miro.medium.com/max/1622/1*fzXAoj0rlv3JKocUFBaqcw.png\" width=\"811\" srcset=\"https://miro.medium.com/max/552/1*fzXAoj0rlv3JKocUFBaqcw.png 276w, https://miro.medium.com/max/1104/1*fzXAoj0rlv3JKocUFBaqcw.png 552w, https://miro.medium.com/max/1280/1*fzXAoj0rlv3JKocUFBaqcw.png 640w, https://miro.medium.com/max/1400/1*fzXAoj0rlv3JKocUFBaqcw.png 700w\" sizes=\"700px\"></figure><h2 id=\"e36a\" class=\"ii\">Deployment</h2><p id=\"86be\" class=\"hi hj dp hk b hl je hn ho hp jf hr hs ht jg hv hw hx jh hz ia ib ji id ie if dh em\">Deploying an nginx service is what you would expect, with the addition of the Microk8s prefix:</p><figure class=\"gp gq gr gs gt gu cz da paragraph-image\"><img alt=\"Image for post\" class=\"t u v gx aj\" src=\"https://miro.medium.com/max/1760/1*ASTvIy8Y0CRZRwgLLMmnbQ.png\" width=\"880\" srcset=\"https://miro.medium.com/max/552/1*ASTvIy8Y0CRZRwgLLMmnbQ.png 276w, https://miro.medium.com/max/1104/1*ASTvIy8Y0CRZRwgLLMmnbQ.png 552w, https://miro.medium.com/max/1280/1*ASTvIy8Y0CRZRwgLLMmnbQ.png 640w, https://miro.medium.com/max/1400/1*ASTvIy8Y0CRZRwgLLMmnbQ.png 700w\" sizes=\"700px\"></figure><p id=\"8e8e\" class=\"hi hj dp hk b hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if dh em\">You can monitor your deployed services using the command:</p><figure class=\"gp gq gr gs gt gu cz da paragraph-image\"><img alt=\"Image for post\" class=\"t u v gx aj\" src=\"https://miro.medium.com/max/1632/1*xtO8eS1bmcCtfAZSFmiLzw.png\" width=\"816\" srcset=\"https://miro.medium.com/max/552/1*xtO8eS1bmcCtfAZSFmiLzw.png 276w, https://miro.medium.com/max/1104/1*xtO8eS1bmcCtfAZSFmiLzw.png 552w, https://miro.medium.com/max/1280/1*xtO8eS1bmcCtfAZSFmiLzw.png 640w, https://miro.medium.com/max/1400/1*xtO8eS1bmcCtfAZSFmiLzw.png 700w\" sizes=\"700px\"></figure><p id=\"5340\" class=\"hi hj dp hk b hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if dh em\">Now you are ready to access your deployed web service by pointing the following web URL to your preferred web browser: <a href=\"http://10.152.183.125/\" class=\"cm ig\">http://10.152.183.125</a></p><h2 id=\"5d93\" class=\"ii\">Conclusions</h2><p id=\"9ef5\" class=\"hi hj dp hk b hl je hn ho hp jf hr hs ht jg hv hw hx jh hz ia ib ji id ie if dh em\">After looking at all solutions, here are our results&amp;</p><p id=\"6d5a\" class=\"hi hj dp hk b hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if dh em\">Minikube is a mature solution available for all major operating systems. Its main advantage is that it provides a unified way of working with a local Kubernetes cluster regardless of the operating system. It is perfect for people that are using multiple OS machines and have some basic familiarity with Kubernetes and Docker.</p><p id=\"a9ce\" class=\"hi hj dp hk b hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if dh em\">Pros:</p><ol class><li id=\"f3f5\" class=\"hi hj dp hk b hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if kf jy jz em\">Mature solution</li><li id=\"f35c\" class=\"hi hj dp hk b hl ka hn ho hp kb hr hs ht kc hv hw hx kd hz ia ib ke id ie if kf jy jz em\">Works on Windows (any version and edition), Mac and Linux</li><li id=\"3bc7\" class=\"hi hj dp hk b hl ka hn ho hp kb hr hs ht kc hv hw hx kd hz ia ib ke id ie if kf jy jz em\">Multiple drivers that can match any environment</li><li id=\"8384\" class=\"hi hj dp hk b hl ka hn ho hp kb hr hs ht kc hv hw hx kd hz ia ib ke id ie if kf jy jz em\">Can work with or without an intermediate VM on Linux (vmdriver=none)</li><li id=\"4eb3\" class=\"hi hj dp hk b hl ka hn ho hp kb hr hs ht kc hv hw hx kd hz ia ib ke id ie if kf jy jz em\">Installs several plugins (such as dashboard) by default</li><li id=\"0552\" class=\"hi hj dp hk b hl ka hn ho hp kb hr hs ht kc hv hw hx kd hz ia ib ke id ie if kf jy jz em\">Very flexible on installation requirements and upgrades</li></ol><p id=\"587f\" class=\"hi hj dp hk b hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if dh em\">Cons:</p><ol class><li id=\"89f4\" class=\"hi hj dp hk b hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if kf jy jz em\">Installation and removal not as streamlined as other solutions</li><li id=\"4f0b\" class=\"hi hj dp hk b hl ka hn ho hp kb hr hs ht kc hv hw hx kd hz ia ib ke id ie if kf jy jz em\">Can conflict with a local installation of other tools (such as Virtualbox)</li></ol><p id=\"50bd\" class=\"hi hj dp hk b hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if dh em\">MicroK8s is a very interesting solution as it runs directly on your machine with no other VM in between.</p><p id=\"d71f\" class=\"hi hj dp hk b hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if dh em\">Pros:</p><ol class><li id=\"5141\" class=\"hi hj dp hk b hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if kf jy jz em\">Very easy to install, upgrade, remove</li><li id=\"759f\" class=\"hi hj dp hk b hl ka hn ho hp kb hr hs ht kc hv hw hx kd hz ia ib ke id ie if kf jy jz em\">Completely isolated from other tools in your machine</li><li id=\"c787\" class=\"hi hj dp hk b hl ka hn ho hp kb hr hs ht kc hv hw hx kd hz ia ib ke id ie if kf jy jz em\">Does not need a VM, all services run locally</li></ol><p id=\"5b4d\" class=\"hi hj dp hk b hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if dh em\">Cons:</p><ol class><li id=\"752e\" class=\"hi hj dp hk b hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if kf jy jz em\">Only available for Snap supported Linux Distributions</li><li id=\"29d1\" class=\"hi hj dp hk b hl ka hn ho hp kb hr hs ht kc hv hw hx kd hz ia ib ke id ie if kf jy jz em\">Relatively new, possible unstable</li><li id=\"411f\" class=\"hi hj dp hk b hl ka hn ho hp kb hr hs ht kc hv hw hx kd hz ia ib ke id ie if kf jy jz em\">Minikube can also run directly on Linux (vm=driver none), so MicroK8s value proposition is diminished</li></ol><figure class=\"gp gq gr gs gt gu cz da paragraph-image\"><img alt=\"Image for post\" class=\"t u v gx aj\" src=\"https://miro.medium.com/max/1454/0*Piks8Tu6xUYpF4DU\" width=\"727\" srcset=\"https://miro.medium.com/max/552/0*Piks8Tu6xUYpF4DU 276w, https://miro.medium.com/max/1104/0*Piks8Tu6xUYpF4DU 552w, https://miro.medium.com/max/1280/0*Piks8Tu6xUYpF4DU 640w, https://miro.medium.com/max/1400/0*Piks8Tu6xUYpF4DU 700w\" sizes=\"700px\"></figure><p id=\"9b5e\" class=\"hi hj dp hk b hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if dh em\"><strong class=\"hk ih\">Follow us on </strong><a href=\"https://twitter.com/joinfaun\" class=\"cm ig\"><strong class=\"hk ih\">Twitter</strong></a><strong class=\"hk ih\"> </strong>=&amp;<strong class=\"hk ih\"> and </strong><a href=\"https://www.facebook.com/faun.dev/\" class=\"cm ig\"><strong class=\"hk ih\">Facebook</strong></a><strong class=\"hk ih\"> </strong>=e<strong class=\"hk ih\"> and </strong><a href=\"https://instagram.com/fauncommunity/\" class=\"cm ig\"><strong class=\"hk ih\">Instagram</strong></a><strong class=\"hk ih\"> </strong>=&#xFFFD; <strong class=\"hk ih\">and join our </strong><a href=\"https://www.facebook.com/groups/364904580892967/\" class=\"cm ig\"><strong class=\"hk ih\">Facebook</strong></a><strong class=\"hk ih\"> and </strong><a href=\"https://www.linkedin.com/company/faundev\" class=\"cm ig\"><strong class=\"hk ih\">Linkedin</strong></a><strong class=\"hk ih\"> Groups </strong>=&#xFFFD;<strong class=\"hk ih\">.</strong></p><p id=\"c972\" class=\"hi hj dp hk b hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if dh em\"><strong class=\"hk ih\">To join our community Slack team chat </strong>=&#xFFFD;\u000f <strong class=\"hk ih\">read our weekly Faun topics </strong>=&#xFFFD;\u000f,<strong class=\"hk ih\"> and connect with the community </strong>=&#xFFFD;<strong class=\"hk ih\"> click here\u0007</strong></p></div></div><div class=\"gu aj\"><figure class=\"gp gq gr gs gt gu aj paragraph-image\"><img alt=\"Image for post\" class=\"t u v gx aj\" src=\"https://miro.medium.com/max/3000/1*6P3WpLjGv5v1ucm5dgkucg.png\" width=\"1500\" srcset=\"https://miro.medium.com/max/552/1*6P3WpLjGv5v1ucm5dgkucg.png 276w, https://miro.medium.com/max/1104/1*6P3WpLjGv5v1ucm5dgkucg.png 552w, https://miro.medium.com/max/1280/1*6P3WpLjGv5v1ucm5dgkucg.png 640w, https://miro.medium.com/max/1456/1*6P3WpLjGv5v1ucm5dgkucg.png 728w, https://miro.medium.com/max/1632/1*6P3WpLjGv5v1ucm5dgkucg.png 816w, https://miro.medium.com/max/1808/1*6P3WpLjGv5v1ucm5dgkucg.png 904w, https://miro.medium.com/max/1984/1*6P3WpLjGv5v1ucm5dgkucg.png 992w, https://miro.medium.com/max/2160/1*6P3WpLjGv5v1ucm5dgkucg.png 1080w, https://miro.medium.com/max/2700/1*6P3WpLjGv5v1ucm5dgkucg.png 1350w, https://miro.medium.com/max/3000/1*6P3WpLjGv5v1ucm5dgkucg.png 1500w\" sizes=\"1500px\"></figure></div></section></div></article></div>",
      "contentAsText": "In this article, we will focus on Linux. Minikube is still a contender here. Unfortunately, the Docker desktop is not available for Linux. Instead, we are going to look at MicroK8s, a Linux only solution for a lightweight local Kubernetes cluster.We are evaluating these solutions and providing a short comparison based on ease of installation, deployment, and management.To get in-Depth knowledge on Kubernetes you can enroll for a live demo on Kubernetes online trainingMinikubeMinikube runs a single-node Kubernetes cluster inside a VM (e.g. Virtualbox ) in your local development environment. The result is a local Kubernetes endpoint that you can use with the kubectl client. Minikube supports most typical Kubernetes features such as DNS, Dashboards, CNI, NodePorts, Config Maps, etc. It also supports multiple hypervisors, such as Virtualbox, kvm, etc.InstallationIn order to install Minikube to Linux, you can follow the steps described in the official documentation( https://github.com/kubernetes/minikube). In our evaluation, we used Ubuntu 18.04 LTS with VirtualBox support using the following commands:Take your career to new heights of success with Kubernetes TrainingAfter installation of Minikube, the kubectl tool needs to be installed in order to deploy and manage applications on Kubernetes. You can install kubectl by adding a new APT repository using the following command:Finally, after a successful installation, you can start your minikube by issuing the command:Microk8sMicrok8s is a new solution for running a lightweight Kubernetes local cluster. It was developed by the Kubernetes team at Canonical. It is designed to be a fast and lightweight upstream Kubernetes installation isolated from your local environment. This isolation is achieved by packaging all the binaries for Kubernetes, Docker.io, iptables, and CNI in a single snap package (available only in Ubuntu and compatible distributions).By installing Microk8s using snap, you are able to create a \u001cclean\u001d deploy of the latest upstream Kubernetes on your local machine without any other overhead. The Snap tool is taking care of all needed operations and can upgrade all associated binaries to their latest versions. By default, Microk8s installs and runs the following services:Api-serverController-managerschedulerkubeletcniAdditional services such as the Kubernetes dashboard can be easily enabled/disabled using the microk8s.enable and microk8s.disable command. The list of available services are:DnsDashboard, including grafana and influxdbStorageIngress, IstioRegistryMetrics ServerInstallationMicrok8s can be installed as a single snap command, directly from the Snap store.This will install the microk8s command and an api-server, controller-manager, scheduler, etcd, kubelet, cni, Kube-proxy, and Docker. To avoid any conflicts with an existing installation of Kubernetes, Microk8s adds a microk8s.kubectl command, configured to exclusively access the new Microk8s install.When following any generic Kubernetes instructions online, make sure to prefix kubectl with Microk8s. To verify that installation was successful, you can use the following commands to retrieve available nodes and available services respectively:ManagementAs mentioned above, Microk8s installs a barebones upstream Kubernetes. This means just the api-server, controller-manager, scheduler, kubelet, cni, and kube-proxy are installed and run. Additional services such as kube-dns and the dashboard can be run using the microk8s. enable command.You can verify that all services are up and running with the following command:You can access any service by pointing the correct CLUSTER_IP to your browser. For example, you can access the dashboard by using the following web address, https://10.152.183.77. See image below for the dashboard:Kubernetes dashboardAt any time, you can pause and restart all Kubernetes services and installed containers without losing any of your configurations by issuing the following command. (Note that this will also disable all commands prefixed with Microk8s.)Removing Microk8s is very easy. You can do so by first disabling all Kubernetes services and then using the snap command to remove the complete installation and configuration files.DeploymentDeploying an nginx service is what you would expect, with the addition of the Microk8s prefix:You can monitor your deployed services using the command:Now you are ready to access your deployed web service by pointing the following web URL to your preferred web browser: http://10.152.183.125ConclusionsAfter looking at all solutions, here are our results&Minikube is a mature solution available for all major operating systems. Its main advantage is that it provides a unified way of working with a local Kubernetes cluster regardless of the operating system. It is perfect for people that are using multiple OS machines and have some basic familiarity with Kubernetes and Docker.Pros:Mature solutionWorks on Windows (any version and edition), Mac and LinuxMultiple drivers that can match any environmentCan work with or without an intermediate VM on Linux (vmdriver=none)Installs several plugins (such as dashboard) by defaultVery flexible on installation requirements and upgradesCons:Installation and removal not as streamlined as other solutionsCan conflict with a local installation of other tools (such as Virtualbox)MicroK8s is a very interesting solution as it runs directly on your machine with no other VM in between.Pros:Very easy to install, upgrade, removeCompletely isolated from other tools in your machineDoes not need a VM, all services run locallyCons:Only available for Snap supported Linux DistributionsRelatively new, possible unstableMinikube can also run directly on Linux (vm=driver none), so MicroK8s value proposition is diminishedFollow us on Twitter =& and Facebook =e and Instagram =� and join our Facebook and Linkedin Groups =�.To join our community Slack team chat =�\u000f read our weekly Faun topics =�\u000f, and connect with the community =� click here\u0007",
      "publishedDate": "2020-02-27T17:55:42.540Z",
      "description": "In this article, we will focus on Linux. Minikube is still a contender here. Unfortunately, the Docker desktop is not available for Linux. Instead, we are going to look at MicroK8s, a Linux only…",
      "ogDescription": "In this article, we will focus on Linux. Minikube is still a contender here. Unfortunately, the Docker desktop is not available for Linux…"
    },
    {
      "url": "https://blog.alcide.io/kubernetes-security-is-not-container-security",
      "title": "Kubernetes Security Is Not Container Security",
      "content": "<div class=\"blog-post-wrapper cell-wrapper\"> <p class=\"section\"> <div id=\"hs_cos_wrapper_post_body\" class=\"hs_cos_wrapper\"> <p><span>Container-Specific Security</span></p>\n<p>I recently had an interesting <a href=\"https://www.reddit.com/r/netsec/comments/jmmyb9/a_practical_introduction_to_container_security/gax2kgv/?utm_source=reddit&amp;utm_medium=web2x&amp;context=3\"><span>discussion</span></a> with <a href=\"https://cloudberry.engineering/about/\"><span>Gianluca Brindisi</span></a> from Spotify about the differences between Kubernetes Security and Container Security. (<a href=\"https://cloudberry.engineering/article/practical-introduction-container-security/\"><span>He wrote an excellent post about container security on his blog here.</span></a>) Typically, the discussion about container security focuses on general questions like the following that aren\u0019t focused on a specific orchestration framework like Kubernetes:</p>\n<ol>\n<li>What images do I run and do they have secure base images?</li>\n<li>Is my image registry secure?</li>\n<li>What operating system permissions do my containers run with?</li>\n<li>Is my container network properly segmented?</li>\n<li>What processes do my containers run and what files do those processes access at runtime?</li>\n</ol>\n<p>These are all important questions when zooming out and looking at container security in general. However in the Kubernetes context, something is lost.</p> <p><span>Kubernetes-Specific Security</span></p>\n<p>Let\u0019s illustrate the type of questions that you should ask in order to secure your <em>Kubernetes</em> cluster in addition to your <em>containers.</em> Some brief answers and tips are included for convenience.</p></div></p>\n<ol>\n<li><strong>Can someone bypass network segmentation by using the Kubernetes APIServer as a proxy and tunnel from one pod to another via </strong><strong><em>kubectl port-forward</em></strong><strong>?</strong> It depends on what Role-Based Access Control (RBAC) permissions that pod\u0019s service account has. (There isn\u0019t much documentation around the pods/portforward role, but you can see it in the source code <a href=\"https://github.com/kubernetes/kubernetes/blob/ef16faf409b7aec6b2171ec3e4a39c3f3f68c030/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/testdata/cluster-roles.yaml#L99\"><span>here</span></a>.)</li>\n<li><strong>If an attacker has breached one pod with restricted operating system permissions, can they escalate privileges by using the Kubernetes APIServer to launch a new pod with greater permissions?</strong> Yes, if that pod\u0019s service account has RBAC permissions to create a new pod and you haven\u0019t set up additional restrictions like<a href=\"https://kubernetes.io/docs/concepts/policy/pod-security-policy/\"><span> Pod Security Policies</span></a>.</li>\n<li><strong>Can an attacker use raw-sockets to wreak havoc on the cluster\u0019s container network? </strong><div><a href=\"/insecure-by-default-kubernetes-networking\">Yes, by default they usually can!</a></div></li>\n<li><strong>Are your security assumptions about Kubernetes namespaces wrong?</strong><span> Specifically, are you </span><a href=\"/three-ways-to-simplify-and-secure-your-infrastructure-using-kubernetes-namespaces\">incorrectly assuming that Kubernetes namespaces are equivalent to security boundaries</a><span> and that privileged pods in one namespace can\u0019t impact pods in another namespace?</span></li>\n</ol> <p><strong>Root Causes for Kubernetes Security Flaws</strong></p>\n<p>Most of the Kubernetes specific security flaws that we see at Alcide stem from one of four root causes:</p>\n<ol>\n<li>A well-intentioned developer or devops engineer once granted <strong>overly-permissive RBAC permissions to a default service account</strong> in order to \u001cmake things work\u001d and no-one today remembers that change or is aware of the lingering consequences.</li>\n<li><strong>The security team isn\u0019t familiar with Kubernetes</strong> and all of it\u0019s obscure pitfalls, or the devops team doesn\u0019t have enough security experience to recognize nuanced security mistakes. This is common even among extremely talented engineers because Kubernetes is both complicated and new.</li>\n<li><strong>People think that Kubernetes and containers do all kinds of magic which Kubernetes/Docker actually don\u0019t</strong><div>. For example, people assume that if an application is running in a container then surely it can\u0019t open a raw socket by default.</div></li>\n<li><strong>Default Kubernetes configurations are too permissive</strong><span> (here too, raw sockets are a good example)</span></li>\n</ol>\n<p><br>These four root causes lead to a variety of security flaws in Kubernetes clusters.</p> <p><strong>Securing Your Cluster</strong></p>\n<p>All of these issues can be fixed by <strong>proactively verifying that your cluster is properly configured,</strong> by making sure that workloads are properly segmented in the correct Kubernetes-native way, and by educating yourself and your teams about the nitty gritty details of Kubernetes and not just containers.</p> <p><strong>The Big Picture - Containers, Image Scanning, and Kubernetes</strong></p>\n<p>Kubernetes is best viewed as a \u001cCloud Operating System\u001d which runs applications called \u001ccontainers\u001d. Focusing on container security is important and it is equivalent to traditional application security. However, ignoring Kubernetes security and only focusing on container security is like ignoring Linux security and only focusing on Nginx/Apache security. The environment in which containers run, what the Container Network Interface (CNI) looks like, and what privileges the operating system called Kubernetes grants the containers (in terms of RBAC permissions) is extremely important if you want to secure your cloud.</p>\n<p>To complete this analogy, you can think of image scanning as the cloud equivalent of source-code scanning which checks if you have known vulnerabilities in your code.&#xA0;</p>\n<p>Image scanning is important but it isn\u0019t a replacement for a firewall, antivirus, or proper operating system configuration.</p>\n<p>In the \u001cold days\u001d, when containers ran only on top of Docker, container security was enough. Nowadays, make sure you don\u0019t overlook the operating system (Kubernetes) and focus only on the apps (containers) because doing so will leave large gaps in your security and compliance.&#xA0;</p> <p><strong>About the author:</strong> Natan Yellin is a senior software engineer at Alcide.io. (Psst, we\u0019re hiring!) <br>He also blogs about low-level Linux at <a href=\"http://natanyellin.com/\"><span>http://natanyellin.com</span></a>. <br>Be sure to follow <a href=\"https://twitter.com/alcideio\"><span>Alcide and </span></a><a href=\"https://twitter.com/aantn\"><span>Natan</span></a>&#xA0;on Twitter to hear about the latest in Kubernetes security before everyone else.</p> <p id=\"hubspot-topic_data\"> Topics: <a class=\"topic-link\" href=\"https://blog.alcide.io/topic/kubernetes\">kubernetes</a>, <a class=\"topic-link\" href=\"https://blog.alcide.io/topic/network-security\">network security</a>, <a class=\"topic-link\" href=\"https://blog.alcide.io/topic/kubernetes-security\">Kubernetes security</a>, <a class=\"topic-link\" href=\"https://blog.alcide.io/topic/container-networking\">container networking</a> </p> </div>",
      "contentAsText": "   Container-Specific Security\nI recently had an interesting discussion with Gianluca Brindisi from Spotify about the differences between Kubernetes Security and Container Security. (He wrote an excellent post about container security on his blog here.) Typically, the discussion about container security focuses on general questions like the following that aren\u0019t focused on a specific orchestration framework like Kubernetes:\n\nWhat images do I run and do they have secure base images?\nIs my image registry secure?\nWhat operating system permissions do my containers run with?\nIs my container network properly segmented?\nWhat processes do my containers run and what files do those processes access at runtime?\n\nThese are all important questions when zooming out and looking at container security in general. However in the Kubernetes context, something is lost. Kubernetes-Specific Security\nLet\u0019s illustrate the type of questions that you should ask in order to secure your Kubernetes cluster in addition to your containers. Some brief answers and tips are included for convenience.\n\nCan someone bypass network segmentation by using the Kubernetes APIServer as a proxy and tunnel from one pod to another via kubectl port-forward? It depends on what Role-Based Access Control (RBAC) permissions that pod\u0019s service account has. (There isn\u0019t much documentation around the pods/portforward role, but you can see it in the source code here.)\nIf an attacker has breached one pod with restricted operating system permissions, can they escalate privileges by using the Kubernetes APIServer to launch a new pod with greater permissions? Yes, if that pod\u0019s service account has RBAC permissions to create a new pod and you haven\u0019t set up additional restrictions like Pod Security Policies.\nCan an attacker use raw-sockets to wreak havoc on the cluster\u0019s container network? Yes, by default they usually can!\nAre your security assumptions about Kubernetes namespaces wrong? Specifically, are you incorrectly assuming that Kubernetes namespaces are equivalent to security boundaries and that privileged pods in one namespace can\u0019t impact pods in another namespace?\n Root Causes for Kubernetes Security Flaws\nMost of the Kubernetes specific security flaws that we see at Alcide stem from one of four root causes:\n\nA well-intentioned developer or devops engineer once granted overly-permissive RBAC permissions to a default service account in order to \u001cmake things work\u001d and no-one today remembers that change or is aware of the lingering consequences.\nThe security team isn\u0019t familiar with Kubernetes and all of it\u0019s obscure pitfalls, or the devops team doesn\u0019t have enough security experience to recognize nuanced security mistakes. This is common even among extremely talented engineers because Kubernetes is both complicated and new.\nPeople think that Kubernetes and containers do all kinds of magic which Kubernetes/Docker actually don\u0019t. For example, people assume that if an application is running in a container then surely it can\u0019t open a raw socket by default.\nDefault Kubernetes configurations are too permissive (here too, raw sockets are a good example)\n\nThese four root causes lead to a variety of security flaws in Kubernetes clusters. Securing Your Cluster\nAll of these issues can be fixed by proactively verifying that your cluster is properly configured, by making sure that workloads are properly segmented in the correct Kubernetes-native way, and by educating yourself and your teams about the nitty gritty details of Kubernetes and not just containers. The Big Picture - Containers, Image Scanning, and Kubernetes\nKubernetes is best viewed as a \u001cCloud Operating System\u001d which runs applications called \u001ccontainers\u001d. Focusing on container security is important and it is equivalent to traditional application security. However, ignoring Kubernetes security and only focusing on container security is like ignoring Linux security and only focusing on Nginx/Apache security. The environment in which containers run, what the Container Network Interface (CNI) looks like, and what privileges the operating system called Kubernetes grants the containers (in terms of RBAC permissions) is extremely important if you want to secure your cloud.\nTo complete this analogy, you can think of image scanning as the cloud equivalent of source-code scanning which checks if you have known vulnerabilities in your code. \nImage scanning is important but it isn\u0019t a replacement for a firewall, antivirus, or proper operating system configuration.\nIn the \u001cold days\u001d, when containers ran only on top of Docker, container security was enough. Nowadays, make sure you don\u0019t overlook the operating system (Kubernetes) and focus only on the apps (containers) because doing so will leave large gaps in your security and compliance.  About the author: Natan Yellin is a senior software engineer at Alcide.io. (Psst, we\u0019re hiring!) He also blogs about low-level Linux at http://natanyellin.com. Be sure to follow Alcide and Natan on Twitter to hear about the latest in Kubernetes security before everyone else.  Topics: kubernetes, network security, Kubernetes security, container networking  ",
      "description": "Kubernetes Security Is Not Container Security",
      "ogDescription": "Kubernetes Security Is Not Container Security"
    },
    {
      "url": "https://github.com/wonderix/shalm",
      "title": "wonderix/shalm",
      "content": "<div><div><article class=\"markdown-body entry-content container-lg\">\n<p>This project brings the starlark scripting language to helm charts.</p>\n<p><a href=\"https://github.com/wonderix/shalm/workflows/build%20and%20test/badge.svg\"><img src=\"https://github.com/wonderix/shalm/workflows/build%20and%20test/badge.svg\" alt=\"build and test\"></a></p>\n<h2><a id=\"user-content-features\" class=\"anchor\" href=\"https://github.com/wonderix/shalm#features\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Features</h2>\n<ul>\n<li>Ease orchestration of deployments</li>\n<li>Strict separation of templates and logic</li>\n<li>Define APIs for helm charts</li>\n<li>Control deployment by overriding methods</li>\n<li>Compatible with helm</li>\n<li>Share a common service like a database manager or an ingress between a set of sub charts</li>\n<li>Use starlark methods in templates (replacement for <code>_helpers.tpl</code>)</li>\n<li>Interact with kubernetes during installation</li>\n<li>Manage user credentials</li>\n<li>Manage certificates</li>\n<li>Act as glue code between helm charts</li>\n<li>Rendering of <a href=\"https://get-ytt.io/\">ytt templates</a></li>\n<li>Also available as kubernetes controller</li>\n<li>Easy embeddable and extendable</li>\n<li>Integration of <a href=\"https://github.com/k14s/kapp\">kapp</a></li>\n</ul>\n<h2><a id=\"user-content-installation\" class=\"anchor\" href=\"https://github.com/wonderix/shalm#installation\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Installation</h2>\n<p>Installing shalm can be done in <a href=\"https://github.com/wonderix/shalm/blob/master/doc/installation.md\">various ways</a></p>\n<h2><a id=\"user-content-architecture\" class=\"anchor\" href=\"https://github.com/wonderix/shalm#architecture\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Architecture</h2>\n<p><a href=\"https://github.com/wonderix/shalm/blob/master/doc/Layer.png\"><img src=\"https://github.com/wonderix/shalm/raw/master/doc/Layer.png\" alt></a></p>\n<h2><a id=\"user-content-usage\" class=\"anchor\" href=\"https://github.com/wonderix/shalm#usage\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Usage</h2>\n<p>How to start shalm from the command line is described <a href=\"https://github.com/wonderix/shalm/blob/master/doc/command_line.md\">here</a></p>\n<h2><a id=\"user-content-getting-started\" class=\"anchor\" href=\"https://github.com/wonderix/shalm#getting-started\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Getting started</h2>\n<p>There is a <a href=\"https://github.com/wonderix/shalm/blob/master/doc/getting_started.md\">small tutorial</a> available.</p>\n<h2><a id=\"user-content-user-guide\" class=\"anchor\" href=\"https://github.com/wonderix/shalm#user-guide\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>User Guide</h2>\n<p>Solutions for a set of problems are shown <a href=\"https://github.com/wonderix/shalm/blob/master/doc/user_guide.md\">here</a></p>\n<h2><a id=\"user-content-repos\" class=\"anchor\" href=\"https://github.com/wonderix/shalm#repos\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Repos</h2>\n<p>Shalm can load charts from a <a href=\"https://github.com/wonderix/shalm/blob/master/doc/repos.md\">various set of locations</a>.</p>\n<h2><a id=\"user-content-kubernetes-controller\" class=\"anchor\" href=\"https://github.com/wonderix/shalm#kubernetes-controller\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Kubernetes Controller</h2>\n<p>Shalm can also run as <a href=\"https://github.com/wonderix/shalm/blob/master/doc/controller.md\">controller</a> inside a kubernets cluster</p>\n<h2><a id=\"user-content-reference\" class=\"anchor\" href=\"https://github.com/wonderix/shalm#reference\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Reference</h2>\n<p>A reference for the complete <a href=\"https://github.com/wonderix/shalm/blob/master/doc/reference.md\">shalm starlark API</a></p>\n<h2><a id=\"user-content-testing\" class=\"anchor\" href=\"https://github.com/wonderix/shalm#testing\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Testing</h2>\n<p>Shalm also supports <a href=\"https://github.com/wonderix/shalm/blob/master/doc/unit_tests.md\">unit testing</a></p>\n<h2><a id=\"user-content-comparison\" class=\"anchor\" href=\"https://github.com/wonderix/shalm#comparison\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Comparison</h2>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>shalm</th>\n<th>helm</th>\n<th>ytt/kapp</th>\n<th>kustomize</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Scripting</td>\n<td>+</td>\n<td>(3.1)</td>\n<td>+</td>\n<td>-</td>\n</tr>\n<tr>\n<td>API definition</td>\n<td>+</td>\n<td>-</td>\n<td>(+)</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Reuse of existing charts</td>\n<td>+</td>\n<td>+</td>\n<td>(+)</td>\n<td>?</td>\n</tr>\n<tr>\n<td>Only simple logic in templates</td>\n<td>+</td>\n<td>+</td>\n<td>-</td>\n<td>+</td>\n</tr>\n<tr>\n<td>Interaction with k8s</td>\n<td>+</td>\n<td>+</td>\n<td>-</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Repository</td>\n<td>+</td>\n<td>+</td>\n<td>-</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Mature technology</td>\n<td>-</td>\n<td>+</td>\n<td>+</td>\n<td>+</td>\n</tr>\n<tr>\n<td>Manage user credentials</td>\n<td>+</td>\n<td>-</td>\n<td>-</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Manage user certificate</td>\n<td>+</td>\n<td>-</td>\n<td>-</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Controller based installation</td>\n<td>+</td>\n<td>-</td>\n<td>+</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Remove outdated objects</td>\n<td>+<sup>(1)</sup></td>\n<td>+</td>\n<td>+</td>\n<td>-</td>\n</tr>\n<tr>\n<td>Migrate existing objects</td>\n<td>+<sup>(1)</sup></td>\n<td>-</td>\n<td>-</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<p><sup>(1)</sup>: Must be implemented inside <code>apply</code> method or by using kapp as installer.</p>\n<h2><a id=\"user-content-difference-to-helm\" class=\"anchor\" href=\"https://github.com/wonderix/shalm#difference-to-helm\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Difference to helm</h2>\n<ul>\n<li>Subcharts are not loaded automatically. They must be loaded using the <code>chart</code> command</li>\n<li>Global variables are not supported.</li>\n<li>The <code>--set</code> command line parameters are passed to the <code>init</code> method of the corresponding chart.\nIt&apos;s not possible to set values (from <code>values.yaml</code>) directly.\nIf you would like to set a lot of values, it&apos;s more convenient to write a separate shalm chart.</li>\n<li><code>shalm</code> doesn&apos;t track installed charts on a kubernetes cluster (except you are using <code>kapp</code> for deployment). It works more like <code>kubectl apply</code></li>\n<li>The <code>.Release.Name</code> value is build as follows: <code>&lt;chart.name&gt;-&lt;chart.suffix&gt;</code>. If no suffix is given, the hyphen is also ommited.</li>\n</ul>\n</article></div></div>",
      "contentAsText": "\nThis project brings the starlark scripting language to helm charts.\n\nFeatures\n\nEase orchestration of deployments\nStrict separation of templates and logic\nDefine APIs for helm charts\nControl deployment by overriding methods\nCompatible with helm\nShare a common service like a database manager or an ingress between a set of sub charts\nUse starlark methods in templates (replacement for _helpers.tpl)\nInteract with kubernetes during installation\nManage user credentials\nManage certificates\nAct as glue code between helm charts\nRendering of ytt templates\nAlso available as kubernetes controller\nEasy embeddable and extendable\nIntegration of kapp\n\nInstallation\nInstalling shalm can be done in various ways\nArchitecture\n\nUsage\nHow to start shalm from the command line is described here\nGetting started\nThere is a small tutorial available.\nUser Guide\nSolutions for a set of problems are shown here\nRepos\nShalm can load charts from a various set of locations.\nKubernetes Controller\nShalm can also run as controller inside a kubernets cluster\nReference\nA reference for the complete shalm starlark API\nTesting\nShalm also supports unit testing\nComparison\n\n\n\n\nshalm\nhelm\nytt/kapp\nkustomize\n\n\n\n\nScripting\n+\n(3.1)\n+\n-\n\n\nAPI definition\n+\n-\n(+)\n-\n\n\nReuse of existing charts\n+\n+\n(+)\n?\n\n\nOnly simple logic in templates\n+\n+\n-\n+\n\n\nInteraction with k8s\n+\n+\n-\n-\n\n\nRepository\n+\n+\n-\n-\n\n\nMature technology\n-\n+\n+\n+\n\n\nManage user credentials\n+\n-\n-\n-\n\n\nManage user certificate\n+\n-\n-\n-\n\n\nController based installation\n+\n-\n+\n-\n\n\nRemove outdated objects\n+(1)\n+\n+\n-\n\n\nMigrate existing objects\n+(1)\n-\n-\n-\n\n\n\n(1): Must be implemented inside apply method or by using kapp as installer.\nDifference to helm\n\nSubcharts are not loaded automatically. They must be loaded using the chart command\nGlobal variables are not supported.\nThe --set command line parameters are passed to the init method of the corresponding chart.\nIt's not possible to set values (from values.yaml) directly.\nIf you would like to set a lot of values, it's more convenient to write a separate shalm chart.\nshalm doesn't track installed charts on a kubernetes cluster (except you are using kapp for deployment). It works more like kubectl apply\nThe .Release.Name value is build as follows: <chart.name>-<chart.suffix>. If no suffix is given, the hyphen is also ommited.\n\n",
      "description": "Contribute to wonderix/shalm development by creating an account on GitHub.",
      "ogDescription": "Contribute to wonderix/shalm development by creating an account on GitHub."
    },
    {
      "url": "https://www.reddit.com/r/kubernetes/comments/jthf7m/what_is_the_real_use_case_for_static_pods_in/",
      "title": "r/kubernetes - What is the real use case for static pods in kubernetes ? Do we really use them ?",
      "content": "<div><div><div class=\"_2FCtq-QzlfuN-SwVMUZMM3 _2v9pwVh0VUYrmhoMv1tHPm t3_jthf7m\"><div class=\"_1hLrLjnE1G_RBCNcN9MVQf\">\n              <img alt src=\"https://www.redditstatic.com/desktop2x/img/renderTimingPixel.png\">\n            </div></div><div class=\"_1hwEKkB_38tIoal6fcdrt9\"><div class=\"_3-miAEojrCvx_4FQ8x3P-s\"><a class=\"_1UoeAeSRhOKSNdY_h3iS1O _1Hw7tY9pMr-T1F4P1C-xNU _2qww3J5KKzsD7e5DO0BvvU\" href=\"https://www.reddit.com/r/kubernetes/comments/jthf7m/what_is_the_real_use_case_for_static_pods_in/\"><span class=\"FHCV02u6Cp2zYL0fhQPsO\">14 comments</span></a></div></div></div></div><hr><h4>Page 2</h4><div><div><div class=\"_3MC4c3Q_Y41YKtl1TcvyMt\"><div class=\"_3UMN4RCVY5288m_fOZlkcg\"><div class=\"_2FCtq-QzlfuN-SwVMUZMM3 _2v9pwVh0VUYrmhoMv1tHPm t3_jthf7m\"><div class=\"_1hLrLjnE1G_RBCNcN9MVQf\">\n              <img alt src=\"https://www.redditstatic.com/desktop2x/img/renderTimingPixel.png\">\n            </div></div><div><a class=\"_1sq8G2ap3_yMYvXINVLxFm RvLtAcdRtbOQbhFB7MD_T\" href=\"https://www.reddit.com/r/kubernetes/comments/jthf7m/what_is_the_real_use_case_for_static_pods_in/\"></a><div class=\"_3-miAEojrCvx_4FQ8x3P-s _3P3ghhoNky7Bzspbfw7--R\"><a class=\"_1UoeAeSRhOKSNdY_h3iS1O _1Hw7tY9pMr-T1F4P1C-xNU _2qww3J5KKzsD7e5DO0BvvU\" href=\"https://www.reddit.com/r/kubernetes/comments/jthf7m/what_is_the_real_use_case_for_static_pods_in/\"><span class=\"FHCV02u6Cp2zYL0fhQPsO\">14 comments</span></a></div></div></div></div></div></div>",
      "contentAsText": "\n              \n            14 commentsPage 2\n              \n            14 comments",
      "publishedDate": "2020-11-14T12:20:44.403Z",
      "description": "13 votes, 14 comments. 52.4k members in the kubernetes community. Kubernetes discussion, news, support, and link sharing.",
      "ogDescription": "13 votes and 14 comments so far on Reddit"
    },
    {
      "url": "https://prometheuskube.com/why-we-switched-from-fluent-bit-to-fluentd-in-2-hours",
      "title": "Why we switched from fluent-bit to Fluentd in 2 hours",
      "content": "<div><div class=\"thrv_wrapper thrv_contentbox_shortcode thrv-content-box tve-elem-default-pad dynamic-group-kbqnfdq0\"> <div class=\"tve-cb\"><h3 class><strong><span><strong><span><strong><span><strong><span>What could have been better?</span></strong></span></strong></span></strong></span></strong></h3></div>\n</div><div class=\"thrv_wrapper thrv_text_element kbqjr88i dynamic-group-kbqnexax tve-froala fr-box fr-basic\"><p>Our monitoring needed to be better. The challenge here isn&apos;t straightforward, though:</p><p>The problem is that <strong>none of the Fluent-bit&apos;s Prometheus metrics showed any errors</strong>. Metrics are there, but they are not increasing. It turns out that this is another bug, filled a while back at <a href=\"https://github.com/fluent/fluent-bit/issues/1935\">issue #1935</a>. Hopefully, Fluentd won&apos;t have the same bug.<p>One way you can catch this situation is to <strong>monitor based on traffic</strong>. In Prometheus, we could alert on the rate of bytes send in a 5-minute window. Something similar to:</p></p><blockquote class><br><em>sum(rate(fluentbit_output_proc_bytes_total{k8s_app=&quot;fluent-bit-logging&quot;}[5m])) by (pod) /1024 / 1024 &gt; 0.1</em></blockquote><p><br>The issue with this approach is that things quiet down during the night and during the day go up. So by the nature of it, it&apos;s very volatile. So it&apos;s hard to come up with one threshold. Instead, what you can do is use two thresholds. One for the day and one for the night. Something like:</p><blockquote class><br><em>sum(rate(fluentbit_output_proc_bytes_total{k8s_app=&quot;fluent-bit-logging&quot;}[5m])) by (pod) /1024 / 1024 &gt; 0.1 and hour() &gt; 9 and hour() &lt; 18<br></em></blockquote><blockquote class><em>sum(rate(fluentbit_output_proc_bytes_total{k8s_app=&quot;fluent-bit-logging&quot;}[5m])) by (pod) /1024 / 1024 &gt; 0 and hour() &gt; 18</em></blockquote><p><br>Another option is to use the <strong>black box monitoring method</strong>. You make a script, which continually logs something. Then it performs a check, whether it is in Elasticsearch or not. You use to alert if logs are not there.<p>Another issue with the situation is that these <strong>alerts are not that actionable.</strong> I mean, what can you do when you are not sending those logs? Fix the bug in Fluent-bit? You will not make it in time. Switch to Fluentd? But that&apos;s not ideal either, and you are not sure what you are getting yourself.</p><p>Anyway, we are going to apply some of these ideas when monitoring Fluentd. And we are working on this now. Good thing that Fluentd has a pretty well documented <a href=\"https://docs.fluentd.org/monitoring-fluentd/monitoring-prometheus\" class=\"tve-froala\">Prometheus integration</a>. We are using this integration to build our monitoring package.</p><p>You don&apos;t want to suffer the same faith as we did? Want to have the same production-ready Prometheus Alerts, Grafana Dashboards, and Runbooks?</p></p></div></div>",
      "contentAsText": " What could have been better?\nOur monitoring needed to be better. The challenge here isn't straightforward, though:The problem is that none of the Fluent-bit's Prometheus metrics showed any errors. Metrics are there, but they are not increasing. It turns out that this is another bug, filled a while back at issue #1935. Hopefully, Fluentd won't have the same bug.One way you can catch this situation is to monitor based on traffic. In Prometheus, we could alert on the rate of bytes send in a 5-minute window. Something similar to:sum(rate(fluentbit_output_proc_bytes_total{k8s_app=\"fluent-bit-logging\"}[5m])) by (pod) /1024 / 1024 > 0.1The issue with this approach is that things quiet down during the night and during the day go up. So by the nature of it, it's very volatile. So it's hard to come up with one threshold. Instead, what you can do is use two thresholds. One for the day and one for the night. Something like:sum(rate(fluentbit_output_proc_bytes_total{k8s_app=\"fluent-bit-logging\"}[5m])) by (pod) /1024 / 1024 > 0.1 and hour() > 9 and hour() < 18sum(rate(fluentbit_output_proc_bytes_total{k8s_app=\"fluent-bit-logging\"}[5m])) by (pod) /1024 / 1024 > 0 and hour() > 18Another option is to use the black box monitoring method. You make a script, which continually logs something. Then it performs a check, whether it is in Elasticsearch or not. You use to alert if logs are not there.Another issue with the situation is that these alerts are not that actionable. I mean, what can you do when you are not sending those logs? Fix the bug in Fluent-bit? You will not make it in time. Switch to Fluentd? But that's not ideal either, and you are not sure what you are getting yourself.Anyway, we are going to apply some of these ideas when monitoring Fluentd. And we are working on this now. Good thing that Fluentd has a pretty well documented Prometheus integration. We are using this integration to build our monitoring package.You don't want to suffer the same faith as we did? Want to have the same production-ready Prometheus Alerts, Grafana Dashboards, and Runbooks?",
      "publishedDate": "2020-11-11T13:11:50.000Z",
      "description": "A story where we fluent-bit completely stoped sending logs to Elasticsearch, and we managed to quickly switch to Fluentd instead.",
      "ogDescription": "A story where we fluent-bit completely stoped sending logs to Elasticsearch, and we managed to quickly switch to Fluentd instead."
    },
    {
      "url": "https://github.com/didier-durand/microk8s-kata-containers",
      "title": "didier-durand/microk8s-kata-containers",
      "content": "<div><div><article class=\"markdown-body entry-content container-lg\"><p><a href=\"https://github.com/didier-durand/microk8s-kata-containers/blob/main/img/kata-logo.png\"><img src=\"https://github.com/didier-durand/microk8s-kata-containers/raw/main/img/kata-logo.png\"></a><a href=\"https://github.com/didier-durand/microk8s-kata-containers/blob/main/img/microk8s-logo.png\"><img src=\"https://github.com/didier-durand/microk8s-kata-containers/raw/main/img/microk8s-logo.png\"></a><a href=\"https://github.com/didier-durand/microk8s-kata-containers/blob/main/img/oci-logo.png\"><img src=\"https://github.com/didier-durand/microk8s-kata-containers/raw/main/img/oci-logo.png\"></a><a href=\"https://github.com/didier-durand/microk8s-kata-containers/blob/main/img/containerd-logo.png\"><img src=\"https://github.com/didier-durand/microk8s-kata-containers/raw/main/img/containerd-logo.png\"></a></p>\n\n<p><a href=\"https://github.com/didier-durand/microk8s-kata-containers/workflows/Kata%20Containers%20on%20MicroK8s/badge.svg\"><img src=\"https://github.com/didier-durand/microk8s-kata-containers/workflows/Kata%20Containers%20on%20MicroK8s/badge.svg\" alt=\"workflow badge\"></a>\n<a href=\"https://github.com/didier-durand/microk8s-kata-containers/workflows/MicroK8s%20Services%20Images/badge.svg\"><img src=\"https://github.com/didier-durand/microk8s-kata-containers/workflows/MicroK8s%20Services%20Images/badge.svg\" alt=\"workflow badge\"></a>\n<a href=\"https://opensource.org/licenses/Apache-2.0\"><img src=\"https://camo.githubusercontent.com/2a2157c971b7ae1deb8eb095799440551c33dcf61ea3d965d86b496a5a65df55/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d417061636865253230322e302d626c75652e737667\" alt=\"License\"></a></p>\n\n<h2><a id=\"user-content-goal\" class=\"anchor\" href=\"https://github.com/didier-durand/microk8s-kata-containers#goal\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Goal</h2>\n<p>[<strong>Nota Bene:</strong> This repository is <strong>Work In Progress (WIP)</strong>: currently, we abruptly replace <em>&quot;<a href=\"https://github.com/opencontainers/runc\">runc</a>&quot;</em> binary, initially packaged with MicroK8s, with a symbolic link (symlink) to <em>&quot;<a href=\"https://github.com/kata-containers/runtime\">kata-runtime</a>&quot;</em>  binary, installed on the Ubuntu instance from project&apos;s GitHub repository and added to the MicroK8s <a href=\"https://en.wikipedia.org/wiki/Snap_(package_manager)\">snap</a> in early steps of this workflow. This initial (very) direct shortcut is possible because both binaries fully respect the <a href=\"https://opencontainers.org/\">OCI runtime specification</a>. Next version of this repo will properly adapt the configuration of <a href=\"https://containerd.io/\">containerd</a> (via changes in containerd.toml) and implement the K8s <a href=\"https://kubernetes.io/docs/concepts/containers/runtime-class/\">RuntimeClass</a> to be able to dynamically choose the runtime on per container basis: proper directives in Deployment yaml manifests will allow simultaneous use of <em>&quot;runc&quot;</em> and <em>&quot;kata-runtime&quot;</em> in parallel by different containers having different execution requirements.]</p>\n<p>This repository encompasses a fully scripted Github workflow (via <a href=\"https://github.com/didier-durand/microk8s-kata-containers/blob/main/.github/workflows/microk8s-kata.yml\">microk8s-kata.yml</a> calling <a href=\"https://github.com/didier-durand/microk8s-kata-containers/blob/main/sh/microk8s-kata.sh\">microk8s-kata.sh</a>) to test the transparent use of the runtime for Kata Containers (Katas) on MicroK8s. It must run on a quite specific Google Cloud Engine (GCE) instance since so-called <em>&quot;<a href=\"https://pve.proxmox.com/wiki/Nested_Virtualization\">nested virtualization</a>&quot;</em> is required by Katas when running on the cloud due to its embedded virtual machine coming on top of the cloud hypervisor managing the Linux host. Some sample containerized services (see <a href=\"https://github.com/didier-durand/microk8s-kata-containers/blob/main/src/go/helloworld/helloworld.go\">helloworld.go</a> and <a href=\"https://github.com/didier-durand/microk8s-kata-containers/blob/main/src/go/autoscale/autoscale.go\">autoscale.go</a> built automatically with this <a href=\"https://github.com/didier-durand/microk8s-kata-containers/blob/main/.github/workflows/build-docker-images.yml\">side job</a>) are deployed from Docker Hub and executed as Kubernetes services on MicroK8s.</p>\n<p>The workflow tests the proper execution of sample containers with &apos;kata-runtime&apos; after running them initially on standard &apos;runc&apos; to validate global setup: beyond run of traditional helloworld-go, autoscale-go is called with parameters ensuring that thorough computations and resource allocation are properly executed by the replacing runtime.</p>\n<p><a href=\"https://microk8s.io/\">MicroK8s</a> by Canonical was chosen on purpose for this project: its source code is extremely close to the upstream version of Kubernetes. Consequently, it allows to build a fully-featured production-grade Kubernetes cluster that can be run autonomously - on a single Limux instance - with very sensible default configuration allowing a quick setup, quite representative of a productive system.</p>\n<p>To automatically confirm the validity of this workflow overtime when new versions of the various components (Kata Containers, MicroK8s, Docker, Ubuntu, etc.) get published, cron schedules it on a recurring basis: execution logs can be seen in <a href=\"https://github.com/didier-durand/microk8s-kata-containers/actions\">Actions tab</a>. Excerpts of last execution are gathered <a href=\"https://github.com/didier-durand/microk8s-kata-containers/blob/main/README.md#execution-report\">further down in this page</a>.</p>\n<p><strong>Forking and re-using on your own is strongly encouraged!</strong> All comments for improvements and extensions will be welcome. Finally, if you like this repo, please give a Github star so that it gets more easily found by others.</p>\n<h2><a id=\"user-content-kata-containers---rationale\" class=\"anchor\" href=\"https://github.com/didier-durand/microk8s-kata-containers#kata-containers---rationale\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Kata Containers - Rationale</h2>\n<p>As per <a href=\"https://katacontainers.io/\">Katas&apos; website</a>: <em>&quot;Kata Containers is an open source community working to build a secure container runtime with lightweight virtual machines that feel and perform like containers, but provide stronger workload isolation using hardware virtualization technology as a second layer of defense.&quot;</em></p>\n<p>This added lightweight virtual machine comes with a dedicated Linux kernel, providing isolation of network, I/O and memory and utilizes hardware-enforced isolation through Intel&apos;s <a href=\"https://en.wikipedia.org/wiki/X86_virtualization#Intel_virtualization_(VT-x)\">VT-x features</a> for virtualization.</p>\n<p><a href=\"https://github.com/didier-durand/microk8s-kata-containers/blob/main/img/kata-vs-docker.jpg\"><img src=\"https://github.com/didier-durand/microk8s-kata-containers/raw/main/img/kata-vs-docker.jpg\"></a></p>\n<p>The use of a per-container dedicated kernel and lightweight virtual machines, provided by either <a href=\"https://www.qemu.org/\">Qemu</a> or <a href=\"https://firecracker-microvm.github.io/\">Amazon&apos;s Firecracker</a>, creates a much stronger isolation between the containers themselves and with the host. For example, if a container misbehaves and messes up with the kernel resources by overconsuming or corrupting them, it&apos;s only <strong>HIS</strong> dedicated kernel that gets damaged, not the unique kernel shared between all containers and host, as when you&apos;re using regular containers. The picture above shows the clear differences between the two architectures. So, Kata Containers are probably the best option currently available for additional security and reliability with untrusted workloads of all kinds (recent versions, external source code, etc.).</p>\n<p>As you would expect, this further level of isolation through additional virtualization comes with a performance / cost penalty but this <a href=\"https://object-storage-ca-ymq-1.vexxhost.net/swift/v1/6e4619c416ff4bd19e1c087f27a43eea/www-assets-prod/presentation-media/kata-containers-and-gvisor-a-quantitave-comparison.pdf\">comparative study</a> between the performances of raw host performances, <em>&quot;runc&quot;</em>, <a href=\"https://gvisor.dev/\">Google&apos;s gVisor</a> containers and Kata Containers demonstrates that the overhead remains quite acceptable in many situations for the additional security that is delivered. Look at slides 19 to 26 of the linked pdf to get the exact numbers.</p>\n<h2><a id=\"user-content-specific-setup\" class=\"anchor\" href=\"https://github.com/didier-durand/microk8s-kata-containers#specific-setup\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Specific Setup</h2>\n<p>Two specific points have to be part of this workflow:</p>\n\n<h2><a id=\"user-content-workflow-steps\" class=\"anchor\" href=\"https://github.com/didier-durand/microk8s-kata-containers#workflow-steps\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Workflow Steps</h2>\n<p>The major steps in this workflow are:</p>\n<ol>\n<li>Check that GCE instance is proper (&apos;GenuineIntel&apos;) - according to the above requirement for Broadwell - via lscpu after it has been created.</li>\n<li>Install Kata Containers runtime directly from the Github repository of the project.</li>\n<li>Check that this added runtime can run on the instance: command <em>&quot;kata-runtime kata-check&quot;</em> MUST produce output <em>&quot;System is capable of running Kata Containers&quot;</em></li>\n<li>Install Docker and check via <em>&quot;docker info&quot;</em> that it sees both its standard runtime <em>&quot;runc&quot;</em> and the newly added <em>&quot;kata-runtime&quot;</em></li>\n<li>Run the latest version of <a href=\"https://en.wikipedia.org/wiki/Alpine_Linux\">Alpine Linux</a> image with selection of kata-runtime (<em>&quot;--runtime=&apos;kata-runtime&quot;</em>) and verify through <em>&quot;docker info&quot;</em> that the running Alpine is effectively using kata-runtime.</li>\n<li>Install MicroK8s via snap and check that it works properly via the deployment of <a href=\"https://github.com/didier-durand/microk8s-kata-containers/blob/main/kubernetes/helloworld-go.yml\">helloworld-go.yml</a> and <a href=\"https://github.com/didier-durand/microk8s-kata-containers/blob/main/kubernetes/autoscale-go.yml\">autoscale-go.yml</a> service manifests, built from from GoLang source code in <a href=\"https://github.com/didier-durand/microk8s-kata-containers/blob/main/src/go\">src/go directory</a>. Stop MicroK8s when validation is successful.</li>\n<li>Open the MicroK8s .snap file to add kata-runtime and repackage a new version (now unsigned) of the .snap file. Please, note use of <em>&quot;unsquashfs&quot;</em> and <em>&quot;mksquashfs&quot;</em> to achieve this refurbishing since the <a href=\"https://en.wikipedia.org/wiki/Snap_(package_manager)\">snap archive format</a> is based on read-only and compressed <a href=\"https://en.wikipedia.org/wiki/SquashFS\">SquashFS</a> Linux file system.</li>\n<li>Remove old MicroK8s installation and re-install a fresh instance based with newly created snap version: <em>&quot;--dangerous&quot;</em> option is now required since the tweaked .snap is no longer signed by its official provider, Canonical.</li>\n<li>Deploy again helloworld-go and autoscale-go on fresh MicroK8s to validate that they work fine with kata-runtime: autoscale-go request is parametrized to make sure that some amount computing resources are consumed to achieve a better validation.</li>\n</ol>\n<h2><a id=\"user-content-how-to-fork--run\" class=\"anchor\" href=\"https://github.com/didier-durand/microk8s-kata-containers#how-to-fork--run\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>How to Fork &amp; Run</h2>\n<p>To start with, you need a Google Cloud account including a project where the GCE APIs have been enabled. Obtain the id of your project from\nGCP dashboard. Additionally, you need to create in this project a service account (SA) and give it proper GKE credentials: right to create, administer and delete GCE images &amp; instances (if your cannot  make the SA a &quot;Project Owner&quot; to simplify the security aspects...). Save the private key of the SA in json format.</p>\n<p>Then, fork our repository and define the required <a href=\"https://docs.github.com/en/actions/reference/encrypted-secrets\">Github Secrets</a> in your fork:</p>\n<ol>\n<li>your GCP project id will be {{ secrets.GCP_PROJECT }}</li>\n<li>The private key of your service account in json format will be ${{ secrets.GCP_SA_KEY }}</li>\n</ol>\n<p>To easily use the workflow from Github, you can launch it with the <a href=\"https://github.blog/changelog/2020-07-06-github-actions-manual-triggers-with-workflow_dispatch/\">manual dispatch feature of Github</a> that you can see as a launch button (the green one in the picture below) in the Action tab of your fork.</p>\n<p><a href=\"https://github.com/didier-durand/microk8s-kata-containers/blob/main/img/microk8s-kata-launch-button.jpg\"><img src=\"https://github.com/didier-durand/microk8s-kata-containers/raw/main/img/microk8s-kata-launch-button.jpg\"></a></p>\n<p>The workflow will execute all the steps described above and terminate gracefully after all validation tests described are completed: it will then delete the GCE instance and the associated image triggering the nested virtualization.</p>\n<p>If you also want to make use of the <a href=\"https://github.com/didier-durand/microk8s-kata-containers/blob/main/.github/workflows/build-docker-images.yml\">side workflow</a> allowing to build the test container images from their GoLang sources, you&apos;ll need to add 2 additional secrets : {{ secrets.DOCKER_USERID }} &amp; {{ secrets.DOCKER_PASSWORD }} corresponding to the login parameters of your <a href=\"https://hub.docker.com/\">Docker Hub account</a>.</p>\n<h2><a id=\"user-content-execution-report\" class=\"anchor\" href=\"https://github.com/didier-durand/microk8s-kata-containers#execution-report\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Execution Report</h2>\n<p>Below are some relevant excerpts of the last execution log:</p>\n<pre><code>### execution date: Thu Nov 19 01:30:05 UTC 2020\n \n### microk8s snap version:\nmicrok8s          v1.19.3    1791   1.19/stable      canonical*         classic\n \n### ubuntu version:\nDistributor ID:\tUbuntu\nDescription:\tUbuntu 20.04.1 LTS\nRelease:\t20.04\nCodename:\tfocal\n \n### docker version:\nClient: Docker Engine - Community\n Version:           19.03.13\n API version:       1.40\n Go version:        go1.13.15\n Git commit:        4484c46d9d\n Built:             Wed Sep 16 17:02:52 2020\n OS/Arch:           linux/amd64\n Experimental:      false\n\nServer: Docker Engine - Community\n Engine:\n  Version:          19.03.13\n  API version:      1.40 (minimum version 1.12)\n  Go version:       go1.13.15\n  Git commit:       4484c46d9d\n  Built:            Wed Sep 16 17:01:20 2020\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.3.7\n  GitCommit:        8fba4e9a7d01810a393d5d25a3621dc101981175\n docker-init:\n  Version:          0.18.0\n  GitCommit:        fec3683\n \n### kata-runtime version:\nkata-runtime  : 1.12.0-rc0\n   commit   : &lt;&lt;unknown&gt;&gt;\n   OCI specs: 1.0.1-dev\n \n### kata-runtime check:\nSystem is capable of running Kata Containers\n \n\n### check existing container runtimes on Ubuntu host:\n-rwxr-xr-x 1 root root 9.7M Sep  9 15:40 /bin/runc\n-rwxr-xr-x 1 root root 31M Oct 22 16:51 /bin/kata-runtime\n\n### check available docker runtimes: \n Runtimes: kata-runtime runc\n\n### test use of kata-runtime with alpine: \nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                      PORTS               NAMES\n2ea9bd4902f3        alpine              &quot;sh&quot;                3 seconds ago       Up Less than a second                           kata-alpine\n4d711e400486        busybox             &quot;uname -a&quot;          13 seconds ago      Exited (0) 10 seconds ago                       priceless_bose\n        &quot;Name&quot;: &quot;/kata-alpine&quot;,\n        &quot;Id&quot;: &quot;2ea9bd4902f3aeafb53aa09f90219b09dd8aa4cfadfb7cceaa8ede146513719b&quot;,\n            &quot;Runtime&quot;: &quot;kata-runtime&quot;,\n\n### install microk8s:\nmicrok8s is running\nhigh-availability: no\n  datastore master nodes: 127.0.0.1:19001\n  datastore standby nodes: none\naddons:\n  enabled:\n    ha-cluster           # Configure high availability on the current node\n  disabled:\n    ambassador           # Ambassador API Gateway and Ingress\n    cilium               # SDN, fast with full network policy\n    dashboard            # The Kubernetes dashboard\n    dns                  # CoreDNS\n    fluentd              # Elasticsearch-Fluentd-Kibana logging and monitoring\n    gpu                  # Automatic enablement of Nvidia CUDA\n    helm                 # Helm 2 - the package manager for Kubernetes\n    helm3                # Helm 3 - Kubernetes package manager\n    host-access          # Allow Pods connecting to Host services smoothly\n    ingress              # Ingress controller for external access\n    istio                # Core Istio service mesh services\n    jaeger               # Kubernetes Jaeger operator with its simple config\n    knative              # The Knative framework on Kubernetes.\n    kubeflow             # Kubeflow for easy ML deployments\n    linkerd              # Linkerd is a service mesh for Kubernetes and other frameworks\n    metallb              # Loadbalancer for your Kubernetes cluster\n    metrics-server       # K8s Metrics Server for API access to service metrics\n    multus               # Multus CNI enables attaching multiple network interfaces to pods\n    prometheus           # Prometheus operator for monitoring and logging\n    rbac                 # Role-Based Access Control for authorisation\n    registry             # Private image registry exposed on localhost:32000\n    storage              # Storage class; allocates storage from host directory\n\n### check container runtime on microk8s snap:\n-rwxr-xr-x 1 root root 15M Nov  6 12:06 /snap/microk8s/current/bin/runc\n\n### TEST WITH RUNC\n\n\n### test microk8s with helloworld-go &amp; autoscale-go: \nservice/helloworld-go created\ndeployment.apps/helloworld-go-deployment created\nservice/autoscale-go created\ndeployment.apps/autoscale-go-deployment created\nNAME                                       READY   STATUS              RESTARTS   AGE\nnginx-test                                 0/1     ContainerCreating   0          2s\nhelloworld-go-deployment-86f5466d4-tjsrj   0/1     ContainerCreating   0          1s\nhelloworld-go-deployment-86f5466d4-ffr4g   0/1     ContainerCreating   0          1s\n\nwaiting for ready pods...\n\nNAME                                       READY   STATUS    RESTARTS   AGE\nnginx-test                                 1/1     Running   1          2m2s\nhelloworld-go-deployment-86f5466d4-tjsrj   1/1     Running   0          2m1s\nhelloworld-go-deployment-86f5466d4-ffr4g   1/1     Running   0          2m1s\nautoscale-go-deployment-5894658957-d7xjf   1/1     Running   0          2m\nautoscale-go-deployment-5894658957-f9747   1/1     Running   0          2m\nNAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE\nkubernetes      ClusterIP   10.152.183.1     &lt;none&gt;        443/TCP        2m33s\nhelloworld-go   NodePort    10.152.183.12    &lt;none&gt;        80:32577/TCP   2m1s\nautoscale-go    NodePort    10.152.183.211   &lt;none&gt;        80:30943/TCP   2m\n\ncalling helloworld-go...\n\nHello World: Kata Containers!\n\ncalling autoscale-go with request for biggest prime under 10 000 and 5 MB memory...\n\nAllocated 5 Mb of memory.\nThe largest prime less than 10000 is 9973.\nSlept for 100.19 milliseconds.\n\n### re-install microk8s incl kata-runtime: \nmicrok8s v1.19.3 installed\nmicrok8s is running\nhigh-availability: no\n  datastore master nodes: 127.0.0.1:19001\n  datastore standby nodes: none\naddons:\n  enabled:\n    ha-cluster           # Configure high availability on the current node\n  disabled:\n    ambassador           # Ambassador API Gateway and Ingress\n    cilium               # SDN, fast with full network policy\n    dashboard            # The Kubernetes dashboard\n    dns                  # CoreDNS\n    fluentd              # Elasticsearch-Fluentd-Kibana logging and monitoring\n    gpu                  # Automatic enablement of Nvidia CUDA\n    helm                 # Helm 2 - the package manager for Kubernetes\n    helm3                # Helm 3 - Kubernetes package manager\n    host-access          # Allow Pods connecting to Host services smoothly\n    ingress              # Ingress controller for external access\n    istio                # Core Istio service mesh services\n    jaeger               # Kubernetes Jaeger operator with its simple config\n    knative              # The Knative framework on Kubernetes.\n    kubeflow             # Kubeflow for easy ML deployments\n    linkerd              # Linkerd is a service mesh for Kubernetes and other frameworks\n    metallb              # Loadbalancer for your Kubernetes cluster\n    metrics-server       # K8s Metrics Server for API access to service metrics\n    multus               # Multus CNI enables attaching multiple network interfaces to pods\n    prometheus           # Prometheus operator for monitoring and logging\n    rbac                 # Role-Based Access Control for authorisation\n    registry             # Private image registry exposed on localhost:32000\n    storage              # Storage class; allocates storage from host directory\n\n### TEST WITH KATA-RUNTIME\n\n\n### test microk8s with helloworld-go &amp; autoscale-go: \nservice/helloworld-go created\ndeployment.apps/helloworld-go-deployment created\nservice/autoscale-go created\ndeployment.apps/autoscale-go-deployment created\nNAME                                       READY   STATUS              RESTARTS   AGE\nnginx-test                                 0/1     ContainerCreating   0          1s\nhelloworld-go-deployment-86f5466d4-8j5nf   0/1     ContainerCreating   0          0s\nhelloworld-go-deployment-86f5466d4-556nf   0/1     ContainerCreating   0          0s\nautoscale-go-deployment-5894658957-px5sm   0/1     Pending             0          0s\nautoscale-go-deployment-5894658957-tqh7g   0/1     ContainerCreating   0          0s\n\nwaiting for ready pods...\n\nNAME                                       READY   STATUS    RESTARTS   AGE\nnginx-test                                 1/1     Running   0          2m2s\nhelloworld-go-deployment-86f5466d4-8j5nf   1/1     Running   0          2m1s\nautoscale-go-deployment-5894658957-tqh7g   1/1     Running   0          2m1s\nhelloworld-go-deployment-86f5466d4-556nf   1/1     Running   0          2m1s\nautoscale-go-deployment-5894658957-px5sm   1/1     Running   0          2m1s\nNAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nkubernetes      ClusterIP   10.152.183.1    &lt;none&gt;        443/TCP        2m35s\nhelloworld-go   NodePort    10.152.183.74   &lt;none&gt;        80:30748/TCP   2m1s\nautoscale-go    NodePort    10.152.183.26   &lt;none&gt;        80:30618/TCP   2m1s\n\ncalling helloworld-go...\n\nHello World: Kata Containers!\n\ncalling autoscale-go with request for biggest prime under 10 000 and 5 MB memory...\n\nAllocated 5 Mb of memory.\nThe largest prime less than 10000 is 9973.\nSlept for 100.19 milliseconds.\n\n### check proper symlink from microk8s runc:\nlrwxrwxrwx 1 root root 30 Nov 19 01:24 /snap/microk8s/current/bin/runc -&gt; squashfs-root/bin/kata-runtime\n-rwxr-xr-x 1 root root 31560112 Oct 22 16:51 /bin/kata-runtime\n-rwxr-xr-x 1 root root 31560112 Nov 19 01:24 /snap/microk8s/current/bin/kata-runtime\n</code></pre>\n</article></div></div><hr><h4>Page 2</h4><div><div><article class=\"markdown-body entry-content container-lg\"><p><a href=\"/didier-durand/microk8s-kata-containers/blob/5a6ed44f3f0ba5fef39af928af632ab94781eb3e/img/kata-logo.png\"><img src=\"/didier-durand/microk8s-kata-containers/raw/5a6ed44f3f0ba5fef39af928af632ab94781eb3e/img/kata-logo.png\"></a><a href=\"/didier-durand/microk8s-kata-containers/blob/5a6ed44f3f0ba5fef39af928af632ab94781eb3e/img/microk8s-logo.png\"><img src=\"/didier-durand/microk8s-kata-containers/raw/5a6ed44f3f0ba5fef39af928af632ab94781eb3e/img/microk8s-logo.png\"></a><a href=\"/didier-durand/microk8s-kata-containers/blob/5a6ed44f3f0ba5fef39af928af632ab94781eb3e/img/oci-logo.png\"><img src=\"/didier-durand/microk8s-kata-containers/raw/5a6ed44f3f0ba5fef39af928af632ab94781eb3e/img/oci-logo.png\"></a><a href=\"/didier-durand/microk8s-kata-containers/blob/5a6ed44f3f0ba5fef39af928af632ab94781eb3e/img/containerd-logo.png\"><img src=\"/didier-durand/microk8s-kata-containers/raw/5a6ed44f3f0ba5fef39af928af632ab94781eb3e/img/containerd-logo.png\"></a></p>\n\n<p><a href=\"https://github.com/didier-durand/microk8s-kata-containers/workflows/Kata%20Containers%20on%20MicroK8s/badge.svg\"><img src=\"https://github.com/didier-durand/microk8s-kata-containers/workflows/Kata%20Containers%20on%20MicroK8s/badge.svg\" alt=\"workflow badge\"></a>\n<a href=\"https://github.com/didier-durand/microk8s-kata-containers/workflows/MicroK8s%20Services%20Images/badge.svg\"><img src=\"https://github.com/didier-durand/microk8s-kata-containers/workflows/MicroK8s%20Services%20Images/badge.svg\" alt=\"workflow badge\"></a>\n<a href=\"https://opensource.org/licenses/Apache-2.0\"><img src=\"https://camo.githubusercontent.com/2a2157c971b7ae1deb8eb095799440551c33dcf61ea3d965d86b496a5a65df55/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d417061636865253230322e302d626c75652e737667\" alt=\"License\"></a></p>\n\n<h2><a id=\"user-content-goal\" class=\"anchor\" href=\"#goal\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Goal</h2>\n<p>[<strong>Nota Bene:</strong> This repository is <strong>Work In Progress (WIP)</strong>: currently, we abruptly replace <em>&quot;<a href=\"https://github.com/opencontainers/runc\">runc</a>&quot;</em> binary, initially packaged with MicroK8s, with a symbolic link (symlink) to <em>&quot;<a href=\"https://github.com/kata-containers/runtime\">kata-runtime</a>&quot;</em>  binary, installed on the Ubuntu instance from project&apos;s GitHub repository and added to the MicroK8s <a href=\"https://en.wikipedia.org/wiki/Snap_(package_manager)\">snap</a> in early steps of this workflow. This initial (very) direct shortcut is possible because both binaries fully respect the <a href=\"https://opencontainers.org/\">OCI runtime specification</a>. Next version of this repo will properly adapt the configuration of <a href=\"https://containerd.io/\">containerd</a> (via changes in containerd.toml) and implement the K8s <a href=\"https://kubernetes.io/docs/concepts/containers/runtime-class/\">RuntimeClass</a> to be able to dynamically choose the runtime on per container basis: proper directives in Deployment yaml manifests will allow simultaneous use of <em>&quot;runc&quot;</em> and <em>&quot;kata-runtime&quot;</em> in parallel by different containers having different execution requirements.]</p>\n<p>This repository encompasses a fully scripted Github workflow (via <a href=\"/didier-durand/microk8s-kata-containers/blob/5a6ed44f3f0ba5fef39af928af632ab94781eb3e/.github/workflows/microk8s-kata.yml\">microk8s-kata.yml</a> calling <a href=\"/didier-durand/microk8s-kata-containers/blob/5a6ed44f3f0ba5fef39af928af632ab94781eb3e/sh/microk8s-kata.sh\">microk8s-kata.sh</a>) to test the transparent use of the runtime for Kata Containers (Katas) on MicroK8s. It must run on a quite specific Google Cloud Engine (GCE) instance since so-called <em>&quot;<a href=\"https://pve.proxmox.com/wiki/Nested_Virtualization\">nested virtualization</a>&quot;</em> is required by Katas when running on the cloud due to its embedded virtual machine coming on top of the cloud hypervisor managing the Linux host. Some sample containerized services (see <a href=\"/didier-durand/microk8s-kata-containers/blob/5a6ed44f3f0ba5fef39af928af632ab94781eb3e/src/go/helloworld/helloworld.go\">helloworld.go</a> and <a href=\"/didier-durand/microk8s-kata-containers/blob/5a6ed44f3f0ba5fef39af928af632ab94781eb3e/src/go/autoscale/autoscale.go\">autoscale.go</a> built automatically with this <a href=\"/didier-durand/microk8s-kata-containers/blob/5a6ed44f3f0ba5fef39af928af632ab94781eb3e/.github/workflows/build-docker-images.yml\">side job</a>) are deployed from Docker Hub and executed as Kubernetes services on MicroK8s.</p>\n<p>The workflow tests the proper execution of sample containers with &apos;kata-runtime&apos; after running them initially on standard &apos;runc&apos; to validate global setup: beyond run of traditional helloworld-go, autoscale-go is called with parameters ensuring that thorough computations and resource allocation are properly executed by the replacing runtime.</p>\n<p><a href=\"https://microk8s.io/\">MicroK8s</a> by Canonical was chosen on purpose for this project: its source code is extremely close to the upstream version of Kubernetes. Consequently, it allows to build a fully-featured production-grade Kubernetes cluster that can be run autonomously - on a single Limux instance - with very sensible default configuration allowing a quick setup, quite representative of a productive system.</p>\n<p>To automatically confirm the validity of this workflow overtime when new versions of the various components (Kata Containers, MicroK8s, Docker, Ubuntu, etc.) get published, cron schedules it on a recurring basis: execution logs can be seen in <a href=\"https://github.com/didier-durand/microk8s-kata-containers/actions\">Actions tab</a>. Excerpts of last execution are gathered <a href=\"/didier-durand/microk8s-kata-containers/blob/5a6ed44f3f0ba5fef39af928af632ab94781eb3e/README.md#execution-report\">further down in this page</a>.</p>\n<p><strong>Forking and re-using on your own is strongly encouraged!</strong> All comments for improvements and extensions will be welcome. Finally, if you like this repo, please give a Github star so that it gets more easily found by others.</p>\n<h2><a id=\"user-content-kata-containers---rationale\" class=\"anchor\" href=\"#kata-containers---rationale\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Kata Containers - Rationale</h2>\n<p>As per <a href=\"https://katacontainers.io/\">Katas&apos; website</a>: <em>&quot;Kata Containers is an open source community working to build a secure container runtime with lightweight virtual machines that feel and perform like containers, but provide stronger workload isolation using hardware virtualization technology as a second layer of defense.&quot;</em></p>\n<p>This added lightweight virtual machine comes with a dedicated Linux kernel, providing isolation of network, I/O and memory and utilizes hardware-enforced isolation through Intel&apos;s <a href=\"https://en.wikipedia.org/wiki/X86_virtualization#Intel_virtualization_(VT-x)\">VT-x features</a> for virtualization.</p>\n<p><a href=\"/didier-durand/microk8s-kata-containers/blob/5a6ed44f3f0ba5fef39af928af632ab94781eb3e/img/kata-vs-docker.jpg\"><img src=\"/didier-durand/microk8s-kata-containers/raw/5a6ed44f3f0ba5fef39af928af632ab94781eb3e/img/kata-vs-docker.jpg\"></a></p>\n<p>The use of a per-container dedicated kernel and lightweight virtual machines, provided by either <a href=\"https://www.qemu.org/\">Qemu</a> or <a href=\"https://firecracker-microvm.github.io/\">Amazon&apos;s Firecracker</a>, creates a much stronger isolation between the containers themselves and with the host. For example, if a container misbehaves and messes up with the kernel resources by overconsuming or corrupting them, it&apos;s only <strong>HIS</strong> dedicated kernel that gets damaged, not the unique kernel shared between all containers and host, as when you&apos;re using regular containers. The picture above shows the clear differences between the two architectures. So, Kata Containers are probably the best option currently available for additional security and reliability with untrusted workloads of all kinds (recent versions, external source code, etc.).</p>\n<p>As you would expect, this further level of isolation through additional virtualization comes with a performance / cost penalty but this <a href=\"https://object-storage-ca-ymq-1.vexxhost.net/swift/v1/6e4619c416ff4bd19e1c087f27a43eea/www-assets-prod/presentation-media/kata-containers-and-gvisor-a-quantitave-comparison.pdf\">comparative study</a> between the performances of raw host performances, <em>&quot;runc&quot;</em>, <a href=\"https://gvisor.dev/\">Google&apos;s gVisor</a> containers and Kata Containers demonstrates that the overhead remains quite acceptable in many situations for the additional security that is delivered. Look at slides 19 to 26 of the linked pdf to get the exact numbers.</p>\n<h2><a id=\"user-content-specific-setup\" class=\"anchor\" href=\"#specific-setup\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Specific Setup</h2>\n<p>Two specific points have to be part of this workflow:</p>\n\n<h2><a id=\"user-content-workflow-steps\" class=\"anchor\" href=\"#workflow-steps\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Workflow Steps</h2>\n<p>The major steps in this workflow are:</p>\n<ol>\n<li>Check that GCE instance is proper (&apos;GenuineIntel&apos;) - according to the above requirement for Broadwell - via lscpu after it has been created.</li>\n<li>Install Kata Containers runtime directly from the Github repository of the project.</li>\n<li>Check that this added runtime can run on the instance: command <em>&quot;kata-runtime kata-check&quot;</em> MUST produce output <em>&quot;System is capable of running Kata Containers&quot;</em></li>\n<li>Install Docker and check via <em>&quot;docker info&quot;</em> that it sees both its standard runtime <em>&quot;runc&quot;</em> and the newly added <em>&quot;kata-runtime&quot;</em></li>\n<li>Run the latest version of <a href=\"https://en.wikipedia.org/wiki/Alpine_Linux\">Alpine Linux</a> image with selection of kata-runtime (<em>&quot;--runtime=&apos;kata-runtime&quot;</em>) and verify through <em>&quot;docker info&quot;</em> that the running Alpine is effectively using kata-runtime.</li>\n<li>Install MicroK8s via snap and check that it works properly via the deployment of <a href=\"/didier-durand/microk8s-kata-containers/blob/5a6ed44f3f0ba5fef39af928af632ab94781eb3e/kubernetes/helloworld-go.yml\">helloworld-go.yml</a> and <a href=\"/didier-durand/microk8s-kata-containers/blob/5a6ed44f3f0ba5fef39af928af632ab94781eb3e/kubernetes/autoscale-go.yml\">autoscale-go.yml</a> service manifests, built from from GoLang source code in <a href=\"/didier-durand/microk8s-kata-containers/blob/5a6ed44f3f0ba5fef39af928af632ab94781eb3e/src/go\">src/go directory</a>. Stop MicroK8s when validation is successful.</li>\n<li>Open the MicroK8s .snap file to add kata-runtime and repackage a new version (now unsigned) of the .snap file. Please, note use of <em>&quot;unsquashfs&quot;</em> and <em>&quot;mksquashfs&quot;</em> to achieve this refurbishing since the <a href=\"https://en.wikipedia.org/wiki/Snap_(package_manager)\">snap archive format</a> is based on read-only and compressed <a href=\"https://en.wikipedia.org/wiki/SquashFS\">SquashFS</a> Linux file system.</li>\n<li>Remove old MicroK8s installation and re-install a fresh instance based with newly created snap version: <em>&quot;--dangerous&quot;</em> option is now required since the tweaked .snap is no longer signed by its official provider, Canonical.</li>\n<li>Deploy again helloworld-go and autoscale-go on fresh MicroK8s to validate that they work fine with kata-runtime: autoscale-go request is parametrized to make sure that some amount computing resources are consumed to achieve a better validation.</li>\n</ol>\n<h2><a id=\"user-content-how-to-fork--run\" class=\"anchor\" href=\"#how-to-fork--run\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>How to Fork &amp; Run</h2>\n<p>To start with, you need a Google Cloud account including a project where the GCE APIs have been enabled. Obtain the id of your project from\nGCP dashboard. Additionally, you need to create in this project a service account (SA) and give it proper GKE credentials: right to create, administer and delete GCE images &amp; instances (if your cannot  make the SA a &quot;Project Owner&quot; to simplify the security aspects...). Save the private key of the SA in json format.</p>\n<p>Then, fork our repository and define the required <a href=\"https://docs.github.com/en/actions/reference/encrypted-secrets\">Github Secrets</a> in your fork:</p>\n<ol>\n<li>your GCP project id will be {{ secrets.GCP_PROJECT }}</li>\n<li>The private key of your service account in json format will be ${{ secrets.GCP_SA_KEY }}</li>\n</ol>\n<p>To easily use the workflow from Github, you can launch it with the <a href=\"https://github.blog/changelog/2020-07-06-github-actions-manual-triggers-with-workflow_dispatch/\">manual dispatch feature of Github</a> that you can see as a launch button (the green one in the picture below) in the Action tab of your fork.</p>\n<p><a href=\"/didier-durand/microk8s-kata-containers/blob/5a6ed44f3f0ba5fef39af928af632ab94781eb3e/img/microk8s-kata-launch-button.jpg\"><img src=\"/didier-durand/microk8s-kata-containers/raw/5a6ed44f3f0ba5fef39af928af632ab94781eb3e/img/microk8s-kata-launch-button.jpg\"></a></p>\n<p>The workflow will execute all the steps described above and terminate gracefully after all validation tests described are completed: it will then delete the GCE instance and the associated image triggering the nested virtualization.</p>\n<p>If you also want to make use of the <a href=\"/didier-durand/microk8s-kata-containers/blob/5a6ed44f3f0ba5fef39af928af632ab94781eb3e/.github/workflows/build-docker-images.yml\">side workflow</a> allowing to build the test container images from their GoLang sources, you&apos;ll need to add 2 additional secrets : {{ secrets.DOCKER_USERID }} &amp; {{ secrets.DOCKER_PASSWORD }} corresponding to the login parameters of your <a href=\"https://hub.docker.com/\">Docker Hub account</a>.</p>\n<h2><a id=\"user-content-execution-report\" class=\"anchor\" href=\"#execution-report\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Execution Report</h2>\n<p>Below are some relevant excerpts of the last execution log:</p>\n<pre><code>### execution date: Thu Nov 19 01:30:05 UTC 2020\n \n### microk8s snap version:\nmicrok8s          v1.19.3    1791   1.19/stable      canonical*         classic\n \n### ubuntu version:\nDistributor ID:\tUbuntu\nDescription:\tUbuntu 20.04.1 LTS\nRelease:\t20.04\nCodename:\tfocal\n \n### docker version:\nClient: Docker Engine - Community\n Version:           19.03.13\n API version:       1.40\n Go version:        go1.13.15\n Git commit:        4484c46d9d\n Built:             Wed Sep 16 17:02:52 2020\n OS/Arch:           linux/amd64\n Experimental:      false\n\nServer: Docker Engine - Community\n Engine:\n  Version:          19.03.13\n  API version:      1.40 (minimum version 1.12)\n  Go version:       go1.13.15\n  Git commit:       4484c46d9d\n  Built:            Wed Sep 16 17:01:20 2020\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.3.7\n  GitCommit:        8fba4e9a7d01810a393d5d25a3621dc101981175\n docker-init:\n  Version:          0.18.0\n  GitCommit:        fec3683\n \n### kata-runtime version:\nkata-runtime  : 1.12.0-rc0\n   commit   : &lt;&lt;unknown&gt;&gt;\n   OCI specs: 1.0.1-dev\n \n### kata-runtime check:\nSystem is capable of running Kata Containers\n \n\n### check existing container runtimes on Ubuntu host:\n-rwxr-xr-x 1 root root 9.7M Sep  9 15:40 /bin/runc\n-rwxr-xr-x 1 root root 31M Oct 22 16:51 /bin/kata-runtime\n\n### check available docker runtimes: \n Runtimes: kata-runtime runc\n\n### test use of kata-runtime with alpine: \nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                      PORTS               NAMES\n2ea9bd4902f3        alpine              &quot;sh&quot;                3 seconds ago       Up Less than a second                           kata-alpine\n4d711e400486        busybox             &quot;uname -a&quot;          13 seconds ago      Exited (0) 10 seconds ago                       priceless_bose\n        &quot;Name&quot;: &quot;/kata-alpine&quot;,\n        &quot;Id&quot;: &quot;2ea9bd4902f3aeafb53aa09f90219b09dd8aa4cfadfb7cceaa8ede146513719b&quot;,\n            &quot;Runtime&quot;: &quot;kata-runtime&quot;,\n\n### install microk8s:\nmicrok8s is running\nhigh-availability: no\n  datastore master nodes: 127.0.0.1:19001\n  datastore standby nodes: none\naddons:\n  enabled:\n    ha-cluster           # Configure high availability on the current node\n  disabled:\n    ambassador           # Ambassador API Gateway and Ingress\n    cilium               # SDN, fast with full network policy\n    dashboard            # The Kubernetes dashboard\n    dns                  # CoreDNS\n    fluentd              # Elasticsearch-Fluentd-Kibana logging and monitoring\n    gpu                  # Automatic enablement of Nvidia CUDA\n    helm                 # Helm 2 - the package manager for Kubernetes\n    helm3                # Helm 3 - Kubernetes package manager\n    host-access          # Allow Pods connecting to Host services smoothly\n    ingress              # Ingress controller for external access\n    istio                # Core Istio service mesh services\n    jaeger               # Kubernetes Jaeger operator with its simple config\n    knative              # The Knative framework on Kubernetes.\n    kubeflow             # Kubeflow for easy ML deployments\n    linkerd              # Linkerd is a service mesh for Kubernetes and other frameworks\n    metallb              # Loadbalancer for your Kubernetes cluster\n    metrics-server       # K8s Metrics Server for API access to service metrics\n    multus               # Multus CNI enables attaching multiple network interfaces to pods\n    prometheus           # Prometheus operator for monitoring and logging\n    rbac                 # Role-Based Access Control for authorisation\n    registry             # Private image registry exposed on localhost:32000\n    storage              # Storage class; allocates storage from host directory\n\n### check container runtime on microk8s snap:\n-rwxr-xr-x 1 root root 15M Nov  6 12:06 /snap/microk8s/current/bin/runc\n\n### TEST WITH RUNC\n\n\n### test microk8s with helloworld-go &amp; autoscale-go: \nservice/helloworld-go created\ndeployment.apps/helloworld-go-deployment created\nservice/autoscale-go created\ndeployment.apps/autoscale-go-deployment created\nNAME                                       READY   STATUS              RESTARTS   AGE\nnginx-test                                 0/1     ContainerCreating   0          2s\nhelloworld-go-deployment-86f5466d4-tjsrj   0/1     ContainerCreating   0          1s\nhelloworld-go-deployment-86f5466d4-ffr4g   0/1     ContainerCreating   0          1s\n\nwaiting for ready pods...\n\nNAME                                       READY   STATUS    RESTARTS   AGE\nnginx-test                                 1/1     Running   1          2m2s\nhelloworld-go-deployment-86f5466d4-tjsrj   1/1     Running   0          2m1s\nhelloworld-go-deployment-86f5466d4-ffr4g   1/1     Running   0          2m1s\nautoscale-go-deployment-5894658957-d7xjf   1/1     Running   0          2m\nautoscale-go-deployment-5894658957-f9747   1/1     Running   0          2m\nNAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE\nkubernetes      ClusterIP   10.152.183.1     &lt;none&gt;        443/TCP        2m33s\nhelloworld-go   NodePort    10.152.183.12    &lt;none&gt;        80:32577/TCP   2m1s\nautoscale-go    NodePort    10.152.183.211   &lt;none&gt;        80:30943/TCP   2m\n\ncalling helloworld-go...\n\nHello World: Kata Containers!\n\ncalling autoscale-go with request for biggest prime under 10 000 and 5 MB memory...\n\nAllocated 5 Mb of memory.\nThe largest prime less than 10000 is 9973.\nSlept for 100.19 milliseconds.\n\n### re-install microk8s incl kata-runtime: \nmicrok8s v1.19.3 installed\nmicrok8s is running\nhigh-availability: no\n  datastore master nodes: 127.0.0.1:19001\n  datastore standby nodes: none\naddons:\n  enabled:\n    ha-cluster           # Configure high availability on the current node\n  disabled:\n    ambassador           # Ambassador API Gateway and Ingress\n    cilium               # SDN, fast with full network policy\n    dashboard            # The Kubernetes dashboard\n    dns                  # CoreDNS\n    fluentd              # Elasticsearch-Fluentd-Kibana logging and monitoring\n    gpu                  # Automatic enablement of Nvidia CUDA\n    helm                 # Helm 2 - the package manager for Kubernetes\n    helm3                # Helm 3 - Kubernetes package manager\n    host-access          # Allow Pods connecting to Host services smoothly\n    ingress              # Ingress controller for external access\n    istio                # Core Istio service mesh services\n    jaeger               # Kubernetes Jaeger operator with its simple config\n    knative              # The Knative framework on Kubernetes.\n    kubeflow             # Kubeflow for easy ML deployments\n    linkerd              # Linkerd is a service mesh for Kubernetes and other frameworks\n    metallb              # Loadbalancer for your Kubernetes cluster\n    metrics-server       # K8s Metrics Server for API access to service metrics\n    multus               # Multus CNI enables attaching multiple network interfaces to pods\n    prometheus           # Prometheus operator for monitoring and logging\n    rbac                 # Role-Based Access Control for authorisation\n    registry             # Private image registry exposed on localhost:32000\n    storage              # Storage class; allocates storage from host directory\n\n### TEST WITH KATA-RUNTIME\n\n\n### test microk8s with helloworld-go &amp; autoscale-go: \nservice/helloworld-go created\ndeployment.apps/helloworld-go-deployment created\nservice/autoscale-go created\ndeployment.apps/autoscale-go-deployment created\nNAME                                       READY   STATUS              RESTARTS   AGE\nnginx-test                                 0/1     ContainerCreating   0          1s\nhelloworld-go-deployment-86f5466d4-8j5nf   0/1     ContainerCreating   0          0s\nhelloworld-go-deployment-86f5466d4-556nf   0/1     ContainerCreating   0          0s\nautoscale-go-deployment-5894658957-px5sm   0/1     Pending             0          0s\nautoscale-go-deployment-5894658957-tqh7g   0/1     ContainerCreating   0          0s\n\nwaiting for ready pods...\n\nNAME                                       READY   STATUS    RESTARTS   AGE\nnginx-test                                 1/1     Running   0          2m2s\nhelloworld-go-deployment-86f5466d4-8j5nf   1/1     Running   0          2m1s\nautoscale-go-deployment-5894658957-tqh7g   1/1     Running   0          2m1s\nhelloworld-go-deployment-86f5466d4-556nf   1/1     Running   0          2m1s\nautoscale-go-deployment-5894658957-px5sm   1/1     Running   0          2m1s\nNAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nkubernetes      ClusterIP   10.152.183.1    &lt;none&gt;        443/TCP        2m35s\nhelloworld-go   NodePort    10.152.183.74   &lt;none&gt;        80:30748/TCP   2m1s\nautoscale-go    NodePort    10.152.183.26   &lt;none&gt;        80:30618/TCP   2m1s\n\ncalling helloworld-go...\n\nHello World: Kata Containers!\n\ncalling autoscale-go with request for biggest prime under 10 000 and 5 MB memory...\n\nAllocated 5 Mb of memory.\nThe largest prime less than 10000 is 9973.\nSlept for 100.19 milliseconds.\n\n### check proper symlink from microk8s runc:\nlrwxrwxrwx 1 root root 30 Nov 19 01:24 /snap/microk8s/current/bin/runc -&gt; squashfs-root/bin/kata-runtime\n-rwxr-xr-x 1 root root 31560112 Oct 22 16:51 /bin/kata-runtime\n-rwxr-xr-x 1 root root 31560112 Nov 19 01:24 /snap/microk8s/current/bin/kata-runtime\n</code></pre>\n</article></div></div>",
      "contentAsText": "\n\n\n\n\n\nGoal\n[Nota Bene: This repository is Work In Progress (WIP): currently, we abruptly replace \"runc\" binary, initially packaged with MicroK8s, with a symbolic link (symlink) to \"kata-runtime\"  binary, installed on the Ubuntu instance from project's GitHub repository and added to the MicroK8s snap in early steps of this workflow. This initial (very) direct shortcut is possible because both binaries fully respect the OCI runtime specification. Next version of this repo will properly adapt the configuration of containerd (via changes in containerd.toml) and implement the K8s RuntimeClass to be able to dynamically choose the runtime on per container basis: proper directives in Deployment yaml manifests will allow simultaneous use of \"runc\" and \"kata-runtime\" in parallel by different containers having different execution requirements.]\nThis repository encompasses a fully scripted Github workflow (via microk8s-kata.yml calling microk8s-kata.sh) to test the transparent use of the runtime for Kata Containers (Katas) on MicroK8s. It must run on a quite specific Google Cloud Engine (GCE) instance since so-called \"nested virtualization\" is required by Katas when running on the cloud due to its embedded virtual machine coming on top of the cloud hypervisor managing the Linux host. Some sample containerized services (see helloworld.go and autoscale.go built automatically with this side job) are deployed from Docker Hub and executed as Kubernetes services on MicroK8s.\nThe workflow tests the proper execution of sample containers with 'kata-runtime' after running them initially on standard 'runc' to validate global setup: beyond run of traditional helloworld-go, autoscale-go is called with parameters ensuring that thorough computations and resource allocation are properly executed by the replacing runtime.\nMicroK8s by Canonical was chosen on purpose for this project: its source code is extremely close to the upstream version of Kubernetes. Consequently, it allows to build a fully-featured production-grade Kubernetes cluster that can be run autonomously - on a single Limux instance - with very sensible default configuration allowing a quick setup, quite representative of a productive system.\nTo automatically confirm the validity of this workflow overtime when new versions of the various components (Kata Containers, MicroK8s, Docker, Ubuntu, etc.) get published, cron schedules it on a recurring basis: execution logs can be seen in Actions tab. Excerpts of last execution are gathered further down in this page.\nForking and re-using on your own is strongly encouraged! All comments for improvements and extensions will be welcome. Finally, if you like this repo, please give a Github star so that it gets more easily found by others.\nKata Containers - Rationale\nAs per Katas' website: \"Kata Containers is an open source community working to build a secure container runtime with lightweight virtual machines that feel and perform like containers, but provide stronger workload isolation using hardware virtualization technology as a second layer of defense.\"\nThis added lightweight virtual machine comes with a dedicated Linux kernel, providing isolation of network, I/O and memory and utilizes hardware-enforced isolation through Intel's VT-x features for virtualization.\n\nThe use of a per-container dedicated kernel and lightweight virtual machines, provided by either Qemu or Amazon's Firecracker, creates a much stronger isolation between the containers themselves and with the host. For example, if a container misbehaves and messes up with the kernel resources by overconsuming or corrupting them, it's only HIS dedicated kernel that gets damaged, not the unique kernel shared between all containers and host, as when you're using regular containers. The picture above shows the clear differences between the two architectures. So, Kata Containers are probably the best option currently available for additional security and reliability with untrusted workloads of all kinds (recent versions, external source code, etc.).\nAs you would expect, this further level of isolation through additional virtualization comes with a performance / cost penalty but this comparative study between the performances of raw host performances, \"runc\", Google's gVisor containers and Kata Containers demonstrates that the overhead remains quite acceptable in many situations for the additional security that is delivered. Look at slides 19 to 26 of the linked pdf to get the exact numbers.\nSpecific Setup\nTwo specific points have to be part of this workflow:\n\nWorkflow Steps\nThe major steps in this workflow are:\n\nCheck that GCE instance is proper ('GenuineIntel') - according to the above requirement for Broadwell - via lscpu after it has been created.\nInstall Kata Containers runtime directly from the Github repository of the project.\nCheck that this added runtime can run on the instance: command \"kata-runtime kata-check\" MUST produce output \"System is capable of running Kata Containers\"\nInstall Docker and check via \"docker info\" that it sees both its standard runtime \"runc\" and the newly added \"kata-runtime\"\nRun the latest version of Alpine Linux image with selection of kata-runtime (\"--runtime='kata-runtime\") and verify through \"docker info\" that the running Alpine is effectively using kata-runtime.\nInstall MicroK8s via snap and check that it works properly via the deployment of helloworld-go.yml and autoscale-go.yml service manifests, built from from GoLang source code in src/go directory. Stop MicroK8s when validation is successful.\nOpen the MicroK8s .snap file to add kata-runtime and repackage a new version (now unsigned) of the .snap file. Please, note use of \"unsquashfs\" and \"mksquashfs\" to achieve this refurbishing since the snap archive format is based on read-only and compressed SquashFS Linux file system.\nRemove old MicroK8s installation and re-install a fresh instance based with newly created snap version: \"--dangerous\" option is now required since the tweaked .snap is no longer signed by its official provider, Canonical.\nDeploy again helloworld-go and autoscale-go on fresh MicroK8s to validate that they work fine with kata-runtime: autoscale-go request is parametrized to make sure that some amount computing resources are consumed to achieve a better validation.\n\nHow to Fork & Run\nTo start with, you need a Google Cloud account including a project where the GCE APIs have been enabled. Obtain the id of your project from\nGCP dashboard. Additionally, you need to create in this project a service account (SA) and give it proper GKE credentials: right to create, administer and delete GCE images & instances (if your cannot  make the SA a \"Project Owner\" to simplify the security aspects...). Save the private key of the SA in json format.\nThen, fork our repository and define the required Github Secrets in your fork:\n\nyour GCP project id will be {{ secrets.GCP_PROJECT }}\nThe private key of your service account in json format will be ${{ secrets.GCP_SA_KEY }}\n\nTo easily use the workflow from Github, you can launch it with the manual dispatch feature of Github that you can see as a launch button (the green one in the picture below) in the Action tab of your fork.\n\nThe workflow will execute all the steps described above and terminate gracefully after all validation tests described are completed: it will then delete the GCE instance and the associated image triggering the nested virtualization.\nIf you also want to make use of the side workflow allowing to build the test container images from their GoLang sources, you'll need to add 2 additional secrets : {{ secrets.DOCKER_USERID }} & {{ secrets.DOCKER_PASSWORD }} corresponding to the login parameters of your Docker Hub account.\nExecution Report\nBelow are some relevant excerpts of the last execution log:\n### execution date: Thu Nov 19 01:30:05 UTC 2020\n \n### microk8s snap version:\nmicrok8s          v1.19.3    1791   1.19/stable      canonical*         classic\n \n### ubuntu version:\nDistributor ID:\tUbuntu\nDescription:\tUbuntu 20.04.1 LTS\nRelease:\t20.04\nCodename:\tfocal\n \n### docker version:\nClient: Docker Engine - Community\n Version:           19.03.13\n API version:       1.40\n Go version:        go1.13.15\n Git commit:        4484c46d9d\n Built:             Wed Sep 16 17:02:52 2020\n OS/Arch:           linux/amd64\n Experimental:      false\n\nServer: Docker Engine - Community\n Engine:\n  Version:          19.03.13\n  API version:      1.40 (minimum version 1.12)\n  Go version:       go1.13.15\n  Git commit:       4484c46d9d\n  Built:            Wed Sep 16 17:01:20 2020\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.3.7\n  GitCommit:        8fba4e9a7d01810a393d5d25a3621dc101981175\n docker-init:\n  Version:          0.18.0\n  GitCommit:        fec3683\n \n### kata-runtime version:\nkata-runtime  : 1.12.0-rc0\n   commit   : <<unknown>>\n   OCI specs: 1.0.1-dev\n \n### kata-runtime check:\nSystem is capable of running Kata Containers\n \n\n### check existing container runtimes on Ubuntu host:\n-rwxr-xr-x 1 root root 9.7M Sep  9 15:40 /bin/runc\n-rwxr-xr-x 1 root root 31M Oct 22 16:51 /bin/kata-runtime\n\n### check available docker runtimes: \n Runtimes: kata-runtime runc\n\n### test use of kata-runtime with alpine: \nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                      PORTS               NAMES\n2ea9bd4902f3        alpine              \"sh\"                3 seconds ago       Up Less than a second                           kata-alpine\n4d711e400486        busybox             \"uname -a\"          13 seconds ago      Exited (0) 10 seconds ago                       priceless_bose\n        \"Name\": \"/kata-alpine\",\n        \"Id\": \"2ea9bd4902f3aeafb53aa09f90219b09dd8aa4cfadfb7cceaa8ede146513719b\",\n            \"Runtime\": \"kata-runtime\",\n\n### install microk8s:\nmicrok8s is running\nhigh-availability: no\n  datastore master nodes: 127.0.0.1:19001\n  datastore standby nodes: none\naddons:\n  enabled:\n    ha-cluster           # Configure high availability on the current node\n  disabled:\n    ambassador           # Ambassador API Gateway and Ingress\n    cilium               # SDN, fast with full network policy\n    dashboard            # The Kubernetes dashboard\n    dns                  # CoreDNS\n    fluentd              # Elasticsearch-Fluentd-Kibana logging and monitoring\n    gpu                  # Automatic enablement of Nvidia CUDA\n    helm                 # Helm 2 - the package manager for Kubernetes\n    helm3                # Helm 3 - Kubernetes package manager\n    host-access          # Allow Pods connecting to Host services smoothly\n    ingress              # Ingress controller for external access\n    istio                # Core Istio service mesh services\n    jaeger               # Kubernetes Jaeger operator with its simple config\n    knative              # The Knative framework on Kubernetes.\n    kubeflow             # Kubeflow for easy ML deployments\n    linkerd              # Linkerd is a service mesh for Kubernetes and other frameworks\n    metallb              # Loadbalancer for your Kubernetes cluster\n    metrics-server       # K8s Metrics Server for API access to service metrics\n    multus               # Multus CNI enables attaching multiple network interfaces to pods\n    prometheus           # Prometheus operator for monitoring and logging\n    rbac                 # Role-Based Access Control for authorisation\n    registry             # Private image registry exposed on localhost:32000\n    storage              # Storage class; allocates storage from host directory\n\n### check container runtime on microk8s snap:\n-rwxr-xr-x 1 root root 15M Nov  6 12:06 /snap/microk8s/current/bin/runc\n\n### TEST WITH RUNC\n\n\n### test microk8s with helloworld-go & autoscale-go: \nservice/helloworld-go created\ndeployment.apps/helloworld-go-deployment created\nservice/autoscale-go created\ndeployment.apps/autoscale-go-deployment created\nNAME                                       READY   STATUS              RESTARTS   AGE\nnginx-test                                 0/1     ContainerCreating   0          2s\nhelloworld-go-deployment-86f5466d4-tjsrj   0/1     ContainerCreating   0          1s\nhelloworld-go-deployment-86f5466d4-ffr4g   0/1     ContainerCreating   0          1s\n\nwaiting for ready pods...\n\nNAME                                       READY   STATUS    RESTARTS   AGE\nnginx-test                                 1/1     Running   1          2m2s\nhelloworld-go-deployment-86f5466d4-tjsrj   1/1     Running   0          2m1s\nhelloworld-go-deployment-86f5466d4-ffr4g   1/1     Running   0          2m1s\nautoscale-go-deployment-5894658957-d7xjf   1/1     Running   0          2m\nautoscale-go-deployment-5894658957-f9747   1/1     Running   0          2m\nNAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE\nkubernetes      ClusterIP   10.152.183.1     <none>        443/TCP        2m33s\nhelloworld-go   NodePort    10.152.183.12    <none>        80:32577/TCP   2m1s\nautoscale-go    NodePort    10.152.183.211   <none>        80:30943/TCP   2m\n\ncalling helloworld-go...\n\nHello World: Kata Containers!\n\ncalling autoscale-go with request for biggest prime under 10 000 and 5 MB memory...\n\nAllocated 5 Mb of memory.\nThe largest prime less than 10000 is 9973.\nSlept for 100.19 milliseconds.\n\n### re-install microk8s incl kata-runtime: \nmicrok8s v1.19.3 installed\nmicrok8s is running\nhigh-availability: no\n  datastore master nodes: 127.0.0.1:19001\n  datastore standby nodes: none\naddons:\n  enabled:\n    ha-cluster           # Configure high availability on the current node\n  disabled:\n    ambassador           # Ambassador API Gateway and Ingress\n    cilium               # SDN, fast with full network policy\n    dashboard            # The Kubernetes dashboard\n    dns                  # CoreDNS\n    fluentd              # Elasticsearch-Fluentd-Kibana logging and monitoring\n    gpu                  # Automatic enablement of Nvidia CUDA\n    helm                 # Helm 2 - the package manager for Kubernetes\n    helm3                # Helm 3 - Kubernetes package manager\n    host-access          # Allow Pods connecting to Host services smoothly\n    ingress              # Ingress controller for external access\n    istio                # Core Istio service mesh services\n    jaeger               # Kubernetes Jaeger operator with its simple config\n    knative              # The Knative framework on Kubernetes.\n    kubeflow             # Kubeflow for easy ML deployments\n    linkerd              # Linkerd is a service mesh for Kubernetes and other frameworks\n    metallb              # Loadbalancer for your Kubernetes cluster\n    metrics-server       # K8s Metrics Server for API access to service metrics\n    multus               # Multus CNI enables attaching multiple network interfaces to pods\n    prometheus           # Prometheus operator for monitoring and logging\n    rbac                 # Role-Based Access Control for authorisation\n    registry             # Private image registry exposed on localhost:32000\n    storage              # Storage class; allocates storage from host directory\n\n### TEST WITH KATA-RUNTIME\n\n\n### test microk8s with helloworld-go & autoscale-go: \nservice/helloworld-go created\ndeployment.apps/helloworld-go-deployment created\nservice/autoscale-go created\ndeployment.apps/autoscale-go-deployment created\nNAME                                       READY   STATUS              RESTARTS   AGE\nnginx-test                                 0/1     ContainerCreating   0          1s\nhelloworld-go-deployment-86f5466d4-8j5nf   0/1     ContainerCreating   0          0s\nhelloworld-go-deployment-86f5466d4-556nf   0/1     ContainerCreating   0          0s\nautoscale-go-deployment-5894658957-px5sm   0/1     Pending             0          0s\nautoscale-go-deployment-5894658957-tqh7g   0/1     ContainerCreating   0          0s\n\nwaiting for ready pods...\n\nNAME                                       READY   STATUS    RESTARTS   AGE\nnginx-test                                 1/1     Running   0          2m2s\nhelloworld-go-deployment-86f5466d4-8j5nf   1/1     Running   0          2m1s\nautoscale-go-deployment-5894658957-tqh7g   1/1     Running   0          2m1s\nhelloworld-go-deployment-86f5466d4-556nf   1/1     Running   0          2m1s\nautoscale-go-deployment-5894658957-px5sm   1/1     Running   0          2m1s\nNAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nkubernetes      ClusterIP   10.152.183.1    <none>        443/TCP        2m35s\nhelloworld-go   NodePort    10.152.183.74   <none>        80:30748/TCP   2m1s\nautoscale-go    NodePort    10.152.183.26   <none>        80:30618/TCP   2m1s\n\ncalling helloworld-go...\n\nHello World: Kata Containers!\n\ncalling autoscale-go with request for biggest prime under 10 000 and 5 MB memory...\n\nAllocated 5 Mb of memory.\nThe largest prime less than 10000 is 9973.\nSlept for 100.19 milliseconds.\n\n### check proper symlink from microk8s runc:\nlrwxrwxrwx 1 root root 30 Nov 19 01:24 /snap/microk8s/current/bin/runc -> squashfs-root/bin/kata-runtime\n-rwxr-xr-x 1 root root 31560112 Oct 22 16:51 /bin/kata-runtime\n-rwxr-xr-x 1 root root 31560112 Nov 19 01:24 /snap/microk8s/current/bin/kata-runtime\n\nPage 2\n\n\n\n\n\nGoal\n[Nota Bene: This repository is Work In Progress (WIP): currently, we abruptly replace \"runc\" binary, initially packaged with MicroK8s, with a symbolic link (symlink) to \"kata-runtime\"  binary, installed on the Ubuntu instance from project's GitHub repository and added to the MicroK8s snap in early steps of this workflow. This initial (very) direct shortcut is possible because both binaries fully respect the OCI runtime specification. Next version of this repo will properly adapt the configuration of containerd (via changes in containerd.toml) and implement the K8s RuntimeClass to be able to dynamically choose the runtime on per container basis: proper directives in Deployment yaml manifests will allow simultaneous use of \"runc\" and \"kata-runtime\" in parallel by different containers having different execution requirements.]\nThis repository encompasses a fully scripted Github workflow (via microk8s-kata.yml calling microk8s-kata.sh) to test the transparent use of the runtime for Kata Containers (Katas) on MicroK8s. It must run on a quite specific Google Cloud Engine (GCE) instance since so-called \"nested virtualization\" is required by Katas when running on the cloud due to its embedded virtual machine coming on top of the cloud hypervisor managing the Linux host. Some sample containerized services (see helloworld.go and autoscale.go built automatically with this side job) are deployed from Docker Hub and executed as Kubernetes services on MicroK8s.\nThe workflow tests the proper execution of sample containers with 'kata-runtime' after running them initially on standard 'runc' to validate global setup: beyond run of traditional helloworld-go, autoscale-go is called with parameters ensuring that thorough computations and resource allocation are properly executed by the replacing runtime.\nMicroK8s by Canonical was chosen on purpose for this project: its source code is extremely close to the upstream version of Kubernetes. Consequently, it allows to build a fully-featured production-grade Kubernetes cluster that can be run autonomously - on a single Limux instance - with very sensible default configuration allowing a quick setup, quite representative of a productive system.\nTo automatically confirm the validity of this workflow overtime when new versions of the various components (Kata Containers, MicroK8s, Docker, Ubuntu, etc.) get published, cron schedules it on a recurring basis: execution logs can be seen in Actions tab. Excerpts of last execution are gathered further down in this page.\nForking and re-using on your own is strongly encouraged! All comments for improvements and extensions will be welcome. Finally, if you like this repo, please give a Github star so that it gets more easily found by others.\nKata Containers - Rationale\nAs per Katas' website: \"Kata Containers is an open source community working to build a secure container runtime with lightweight virtual machines that feel and perform like containers, but provide stronger workload isolation using hardware virtualization technology as a second layer of defense.\"\nThis added lightweight virtual machine comes with a dedicated Linux kernel, providing isolation of network, I/O and memory and utilizes hardware-enforced isolation through Intel's VT-x features for virtualization.\n\nThe use of a per-container dedicated kernel and lightweight virtual machines, provided by either Qemu or Amazon's Firecracker, creates a much stronger isolation between the containers themselves and with the host. For example, if a container misbehaves and messes up with the kernel resources by overconsuming or corrupting them, it's only HIS dedicated kernel that gets damaged, not the unique kernel shared between all containers and host, as when you're using regular containers. The picture above shows the clear differences between the two architectures. So, Kata Containers are probably the best option currently available for additional security and reliability with untrusted workloads of all kinds (recent versions, external source code, etc.).\nAs you would expect, this further level of isolation through additional virtualization comes with a performance / cost penalty but this comparative study between the performances of raw host performances, \"runc\", Google's gVisor containers and Kata Containers demonstrates that the overhead remains quite acceptable in many situations for the additional security that is delivered. Look at slides 19 to 26 of the linked pdf to get the exact numbers.\nSpecific Setup\nTwo specific points have to be part of this workflow:\n\nWorkflow Steps\nThe major steps in this workflow are:\n\nCheck that GCE instance is proper ('GenuineIntel') - according to the above requirement for Broadwell - via lscpu after it has been created.\nInstall Kata Containers runtime directly from the Github repository of the project.\nCheck that this added runtime can run on the instance: command \"kata-runtime kata-check\" MUST produce output \"System is capable of running Kata Containers\"\nInstall Docker and check via \"docker info\" that it sees both its standard runtime \"runc\" and the newly added \"kata-runtime\"\nRun the latest version of Alpine Linux image with selection of kata-runtime (\"--runtime='kata-runtime\") and verify through \"docker info\" that the running Alpine is effectively using kata-runtime.\nInstall MicroK8s via snap and check that it works properly via the deployment of helloworld-go.yml and autoscale-go.yml service manifests, built from from GoLang source code in src/go directory. Stop MicroK8s when validation is successful.\nOpen the MicroK8s .snap file to add kata-runtime and repackage a new version (now unsigned) of the .snap file. Please, note use of \"unsquashfs\" and \"mksquashfs\" to achieve this refurbishing since the snap archive format is based on read-only and compressed SquashFS Linux file system.\nRemove old MicroK8s installation and re-install a fresh instance based with newly created snap version: \"--dangerous\" option is now required since the tweaked .snap is no longer signed by its official provider, Canonical.\nDeploy again helloworld-go and autoscale-go on fresh MicroK8s to validate that they work fine with kata-runtime: autoscale-go request is parametrized to make sure that some amount computing resources are consumed to achieve a better validation.\n\nHow to Fork & Run\nTo start with, you need a Google Cloud account including a project where the GCE APIs have been enabled. Obtain the id of your project from\nGCP dashboard. Additionally, you need to create in this project a service account (SA) and give it proper GKE credentials: right to create, administer and delete GCE images & instances (if your cannot  make the SA a \"Project Owner\" to simplify the security aspects...). Save the private key of the SA in json format.\nThen, fork our repository and define the required Github Secrets in your fork:\n\nyour GCP project id will be {{ secrets.GCP_PROJECT }}\nThe private key of your service account in json format will be ${{ secrets.GCP_SA_KEY }}\n\nTo easily use the workflow from Github, you can launch it with the manual dispatch feature of Github that you can see as a launch button (the green one in the picture below) in the Action tab of your fork.\n\nThe workflow will execute all the steps described above and terminate gracefully after all validation tests described are completed: it will then delete the GCE instance and the associated image triggering the nested virtualization.\nIf you also want to make use of the side workflow allowing to build the test container images from their GoLang sources, you'll need to add 2 additional secrets : {{ secrets.DOCKER_USERID }} & {{ secrets.DOCKER_PASSWORD }} corresponding to the login parameters of your Docker Hub account.\nExecution Report\nBelow are some relevant excerpts of the last execution log:\n### execution date: Thu Nov 19 01:30:05 UTC 2020\n \n### microk8s snap version:\nmicrok8s          v1.19.3    1791   1.19/stable      canonical*         classic\n \n### ubuntu version:\nDistributor ID:\tUbuntu\nDescription:\tUbuntu 20.04.1 LTS\nRelease:\t20.04\nCodename:\tfocal\n \n### docker version:\nClient: Docker Engine - Community\n Version:           19.03.13\n API version:       1.40\n Go version:        go1.13.15\n Git commit:        4484c46d9d\n Built:             Wed Sep 16 17:02:52 2020\n OS/Arch:           linux/amd64\n Experimental:      false\n\nServer: Docker Engine - Community\n Engine:\n  Version:          19.03.13\n  API version:      1.40 (minimum version 1.12)\n  Go version:       go1.13.15\n  Git commit:       4484c46d9d\n  Built:            Wed Sep 16 17:01:20 2020\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.3.7\n  GitCommit:        8fba4e9a7d01810a393d5d25a3621dc101981175\n docker-init:\n  Version:          0.18.0\n  GitCommit:        fec3683\n \n### kata-runtime version:\nkata-runtime  : 1.12.0-rc0\n   commit   : <<unknown>>\n   OCI specs: 1.0.1-dev\n \n### kata-runtime check:\nSystem is capable of running Kata Containers\n \n\n### check existing container runtimes on Ubuntu host:\n-rwxr-xr-x 1 root root 9.7M Sep  9 15:40 /bin/runc\n-rwxr-xr-x 1 root root 31M Oct 22 16:51 /bin/kata-runtime\n\n### check available docker runtimes: \n Runtimes: kata-runtime runc\n\n### test use of kata-runtime with alpine: \nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                      PORTS               NAMES\n2ea9bd4902f3        alpine              \"sh\"                3 seconds ago       Up Less than a second                           kata-alpine\n4d711e400486        busybox             \"uname -a\"          13 seconds ago      Exited (0) 10 seconds ago                       priceless_bose\n        \"Name\": \"/kata-alpine\",\n        \"Id\": \"2ea9bd4902f3aeafb53aa09f90219b09dd8aa4cfadfb7cceaa8ede146513719b\",\n            \"Runtime\": \"kata-runtime\",\n\n### install microk8s:\nmicrok8s is running\nhigh-availability: no\n  datastore master nodes: 127.0.0.1:19001\n  datastore standby nodes: none\naddons:\n  enabled:\n    ha-cluster           # Configure high availability on the current node\n  disabled:\n    ambassador           # Ambassador API Gateway and Ingress\n    cilium               # SDN, fast with full network policy\n    dashboard            # The Kubernetes dashboard\n    dns                  # CoreDNS\n    fluentd              # Elasticsearch-Fluentd-Kibana logging and monitoring\n    gpu                  # Automatic enablement of Nvidia CUDA\n    helm                 # Helm 2 - the package manager for Kubernetes\n    helm3                # Helm 3 - Kubernetes package manager\n    host-access          # Allow Pods connecting to Host services smoothly\n    ingress              # Ingress controller for external access\n    istio                # Core Istio service mesh services\n    jaeger               # Kubernetes Jaeger operator with its simple config\n    knative              # The Knative framework on Kubernetes.\n    kubeflow             # Kubeflow for easy ML deployments\n    linkerd              # Linkerd is a service mesh for Kubernetes and other frameworks\n    metallb              # Loadbalancer for your Kubernetes cluster\n    metrics-server       # K8s Metrics Server for API access to service metrics\n    multus               # Multus CNI enables attaching multiple network interfaces to pods\n    prometheus           # Prometheus operator for monitoring and logging\n    rbac                 # Role-Based Access Control for authorisation\n    registry             # Private image registry exposed on localhost:32000\n    storage              # Storage class; allocates storage from host directory\n\n### check container runtime on microk8s snap:\n-rwxr-xr-x 1 root root 15M Nov  6 12:06 /snap/microk8s/current/bin/runc\n\n### TEST WITH RUNC\n\n\n### test microk8s with helloworld-go & autoscale-go: \nservice/helloworld-go created\ndeployment.apps/helloworld-go-deployment created\nservice/autoscale-go created\ndeployment.apps/autoscale-go-deployment created\nNAME                                       READY   STATUS              RESTARTS   AGE\nnginx-test                                 0/1     ContainerCreating   0          2s\nhelloworld-go-deployment-86f5466d4-tjsrj   0/1     ContainerCreating   0          1s\nhelloworld-go-deployment-86f5466d4-ffr4g   0/1     ContainerCreating   0          1s\n\nwaiting for ready pods...\n\nNAME                                       READY   STATUS    RESTARTS   AGE\nnginx-test                                 1/1     Running   1          2m2s\nhelloworld-go-deployment-86f5466d4-tjsrj   1/1     Running   0          2m1s\nhelloworld-go-deployment-86f5466d4-ffr4g   1/1     Running   0          2m1s\nautoscale-go-deployment-5894658957-d7xjf   1/1     Running   0          2m\nautoscale-go-deployment-5894658957-f9747   1/1     Running   0          2m\nNAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE\nkubernetes      ClusterIP   10.152.183.1     <none>        443/TCP        2m33s\nhelloworld-go   NodePort    10.152.183.12    <none>        80:32577/TCP   2m1s\nautoscale-go    NodePort    10.152.183.211   <none>        80:30943/TCP   2m\n\ncalling helloworld-go...\n\nHello World: Kata Containers!\n\ncalling autoscale-go with request for biggest prime under 10 000 and 5 MB memory...\n\nAllocated 5 Mb of memory.\nThe largest prime less than 10000 is 9973.\nSlept for 100.19 milliseconds.\n\n### re-install microk8s incl kata-runtime: \nmicrok8s v1.19.3 installed\nmicrok8s is running\nhigh-availability: no\n  datastore master nodes: 127.0.0.1:19001\n  datastore standby nodes: none\naddons:\n  enabled:\n    ha-cluster           # Configure high availability on the current node\n  disabled:\n    ambassador           # Ambassador API Gateway and Ingress\n    cilium               # SDN, fast with full network policy\n    dashboard            # The Kubernetes dashboard\n    dns                  # CoreDNS\n    fluentd              # Elasticsearch-Fluentd-Kibana logging and monitoring\n    gpu                  # Automatic enablement of Nvidia CUDA\n    helm                 # Helm 2 - the package manager for Kubernetes\n    helm3                # Helm 3 - Kubernetes package manager\n    host-access          # Allow Pods connecting to Host services smoothly\n    ingress              # Ingress controller for external access\n    istio                # Core Istio service mesh services\n    jaeger               # Kubernetes Jaeger operator with its simple config\n    knative              # The Knative framework on Kubernetes.\n    kubeflow             # Kubeflow for easy ML deployments\n    linkerd              # Linkerd is a service mesh for Kubernetes and other frameworks\n    metallb              # Loadbalancer for your Kubernetes cluster\n    metrics-server       # K8s Metrics Server for API access to service metrics\n    multus               # Multus CNI enables attaching multiple network interfaces to pods\n    prometheus           # Prometheus operator for monitoring and logging\n    rbac                 # Role-Based Access Control for authorisation\n    registry             # Private image registry exposed on localhost:32000\n    storage              # Storage class; allocates storage from host directory\n\n### TEST WITH KATA-RUNTIME\n\n\n### test microk8s with helloworld-go & autoscale-go: \nservice/helloworld-go created\ndeployment.apps/helloworld-go-deployment created\nservice/autoscale-go created\ndeployment.apps/autoscale-go-deployment created\nNAME                                       READY   STATUS              RESTARTS   AGE\nnginx-test                                 0/1     ContainerCreating   0          1s\nhelloworld-go-deployment-86f5466d4-8j5nf   0/1     ContainerCreating   0          0s\nhelloworld-go-deployment-86f5466d4-556nf   0/1     ContainerCreating   0          0s\nautoscale-go-deployment-5894658957-px5sm   0/1     Pending             0          0s\nautoscale-go-deployment-5894658957-tqh7g   0/1     ContainerCreating   0          0s\n\nwaiting for ready pods...\n\nNAME                                       READY   STATUS    RESTARTS   AGE\nnginx-test                                 1/1     Running   0          2m2s\nhelloworld-go-deployment-86f5466d4-8j5nf   1/1     Running   0          2m1s\nautoscale-go-deployment-5894658957-tqh7g   1/1     Running   0          2m1s\nhelloworld-go-deployment-86f5466d4-556nf   1/1     Running   0          2m1s\nautoscale-go-deployment-5894658957-px5sm   1/1     Running   0          2m1s\nNAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nkubernetes      ClusterIP   10.152.183.1    <none>        443/TCP        2m35s\nhelloworld-go   NodePort    10.152.183.74   <none>        80:30748/TCP   2m1s\nautoscale-go    NodePort    10.152.183.26   <none>        80:30618/TCP   2m1s\n\ncalling helloworld-go...\n\nHello World: Kata Containers!\n\ncalling autoscale-go with request for biggest prime under 10 000 and 5 MB memory...\n\nAllocated 5 Mb of memory.\nThe largest prime less than 10000 is 9973.\nSlept for 100.19 milliseconds.\n\n### check proper symlink from microk8s runc:\nlrwxrwxrwx 1 root root 30 Nov 19 01:24 /snap/microk8s/current/bin/runc -> squashfs-root/bin/kata-runtime\n-rwxr-xr-x 1 root root 31560112 Oct 22 16:51 /bin/kata-runtime\n-rwxr-xr-x 1 root root 31560112 Nov 19 01:24 /snap/microk8s/current/bin/kata-runtime\n\n",
      "description": "Kata Containers with MicroK8s. Contribute to didier-durand/microk8s-kata-containers development by creating an account on GitHub.",
      "ogDescription": "Kata Containers with MicroK8s. Contribute to didier-durand/microk8s-kata-containers development by creating an account on GitHub."
    },
    {
      "url": "https://itnext.io/implementing-a-restricted-first-pod-security-policyarchitecture-af4e906593b0",
      "title": "Implementing a Restricted-First Pod Security PolicyArchitecture",
      "content": "<div><article><section class=\"cx cy cz da aj db dc s\"></section><div><section class=\"dh di dj dk dl\"><div class=\"n p\"><div class=\"ab ac ae af ag dm ai aj\"><figure class=\"gp gq gr gs gt gu cz da paragraph-image\"><img alt=\"Image for post\" class=\"t u v hc aj\" src=\"https://miro.medium.com/max/6928/0*9DiXE0WQz-pzGpGx\" width=\"3464\" srcset=\"https://miro.medium.com/max/552/0*9DiXE0WQz-pzGpGx 276w, https://miro.medium.com/max/1104/0*9DiXE0WQz-pzGpGx 552w, https://miro.medium.com/max/1280/0*9DiXE0WQz-pzGpGx 640w, https://miro.medium.com/max/1400/0*9DiXE0WQz-pzGpGx 700w\" sizes=\"700px\"><figcaption class=\"hm hn db cz da ho hp cg b ew ci fz\">Photo by <a href=\"https://unsplash.com/@speedoshots?utm_source=medium&amp;utm_medium=referral\" class=\"cm hq\">MiBosz Klinowski</a> on <a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\" class=\"cm hq\">Unsplash</a></figcaption></figure></div></div></section><section class=\"dh di dj dk dl\"><div class=\"n p\"><div class=\"ab ac ae af ag dm ai aj\"><p id=\"c9a1\" class=\"iw ix dp iy b iz ja id jb jc jd ih je jf jg jh ji jj jk jl jm jn jo jp jq jr dh em\">There are a myriad of ways to secure a Kubernetes cluster, whether through implementing Network Policies to control ingress/egress traffic, Role Based Access Control, or multi-tenancy. One of the most effective ways to manage what gets run on your cluster is through the creation of Pod Security Policies.</p><p id=\"90b3\" class=\"iw ix dp iy b iz js id jb jc jt ih je jf ju jh ji jj jv jl jm jn jw jp jq jr dh em\">A Pod Security Policy defines a set of conditions a pod must run with in order to run on the cluster. These conditions span host-level access, to a range of UIDs a container can run as, and even what volumes a pod can use.</p><p id=\"b529\" class=\"iw ix dp iy b iz js id jb jc jt ih je jf ju jh ji jj jv jl jm jn jw jp jq jr dh em\">In this article, I will lay out a blueprint for applying a secure-first mindset for your cluster through the implementation of Pod Security Policies. With a secure or restricted-first mindset, you will by default, lock-down your cluster to run secure workloads and through review, make exceptions for those workloads which require privileged access.</p></div></div></section><section class=\"dh di dj dk dl\"><div class=\"n p\"><div class=\"ab ac ae af ag dm ai aj\"><p id=\"7163\" class=\"iw ix dp iy b iz ja id jb jc jd ih je jf jg jh ji jj jk jl jm jn jo jp jq jr dh em\">As we are approaching our cluster from a secure-first mindset, we need to define a default Pod Security Policy that extends to all authenticated users and service accounts. Provided below is a sample restricted PSP you can use to get started and then tweak based on your needs:</p><figure class=\"gp gq gr gs gt gu\"></figure><p id=\"691a\" class=\"iw ix dp iy b iz js id jb jc jt ih je jf ju jh ji jj jv jl jm jn jw jp jq jr dh em\">Execute the following command to create your new Pod Security Policy:</p><pre class=\"gp gq gr gs gt jy jz bv\"></pre><p id=\"c39f\" class=\"iw ix dp iy b iz js id jb jc jt ih je jf ju jh ji jj jv jl jm jn jw jp jq jr dh em\">Next, we\u0019ll want to ensure that this Pod Security Policy applies to all service accounts as we\u0019re approaching this from a secure-first mindset. Use the RBAC resources below to accomplish this:</p><figure class=\"gp gq gr gs gt gu\"></figure><pre class=\"gp gq gr gs gt jy jz bv\"></pre><blockquote class=\"kg kh ki\"><p id=\"b07b\" class=\"iw ix kj iy b iz js id jb jc jt ih je jf ju jh ji jj jv jl jm jn jw jp jq jr dh em\">The above resources will first create a <strong class=\"iy kk\">ClusterRole </strong>resource that is authorized to use the <strong class=\"iy kk\">restricted-psp </strong>Pod Security Policy. We then create a <strong class=\"iy kk\">ClusterRoleBinding </strong>which binds our <strong class=\"iy kk\">ClusterRole</strong> to all service accounts across the cluster. This effectively means that when you deploy any pod on the cluster, it must confine to the conditions set forth by our <strong class=\"iy kk\">restricted-psp </strong>Pod Security Policy.</p></blockquote><h2 id=\"9cdb\" class=\"hy\">Privileged Access is the Exception</h2><p id=\"fcf1\" class=\"iw ix dp iy b iz ja id jb jc jd ih je jf jg jh ji jj jk jl jm jn jo jp jq jr dh em\">Now that we have applied a default restrictive Pod Security Policy, we are now ready to create our privileged PSP to allow workloads which require elevated levels of access to run. Some examples of these workloads include the control plane pods like kube-proxy, as well as ingress pods such as nginx.</p><p id=\"9438\" class=\"iw ix dp iy b iz js id jb jc jt ih je jf ju jh ji jj jv jl jm jn jw jp jq jr dh em\">Create the following privileged PSP using the template below, you can tweak it as needed based on your requirements:</p><figure class=\"gp gq gr gs gt gu\"></figure><pre class=\"gp gq gr gs gt jy jz bv\"></pre><blockquote class=\"kg kh ki\"><p id=\"90ac\" class=\"iw ix kj iy b iz js id jb jc jt ih je jf ju jh ji jj jv jl jm jn jw jp jq jr dh em\">This PSP effectively allows privileged access and escalation, allows containers to run as any user, allows containers to use any kind of volume, allows host access (filesystem and network), and allows any linux capability. You must exercise great judgement when determining how much privilege your workloads need. You should first understand what the minimum set of privileges those workloads need to run, then refine the PSP accordingly.</p></blockquote><p id=\"de9b\" class=\"iw ix dp iy b iz js id jb jc jt ih je jf ju jh ji jj jv jl jm jn jw jp jq jr dh em\">Next, we need to define a ClusterRole which is authorized to use our privileged PSP:</p><figure class=\"gp gq gr gs gt gu\"></figure><pre class=\"gp gq gr gs gt jy jz bv\"></pre><p id=\"3995\" class=\"iw ix dp iy b iz js id jb jc jt ih je jf ju jh ji jj jv jl jm jn jw jp jq jr dh em\">Now, we need to be able to determine which workloads require privileged access and then bind those workloads to our privileged PSP. This should be done on a case by case basis for your cluster, so as not to open the entire cluster up to privileged access.</p><p id=\"325f\" class=\"iw ix dp iy b iz js id jb jc jt ih je jf ju jh ji jj jv jl jm jn jw jp jq jr dh em\">To accomplish this, I would look at 2 questions to ask:</p><ol class><li id=\"20cc\" class=\"iw ix dp iy b iz js id jb jc jt ih je jf ju jh ji jj jv jl jm jn jw jp jq jr kq kr ks em\">Is there a namespace on your cluster (e.g. kube-system) where all workloads operate similarly and will require privileged access?</li><li id=\"9379\" class=\"iw ix dp iy b iz kt id jb jc ku ih je jf kv jh ji jj kw jl jm jn kx jp jq jr kq kr ks em\">Are there only specific workloads in a namespace which require privileged access?</li></ol><p id=\"5d28\" class=\"iw ix dp iy b iz js id jb jc jt ih je jf ju jh ji jj jv jl jm jn jw jp jq jr dh em\">For scenario 1, use the following RBAC resource template to allow all service accounts in a specific namespace to use the privileged pod security policy:</p><figure class=\"gp gq gr gs gt gu\"></figure><pre class=\"gp gq gr gs gt jy jz bv\"></pre><p id=\"7e18\" class=\"iw ix dp iy b iz js id jb jc jt ih je jf ju jh ji jj jv jl jm jn jw jp jq jr dh em\">For scenario 2, use the following RBAC resource template to allow a specific service account in a namespace to use the privileged pod security policy:</p><figure class=\"gp gq gr gs gt gu\"></figure><pre class=\"gp gq gr gs gt jy jz bv\"></pre><h2 id=\"ba87\" class=\"hy\">Enable the Pod Security Policy Admission Controller</h2><p id=\"d2c3\" class=\"iw ix dp iy b iz ja id jb jc jd ih je jf jg jh ji jj jk jl jm jn jo jp jq jr dh em\">Now that we have created our pod security policies and RBAC resources, we need to enable the Pod Security Policy admission controller on the Kubernetes API Server. Make sure that all of these resources have been created, otherwise, no workloads will be able to run once the admission controller is enabled (it requires that at least 1 Pod Security Policy be present).</p><p id=\"56d2\" class=\"iw ix dp iy b iz js id jb jc jt ih je jf ju jh ji jj jv jl jm jn jw jp jq jr dh em\">This is done by updating the <strong class=\"iy kk\">enable-admission-plugins </strong>flag on the Kubernetes API Server to include <strong class=\"iy kk\">PodSecurityPolicy</strong> in the list. If you are using a managed Kubernetes provider (EKS, AKS, etc), your provider is responsible for ensuring this is enabled, otherwise, you can use the steps above.</p><p id=\"4ab1\" class=\"iw ix dp iy b iz js id jb jc jt ih je jf ju jh ji jj jv jl jm jn jw jp jq jr dh em\">To see what admission plugins are currently available, run:</p><pre class=\"gp gq gr gs gt jy jz bv\"></pre><h2 id=\"0715\" class=\"hy\">Validation</h2><p id=\"cb3f\" class=\"iw ix dp iy b iz ja id jb jc jd ih je jf jg jh ji jj jk jl jm jn jo jp jq jr dh em\">To can see what PodSecurityPolicy your pod is using by executing the following command:</p><pre class=\"gp gq gr gs gt jy jz bv\"></pre><h2 id=\"b1c3\" class=\"hy\">Conclusion</h2><p id=\"e804\" class=\"iw ix dp iy b iz ja id jb jc jd ih je jf jg jh ji jj jk jl jm jn jo jp jq jr dh em\">In summary, pod security policies allow you to define a set of conditions that workloads must confine to in order to run. You have access to a wide array of conditions you can define for a policy, offering great flexibility in enforcing a secure architecture:</p><ul class><li id=\"02b9\" class=\"iw ix dp iy b iz js id jb jc jt ih je jf ju jh ji jj jv jl jm jn jw jp jq jr ky kr ks em\">Volume Types</li><li id=\"323b\" class=\"iw ix dp iy b iz kt id jb jc ku ih je jf kv jh ji jj kw jl jm jn kx jp jq jr ky kr ks em\">Host Filesystem and Network Access</li><li id=\"cf59\" class=\"iw ix dp iy b iz kt id jb jc ku ih je jf kv jh ji jj kw jl jm jn kx jp jq jr ky kr ks em\">Privileged Access and Escalation</li><li id=\"3e62\" class=\"iw ix dp iy b iz kt id jb jc ku ih je jf kv jh ji jj kw jl jm jn kx jp jq jr ky kr ks em\">Linux Capabilities</li><li id=\"9a93\" class=\"iw ix dp iy b iz kt id jb jc ku ih je jf kv jh ji jj kw jl jm jn kx jp jq jr ky kr ks em\">RunAsUser for containers</li></ul><p id=\"7acb\" class=\"iw ix dp iy b iz js id jb jc jt ih je jf ju jh ji jj jv jl jm jn jw jp jq jr dh em\">And much more&amp;</p><p id=\"15dc\" class=\"iw ix dp iy b iz js id jb jc jt ih je jf ju jh ji jj jv jl jm jn jw jp jq jr dh em\">With the blueprint above, you will be able to operate your Kubernetes cluster from a secure-first mindset, while still excepting privileged access where needed.</p><h2 id=\"45f2\" class=\"hy\">Resources</h2><p id=\"4912\" class=\"iw ix dp iy b iz ja id jb jc jd ih je jf jg jh ji jj jk jl jm jn jo jp jq jr dh em\"><a href=\"https://kubernetes.io/docs/concepts/policy/pod-security-policy/\" class=\"cm hq\">Pod Security Policies: Kubernetes Docs</a></p></div></div></section></div></article></div>",
      "contentAsText": "Photo by MiBosz Klinowski on UnsplashThere are a myriad of ways to secure a Kubernetes cluster, whether through implementing Network Policies to control ingress/egress traffic, Role Based Access Control, or multi-tenancy. One of the most effective ways to manage what gets run on your cluster is through the creation of Pod Security Policies.A Pod Security Policy defines a set of conditions a pod must run with in order to run on the cluster. These conditions span host-level access, to a range of UIDs a container can run as, and even what volumes a pod can use.In this article, I will lay out a blueprint for applying a secure-first mindset for your cluster through the implementation of Pod Security Policies. With a secure or restricted-first mindset, you will by default, lock-down your cluster to run secure workloads and through review, make exceptions for those workloads which require privileged access.As we are approaching our cluster from a secure-first mindset, we need to define a default Pod Security Policy that extends to all authenticated users and service accounts. Provided below is a sample restricted PSP you can use to get started and then tweak based on your needs:Execute the following command to create your new Pod Security Policy:Next, we\u0019ll want to ensure that this Pod Security Policy applies to all service accounts as we\u0019re approaching this from a secure-first mindset. Use the RBAC resources below to accomplish this:The above resources will first create a ClusterRole resource that is authorized to use the restricted-psp Pod Security Policy. We then create a ClusterRoleBinding which binds our ClusterRole to all service accounts across the cluster. This effectively means that when you deploy any pod on the cluster, it must confine to the conditions set forth by our restricted-psp Pod Security Policy.Privileged Access is the ExceptionNow that we have applied a default restrictive Pod Security Policy, we are now ready to create our privileged PSP to allow workloads which require elevated levels of access to run. Some examples of these workloads include the control plane pods like kube-proxy, as well as ingress pods such as nginx.Create the following privileged PSP using the template below, you can tweak it as needed based on your requirements:This PSP effectively allows privileged access and escalation, allows containers to run as any user, allows containers to use any kind of volume, allows host access (filesystem and network), and allows any linux capability. You must exercise great judgement when determining how much privilege your workloads need. You should first understand what the minimum set of privileges those workloads need to run, then refine the PSP accordingly.Next, we need to define a ClusterRole which is authorized to use our privileged PSP:Now, we need to be able to determine which workloads require privileged access and then bind those workloads to our privileged PSP. This should be done on a case by case basis for your cluster, so as not to open the entire cluster up to privileged access.To accomplish this, I would look at 2 questions to ask:Is there a namespace on your cluster (e.g. kube-system) where all workloads operate similarly and will require privileged access?Are there only specific workloads in a namespace which require privileged access?For scenario 1, use the following RBAC resource template to allow all service accounts in a specific namespace to use the privileged pod security policy:For scenario 2, use the following RBAC resource template to allow a specific service account in a namespace to use the privileged pod security policy:Enable the Pod Security Policy Admission ControllerNow that we have created our pod security policies and RBAC resources, we need to enable the Pod Security Policy admission controller on the Kubernetes API Server. Make sure that all of these resources have been created, otherwise, no workloads will be able to run once the admission controller is enabled (it requires that at least 1 Pod Security Policy be present).This is done by updating the enable-admission-plugins flag on the Kubernetes API Server to include PodSecurityPolicy in the list. If you are using a managed Kubernetes provider (EKS, AKS, etc), your provider is responsible for ensuring this is enabled, otherwise, you can use the steps above.To see what admission plugins are currently available, run:ValidationTo can see what PodSecurityPolicy your pod is using by executing the following command:ConclusionIn summary, pod security policies allow you to define a set of conditions that workloads must confine to in order to run. You have access to a wide array of conditions you can define for a policy, offering great flexibility in enforcing a secure architecture:Volume TypesHost Filesystem and Network AccessPrivileged Access and EscalationLinux CapabilitiesRunAsUser for containersAnd much more&With the blueprint above, you will be able to operate your Kubernetes cluster from a secure-first mindset, while still excepting privileged access where needed.ResourcesPod Security Policies: Kubernetes Docs",
      "publishedDate": "2020-11-03T17:59:05.323Z",
      "description": "Pod Security Policies allow you to define fine-grained conditions workloads must conform to in order to run on your Kubernetes cluster. I will provide a blueprint here to deploy a secure-first PSP while excepting privileged access when needed for a robust security architecture.",
      "ogDescription": "Pod Security Policies"
    },
    {
      "url": "https://blog.getambassador.io/get-a-sandbox-domain-name-and-tls-cert-for-testing-k8s-apps-in-under-5-minutes-cf97e1041327",
      "title": "Get a Sandbox, Domain Name, and TLS Cert for Testing K8s Apps in Under 5 Minutes",
      "content": "<div><article><section class=\"cx cy cz da aj db dc s\"></section><div><section class=\"dh di dj dk dl\"><div class=\"n p\"><div class=\"ab ac ae af ag dm ai aj\"><figure class=\"hg hh hi hj hk hl cz da paragraph-image\"><img alt=\"Image for post\" class=\"t u v ht aj\" src=\"https://miro.medium.com/max/11520/1*O50XnMdYISSOLNkpMT-9SA.jpeg\" width=\"5760\" srcset=\"https://miro.medium.com/max/552/1*O50XnMdYISSOLNkpMT-9SA.jpeg 276w, https://miro.medium.com/max/1104/1*O50XnMdYISSOLNkpMT-9SA.jpeg 552w, https://miro.medium.com/max/1280/1*O50XnMdYISSOLNkpMT-9SA.jpeg 640w, https://miro.medium.com/max/1400/1*O50XnMdYISSOLNkpMT-9SA.jpeg 700w\" sizes=\"700px\"><figcaption class=\"id ie db cz da if ig cg b fo ci fe\">Photo by <a href=\"https://unsplash.com/@markusspiske?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\" class=\"cm ih\">Markus Spiske</a> on <a href=\"https://unsplash.com/s/photos/sandbox?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\" class=\"cm ih\">Unsplash</a></figcaption></figure><p id=\"3e3d\" class=\"ii ij dp ik b eo il im in er io ip iq ir is it iu iv iw ix iy iz ja jb jc jd dh em\">When prototyping a public-facing API or website it is often the case that you need a fully qualified domain name such as <a href=\"http://www.my-new-site.com/\" class=\"cm ih\">www.my-new-site.com</a> to verify some aspect of the underlying application. The classic example is when testing secure access (TLS) to your application.</p><p id=\"a8ec\" class=\"ii ij dp ik b eo il im in er io ip iq ir is it iu iv iw ix iy iz ja jb jc jd dh em\">At the beginning stages of your project it\u0019s often the case that you don\u0019t have your production Kubernetes cluster ready to go or your domain name and DNS configuration complete, but you still need a domain name and TLS certificate to be able to run truly realistic tests. How can you achieve this? Read on&amp;</p><h2 id=\"f4a7\" class=\"je\">Testing with a domain name with TLS certificate</h2><p id=\"1e51\" class=\"ii ij dp ik b eo ka im in er kb ip iq ir kc it iu iv kd ix iy iz ke jb jc jd dh em\">Following the usual chain of thought, yes, you can attach a certificate to an IP address. However, this <a href=\"https://stackoverflow.com/questions/2043617/is-it-possible-to-have-ssl-certificate-for-ip-address-not-domain-name\" class=\"cm ih\">isn\u0019t generally recommended</a>, and also it doesn\u0019t satisfy our requirement of using a domain name.</p><p id=\"2d9f\" class=\"ii ij dp ik b eo il im in er io ip iq ir is it iu iv iw ix iy iz ja jb jc jd dh em\">You could use services like <a href=\"https://nip.io/\" class=\"cm ih\">nip.io</a>, but issuing TLS certificates via a service like <a href=\"https://letsencrypt.org/\" class=\"cm ih\">Let\u0019s Encrypt</a> can bump into rate limiting challenges.</p><p id=\"75e4\" class=\"ii ij dp ik b eo il im in er io ip iq ir is it iu iv iw ix iy iz ja jb jc jd dh em\">A better approach would be to generate your own domain name, register a TLS certificate for this, and then point this at your app. But this all takes time and knowledge, particularly depending on your role and your organization.</p><p id=\"b65e\" class=\"ii ij dp ik b eo il im in er io ip iq ir is it iu iv iw ix iy iz ja jb jc jd dh em\">What if I told you there is a better way to do this when building and deploying applications onto Kubernetes?</p><h2 id=\"9f76\" class=\"je\">Initialize your testing problems away!</h2><p id=\"6631\" class=\"ii ij dp ik b eo ka im in er kb ip iq ir kc it iu iv kd ix iy iz ke jb jc jd dh em\">The <a href=\"https://app.getambassador.io/initializer\" class=\"cm ih\">K8s Initializer</a> allows you to generate a unique <a href=\"https://edgestack.me/\" class=\"cm ih\">edgestack.me</a> subdomain and provides the functionality to automatically acquire a TLS certificate using Let\u0019s Encrypt and the <a href=\"https://www.getambassador.io/docs/latest/topics/running/host-crd/#acme-support\" class=\"cm ih\">ACME protocol.</a></p><p id=\"b8ea\" class=\"ii ij dp ik b eo il im in er io ip iq ir is it iu iv iw ix iy iz ja jb jc jd dh em\">When configuring the Ambassador Edge Stack for your specific environment and testing needs, you may select the \u001cAutomatically generate and assign a publicly shareable domain name for my installation\u001d option.</p><p id=\"0183\" class=\"ii ij dp ik b eo il im in er io ip iq ir is it iu iv iw ix iy iz ja jb jc jd dh em\">This option is available for all public cloud providers: Google Kubernetes Engine, Azure Kubernetes Service, and Amazon offerings \u0014 given a supported public load balancer and having Ambassador terminate TLS using a Let\u0019s Encrypt certificate, which is our recommended option. The only input requirement is that you provide a valid email address that will be shared with Let\u0019s Encrypt when registering and acquiring the TLS certificate matching the randomly generated domain name.</p><figure class=\"hg hh hi hj hk hl cz da paragraph-image\"><img alt=\"Image for post\" class=\"t u v ht aj\" src=\"https://miro.medium.com/max/2356/0*9bVcyr3GkUgFrysu\" width=\"1178\" srcset=\"https://miro.medium.com/max/552/0*9bVcyr3GkUgFrysu 276w, https://miro.medium.com/max/1104/0*9bVcyr3GkUgFrysu 552w, https://miro.medium.com/max/1280/0*9bVcyr3GkUgFrysu 640w, https://miro.medium.com/max/1400/0*9bVcyr3GkUgFrysu 700w\" sizes=\"700px\"></figure><p id=\"aa3c\" class=\"ii ij dp ik b eo il im in er io ip iq ir is it iu iv iw ix iy iz ja jb jc jd dh em\">Moving on to the \u001cReview and Install\u001d part of the K8s Initializer guided process, the installation instructions will lead you through your sandbox setup by applying Kubernetes Custom Resource Definitions followed by Kubernetes manifests for all your selected components at once. Right after the Ambassador Edge Stack has started, the cloud provider will automatically assign an IP or hostname to the attached load balancer and ambassador service.</p><p id=\"1432\" class=\"ii ij dp ik b eo il im in er io ip iq ir is it iu iv iw ix iy iz ja jb jc jd dh em\">It may take a few moments, depending on your cloud provider, but you\u0019ll be able to retrieve the assigned public endpoint by following and copy/pasting the instructions on the K8s Initializer screen:</p><figure class=\"hg hh hi hj hk hl cz da paragraph-image\"><img alt=\"Image for post\" class=\"t u v ht aj\" src=\"https://miro.medium.com/max/3104/0*SGOtRJ1RPAu17H8f\" width=\"1552\" srcset=\"https://miro.medium.com/max/552/0*SGOtRJ1RPAu17H8f 276w, https://miro.medium.com/max/1104/0*SGOtRJ1RPAu17H8f 552w, https://miro.medium.com/max/1280/0*SGOtRJ1RPAu17H8f 640w, https://miro.medium.com/max/1400/0*SGOtRJ1RPAu17H8f 700w\" sizes=\"700px\"></figure><p id=\"abfd\" class=\"ii ij dp ik b eo il im in er io ip iq ir is it iu iv iw ix iy iz ja jb jc jd dh em\">After inputting the public external address assigned by your cloud provider for this installation in the text field, clicking the \u001cAcquire a domain name\u001d button will kick off the DNS registration process. Ensuring Ambassador Edge Stack is indeed serving requests for this endpoint, a random domain name will be assigned and visible on the screen.</p><p id=\"9ed0\" class=\"ii ij dp ik b eo il im in er io ip iq ir is it iu iv iw ix iy iz ja jb jc jd dh em\">Please note that you may experience a bit of delay with DNS propagation depending on your Internet service provider.</p><p id=\"05a7\" class=\"ii ij dp ik b eo il im in er io ip iq ir is it iu iv iw ix iy iz ja jb jc jd dh em\">Right after a domain name was acquired, the K8s Initializer instructions page will automatically refresh and present you with up-to-date information matching the newly accessible public endpoint. At this time, you\u0019ll be able to follow through with the installation of the customized resources to support your configuration options, including the Host resource for automatic TLS!</p><p id=\"1419\" class=\"ii ij dp ik b eo il im in er io ip iq ir is it iu iv iw ix iy iz ja jb jc jd dh em\">This Host resource can also be inspected as the K8s Initializer allows you to drill down into the generated Kubernetes manifests.</p><figure class=\"hg hh hi hj hk hl cz da paragraph-image\"><img alt=\"Image for post\" class=\"t u v ht aj\" src=\"https://miro.medium.com/max/3200/0*Olf-pnUgbyCXnbxA\" width=\"1600\" srcset=\"https://miro.medium.com/max/552/0*Olf-pnUgbyCXnbxA 276w, https://miro.medium.com/max/1104/0*Olf-pnUgbyCXnbxA 552w, https://miro.medium.com/max/1280/0*Olf-pnUgbyCXnbxA 640w, https://miro.medium.com/max/1400/0*Olf-pnUgbyCXnbxA 700w\" sizes=\"700px\"></figure><h2 id=\"3bf7\" class=\"je\">Share your demo publicly</h2><p id=\"52f1\" class=\"ii ij dp ik b eo ka im in er kb ip iq ir kc it iu iv kd ix iy iz ke jb jc jd dh em\">Now that you have set up your playground environment on a custom domain, you have the ability to share your demo publicly and get teammates involved (an opportunity you wouldn\u0019t have if you were just running your demo from localhost).</p><p id=\"01c4\" class=\"ii ij dp ik b eo il im in er io ip iq ir is it iu iv iw ix iy iz ja jb jc jd dh em\">This means, for example, that if you configured Jaeger as part of your tech stack you can point teammates directly to the jaeger UI at <a href=\"https:///$GENERATED_CUSTOM_DOMAIN/jaeger/\" class=\"cm ih\">https://$GENERATED_CUSTOM_DOMAIN/jaeger/.</a></p><p id=\"2f2d\" class=\"ii ij dp ik b eo il im in er io ip iq ir is it iu iv iw ix iy iz ja jb jc jd dh em\">This approach is incredibly useful when evaluating new tools to add to your tech stack, and for when you want to collaborate around a proof-of-concept set-up.</p><h2 id=\"1447\" class=\"je\">What\u0019s your next biggest K8s app development challenge? Send us your feedback</h2><p id=\"48c6\" class=\"ii ij dp ik b eo ka im in er kb ip iq ir kc it iu iv kd ix iy iz ke jb jc jd dh em\">The cloud native space is always evolving, and if you\u0019re building applications in Kubernetes you\u0019re probably in a constant state of evaluating the latest and greatest tools. The K8s Initializer lets you evaluate these tools in an environment similar to your production set-up, while also giving you the tools you need to collaborate with your teammates.</p><p id=\"9e2a\" class=\"ii ij dp ik b eo il im in er io ip iq ir is it iu iv iw ix iy iz ja jb jc jd dh em\">We\u0019d love to hear your feedback! If there are other tools you\u0019d like to see supported by the <a href=\"http://app.getambassador.io/initializer\" class=\"cm ih\">K8s Initializer</a>, or if you have any questions please join the #K8s-Initializer channel in <a href=\"https://d6e.co/slack\" class=\"cm ih\">our Slack</a> or tweet us <a href=\"http://twitter.com/ambassadorlabs\" class=\"cm ih\">@ambassadorlabs</a>.</p></div></div></section></div></article></div>",
      "contentAsText": "Photo by Markus Spiske on UnsplashWhen prototyping a public-facing API or website it is often the case that you need a fully qualified domain name such as www.my-new-site.com to verify some aspect of the underlying application. The classic example is when testing secure access (TLS) to your application.At the beginning stages of your project it\u0019s often the case that you don\u0019t have your production Kubernetes cluster ready to go or your domain name and DNS configuration complete, but you still need a domain name and TLS certificate to be able to run truly realistic tests. How can you achieve this? Read on&Testing with a domain name with TLS certificateFollowing the usual chain of thought, yes, you can attach a certificate to an IP address. However, this isn\u0019t generally recommended, and also it doesn\u0019t satisfy our requirement of using a domain name.You could use services like nip.io, but issuing TLS certificates via a service like Let\u0019s Encrypt can bump into rate limiting challenges.A better approach would be to generate your own domain name, register a TLS certificate for this, and then point this at your app. But this all takes time and knowledge, particularly depending on your role and your organization.What if I told you there is a better way to do this when building and deploying applications onto Kubernetes?Initialize your testing problems away!The K8s Initializer allows you to generate a unique edgestack.me subdomain and provides the functionality to automatically acquire a TLS certificate using Let\u0019s Encrypt and the ACME protocol.When configuring the Ambassador Edge Stack for your specific environment and testing needs, you may select the \u001cAutomatically generate and assign a publicly shareable domain name for my installation\u001d option.This option is available for all public cloud providers: Google Kubernetes Engine, Azure Kubernetes Service, and Amazon offerings \u0014 given a supported public load balancer and having Ambassador terminate TLS using a Let\u0019s Encrypt certificate, which is our recommended option. The only input requirement is that you provide a valid email address that will be shared with Let\u0019s Encrypt when registering and acquiring the TLS certificate matching the randomly generated domain name.Moving on to the \u001cReview and Install\u001d part of the K8s Initializer guided process, the installation instructions will lead you through your sandbox setup by applying Kubernetes Custom Resource Definitions followed by Kubernetes manifests for all your selected components at once. Right after the Ambassador Edge Stack has started, the cloud provider will automatically assign an IP or hostname to the attached load balancer and ambassador service.It may take a few moments, depending on your cloud provider, but you\u0019ll be able to retrieve the assigned public endpoint by following and copy/pasting the instructions on the K8s Initializer screen:After inputting the public external address assigned by your cloud provider for this installation in the text field, clicking the \u001cAcquire a domain name\u001d button will kick off the DNS registration process. Ensuring Ambassador Edge Stack is indeed serving requests for this endpoint, a random domain name will be assigned and visible on the screen.Please note that you may experience a bit of delay with DNS propagation depending on your Internet service provider.Right after a domain name was acquired, the K8s Initializer instructions page will automatically refresh and present you with up-to-date information matching the newly accessible public endpoint. At this time, you\u0019ll be able to follow through with the installation of the customized resources to support your configuration options, including the Host resource for automatic TLS!This Host resource can also be inspected as the K8s Initializer allows you to drill down into the generated Kubernetes manifests.Share your demo publiclyNow that you have set up your playground environment on a custom domain, you have the ability to share your demo publicly and get teammates involved (an opportunity you wouldn\u0019t have if you were just running your demo from localhost).This means, for example, that if you configured Jaeger as part of your tech stack you can point teammates directly to the jaeger UI at https://$GENERATED_CUSTOM_DOMAIN/jaeger/.This approach is incredibly useful when evaluating new tools to add to your tech stack, and for when you want to collaborate around a proof-of-concept set-up.What\u0019s your next biggest K8s app development challenge? Send us your feedbackThe cloud native space is always evolving, and if you\u0019re building applications in Kubernetes you\u0019re probably in a constant state of evaluating the latest and greatest tools. The K8s Initializer lets you evaluate these tools in an environment similar to your production set-up, while also giving you the tools you need to collaborate with your teammates.We\u0019d love to hear your feedback! If there are other tools you\u0019d like to see supported by the K8s Initializer, or if you have any questions please join the #K8s-Initializer channel in our Slack or tweet us @ambassadorlabs.",
      "publishedDate": "2020-11-11T18:56:40.252Z",
      "description": "When prototyping a public-facing API or website it is often the case that you need a fully qualified domain name such as www.my-new-site.com to verify some aspect of the underlying application. The…",
      "ogDescription": "Run fast, realistic tests with a custom TLS-secured domain"
    },
    {
      "url": "https://codilime.com/how-to-create-a-custom-resource-with-kubernetes-operator/",
      "title": "How to create a custom resource with Kubernetes Operator",
      "content": "<article> <p><b>While developing projects on the Kubernetes platform I came across an interesting problem. I had quite a few scripts that ran in containers and needed to be triggered only once on every node in my Kubernetes cluster. This could not be solved using default Kubernetes resources such as DaemonSet and Job. So I decided to write my own resource using Kubernetes Operator Framework. How I went about it is the subject of this blog post.</b></p>\n<p>When I confronted this problem, my first thought was to use a DaemonSet resource that utilizes <i>initContainers</i> and then starts a dummy busybox container running <span>tail -f /dev/null</span> or another command that does nothing. However, after analyzing the question more thoroughly, I realized that it would be very problematic on a production setup with many such DaemonSets. Each dummy container eats up resources to execute a useless task. Moreover, from the perspective of the cluster administrator, such a DaemonSet seems exactly the same as other legitimate services. Should an error occur, it may be confusing and require a deep dive into running resources.</p>\n<p>My second idea was to use a simple Job with a huge number of <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/job/#parallel-jobs\"><i>Completions </i>and<i> Parallelism</i></a>. Using <a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity\">anti-affinity</a> specification to ensure that only one Pod will run on a single node at a time, I would achieve a simulated effect of the DaemonSet\u0019s behavior. Though a new node would appear in the cluster, a new Pod would be scheduled.</p>\n<p>Yet this idea, with a fixed number of <i>Completions,</i> seemed a bit amateurish. Even if I defined it to be 1000\u0014what if a 1001st node were to appear in the cluster? Further, from the perspective of the cluster administrator, such a Job seems like a defective resource that cannot finish all <i>Completions</i>.</p> <h2><b>Creating a custom resource in Kubernetes</b></h2>\n<p>But if I could go a bit more low-level and create a resource that merges the functionality of the DaemonSet and the Job? I went online to find out whether someone has already created such a resource. But all I turned up were a few Issues on Kubernetes GitHub posted by people who\u0019d had the same problem. For example, <a href=\"https://github.com/kubernetes/kubernetes/issues/64623\">in this issue</a> the idea of adding such a DaemonJob to the official Kubernetes resources has been discussed for over two years, but a conclusion has remained elusive. Long story short, I would be forced to create such a resource by myself. I chose to use Operator Framework and code a resource that would be defined in a very similar style as a standard Kubernetes Job resource but would automatically schedule a pod on every node, &#xFFFD; la DaemonSet. Before I go further, let\u0019s have a brief overview of the Kubernetes operator architecture.</p> <h2><b>An overview of Kubernetes operator architecture</b></h2>\n<p>A Kubernetes operator has two main components: Custom Resource Definition (CRD) and Operator Controller. Custom Resource Definition is a standard Kubernetes resource that allows users to extend Kubernetes API with new resources. However, this only enables users to apply manifests of newly created resources but will have no effect on the cluster. This is because Kubernetes architecture contains the Kubernetes controller which has all the logic behind every resource in the cluster. Since a user-defined resource has not been coded in the built-in Kubernetes controller, the user has to provide its controller separately. This is where the operator controller comes up, usually deployed as a separate pod on the cluster with code that contains logic on how to manage custom resources.</p>\n<p>The controller has the main parts which are important for every operator developer. The first is the definition of what the controller watches. It defines changes in specific resources or events that will trigger the operator\u0019s control logic. For example, if a custom resource\u0019s work is based on information provided in ConfigMap, then the operator should watch changes of ConfigMap to trigger code on every change and adjust the resource to the new configuration. The other important part is the <i>Reconcile</i> loop. This is in fact a function that contains the whole logic behind the operator and is triggered by changes in the resources being watched.</p> <h2><b>How to start working with Kubernetes Operator Framework</b></h2>\n<p>Development of such a project code template may seem a tremendous challenge for a developer. However, Operator Framework provides a clever CLI tool called <a href=\"https://github.com/operator-framework/operator-sdk\">operator-sdk</a>, which will do most of the job. Whole code infrastructure may be generated with just two commands and then the developer just edits specific structures or functions to adjust the resource-specific logic.</p>\n<p>Operator Framework allows users to create multiple Kubernetes custom resources in the scope of a single operator. In such a case, every resource is managed by a single controller, which can be used to build more complicated projects. These can often have multiple components, with each deployed as a separate custom resource. In order to initialize an empty operator, you may just run simply <span>operator-sdk init</span> <a href=\"https://sdk.operatorframework.io/docs/building-operators/golang/quickstart/#create-a-project\">(docs)</a> command with proper flags for our project and the whole basic architecture will be created.</p>\n<p>Having initialized the operator, you can now add code for specific resources. For the problem described in this blogpost, I will create just one resource. The new resource can be added with the command <span>operator-sdk create api</span> <a href=\"https://sdk.operatorframework.io/docs/building-operators/golang/quickstart/#create-an-api\">(docs)</a>, which will generate API code for the specific resource described with passed parameters that are additional flags added after a CLI command that specify what should exactly happen after running this command. This means the developer will be able to specify only the structure of a custom resource, but will not have code generated for the controller part. To additionally generate controller code it\u0019s necessary to provide the flag <span>&#x2013;controller=True</span>&#xFFFD; when creating the API. Why does operator-sdk not do that by default? In more advanced projects that periodically release new versions of the product, the API may change and leave old manifests not working\u0014and all customers using the old manifests facing the need to migrate to the new version. However, because a new API can be created, both the old and new API of a resource are supported, and both versions can be managed with a single controller, at least until legacy support is no longer required.</p>\n<p>Having a ready project structure for development of a new custom resource, you can start writing the code. All definitions of API should be written in files with names ending with <i>_types.go</i> located under <i>api/&lt;API version&gt;</i> directory. And all controllers code is located under controllers/ directory in files with names ending with <i>_controller.go</i>. These are the main two file types where developers should edit code. All other files are generated automatically. When a developer finishes editing code of the custom resource then all he has to do is run the make <span>manifests</span> command and the operator code will be ready to be installed on the cluster <a href=\"https://sdk.operatorframework.io/docs/building-operators/golang/tutorial/#build-and-run-the-operator\">(docs)</a>.</p> <h2><b>DeamonJob step-by-step</b></h2>\n<p>Given all this, I started implementing the idea of a DaemonJob. The API was the easy part\u0014just remove the <i>Completions</i> and <i>Parallelism</i> fields from the Job API and leave everything as is. The API code in the Kubernetes resources is written as Go structs and most of it looks the same <a href=\"https://github.com/Dysproz/DaemonJob/blob/master/api/v1/daemonjob_types.go\">(code)</a>. From the user\u0019s perspective, a sample manifest to be applied on the cluster would look like this:</p>\n<pre class=\"brush: plain; title: ; notranslate\">\r\napiVersion: dj.dysproz.io/v1\r\nkind: DaemonJob\r\n  metadata:\r\nname: daemonjob-sample\r\n  spec:\r\n    template:\r\n      spec:\r\n        containers:\r\n         - name: test-job\r\n           image: busybox\r\n           command:\r\n            - sleep\r\n            - &quot;20&quot;\r\n        nodeSelector:\r\n          app: v1\r\n        restartPolicy: OnFailure\r\n</pre>\n<p>In this example, on every node which has label <i>app=v1,</i> a busybox container will be created and will run a <i>sleep</i> command for 20 seconds. As you may be able to see, the above manifest is very similar to one users would create for a simple Kubernetes Job resource.</p>\n<p>However, the logic behind the controller for such a resource presented a more daunting challenge <a href=\"https://github.com/Dysproz/DaemonJob/blob/master/controllers/daemonjob_controller.go\">(code)</a>.</p>\n<p>It required, first of all, that the resources to watch be defined. In this case, the operator would have to look just for changes on nodes which may seem fairly easy.&#xFFFD; This brings me to the first problem. Kubernetes operator code is usually designed to create resources that are owned by a single custom resource and watch only these resources. For example, if a custom resource creates ConfigMap with configuration and watches for ConfigMap, then it should look only for changes on ConfigMaps that are owned by this custom resource.</p>\n<p>When a watched resource triggers the Reconcile loop of custom resources, an argument request is passed containing two fields: custom resource name and namespace. Although nodes in the Kubernetes cluster are represented as regular resources, there is no built-in function to watch specific resources without ownership of a custom resource. As a result, the Reconcile loop would be triggered with the request name pointing to a node that triggered action rather than to a specific custom resource.</p>\n<p>To keep up with the original design of developing operators, it is necessary to build a custom method that will trigger the Reconcile loop with a proper request name for every DaemonJob resource when any node changes its state. This can be achieved with the following code:</p>\n<pre class=\"brush: plain; title: ; notranslate\">\r\n    func (r *DaemonJobReconciler) SetupWithManager(mgr ctrl.Manager) error {\r\n        if err := ctrl.NewControllerManagedBy(mgr).\r\n            For(&amp;djv1.DaemonJob{}).\r\n            Complete(r); err != nil {\r\n            return err\r\n        }\r\n    \r\n        if err := ctrl.NewControllerManagedBy(mgr).\r\n            For(&amp;corev1.Node{}).\r\n            Watches(&amp;source.Kind{Type: &amp;corev1.Node{}}, &amp;handler.EnqueueRequestsFromMapFunc{\r\n                ToRequests: handler.ToRequestsFunc(func(nodeObject handler.MapObject) []reconcile.Request {\r\n                    var djObjects djv1.DaemonJobList\r\n                    _ = mgr.GetClient().List(context.TODO(), &amp;djObjects)\r\n                    var requests = []reconcile.Request{}\r\n                    for _, djObject := range djObjects.Items {\r\n                        requests = append(requests, reconcile.Request{\r\n                            NamespacedName: types.NamespacedName{\r\n                                Name:      djObject.Name,\r\n                                Namespace: djObject.Namespace,\r\n                            },\r\n                        })\r\n                    }\r\n                    return requests\r\n                }),\r\n            }).\r\n            Complete(r); err != nil {\r\n            return err\r\n        }\r\n    \r\n        return nil\r\n    }\r\n</pre>\n<p>The first operation adds a watch for the custom resource itself. This means that a Reconcile loop will be triggered whenever a new manifest with this resource is applied on the cluster or edited by the user during runtime.</p>\n<p>The second operation defines a custom watch for changes in node resources. If any node changes its state, an anonymous function is triggered. The anonymous function lists all DaemonJob resources that exist in the cluster and then prepares requests for every resource found. As a result, node watch will simultaneously trigger a set of requests to reconcile DaemonJob resources and update their state.</p>\n<p>Once the logic behind the proper triggers of the Reconcile loop is handled, it is time to concentrate on the Reconcile loop itself.</p>\n<p>The Controller logic is based on two steps. The first is to define how many nodes are currently in the cluster and which nodes should run the Job (which may be defined with, for example, a nodeSelector field in the manifest). The second step is to prepare a Job resource that will have defined that number as the number of Completions.</p>\n<p>Starting the Reconcile loop by setting some basic variables, it has to gather the filled structure of the DaemonJob resource applied in the cluster to be used to fill out the Job resource. In this way, you create a data structure to which you can apply the logic embedded in the code.</p>\n<pre class=\"brush: plain; title: ; notranslate\">\r\n    func (r *DaemonJobReconciler) Reconcile(req ctrl.Request) (ctrl.Result, error) {\r\n        _ = context.Background()\r\n        _ = r.Log.WithValues(&quot;daemonjob&quot;, req.NamespacedName)\r\n        r.Log.Info(&quot;Reconciling DaemonJob&quot;, &quot;request name&quot;, req.Name, &quot;request namespace&quot;, req.Namespace)\r\n        instance := &amp;djv1.DaemonJob{}\r\n        instanceType := &quot;daemonjob&quot;\r\n        ctx := context.TODO()\r\n    \r\n        if err := r.Client.Get(ctx, req.NamespacedName, instance); err != nil {\r\n            if errors.IsNotFound(err) {\r\n                return reconcile.Result{}, nil\r\n            }\r\n            return reconcile.Result{}, err\r\n        }\r\n    \r\n        if !instance.GetDeletionTimestamp().IsZero() {\r\n            return reconcile.Result{}, nil\r\n        }\r\n</pre>\n<p>Next, a selector needs to be prepared to filter nodes based on the selector passed in DaemonJob manifest and list all nodes that apply to these constraints. The length of the gathered nodes will be the number of desired completions of the final Job.</p> <pre class=\"brush: plain; title: ; notranslate\">\r\n    nodeSelector := client.MatchingLabels{}\r\n    if instance.Spec.Template.Spec.NodeSelector != nil {\r\n        nodeSelector = instance.Spec.Template.Spec.NodeSelector\r\n    }\r\n    var nodesListOptions client.MatchingLabels = nodeSelector\r\n    var nodes corev1.NodeList\r\n    if err := r.Client.List(ctx, &amp;nodes, nodesListOptions); err != nil &amp;&amp; errors.IsNotFound(err) {\r\n        return reconcile.Result{}, nil\r\n    }\r\n    jobReplicas := int32(len(nodes.Items))\r\n</pre>\n<p>With all necessary values for a Job resource, the structure desired can be filled out:</p> <pre class=\"brush: plain; title: ; notranslate\">\r\n    func getJob(instance *djv1.DaemonJob, replicas *int32, reqName, instanceType string) *batchv1.Job {\r\n        var jobAffinity = corev1.Affinity{\r\n            PodAntiAffinity: &amp;corev1.PodAntiAffinity{\r\n                RequiredDuringSchedulingIgnoredDuringExecution: []corev1.PodAffinityTerm{{\r\n                    LabelSelector: &amp;metav1.LabelSelector{\r\n                        MatchExpressions: []metav1.LabelSelectorRequirement{{\r\n                            Key:      instanceType,\r\n                            Operator: &quot;In&quot;,\r\n                            Values:   []string{reqName},\r\n                        }},\r\n                    },\r\n                    TopologyKey: &quot;kubernetes.io/hostname&quot;,\r\n                }},\r\n            },\r\n        }\r\n    \r\n        var podSpec = instance.Spec.Template\r\n        podSpec.Spec.Affinity = &amp;jobAffinity\r\n    \r\n        if podSpec.Spec.RestartPolicy == &quot;Always&quot; {\r\n            podSpec.Spec.RestartPolicy = &quot;OnFailure&quot;\r\n        }\r\n    \r\n        return &amp;batchv1.Job{\r\n            TypeMeta: metav1.TypeMeta{\r\n                Kind:       &quot;Job&quot;,\r\n                APIVersion: &quot;batch/v1&quot;,\r\n            },\r\n            ObjectMeta: metav1.ObjectMeta{\r\n                Name:      instance.Name + &quot;-job&quot;,\r\n                Namespace: instance.Namespace,\r\n                Labels:    instance.Labels,\r\n            },\r\n            Spec: batchv1.JobSpec{\r\n                Parallelism:             replicas,\r\n                Completions:             replicas,\r\n                Selector:                instance.Spec.Selector,\r\n                Template:                podSpec,\r\n                ManualSelector:          instance.Spec.ManualSelector,\r\n                TTLSecondsAfterFinished: instance.Spec.TTLSecondsAfterFinished,\r\n                BackoffLimit:            instance.Spec.BackoffLimit,\r\n                ActiveDeadlineSeconds:   instance.Spec.ActiveDeadlineSeconds,\r\n            },\r\n        }\r\n    }\r\n</pre>\n<p>Here, aside from copying the same values as were passed in the manifest to fields that do not vary, it is important to specify three elements. The first is the anti-affinity. The above code specifies that a pod created from a Job cannot run on the same node another Pod is running on. To achieve this affinity, the mechanism uses the built-in node label kubernetes.io/hostname which contains the hostname of a node and ensures that every pod created from a Job resource will have a unique hostname value. This results in running only one pod on a single node.</p>\n<p>Apart from setting anti-affinity, the function writes a number of nodes in the cluster to schedule Job pods into the <i>Completions</i> and <i>Parallelism</i> fields. Other fields are filled based on values the user passes on to the DaemonJob manifest.</p> <h2><b>Final steps</b></h2>\n<p>In the last step, it is vital to create a Job in the cluster or update of an already existing Job. Usually, this can be handled with a simple CreateOrUpdate method from <span>sigs.k8s.io/controller-runtime</span> package. CreateOrUpdate method ensures that a specified resource exists on the cluster and, if it does, that it has specified field values. However, Job resource does not allow you to update the Completions or Parallelism fields on existing resources. So here&#x2019;s a little trick that will allow you to edit these parameters.</p>\n<pre class=\"brush: plain; title: ; notranslate\">\r\nvar clusterJob batchv1.Job\r\nclusterJob.ObjectMeta = job.ObjectMeta\r\n_, err = ctrl.CreateOrUpdate(ctx, r, &amp;clusterJob, func() error {\r\n    modifyJob(job, &amp;clusterJob)\r\n    return controllerutil.SetControllerReference(instance, &amp;clusterJob, r.Scheme)\r\n})\r\nif err != nil {\r\n    if errors.IsInvalid(err) {\r\n        _ = r.Client.Delete(ctx, &amp;batchv1.Job{ObjectMeta: metav1.ObjectMeta{Name: job.Name, Namespace: job.Namespace}}, client.PropagationPolicy(&quot;Background&quot;))\r\n        return reconcile.Result{RequeueAfter: 5}, nil\r\n    }\r\n    return reconcile.Result{}, err\r\n}</pre>\n<p>When the CreateOrUpdate function fails with an error IsInvalid (which indicates that an update on an existing resource could not be completed), the resource is deleted and the Reconcile loop rescheduled after five seconds.</p> <h2><b>Summary</b></h2>\n<p>As you can see, this fully working resource was created with Kubernetes operator. It can now be applied to the cluster and used together with other standard Kubernetes resources to develop advanced projects on the Kubernetes platform. I hope that my blog post will help you do just that. The full code can be found in <a href=\"https://github.com/Dysproz/DaemonJob\">my GitHub repository</a>.</p> <p><a href=\"https://codilime.com/kubernetes/\"><img class=\"alignnone size-full wp-image-60493\" src=\"https://codilime.com/wp-content/uploads/2020/11/codilime_services_banner_kubernetes.png\" alt=\"Kubernetes services CodiLime\" width=\"1500\" srcset=\"https://codilime.com/wp-content/uploads/2020/11/codilime_services_banner_kubernetes.png 1500w, https://codilime.com/wp-content/uploads/2020/11/codilime_services_banner_kubernetes-300x70.png 300w, https://codilime.com/wp-content/uploads/2020/11/codilime_services_banner_kubernetes-768x179.png 768w, https://codilime.com/wp-content/uploads/2020/11/codilime_services_banner_kubernetes-1024x239.png 1024w\" sizes=\"(max-width: 1500px) 100vw, 1500px\"></a></p> </article>",
      "contentAsText": " While developing projects on the Kubernetes platform I came across an interesting problem. I had quite a few scripts that ran in containers and needed to be triggered only once on every node in my Kubernetes cluster. This could not be solved using default Kubernetes resources such as DaemonSet and Job. So I decided to write my own resource using Kubernetes Operator Framework. How I went about it is the subject of this blog post.\nWhen I confronted this problem, my first thought was to use a DaemonSet resource that utilizes initContainers and then starts a dummy busybox container running tail -f /dev/null or another command that does nothing. However, after analyzing the question more thoroughly, I realized that it would be very problematic on a production setup with many such DaemonSets. Each dummy container eats up resources to execute a useless task. Moreover, from the perspective of the cluster administrator, such a DaemonSet seems exactly the same as other legitimate services. Should an error occur, it may be confusing and require a deep dive into running resources.\nMy second idea was to use a simple Job with a huge number of Completions and Parallelism. Using anti-affinity specification to ensure that only one Pod will run on a single node at a time, I would achieve a simulated effect of the DaemonSet\u0019s behavior. Though a new node would appear in the cluster, a new Pod would be scheduled.\nYet this idea, with a fixed number of Completions, seemed a bit amateurish. Even if I defined it to be 1000\u0014what if a 1001st node were to appear in the cluster? Further, from the perspective of the cluster administrator, such a Job seems like a defective resource that cannot finish all Completions. Creating a custom resource in Kubernetes\nBut if I could go a bit more low-level and create a resource that merges the functionality of the DaemonSet and the Job? I went online to find out whether someone has already created such a resource. But all I turned up were a few Issues on Kubernetes GitHub posted by people who\u0019d had the same problem. For example, in this issue the idea of adding such a DaemonJob to the official Kubernetes resources has been discussed for over two years, but a conclusion has remained elusive. Long story short, I would be forced to create such a resource by myself. I chose to use Operator Framework and code a resource that would be defined in a very similar style as a standard Kubernetes Job resource but would automatically schedule a pod on every node, � la DaemonSet. Before I go further, let\u0019s have a brief overview of the Kubernetes operator architecture. An overview of Kubernetes operator architecture\nA Kubernetes operator has two main components: Custom Resource Definition (CRD) and Operator Controller. Custom Resource Definition is a standard Kubernetes resource that allows users to extend Kubernetes API with new resources. However, this only enables users to apply manifests of newly created resources but will have no effect on the cluster. This is because Kubernetes architecture contains the Kubernetes controller which has all the logic behind every resource in the cluster. Since a user-defined resource has not been coded in the built-in Kubernetes controller, the user has to provide its controller separately. This is where the operator controller comes up, usually deployed as a separate pod on the cluster with code that contains logic on how to manage custom resources.\nThe controller has the main parts which are important for every operator developer. The first is the definition of what the controller watches. It defines changes in specific resources or events that will trigger the operator\u0019s control logic. For example, if a custom resource\u0019s work is based on information provided in ConfigMap, then the operator should watch changes of ConfigMap to trigger code on every change and adjust the resource to the new configuration. The other important part is the Reconcile loop. This is in fact a function that contains the whole logic behind the operator and is triggered by changes in the resources being watched. How to start working with Kubernetes Operator Framework\nDevelopment of such a project code template may seem a tremendous challenge for a developer. However, Operator Framework provides a clever CLI tool called operator-sdk, which will do most of the job. Whole code infrastructure may be generated with just two commands and then the developer just edits specific structures or functions to adjust the resource-specific logic.\nOperator Framework allows users to create multiple Kubernetes custom resources in the scope of a single operator. In such a case, every resource is managed by a single controller, which can be used to build more complicated projects. These can often have multiple components, with each deployed as a separate custom resource. In order to initialize an empty operator, you may just run simply operator-sdk init (docs) command with proper flags for our project and the whole basic architecture will be created.\nHaving initialized the operator, you can now add code for specific resources. For the problem described in this blogpost, I will create just one resource. The new resource can be added with the command operator-sdk create api (docs), which will generate API code for the specific resource described with passed parameters that are additional flags added after a CLI command that specify what should exactly happen after running this command. This means the developer will be able to specify only the structure of a custom resource, but will not have code generated for the controller part. To additionally generate controller code it\u0019s necessary to provide the flag –controller=True� when creating the API. Why does operator-sdk not do that by default? In more advanced projects that periodically release new versions of the product, the API may change and leave old manifests not working\u0014and all customers using the old manifests facing the need to migrate to the new version. However, because a new API can be created, both the old and new API of a resource are supported, and both versions can be managed with a single controller, at least until legacy support is no longer required.\nHaving a ready project structure for development of a new custom resource, you can start writing the code. All definitions of API should be written in files with names ending with _types.go located under api/<API version> directory. And all controllers code is located under controllers/ directory in files with names ending with _controller.go. These are the main two file types where developers should edit code. All other files are generated automatically. When a developer finishes editing code of the custom resource then all he has to do is run the make manifests command and the operator code will be ready to be installed on the cluster (docs). DeamonJob step-by-step\nGiven all this, I started implementing the idea of a DaemonJob. The API was the easy part\u0014just remove the Completions and Parallelism fields from the Job API and leave everything as is. The API code in the Kubernetes resources is written as Go structs and most of it looks the same (code). From the user\u0019s perspective, a sample manifest to be applied on the cluster would look like this:\napiVersion: dj.dysproz.io/v1\nkind: DaemonJob\n  metadata:\nname: daemonjob-sample\n  spec:\n    template:\n      spec:\n        containers:\n         - name: test-job\n           image: busybox\n           command:\n            - sleep\n            - \"20\"\n        nodeSelector:\n          app: v1\n        restartPolicy: OnFailure\n\nIn this example, on every node which has label app=v1, a busybox container will be created and will run a sleep command for 20 seconds. As you may be able to see, the above manifest is very similar to one users would create for a simple Kubernetes Job resource.\nHowever, the logic behind the controller for such a resource presented a more daunting challenge (code).\nIt required, first of all, that the resources to watch be defined. In this case, the operator would have to look just for changes on nodes which may seem fairly easy.� This brings me to the first problem. Kubernetes operator code is usually designed to create resources that are owned by a single custom resource and watch only these resources. For example, if a custom resource creates ConfigMap with configuration and watches for ConfigMap, then it should look only for changes on ConfigMaps that are owned by this custom resource.\nWhen a watched resource triggers the Reconcile loop of custom resources, an argument request is passed containing two fields: custom resource name and namespace. Although nodes in the Kubernetes cluster are represented as regular resources, there is no built-in function to watch specific resources without ownership of a custom resource. As a result, the Reconcile loop would be triggered with the request name pointing to a node that triggered action rather than to a specific custom resource.\nTo keep up with the original design of developing operators, it is necessary to build a custom method that will trigger the Reconcile loop with a proper request name for every DaemonJob resource when any node changes its state. This can be achieved with the following code:\n    func (r *DaemonJobReconciler) SetupWithManager(mgr ctrl.Manager) error {\n        if err := ctrl.NewControllerManagedBy(mgr).\n            For(&djv1.DaemonJob{}).\n            Complete(r); err != nil {\n            return err\n        }\n    \n        if err := ctrl.NewControllerManagedBy(mgr).\n            For(&corev1.Node{}).\n            Watches(&source.Kind{Type: &corev1.Node{}}, &handler.EnqueueRequestsFromMapFunc{\n                ToRequests: handler.ToRequestsFunc(func(nodeObject handler.MapObject) []reconcile.Request {\n                    var djObjects djv1.DaemonJobList\n                    _ = mgr.GetClient().List(context.TODO(), &djObjects)\n                    var requests = []reconcile.Request{}\n                    for _, djObject := range djObjects.Items {\n                        requests = append(requests, reconcile.Request{\n                            NamespacedName: types.NamespacedName{\n                                Name:      djObject.Name,\n                                Namespace: djObject.Namespace,\n                            },\n                        })\n                    }\n                    return requests\n                }),\n            }).\n            Complete(r); err != nil {\n            return err\n        }\n    \n        return nil\n    }\n\nThe first operation adds a watch for the custom resource itself. This means that a Reconcile loop will be triggered whenever a new manifest with this resource is applied on the cluster or edited by the user during runtime.\nThe second operation defines a custom watch for changes in node resources. If any node changes its state, an anonymous function is triggered. The anonymous function lists all DaemonJob resources that exist in the cluster and then prepares requests for every resource found. As a result, node watch will simultaneously trigger a set of requests to reconcile DaemonJob resources and update their state.\nOnce the logic behind the proper triggers of the Reconcile loop is handled, it is time to concentrate on the Reconcile loop itself.\nThe Controller logic is based on two steps. The first is to define how many nodes are currently in the cluster and which nodes should run the Job (which may be defined with, for example, a nodeSelector field in the manifest). The second step is to prepare a Job resource that will have defined that number as the number of Completions.\nStarting the Reconcile loop by setting some basic variables, it has to gather the filled structure of the DaemonJob resource applied in the cluster to be used to fill out the Job resource. In this way, you create a data structure to which you can apply the logic embedded in the code.\n    func (r *DaemonJobReconciler) Reconcile(req ctrl.Request) (ctrl.Result, error) {\n        _ = context.Background()\n        _ = r.Log.WithValues(\"daemonjob\", req.NamespacedName)\n        r.Log.Info(\"Reconciling DaemonJob\", \"request name\", req.Name, \"request namespace\", req.Namespace)\n        instance := &djv1.DaemonJob{}\n        instanceType := \"daemonjob\"\n        ctx := context.TODO()\n    \n        if err := r.Client.Get(ctx, req.NamespacedName, instance); err != nil {\n            if errors.IsNotFound(err) {\n                return reconcile.Result{}, nil\n            }\n            return reconcile.Result{}, err\n        }\n    \n        if !instance.GetDeletionTimestamp().IsZero() {\n            return reconcile.Result{}, nil\n        }\n\nNext, a selector needs to be prepared to filter nodes based on the selector passed in DaemonJob manifest and list all nodes that apply to these constraints. The length of the gathered nodes will be the number of desired completions of the final Job.     nodeSelector := client.MatchingLabels{}\n    if instance.Spec.Template.Spec.NodeSelector != nil {\n        nodeSelector = instance.Spec.Template.Spec.NodeSelector\n    }\n    var nodesListOptions client.MatchingLabels = nodeSelector\n    var nodes corev1.NodeList\n    if err := r.Client.List(ctx, &nodes, nodesListOptions); err != nil && errors.IsNotFound(err) {\n        return reconcile.Result{}, nil\n    }\n    jobReplicas := int32(len(nodes.Items))\n\nWith all necessary values for a Job resource, the structure desired can be filled out:     func getJob(instance *djv1.DaemonJob, replicas *int32, reqName, instanceType string) *batchv1.Job {\n        var jobAffinity = corev1.Affinity{\n            PodAntiAffinity: &corev1.PodAntiAffinity{\n                RequiredDuringSchedulingIgnoredDuringExecution: []corev1.PodAffinityTerm{{\n                    LabelSelector: &metav1.LabelSelector{\n                        MatchExpressions: []metav1.LabelSelectorRequirement{{\n                            Key:      instanceType,\n                            Operator: \"In\",\n                            Values:   []string{reqName},\n                        }},\n                    },\n                    TopologyKey: \"kubernetes.io/hostname\",\n                }},\n            },\n        }\n    \n        var podSpec = instance.Spec.Template\n        podSpec.Spec.Affinity = &jobAffinity\n    \n        if podSpec.Spec.RestartPolicy == \"Always\" {\n            podSpec.Spec.RestartPolicy = \"OnFailure\"\n        }\n    \n        return &batchv1.Job{\n            TypeMeta: metav1.TypeMeta{\n                Kind:       \"Job\",\n                APIVersion: \"batch/v1\",\n            },\n            ObjectMeta: metav1.ObjectMeta{\n                Name:      instance.Name + \"-job\",\n                Namespace: instance.Namespace,\n                Labels:    instance.Labels,\n            },\n            Spec: batchv1.JobSpec{\n                Parallelism:             replicas,\n                Completions:             replicas,\n                Selector:                instance.Spec.Selector,\n                Template:                podSpec,\n                ManualSelector:          instance.Spec.ManualSelector,\n                TTLSecondsAfterFinished: instance.Spec.TTLSecondsAfterFinished,\n                BackoffLimit:            instance.Spec.BackoffLimit,\n                ActiveDeadlineSeconds:   instance.Spec.ActiveDeadlineSeconds,\n            },\n        }\n    }\n\nHere, aside from copying the same values as were passed in the manifest to fields that do not vary, it is important to specify three elements. The first is the anti-affinity. The above code specifies that a pod created from a Job cannot run on the same node another Pod is running on. To achieve this affinity, the mechanism uses the built-in node label kubernetes.io/hostname which contains the hostname of a node and ensures that every pod created from a Job resource will have a unique hostname value. This results in running only one pod on a single node.\nApart from setting anti-affinity, the function writes a number of nodes in the cluster to schedule Job pods into the Completions and Parallelism fields. Other fields are filled based on values the user passes on to the DaemonJob manifest. Final steps\nIn the last step, it is vital to create a Job in the cluster or update of an already existing Job. Usually, this can be handled with a simple CreateOrUpdate method from sigs.k8s.io/controller-runtime package. CreateOrUpdate method ensures that a specified resource exists on the cluster and, if it does, that it has specified field values. However, Job resource does not allow you to update the Completions or Parallelism fields on existing resources. So here’s a little trick that will allow you to edit these parameters.\nvar clusterJob batchv1.Job\nclusterJob.ObjectMeta = job.ObjectMeta\n_, err = ctrl.CreateOrUpdate(ctx, r, &clusterJob, func() error {\n    modifyJob(job, &clusterJob)\n    return controllerutil.SetControllerReference(instance, &clusterJob, r.Scheme)\n})\nif err != nil {\n    if errors.IsInvalid(err) {\n        _ = r.Client.Delete(ctx, &batchv1.Job{ObjectMeta: metav1.ObjectMeta{Name: job.Name, Namespace: job.Namespace}}, client.PropagationPolicy(\"Background\"))\n        return reconcile.Result{RequeueAfter: 5}, nil\n    }\n    return reconcile.Result{}, err\n}\nWhen the CreateOrUpdate function fails with an error IsInvalid (which indicates that an update on an existing resource could not be completed), the resource is deleted and the Reconcile loop rescheduled after five seconds. Summary\nAs you can see, this fully working resource was created with Kubernetes operator. It can now be applied to the cluster and used together with other standard Kubernetes resources to develop advanced projects on the Kubernetes platform. I hope that my blog post will help you do just that. The full code can be found in my GitHub repository.  ",
      "publishedDate": "2020-11-12T13:50:00.000Z",
      "description": "Running many scripts on every Kubernetes node can be tricky. Read on to see how to do it using Kubernetes Operator to create a custom resource.",
      "ogDescription": "Running many scripts on every Kubernetes node can be tricky. Read on to see how to do it using Kubernetes Operator to create a custom resource."
    },
    {
      "url": "https://github.com/ricardomaraschini/tagger",
      "title": "ricardomaraschini/tagger",
      "content": "<div><div><article class=\"markdown-body entry-content container-lg\"><p><a href=\"https://github.com/ricardomaraschini/tagger/blob/main/assets/tagger.png\"><img src=\"https://github.com/ricardomaraschini/tagger/raw/main/assets/tagger.png\" alt=\"tagger logo\"></a></p>\n<p>Tagger keeps references to externally hosted Docker images internally in a Kubernetes cluster\nby mapping their <code>tags</code> (such as <code>latest</code>) into their references by <code>hash</code>. Allow Kubernetes\nadministrators to host these images internally if needed.</p>\n<h3><a id=\"user-content-concepts\" class=\"anchor\" href=\"https://github.com/ricardomaraschini/tagger#concepts\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Concepts</h3>\n<p>Images in remote repositories are tagged using a string (e.g. <code>latest</code>), these tags are not\npermanent, i.e. the repository owner can push a new version for the tag. Luckily we can also\nrefer to an image by its hash (generally sha256) so one can either pull an image by its tag\nor by its hash.</p>\n<p>Tagger takes advantage of this repository feature and creates references to image tags using\ntheir &quot;fixed point in time&quot; hashes. For instance an image <code>centos:latest</code> can be pulled by\nits hash as well, such as <code>centos@sha256:012345...</code>.</p>\n<p>Every time one tags an image <code>tagger</code> creates a new generation for that image, making it easier\nto downgrade to previously tagged versions in case of issues with the new generation.</p>\n<h3><a id=\"user-content-caching-images-locally\" class=\"anchor\" href=\"https://github.com/ricardomaraschini/tagger#caching-images-locally\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Caching images locally</h3>\n<p>Tagger allow administrators to tag and cache images locally within the cluster. You just need\nto have a image registry running internal to the cluster and ask <code>tagger</code> to cache the image.\nBy doing so a copy of the remote image is going to be made into the internal registry and all\ndeployments leveraging such image will automatically pass to use the cached copy.</p>\n<h3><a id=\"user-content-webhooks-from-quayio-and-docker-hub\" class=\"anchor\" href=\"https://github.com/ricardomaraschini/tagger#webhooks-from-quayio-and-docker-hub\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Webhooks from quay.io and Docker hub</h3>\n<p>After the <code>tagger</code> deployment two internal services will be created, one for Quay and one for\nDocker. These services can then be exposed externally if you want to accept webhooks coming in\nfrom either quay.io or docker.io. Support for these webhooks is still under development but it\nshould work for most of the use cases.</p>\n<h3><a id=\"user-content-use\" class=\"anchor\" href=\"https://github.com/ricardomaraschini/tagger#use\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Use</h3>\n<p>A brief hands on presentation</p>\n<p><a href=\"https://asciinema.org/a/372131\"><img src=\"https://asciinema.org/a/372131.png\" alt=\"asciicast\"></a></p>\n<h3><a id=\"user-content-disclaimer\" class=\"anchor\" href=\"https://github.com/ricardomaraschini/tagger#disclaimer\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Disclaimer</h3>\n<p>The private key present on this project does not represent a problem, it is not being used\nanywhere yet and to keep keys in here makes <em>things</em> easier (specially at this stage).</p>\n<h3><a id=\"user-content-deploying\" class=\"anchor\" href=\"https://github.com/ricardomaraschini/tagger#deploying\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Deploying</h3>\n<p>The deployment of this operator is a little bit cumbersome, as I move forward this process will\nget simpler. For now you gonna need to follow the procedure below.</p>\n<pre><code>$ # you should customize certs and keys in use. please remember to feed\n$ # manifests/02_secret.yaml and manifests/04_webhook.yaml with the keys\n$ kubectl create namespace tagger\n$ kubectl create -f ./manifests/00_crd.yaml\n$ kubectl create -f ./manifests/01_rbac.yaml\n$ kubectl create -f ./manifests/02_secret.yaml\n$ kubectl create -f ./manifests/03_deploy.yaml\n$ kubectl create -f ./manifests/04_webhook.yaml\n</code></pre>\n</article></div></div>",
      "contentAsText": "\nTagger keeps references to externally hosted Docker images internally in a Kubernetes cluster\nby mapping their tags (such as latest) into their references by hash. Allow Kubernetes\nadministrators to host these images internally if needed.\nConcepts\nImages in remote repositories are tagged using a string (e.g. latest), these tags are not\npermanent, i.e. the repository owner can push a new version for the tag. Luckily we can also\nrefer to an image by its hash (generally sha256) so one can either pull an image by its tag\nor by its hash.\nTagger takes advantage of this repository feature and creates references to image tags using\ntheir \"fixed point in time\" hashes. For instance an image centos:latest can be pulled by\nits hash as well, such as centos@sha256:012345....\nEvery time one tags an image tagger creates a new generation for that image, making it easier\nto downgrade to previously tagged versions in case of issues with the new generation.\nCaching images locally\nTagger allow administrators to tag and cache images locally within the cluster. You just need\nto have a image registry running internal to the cluster and ask tagger to cache the image.\nBy doing so a copy of the remote image is going to be made into the internal registry and all\ndeployments leveraging such image will automatically pass to use the cached copy.\nWebhooks from quay.io and Docker hub\nAfter the tagger deployment two internal services will be created, one for Quay and one for\nDocker. These services can then be exposed externally if you want to accept webhooks coming in\nfrom either quay.io or docker.io. Support for these webhooks is still under development but it\nshould work for most of the use cases.\nUse\nA brief hands on presentation\n\nDisclaimer\nThe private key present on this project does not represent a problem, it is not being used\nanywhere yet and to keep keys in here makes things easier (specially at this stage).\nDeploying\nThe deployment of this operator is a little bit cumbersome, as I move forward this process will\nget simpler. For now you gonna need to follow the procedure below.\n$ # you should customize certs and keys in use. please remember to feed\n$ # manifests/02_secret.yaml and manifests/04_webhook.yaml with the keys\n$ kubectl create namespace tagger\n$ kubectl create -f ./manifests/00_crd.yaml\n$ kubectl create -f ./manifests/01_rbac.yaml\n$ kubectl create -f ./manifests/02_secret.yaml\n$ kubectl create -f ./manifests/03_deploy.yaml\n$ kubectl create -f ./manifests/04_webhook.yaml\n\n",
      "description": "ImageTag. Contribute to ricardomaraschini/tagger development by creating an account on GitHub.",
      "ogDescription": "ImageTag. Contribute to ricardomaraschini/tagger development by creating an account on GitHub."
    },
    {
      "url": "https://medium.com/@clglavan/delete-k8s-resources-in-a-namespace-based-on-a-prefix-f1a72a540857",
      "title": "Delete k8s resources in a namespace, based on a prefix",
      "content": "<div><article><section class=\"ea eb ec ed w ee bm s\"></section><div><section class=\"db ek el cw em\"><div class=\"n p\"><div class=\"ag ah ai aj ak en am w\"><figure class=\"gn go ec ed paragraph-image\"><img alt=\"Image for post\" class=\"ef dv dr gu w\" src=\"https://miro.medium.com/max/6978/0*vyKVePyPe_Tkmz0Q\" width=\"3489\" srcset=\"https://miro.medium.com/max/552/0*vyKVePyPe_Tkmz0Q 276w, https://miro.medium.com/max/1104/0*vyKVePyPe_Tkmz0Q 552w, https://miro.medium.com/max/1280/0*vyKVePyPe_Tkmz0Q 640w, https://miro.medium.com/max/1400/0*vyKVePyPe_Tkmz0Q 700w\" sizes=\"700px\"><figcaption class=\"hf hg ee ec ed hh hi as b at au bq\">Photo by <a href=\"https://unsplash.com/@cjred?utm_source=medium&amp;utm_medium=referral\" class=\"dg hj\">CJ Dayrit</a> on <a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\" class=\"dg hj\">Unsplash</a></figcaption></figure><p id=\"c03c\" class=\"hk hl ep hm b hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih db fl\">If you need to cleanup your namespace resources based on a prefix and don\u0019t want to do it manually, then this script I wrote is for you.</p><p id=\"e432\" class=\"hk hl ep hm b hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih db fl\">Note that this assumes you use a naming convention for your resources.</p><figure class=\"ii ij ik il im go\"></figure><p id=\"df5e\" class=\"hk hl ep hm b hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih db fl\">This will iterate through resources, in the right order ( as tested so far&amp; ) to delete everything starting with that prefix.</p></div></div></section></div></article></div>",
      "contentAsText": "Photo by CJ Dayrit on UnsplashIf you need to cleanup your namespace resources based on a prefix and don\u0019t want to do it manually, then this script I wrote is for you.Note that this assumes you use a naming convention for your resources.This will iterate through resources, in the right order ( as tested so far& ) to delete everything starting with that prefix.",
      "publishedDate": "2020-10-11T10:25:25.869Z",
      "description": "If you need to cleanup your namespace resources based on a prefix and don’t want to do it manually, then this script I wrote is for you. This will iterate through resources, in the right order ( as…",
      "ogDescription": "If you need to cleanup your namespace resources based on a prefix and don’t want to do it manually, then this script I wrote is for you."
    },
    {
      "url": "https://sfxworks.net/posts/intro-to-my-cluster/",
      "title": "Intro to a k8s homelab",
      "content": "<article class=\"content\"> <p><img src=\"imgs/homelab-grafana.PNG\" alt=\"homelab-grafana\"></p>\n<pre><code>kubectl get nodes -o wide\nNAME            STATUS   ROLES    AGE     VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME\nhome-rpi-1      Ready    &lt;none&gt;   6h22m   v1.19.2   192.168.1.74    &lt;none&gt;        Ubuntu 20.04.1 LTS   5.4.0-1019-raspi   containerd://1.3.3-0ubuntu2\nhome-rpi-2      Ready    &lt;none&gt;   6h22m   v1.19.2   192.168.1.209   &lt;none&gt;        Ubuntu 20.04.1 LTS   5.4.0-1019-raspi   containerd://1.3.3-0ubuntu2\nhome-rpi-3      Ready    &lt;none&gt;   6h17m   v1.19.2   192.168.1.194   &lt;none&gt;        Ubuntu 20.04.1 LTS   5.4.0-1019-raspi   containerd://1.3.3-0ubuntu2\nhome-rpi-4      Ready    &lt;none&gt;   6h11m   v1.19.2   192.168.1.145   &lt;none&gt;        Ubuntu 20.04.1 LTS   5.4.0-1019-raspi   containerd://1.3.3-0ubuntu2\nhome-server-1   Ready    master   3d12h   v1.19.2   192.168.1.111   &lt;none&gt;        Ubuntu 20.04.1 LTS   5.4.0-48-generic   cri-o://1.19.0\nhome-server-2   Ready    &lt;none&gt;   9h      v1.19.2   192.168.1.140   &lt;none&gt;        Ubuntu 20.04.1 LTS   5.4.0-48-generic   cri-o://1.19.0\n</code></pre><p>Homelabs are cool. Happy I made one too. Let&#x2019;s me test things against non-prod clusters like yolo rook configurations and advanced networking with multus. Though this is about building one for now.</p>\n<p>A few things are used to make this all work.</p>\n<table>\n<thead>\n<tr>\n<th>Thing</th>\n<th>Resource</th>\n<th>Prerequisites</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Baremetal Servers</td>\n<td>You can get these many places</td>\n<td>Put an <a href=\"https://ubuntu.com/download/server\">OS</a> on a USB and install it from boot (Or go PXE if you&#x2019;re feeling fancy)</td>\n</tr>\n<tr>\n<td>Deployer</td>\n<td><a href=\"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl\">kubeadm</a></td>\n<td>Instructions followed based on your distro</td>\n</tr>\n<tr>\n<td>Container Runtime Interface (CRI)</td>\n<td><a href=\"https://containerd.io/downloads/\">Containerd</a> or <a href=\"https://cri-o.io/#distribution-packaging\">CRI-O</a> recommended</td>\n<td>Same as above</td>\n</tr>\n<tr>\n<td>Container Network Interface (CNI)</td>\n<td><a href=\"https://www.kube-router.io/\">Kube Router</a></td>\n<td>All devices on same network</td>\n</tr>\n<tr>\n<td>Storage</td>\n<td><a href=\"https://rook.io/docs/rook/v1.4/ceph-storage.html\">Rook Ceph</a></td>\n<td>Raw Devices w/ No Partitions <a href=\"https://rook.io/docs/rook/v1.4/ceph-prerequisites.html\">More Info</a></td>\n</tr>\n<tr>\n<td>Load Balancing</td>\n<td><a href=\"https://metallb.universe.tf/usage/\">Metallb</a></td>\n<td>Layer 2 more w/ a range of IPs reserved for Load Balancer service type</td>\n</tr>\n<tr>\n<td>Monitoring</td>\n<td><a href=\"https://github.com/prometheus-community/helm-charts\">Prometheus Operator</a></td>\n<td>Above installed</td>\n</tr>\n<tr>\n<td>Git Ops</td>\n<td><a href=\"https://docs.fluxcd.io/en/1.21.0/tutorials/get-started-helm/\">FluxCD</a></td>\n<td>A git repo accessible by cluster</td>\n</tr>\n</tbody>\n</table>\n<p>Following the docs in the above repo make for a great home cluster. You can even go as far as to run Virtual Machines using <a href=\"http://kubevirt.io/\">kubevirt</a> once you have it all running.</p>\n<p>Now, this isn&#x2019;t really a step-by-step how-to guide. More of a reference. Though you are free to get started with a quick <a href=\"https://gist.github.com/sfxworks/d41f9878248ee66ab99df540c8c64366\">gist</a> for Ubuntu 20 (likely for all DEB based systems, though untested) to prep your nodes and <a href=\"https://github.com/sfxworks/home-cluster\">my git-ops repo</a> that includes the above. You&#x2019;ll need to fork it though and apply your own configurations based on your needs.</p> <p>If you&#x2019;re going to run a cluster of Raspberry Pi&#x2019;s or mixed cluster of Raspberry Pi&#x2019;s and normal servers (ARM/AMD64 mix) you&#x2019;ll want to do a few things.</p>\n<ol>\n<li>Ensure you have <code>cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1</code> added to <code>/boot/firmware/cmdline.txt</code> or similar for your distro. This enables cgroup features for containers on those nodes.</li>\n<li>Running a mix with Rook Ceph <a href=\"https://github.com/rook/rook/issues/4051\">isn&#x2019;t possible at this time</a> unless you use <a href=\"https://github.com/rook/rook/issues/4051#issuecomment-623172463\">a differerent set of images</a>.\nCheck out the raspbernetes multiarch repo for more info on that <a href=\"https://github.com/raspbernetes/multi-arch-images\">here</a></li>\n</ol>\n<p>I hope this serves as a starting point for your adventures!</p>\n<p><img src=\"imgs/homelab-ceph.PNG\" alt=\"homelab-ceph\"></p> </article>",
      "contentAsText": " \nkubectl get nodes -o wide\nNAME            STATUS   ROLES    AGE     VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME\nhome-rpi-1      Ready    <none>   6h22m   v1.19.2   192.168.1.74    <none>        Ubuntu 20.04.1 LTS   5.4.0-1019-raspi   containerd://1.3.3-0ubuntu2\nhome-rpi-2      Ready    <none>   6h22m   v1.19.2   192.168.1.209   <none>        Ubuntu 20.04.1 LTS   5.4.0-1019-raspi   containerd://1.3.3-0ubuntu2\nhome-rpi-3      Ready    <none>   6h17m   v1.19.2   192.168.1.194   <none>        Ubuntu 20.04.1 LTS   5.4.0-1019-raspi   containerd://1.3.3-0ubuntu2\nhome-rpi-4      Ready    <none>   6h11m   v1.19.2   192.168.1.145   <none>        Ubuntu 20.04.1 LTS   5.4.0-1019-raspi   containerd://1.3.3-0ubuntu2\nhome-server-1   Ready    master   3d12h   v1.19.2   192.168.1.111   <none>        Ubuntu 20.04.1 LTS   5.4.0-48-generic   cri-o://1.19.0\nhome-server-2   Ready    <none>   9h      v1.19.2   192.168.1.140   <none>        Ubuntu 20.04.1 LTS   5.4.0-48-generic   cri-o://1.19.0\nHomelabs are cool. Happy I made one too. Let’s me test things against non-prod clusters like yolo rook configurations and advanced networking with multus. Though this is about building one for now.\nA few things are used to make this all work.\n\n\n\nThing\nResource\nPrerequisites\n\n\n\n\nBaremetal Servers\nYou can get these many places\nPut an OS on a USB and install it from boot (Or go PXE if you’re feeling fancy)\n\n\nDeployer\nkubeadm\nInstructions followed based on your distro\n\n\nContainer Runtime Interface (CRI)\nContainerd or CRI-O recommended\nSame as above\n\n\nContainer Network Interface (CNI)\nKube Router\nAll devices on same network\n\n\nStorage\nRook Ceph\nRaw Devices w/ No Partitions More Info\n\n\nLoad Balancing\nMetallb\nLayer 2 more w/ a range of IPs reserved for Load Balancer service type\n\n\nMonitoring\nPrometheus Operator\nAbove installed\n\n\nGit Ops\nFluxCD\nA git repo accessible by cluster\n\n\n\nFollowing the docs in the above repo make for a great home cluster. You can even go as far as to run Virtual Machines using kubevirt once you have it all running.\nNow, this isn’t really a step-by-step how-to guide. More of a reference. Though you are free to get started with a quick gist for Ubuntu 20 (likely for all DEB based systems, though untested) to prep your nodes and my git-ops repo that includes the above. You’ll need to fork it though and apply your own configurations based on your needs. If you’re going to run a cluster of Raspberry Pi’s or mixed cluster of Raspberry Pi’s and normal servers (ARM/AMD64 mix) you’ll want to do a few things.\n\nEnsure you have cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1 added to /boot/firmware/cmdline.txt or similar for your distro. This enables cgroup features for containers on those nodes.\nRunning a mix with Rook Ceph isn’t possible at this time unless you use a differerent set of images.\nCheck out the raspbernetes multiarch repo for more info on that here\n\nI hope this serves as a starting point for your adventures!\n ",
      "description": "",
      "ogDescription": ""
    },
    {
      "url": "https://github.com/adaptivecorporation/multitenant-nginx-ingress-k8s",
      "title": "adaptivecorporation/multitenant-nginx-ingress-k8s",
      "content": "<div><div><article class=\"markdown-body entry-content container-lg\">\n<p>Installation on bare-metal systems</p>\n<p>Step 1. Install the NGINX controller</p>\n<div class=\"highlight highlight-source-shell\"><pre>kubectl apply -f nginx-controller.yml</pre></div>\n<p>Step 2. Apply the ingress rules</p>\n<div class=\"highlight highlight-source-shell\"><pre>kubectl apply -f ingress.yml</pre></div>\n<p>Step 3. Configure firewall / routing rules and open port 80 (and 443) and point it to the speicified node port. You can find the node ports with this command:</p>\n<div class=\"highlight highlight-source-shell\"><pre>kubectl get svc -n ingress-nginx</pre></div>\n<p>To find the node that the ingress service is running on, use this command:</p>\n<div class=\"highlight highlight-source-shell\"><pre>kubectl get pods -n ingress-nginx -o wide </pre></div>\n</article></div></div><hr><h4>Page 2</h4><div><div><article class=\"markdown-body entry-content container-lg\">\n<p>Installation on bare-metal systems</p>\n<p>Step 1. Install the NGINX controller</p>\n<div class=\"highlight highlight-source-shell\"><pre>kubectl apply -f nginx-controller.yml</pre></div>\n<p>Step 2. Apply the ingress rules</p>\n<div class=\"highlight highlight-source-shell\"><pre>kubectl apply -f ingress.yml</pre></div>\n<p>Step 3. Configure firewall / routing rules and open port 80 (and 443) and point it to the speicified node port. You can find the node ports with this command:</p>\n<div class=\"highlight highlight-source-shell\"><pre>kubectl get svc -n ingress-nginx</pre></div>\n<p>To find the node that the ingress service is running on, use this command:</p>\n<div class=\"highlight highlight-source-shell\"><pre>kubectl get pods -n ingress-nginx -o wide </pre></div>\n</article></div></div>",
      "contentAsText": "\nInstallation on bare-metal systems\nStep 1. Install the NGINX controller\nkubectl apply -f nginx-controller.yml\nStep 2. Apply the ingress rules\nkubectl apply -f ingress.yml\nStep 3. Configure firewall / routing rules and open port 80 (and 443) and point it to the speicified node port. You can find the node ports with this command:\nkubectl get svc -n ingress-nginx\nTo find the node that the ingress service is running on, use this command:\nkubectl get pods -n ingress-nginx -o wide \nPage 2\nInstallation on bare-metal systems\nStep 1. Install the NGINX controller\nkubectl apply -f nginx-controller.yml\nStep 2. Apply the ingress rules\nkubectl apply -f ingress.yml\nStep 3. Configure firewall / routing rules and open port 80 (and 443) and point it to the speicified node port. You can find the node ports with this command:\nkubectl get svc -n ingress-nginx\nTo find the node that the ingress service is running on, use this command:\nkubectl get pods -n ingress-nginx -o wide \n",
      "description": "K8s manifest files for an nginx ingress controller for use on a bare-metal system. - adaptivecorporation/multitenant-nginx-ingress-k8s",
      "ogDescription": "K8s manifest files for an nginx ingress controller for use on a bare-metal system. - adaptivecorporation/multitenant-nginx-ingress-k8s"
    },
    {
      "url": "https://www.reddit.com/r/kubernetes/comments/j89l2p/kubernetes_replication_and_selfhealing/",
      "title": "r/kubernetes - Kubernetes - Replication and self-healing",
      "content": "<div><div><p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">To start with, I would like to explain what does self-healing means in terms of Kubernetes. Self-healing is a fantastic feature of Kubernetes to recover from service or node failure automatically. In the following article, we will consider the benefit of using replication for your micro-services and how the Kubernetes cluster can automatically recover from a service failure.</p><p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">One of the great features of Kubernetes is the ability to replicate pods and their underlying containers across the cluster. So, before we set up our self-healing feature please make sure you have managed replication and here is a simple example of a deployment file that will deploy <code class=\"_34q3PgLsx9zIU5BiSOjFoM\">nginx</code> container with replication factor <code class=\"_34q3PgLsx9zIU5BiSOjFoM\">3</code>:</p><p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">All right, so let&apos;s create the deployment:</p><p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">Now, let&apos;s check whether our <code class=\"_34q3PgLsx9zIU5BiSOjFoM\">nginx-deployment-example</code> was created:</p><p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">You should see your <code class=\"_34q3PgLsx9zIU5BiSOjFoM\">nginx-deployment-example</code> deployment in the default namespace. If we want to see more details about those pods, please run the following command:</p><p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">We will see our 3 <code class=\"_34q3PgLsx9zIU5BiSOjFoM\">nginx-deployment-example</code> pods:</p><p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">Kubernetes ensures that the actual state of the cluster and the desired statue of the cluster are always in-sync. This is made possible through continuous monitoring within the Kubernetes cluster. Whenever the state of a cluster changes from what has been defined, the various components of Kubernetes work to bring it back to its defined state. This automated recovery is often referred to as self-healing. So, let&apos;s copy one of the pods mentioned in the prerequisite and see what happens when we delete it:</p><p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">And after a few seconds, we see that our pod was deleted:</p><p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">Let&apos;s go ahead and list the pods one more time:</p><p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">And we see that the pod <code class=\"_34q3PgLsx9zIU5BiSOjFoM\">nginx-deployment-example-f4cd8584-sgfqq</code> was automatically created to replace our deleted pod <code class=\"_34q3PgLsx9zIU5BiSOjFoM\">nginx-deployment-example-f4cd8584-f494x</code>. And the reason is that <code class=\"_34q3PgLsx9zIU5BiSOjFoM\">nginx</code> deployment is set to have 3 replicas. So, even though one of these was deleted, our Kubernetes cluster works to make sure that the desired state is the actual state that we have. So, now let&apos;s consider the case when there is an actual node failure in your cluster. First, let&apos;s check our nodes:</p><p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">You will see your Master and Worker nodes. Now, let&apos;s figure out what server our pods are running on. To do that, we have to <code class=\"_34q3PgLsx9zIU5BiSOjFoM\">describe</code> it:</p><p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">Under the Events, you can see to which server the pod was assigned. We can also scroll up and under Node will also see where it has been assigned, which is of course will be the same server. Once you&apos;ve identified all servers that are pods running on, you have to pick one server and simulate a node failure by <code class=\"_34q3PgLsx9zIU5BiSOjFoM\">shutting down</code> the server. Once the node has been shut down let us head back to the Master and check on the status of the nodes:</p><p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">We see that the cluster knows that one node is down. Let&apos;s also list our pods:</p><p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">You see that the state of the pod that was running on <code class=\"_34q3PgLsx9zIU5BiSOjFoM\">&quot;failed&quot;</code> node is Unknown, but you see that another pod took its place. Let&apos;s go ahead and list the deployments:</p><p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">And as we expected we have 3 pods available which are now in sync with our Desired amount of pods. Now, let&apos;s try to describe the Unknown pod:</p><p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">You see that the Status of Unknown pod is <code class=\"_34q3PgLsx9zIU5BiSOjFoM\">Terminating</code>, <code class=\"_34q3PgLsx9zIU5BiSOjFoM\">Termination Grace Period</code> will be the <code class=\"_34q3PgLsx9zIU5BiSOjFoM\">30s</code> by default and the reason for this is <code class=\"_34q3PgLsx9zIU5BiSOjFoM\">NodeLost</code>. Also, you will see the messages that specifies that our node which was running our pod is unresponsive. Now, let&apos;s <code class=\"_34q3PgLsx9zIU5BiSOjFoM\">start</code> our <code class=\"_34q3PgLsx9zIU5BiSOjFoM\">&quot;failed&quot;</code> node and wait till it successfully rejoins the cluster. Alright, once <code class=\"_34q3PgLsx9zIU5BiSOjFoM\">&quot;failed&quot;</code> node is up and running, let&apos;s go ahead and check the status of our deployment:</p><p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">We will see that our pod count is in sync. So, let&apos;s list our pods out:</p><p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">We will see that our Kubernetes cluster has finally terminated the old pod, and we are left with our desired count of <code class=\"_34q3PgLsx9zIU5BiSOjFoM\">3</code> pods.</p><p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">As we see, Kubernetes takes a self-healing approach to infrastructure that reduces the criticality of failures, making fire drills less common. Kubernetes heals itself when there is a discrepancy and ensures the cluster always matches the declarative state. In other words, Kubernetes kicks in and fixes a deviation, if detected. For example, if a pod goes down, a new one will be deployed to match the desired state.</p><p class=\"_1qeIAgB0cPwnLhDF9XSiJM\"><em class=\"_7s4syPYtk5hfUIjySXcRE\">This article was originally published on</em> <a href=\"https://appfleet.com/blog/kubernetes-and-self-healing-micro-services/\" class=\"_3t5uN8xUmg0TOwRCOGQEcU\"><em class=\"_7s4syPYtk5hfUIjySXcRE\">https://appfleet.com/blog/kubernetes-and-self-healing-micro-services/</em></a> <em class=\"_7s4syPYtk5hfUIjySXcRE\">and has been authorized by Appfleet for a republish.</em></p></div></div>",
      "contentAsText": "To start with, I would like to explain what does self-healing means in terms of Kubernetes. Self-healing is a fantastic feature of Kubernetes to recover from service or node failure automatically. In the following article, we will consider the benefit of using replication for your micro-services and how the Kubernetes cluster can automatically recover from a service failure.One of the great features of Kubernetes is the ability to replicate pods and their underlying containers across the cluster. So, before we set up our self-healing feature please make sure you have managed replication and here is a simple example of a deployment file that will deploy nginx container with replication factor 3:All right, so let's create the deployment:Now, let's check whether our nginx-deployment-example was created:You should see your nginx-deployment-example deployment in the default namespace. If we want to see more details about those pods, please run the following command:We will see our 3 nginx-deployment-example pods:Kubernetes ensures that the actual state of the cluster and the desired statue of the cluster are always in-sync. This is made possible through continuous monitoring within the Kubernetes cluster. Whenever the state of a cluster changes from what has been defined, the various components of Kubernetes work to bring it back to its defined state. This automated recovery is often referred to as self-healing. So, let's copy one of the pods mentioned in the prerequisite and see what happens when we delete it:And after a few seconds, we see that our pod was deleted:Let's go ahead and list the pods one more time:And we see that the pod nginx-deployment-example-f4cd8584-sgfqq was automatically created to replace our deleted pod nginx-deployment-example-f4cd8584-f494x. And the reason is that nginx deployment is set to have 3 replicas. So, even though one of these was deleted, our Kubernetes cluster works to make sure that the desired state is the actual state that we have. So, now let's consider the case when there is an actual node failure in your cluster. First, let's check our nodes:You will see your Master and Worker nodes. Now, let's figure out what server our pods are running on. To do that, we have to describe it:Under the Events, you can see to which server the pod was assigned. We can also scroll up and under Node will also see where it has been assigned, which is of course will be the same server. Once you've identified all servers that are pods running on, you have to pick one server and simulate a node failure by shutting down the server. Once the node has been shut down let us head back to the Master and check on the status of the nodes:We see that the cluster knows that one node is down. Let's also list our pods:You see that the state of the pod that was running on \"failed\" node is Unknown, but you see that another pod took its place. Let's go ahead and list the deployments:And as we expected we have 3 pods available which are now in sync with our Desired amount of pods. Now, let's try to describe the Unknown pod:You see that the Status of Unknown pod is Terminating, Termination Grace Period will be the 30s by default and the reason for this is NodeLost. Also, you will see the messages that specifies that our node which was running our pod is unresponsive. Now, let's start our \"failed\" node and wait till it successfully rejoins the cluster. Alright, once \"failed\" node is up and running, let's go ahead and check the status of our deployment:We will see that our pod count is in sync. So, let's list our pods out:We will see that our Kubernetes cluster has finally terminated the old pod, and we are left with our desired count of 3 pods.As we see, Kubernetes takes a self-healing approach to infrastructure that reduces the criticality of failures, making fire drills less common. Kubernetes heals itself when there is a discrepancy and ensures the cluster always matches the declarative state. In other words, Kubernetes kicks in and fixes a deviation, if detected. For example, if a pod goes down, a new one will be deployed to match the desired state.This article was originally published on https://appfleet.com/blog/kubernetes-and-self-healing-micro-services/ and has been authorized by Appfleet for a republish.",
      "publishedDate": "2020-10-22T12:20:37.237Z",
      "description": "To start with, I would like to explain what does self-healing means in terms of Kubernetes. Self-healing is a fantastic feature of Kubernetes to …",
      "ogDescription": "24 votes and 2 comments so far on Reddit"
    },
    {
      "url": "https://github.com/vetyy/helm-ecr",
      "title": "vetyy/helm-ecr",
      "content": "<div><div><article class=\"markdown-body entry-content container-lg\">\n<p><a href=\"https://camo.githubusercontent.com/cb0cf671eaaf0b35a36d1180531cdd84490925146abbf95e3bcb93b7d95ed89d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f48656c6d253230332d737570706f727465642d677265656e\"><img src=\"https://camo.githubusercontent.com/cb0cf671eaaf0b35a36d1180531cdd84490925146abbf95e3bcb93b7d95ed89d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f48656c6d253230332d737570706f727465642d677265656e\" alt=\"Helm3 supported\"></a>\n<a href=\"https://github.com/vetyy/helm-ecr/blob/master/LICENSE\"><img src=\"https://camo.githubusercontent.com/dd1c858e94a371529a0a4c359bc95f18f09ba4a5fc0e658950bcb1383ea40fc9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d626c75652e7376673f7374796c653d666c6174\" alt=\"License MIT\"></a>\n<a href=\"https://github.com/vetyy/helm-ecr/releases\"><img src=\"https://camo.githubusercontent.com/0bfd7c8a933f90f8531064fa63bd36f2529162e0c0eba2f9342fde2548918c86/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f76657479792f68656c6d2d6563722e737667\" alt=\"GitHub release\"></a></p>\n<p>Helm plugin that allows installing Helm charts from <a href=\"https://aws.amazon.com/ecr/\">Amazon ECR</a> stored as OCI Artifacts.</p>\n<blockquote>\n<p><g-emoji class=\"g-emoji\">&#xFFFD;\u000f</g-emoji> <strong>Notice</strong>\nThis is not an official plugin and does not use Helm&apos;s new experimental API.\nMain motivation for this plugin was to be able to install charts stored in ECR using existing tools\nlike <a href=\"https://github.com/fluxcd/helm-operator\">Flux Helm Operator</a> until better options are available.</p>\n</blockquote>\n<p>Plugin currently supports only Helm v3.</p>\n<p>This plugin was motivated by <a href=\"https://github.com/hypnoglow/helm-s3\">helm-s3</a> plugin.</p>\n<h2><a id=\"user-content-install\" class=\"anchor\" href=\"https://github.com/vetyy/helm-ecr#install\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Install</h2>\n<p>Install latest version:</p>\n<pre><code>$ helm plugin install https://github.com/vetyy/helm-ecr.git\n</code></pre>\n<p>Install a specific release version:</p>\n<pre><code>$ helm plugin install https://github.com/vetyy/helm-ecr.git --version 0.1.2\n</code></pre>\n<p>To use the plugin, you do not need any special dependencies. The installer will\ndownload versioned release with prebuilt binary from <a href=\"https://github.com/vetyy/helm-ecr/releases\">github releases</a>.</p>\n<h2><a id=\"user-content-overview\" class=\"anchor\" href=\"https://github.com/vetyy/helm-ecr#overview\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Overview</h2>\n<p>Plugin provides a new made up protocol registered as <code>ecr://</code>.\nThere are no additional commands provided by the plugin and it integrates only with native Helm commands.</p>\n<p>Because ECR repository basically stores only a single chart, but multiple versions,\nwe must provide a chart with a name derived from repository url.\nWe decided to use the last part of the repository url as chart name and all attached image tags will be the chart versions.</p>\n<p>Given <code>ecr://aws_account_id.dkr.ecr.region.amazonaws.com/namespace/NAME</code>.\nThe <code>NAME</code> will be the name of the chart. See usage below for more details.</p>\n<h2><a id=\"user-content-usage\" class=\"anchor\" href=\"https://github.com/vetyy/helm-ecr#usage\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Usage</h2>\n<p>Add Helm repo:</p>\n<pre><code>$ helm repo add my-ecr ecr://aws_account_id.dkr.ecr.region.amazonaws.com/namespace/app\n&quot;my-ecr&quot; has been added to your repositories\n</code></pre>\n<p>Discover chart versions:</p>\n<pre><code>$ helm search repo my-ecr -l\nNAME\t                CHART        VERSION\tAPP VERSION\tDESCRIPTION\nmy-ecr/app\t        0.1.1\nmy-ecr/app\t        0.1.0\nmy-ecr/app\t        my-image-tag\n</code></pre>\n<p>Install chart:</p>\n<pre><code>$ helm install my-app my-ecr/app\n</code></pre>\n<p>Alternatively you can install chart without adding the Helm repo,\nbut you must specify the version (image tag) as the last part of the chart name.</p>\n<pre><code>$ helm install my-app ecr://aws_project_id.dkr.ecr.region.amazonaws.com/namespace/app/0.1.0\n</code></pre>\n<h2><a id=\"user-content-helm-operator\" class=\"anchor\" href=\"https://github.com/vetyy/helm-ecr#helm-operator\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Helm Operator</h2>\n<p>You can also use this plugin with Flux Helm Operator and their Kubernetes CRD <code>HelmRelease</code>.</p>\n<div class=\"highlight highlight-source-yaml\"><pre>---\n<span class=\"pl-ent\">apiVersion</span>: <span class=\"pl-s\">helm.fluxcd.io/v1</span>\n<span class=\"pl-ent\">kind</span>: <span class=\"pl-s\">HelmRelease</span>\n<span class=\"pl-ent\">metadata</span>:\n  <span class=\"pl-ent\">labels</span>:\n    <span class=\"pl-ent\">app</span>: <span class=\"pl-s\">my-app</span>\n  <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">my-app</span>\n<span class=\"pl-ent\">spec</span>:\n  <span class=\"pl-ent\">chart</span>:\n    <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">app</span>\n    <span class=\"pl-ent\">repository</span>: <span class=\"pl-s\">ecr://aws_project_id.dkr.ecr.region.amazonaws.com/namespace/app</span>\n    <span class=\"pl-ent\">version</span>: <span class=\"pl-s\">0.1.0</span>\n  <span class=\"pl-ent\">values</span>:\n    <span class=\"pl-ent\">some_key</span>: <span class=\"pl-s\">some_value</span></pre></div>\n<h2><a id=\"user-content-uninstall\" class=\"anchor\" href=\"https://github.com/vetyy/helm-ecr#uninstall\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Uninstall</h2>\n<pre><code>$ helm plugin remove ecr\n</code></pre>\n<h2><a id=\"user-content-development\" class=\"anchor\" href=\"https://github.com/vetyy/helm-ecr#development\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Development</h2>\n<p>On regular plugin installation, helm triggers post-install hook\nthat downloads prebuilt versioned release of the plugin binary and installs it.\nTo disable this behavior, you need to pass <code>HELM_PLUGIN_NO_INSTALL_HOOK=true</code> to the installer:</p>\n<pre><code>$ HELM_PLUGIN_NO_INSTALL_HOOK=true helm plugin install https://github.com/vetyy/helm-ecr.git\nDevelopment mode: not downloading versioned release.\nInstalled plugin: ecr\n</code></pre>\n<p>Next, you may want to ensure if you have all prerequisites to build the plugin from source:</p>\n<pre><code>cd ~/.helm/plugins/helm-ecr\nmake deps build\n</code></pre>\n<p>If you see no messages - build was successful. Try to run some helm commands\nthat involve the plugin, or jump straight into plugin development.</p>\n<h2><a id=\"user-content-license\" class=\"anchor\" href=\"https://github.com/vetyy/helm-ecr#license\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>License</h2>\n<p><a href=\"https://github.com/vetyy/helm-ecr/blob/master/LICENSE\">MIT</a></p>\n</article></div></div>",
      "contentAsText": "\n\n\n\nHelm plugin that allows installing Helm charts from Amazon ECR stored as OCI Artifacts.\n\n�\u000f Notice\nThis is not an official plugin and does not use Helm's new experimental API.\nMain motivation for this plugin was to be able to install charts stored in ECR using existing tools\nlike Flux Helm Operator until better options are available.\n\nPlugin currently supports only Helm v3.\nThis plugin was motivated by helm-s3 plugin.\nInstall\nInstall latest version:\n$ helm plugin install https://github.com/vetyy/helm-ecr.git\n\nInstall a specific release version:\n$ helm plugin install https://github.com/vetyy/helm-ecr.git --version 0.1.2\n\nTo use the plugin, you do not need any special dependencies. The installer will\ndownload versioned release with prebuilt binary from github releases.\nOverview\nPlugin provides a new made up protocol registered as ecr://.\nThere are no additional commands provided by the plugin and it integrates only with native Helm commands.\nBecause ECR repository basically stores only a single chart, but multiple versions,\nwe must provide a chart with a name derived from repository url.\nWe decided to use the last part of the repository url as chart name and all attached image tags will be the chart versions.\nGiven ecr://aws_account_id.dkr.ecr.region.amazonaws.com/namespace/NAME.\nThe NAME will be the name of the chart. See usage below for more details.\nUsage\nAdd Helm repo:\n$ helm repo add my-ecr ecr://aws_account_id.dkr.ecr.region.amazonaws.com/namespace/app\n\"my-ecr\" has been added to your repositories\n\nDiscover chart versions:\n$ helm search repo my-ecr -l\nNAME\t                CHART        VERSION\tAPP VERSION\tDESCRIPTION\nmy-ecr/app\t        0.1.1\nmy-ecr/app\t        0.1.0\nmy-ecr/app\t        my-image-tag\n\nInstall chart:\n$ helm install my-app my-ecr/app\n\nAlternatively you can install chart without adding the Helm repo,\nbut you must specify the version (image tag) as the last part of the chart name.\n$ helm install my-app ecr://aws_project_id.dkr.ecr.region.amazonaws.com/namespace/app/0.1.0\n\nHelm Operator\nYou can also use this plugin with Flux Helm Operator and their Kubernetes CRD HelmRelease.\n---\napiVersion: helm.fluxcd.io/v1\nkind: HelmRelease\nmetadata:\n  labels:\n    app: my-app\n  name: my-app\nspec:\n  chart:\n    name: app\n    repository: ecr://aws_project_id.dkr.ecr.region.amazonaws.com/namespace/app\n    version: 0.1.0\n  values:\n    some_key: some_value\nUninstall\n$ helm plugin remove ecr\n\nDevelopment\nOn regular plugin installation, helm triggers post-install hook\nthat downloads prebuilt versioned release of the plugin binary and installs it.\nTo disable this behavior, you need to pass HELM_PLUGIN_NO_INSTALL_HOOK=true to the installer:\n$ HELM_PLUGIN_NO_INSTALL_HOOK=true helm plugin install https://github.com/vetyy/helm-ecr.git\nDevelopment mode: not downloading versioned release.\nInstalled plugin: ecr\n\nNext, you may want to ensure if you have all prerequisites to build the plugin from source:\ncd ~/.helm/plugins/helm-ecr\nmake deps build\n\nIf you see no messages - build was successful. Try to run some helm commands\nthat involve the plugin, or jump straight into plugin development.\nLicense\nMIT\n",
      "description": "Helm plugin that supports installing Charts from AWS ECR. - vetyy/helm-ecr",
      "ogDescription": "Helm plugin that supports installing Charts from AWS ECR. - vetyy/helm-ecr"
    },
    {
      "url": "https://www.altoros.com/blog/linkedin-aims-to-deploy-thousands-of-hadoop-servers-on-kubernetes/",
      "title": "LinkedIn Aims to Deploy Thousands of Hadoop Servers on Kubernetes",
      "content": "<div><div class=\"cf_ttshare\"><p><a href=\"https://www.altoros.com/research-papers/hadoop-gpu-boost-performance-of-your-big-data-project-by-50x-200x-2/\">Hadoop</a> is a collection of open-source software for utilizing a computer network to solve problems around massive amounts of data and computation. It provides a framework for distributed storage and big data processing. For the past 10 years, LinkedIn has invested heavily in this technology, becoming one of the largest Hadoop data lake operators in the world. The organization has over 10 Hadoop clusters, the largest consisting of 7,000+ servers with a storage capacity of 400+ PB.</p><p><center><a href=\"https://www.altoros.com/blog/wp-content/uploads/2020/09/Hadoop-at-LinkedIn.png\"><img src=\"https://www.altoros.com/blog/wp-content/uploads/2020/09/Hadoop-at-LinkedIn-1024x576.png\" width=\"640\" class=\"aligncenter size-large wp-image-57566\"></a><small>The Hadoop footprint at LinkedIn (<a href=\"https://static.sched.com/hosted_files/kccnceu20/d1/kubeCon2020_Lesson-Learned-on-Running-Hadoop-on-Kubernetes%20%20-%20%20Compatibility%20Mode.pdf\">Image credit</a>)</small></center></p><p>In the past few years, <a href=\"https://www.altoros.com/blog/tag/kubernetes/\">Kubernetes</a> rapidly grew in popularity, and LinkedIn saw the opportunity to use the platform for its <a href=\"https://www.altoros.com/blog/tag/machine-learning/\">artificial intelligence</a> (AI) workloads. However, according to <a href=\"https://www.linkedin.com/in/frank-cong-gu-1059838b/\">Cong Gu</a>, Senior Software Engineer at LinkedIn, before any adoption could occur, the organization needed to address the gap between the security models of Kubernetes and Hadoop.</p><div id=\"attachment_57573\" class=\"wp-caption alignright\"><a href=\"https://www.altoros.com/blog/wp-content/uploads/2020/09/Cong-Gu.jpg\"><img src=\"https://www.altoros.com/blog/wp-content/uploads/2020/09/Cong-Gu-150x150.jpg\" width=\"100\" class=\"size-thumbnail wp-image-57573\"></a><p id=\"caption-attachment-57573\" class=\"wp-caption-text\"><small>Cong Gu</small></p></div><blockquote><p>&#x201C;To enable LinkedIn AI jobs running on Kubernetes, we first need to tackle the problem of accessing a Hadoop data file system (HDFS) from Kubernetes. However, there is a gap in authentication between the two services.&#x201D;</p><p>&#x2014;Cong Gu, LinkedIn</p></blockquote><p>During KubeCon Europe 2020, Cong Gu along with <a href=\"https://www.linkedin.com/in/abinshahab/\">Abin Shahab</a> and <a href=\"https://www.linkedin.com/in/cqiang/\">Chen Qiang</a> of LinkedIn revealed how the organization solved these and other issues when integrating Hadoop and Kubernetes.</p><h3>Binding Hadoop and Kubernetes</h3><p>Hadoop uses <a href=\"https://web.mit.edu/kerberos/\">Kerberos</a>, a three-party protocol built on symmetric key cryptography that ensures anyone accessing a clusters is who they claim to be. The LinkedIn team introduced the concept of delegation tokens&#x2014;a two-party authentication method&#x2014;to avoid the necessity to always authenticate against a Kerberos server.</p><p>By default, the Hadoop delegation token has a lifespan of a day and can be renewed up to seven days. Meanwhile, Kubernetes authenticates via certificates and does not expose the job owner in any of its public-facing APIs. As a result, it is impossible to verify the authorized user from the pod through the native Kubernetes API, and then employ the username to fetch the Hadoop delegation token for HDFS access.</p><p>For the integration purposes, LinkedIn created and open-sourced <a href=\"https://github.com/linkedin/kube2hadoop\">Kube2Hadoop</a>, a project that enables secure HDFS access from Kubernetes. The tool has the following functionality:</p><ul><li>integrates the Hadoop authentication mechanism that uses delegation tokens</li><li>renews delegation tokens to support long-term jobs</li><li>incorporates lightweight directory access protocol (LDAP) for fine-grained access control, enabling users to proxy as themselves or as <a href=\"https://www.usenix.org/system/files/login/articles/105516-Schaumann.pdf\">headless accounts</a></li><li>generates GDPR-compliant auditable logs and helps administrators to figure out who is accessing data at what time</li></ul><p><center><a href=\"https://www.altoros.com/blog/wp-content/uploads/2020/09/kube2hadoop-authentication-workflow.png\"><img src=\"https://www.altoros.com/blog/wp-content/uploads/2020/09/kube2hadoop-authentication-workflow.png\" alt width=\"640\" class=\"aligncenter size-full wp-image-57592\"></a><small>An example of the Kube2Hadoop authentication workflow (<a href=\"https://engineering.linkedin.com/blog/2020/open-sourcing-kube2hadoop\">Image credit</a>)</small></center></p><p>According to the <a href=\"https://engineering.linkedin.com/blog/2020/open-sourcing-kube2hadoop\">article</a> by Cong Gu, Abin Shahab, Chen Qiang, and <a href=\"https://www.linkedin.com/in/keqiuhu/\">Keqiu Hu</a> of LinkedIn, Kube2Hadoop has three major components:</p><ul><li><i>Hadoop Token Service</i> fetches delegation tokens on behalf of a user. It is deployed as a <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/deployment/\">Kubernetes Deployment</a>.</li><li><i>Kube2Hadoop Init Container</i> resides in every worker that needs to access HDFS. The component sends a request to Hadoop Token Service for fetching a delegation token.</li><li><i>IDDecorator</i> writes authenticated userID as an immutable annotation in each pod.</li></ul><p><center><a href=\"https://www.altoros.com/blog/wp-content/uploads/2020/09/kube2hadoop-iddecorator.png\"><img src=\"https://www.altoros.com/blog/wp-content/uploads/2020/09/kube2hadoop-iddecorator.png\" alt width=\"640\" class=\"aligncenter size-full wp-image-57606\"></a><small>An example of the Kube2Hadoop IDDecorator workflow (<a href=\"https://engineering.linkedin.com/blog/2020/open-sourcing-kube2hadoop\">Image credit</a>)</small></center></p><p>Read the original article by the LinkedIn team for the workflow and additional details.</p><p class=\"altor-services-banners\" id=\"altor-712001496\"><!--HubSpot Call-to-Action Code --><span class=\"hs-cta-wrapper\" id=\"hs-cta-wrapper-3aa5aaea-880e-4a6d-892b-941e2b37bf10\"><span class=\"hs-cta-node hs-cta-3aa5aaea-880e-4a6d-892b-941e2b37bf10\" id=\"hs-cta-3aa5aaea-880e-4a6d-892b-941e2b37bf10\"><!--[if lte IE 8]><div id=\"hs-cta-ie-element\"></div><![endif]--><a href=\"https://cta-redirect.hubspot.com/cta/redirect/2950617/3aa5aaea-880e-4a6d-892b-941e2b37bf10\"><img class=\"hs-cta-img\" id=\"hs-cta-img-3aa5aaea-880e-4a6d-892b-941e2b37bf10\" width=\"818\" src=\"https://no-cache.hubspot.com/cta/default/2950617/3aa5aaea-880e-4a6d-892b-941e2b37bf10.png\" alt=\"Kubernetes-Training-Courses\"></a></span></span><!-- end HubSpot Call-to-Action Code --></p><h3>Issues with network throughput and init container</h3><p>When LinkedIn started to experiment with Kube2Hadoop in production, the team encountered two major problems related to network throughput and launch speeds of init containers.</p><p>Initially, users running synchronous distributed training on the host network could consistently get a rate of 900 Mb/s.</p><p><center><a href=\"https://www.altoros.com/blog/wp-content/uploads/2020/09/LinkedIn-Kube2Hadoop-Kubernetes-host-network-throughput.png\"><img src=\"https://www.altoros.com/blog/wp-content/uploads/2020/09/LinkedIn-Kube2Hadoop-Kubernetes-host-network-throughput.png\" alt width=\"640\" class=\"aligncenter size-full wp-image-57715\"></a><small>Throughput on a host network (<a href=\"https://www.youtube.com/watch?v=VFIwJrkFw1s\">Image credit</a>)</small></center></p><p>Once the team switched over to a pod network for Kube2Hadoop to get pod IP addresses, the users&#x2019; <b>network throughput went down</b> to around 140 Mb/s.</p><p><center><a href=\"https://www.altoros.com/blog/wp-content/uploads/2020/09/LinkedIn-Kube2Hadoop-Kubernetes-pod-network-throughput.png\"><img src=\"https://www.altoros.com/blog/wp-content/uploads/2020/09/LinkedIn-Kube2Hadoop-Kubernetes-pod-network-throughput.png\" width=\"640\" class=\"aligncenter size-full wp-image-57716\"></a><small>Throughput on a pod network (<a href=\"https://www.youtube.com/watch?v=VFIwJrkFw1s\">Image credit</a>)</small></center></p><p>The second problem were customer complaints about <b>slow launch speeds of init containers</b>. According to Cong, this slow down was intentional to allow the Kubernetes API server to propagate IP addresses.</p><p><center><a href=\"https://www.altoros.com/blog/wp-content/uploads/2020/09/LinkedIn-Kube2Hadoop-Kubernetes-API-Server-delay.png\"><img src=\"https://www.altoros.com/blog/wp-content/uploads/2020/09/LinkedIn-Kube2Hadoop-Kubernetes-API-Server-delay.png\" width=\"640\" class=\"aligncenter size-full wp-image-57718\"></a><small>Propagation of IP addresses (<a href=\"https://www.youtube.com/watch?v=VFIwJrkFw1s\">Image credit</a>)</small></center></p><p>To resolve both the issues, the LinkedIn team made use of the <code>TokenRequest</code> and <code>TokenReview</code> APIs available in Kubernetes v1.14.</p><blockquote><p>&#x201C;Using tokens instead of IP addresses as authentication, we no longer need to bind ourselves to the pod network. Thus, we can support hosted network jobs. Since we won&#x2019;t face the IP address propagation issue, we can potentially speed up our init container by a very big margin.&#x201D; &#x2014;Cong Gu, LinkedIn</p></blockquote><h3>Integration challenges</h3><p>While running Hadoop on Kubernetes, LinkedIn faced a few challenges related to domain name server (DNS), identity, network, and orchestration. <a href=\"https://www.linkedin.com/in/cqiang/\">Chen Qiang</a>, Engineering Manager and Data Site Reliability Engineer at LinkedIn, provided an overview of each problem and explained how the team resolved them.</p><p>According to Chen, each Hadoop worker or HDFS data node communicate with each other using host names. However, by default, there is <b>no global resolvable hostname</b> associated to each. To address the DNS issue, the LinkedIn team made use of Kubernetes <a href=\"https://www.altoros.com/blog/running-stateful-apps-on-kubernetes-with-statefulsets/\">StatefulSets</a> with a headless service to provide every pod a global resolvable hostname. Additionally, a resolvable hostname was injected into the main container.</p><p><center><a href=\"https://www.altoros.com/blog/wp-content/uploads/2020/09/hadoop-kubernetes-challenges.png\"><img src=\"https://www.altoros.com/blog/wp-content/uploads/2020/09/hadoop-kubernetes-challenges-1024x576.png\" alt width=\"640\" class=\"aligncenter size-large wp-image-57615\"></a><small>Challenges encountered when integrating Hadoop and Kubernetes (<a href=\"https://static.sched.com/hosted_files/kccnceu20/d1/kubeCon2020_Lesson-Learned-on-Running-Hadoop-on-Kubernetes%20%20-%20%20Compatibility%20Mode.pdf\">Image credit</a>)</small></center></p><p>Next, there are many components in Hadoop that communicate with each other, while the cluster is up and running, but there are <b>no fixed IP addresses</b> for Hadoop master services. By creating a Kubernetes Service for every Hadoop administrator instance, LinkedIn delivers a predefined and structured DNS-resolvable hostname that can be predetermined in Hadoop configuration files.</p><div id=\"attachment_57625\" class=\"wp-caption alignright\"><a href=\"https://www.altoros.com/blog/wp-content/uploads/2020/09/Chen-Qiang.jpg\"><img src=\"https://www.altoros.com/blog/wp-content/uploads/2020/09/Chen-Qiang-150x150.jpg\" width=\"100\" class=\"size-thumbnail wp-image-57625\"></a><p id=\"caption-attachment-57625\" class=\"wp-caption-text\"><small>Chen Qiang</small></p></div><p>Additionally, the LinkedIn team encountered an issue around identity related to secure Hadoop clusters with Kerberos enabled. To authenticate, each Hadoop cluster uses a keytab file that includes part of the hostname. However, hostnames have random IP addresses, making it <b>impossible to pregenerate keytabs</b>. To resolve this problem, the team introduced a Keytab Delivery Service (KDS) that uses the authentication mechanism of Kube2Hadoop.</p><p>Finally, there is a strong <b>dependency between Hadoop components</b>, making the bootstrap order critical. While the bootstrap order can be orchestrated externally, this introduces additional complexity and prolongs deployment. In response, the LinkedIn team introduced built-in dependencies using an init container with Kubernetes service discovery. This way, all pods can be deployed simultaneously, effectively reducing cluster deployment time down to two minutes.</p><blockquote><p>&#x201C;Hadoop on Kubernetes may change the way big data infrastructure runs. A lot of Hadoop-native distributed frameworks are now running natively on Kubernetes.&#x201D; &#x2014;Chen Qiang, LinkedIn</p></blockquote><h3>What&#x2019;s next?</h3><p>Moving forward, LinkedIn is looking to expand the effort to run Hadoop on Kubernetes by adding more big data components, such as <a href=\"https://www.altoros.com/blog/optimizing-the-performance-of-apache-spark-queries/\">Spark</a>, Hive, Presto, and Azkaban. The organization is also in the processes of testing long-running Hadoop clusters on Kubernetes in order to replace bare-metal environments.</p><p>As for Kube2Hadoop, the LinkedIn team is planning to add a chief init container that will be in charge of fetching delegation tokens from Hadoop Token Service and then distributing the tokens to workers. This has the potential to improve scaling, especially with deep learning jobs involving thousands of containers.</p><p><center><a href=\"https://www.altoros.com/blog/wp-content/uploads/2020/09/kube2hadoop-chief-init-container.png\"><img src=\"https://www.altoros.com/blog/wp-content/uploads/2020/09/kube2hadoop-chief-init-container.png\" width=\"640\" class=\"aligncenter size-full wp-image-57691\"></a><small>Fetching and distributing tokens through a chief init container (<a href=\"https://www.youtube.com/watch?v=VFIwJrkFw1s\">Image credit</a>)</small></center></p><p>By integrating Hadoop and Kubernetes, LinkedIn can better scale its AI projects. With open-source Kube2Hadoop, organizations are able to combine different Hadoop workloads onto a single resource management platform. This way, companies that have separate online and offline infrastructures can easily leverage their online infrastructure during off-peak hours, effectively utilizing idle resources and potentially reducing millions in hardware cost.</p><h3>Want details? Watch the videos!</h3><p><small><a href=\"https://www.linkedin.com/in/abinshahab/\">Abin Shahab</a> and <a href=\"https://www.linkedin.com/in/frank-cong-gu-1059838b/\">Cong Gu</a> explain how Kube2Hadoop works.</small></p><p><center><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/VFIwJrkFw1s\" class></iframe></center></p><p><small><a href=\"https://www.linkedin.com/in/cqiang/\">Chen Qiang</a> provides an overview of lessons and takeaways LinkedIn encountered in running Hadoop on Kubernetes.</small></p><p><center><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Fht0Nj8GqIs\" class></iframe></center></p><h3>Further reading</h3><h3>About the experts</h3><p><small><a href=\"https://www.altoros.com/blog/wp-content/uploads/2020/09/Cong-Gu-bio.png\"><img src=\"https://www.altoros.com/blog/wp-content/uploads/2020/09/Cong-Gu-bio-150x150.png\" width=\"100\" class=\"alignright size-thumbnail wp-image-57634\"></a><a href=\"https://www.linkedin.com/in/frank-cong-gu-1059838b/\">Cong Gu</a> is Software Engineer at LinkedIn&#x2019;s Big Data Platform team. He joined LinkedIn in 2017 and helps AI engineers by building infrastructure to improve their productivity. Cong has given technical deep dive talks in company-wide settings, as well as at KubeFlow Summit.</small></p><p>&#xA0;<br> <small><a href=\"https://www.altoros.com/blog/wp-content/uploads/2020/09/Abin-Shahab-bio.png\"><img src=\"https://www.altoros.com/blog/wp-content/uploads/2020/09/Abin-Shahab-bio-150x150.png\" width=\"100\" class=\"alignright size-thumbnail wp-image-57633\"></a><a href=\"https://www.linkedin.com/in/abinshahab/\">Abin Shahab</a> is Staff Engineer at Linkedin&#x2019;s Big Data Platform (BDP) team. He joined Linkedin in 2017 and leads the Deep Learning infrastructure team in BDP. Abin is a veteran KubeCon speaker.</small></p><p>&#xA0;<br> <small><a href=\"https://www.altoros.com/blog/wp-content/uploads/2020/09/Chen-Qiang-bio.png\"><img src=\"https://www.altoros.com/blog/wp-content/uploads/2020/09/Chen-Qiang-bio-150x150.png\" width=\"100\" class=\"alignright size-thumbnail wp-image-57635\"></a><a href=\"https://www.linkedin.com/in/cqiang/\">Chen Qiang</a> is Staff Site Reliability Engineer at LinkedIn focusing on big data infrastructure. He specializes in continuous integration and continuous delivery for big data. Chen is also an expert in big data infrastructure, such as Hadoop HDFS, YARN, HBase, Hive, and Oozie.</small></p><p>&#xA0;<br> <small><a href=\"https://www.altoros.com/blog/wp-content/uploads/2020/10/Keqiu-Hu-bio.png\"><img src=\"https://www.altoros.com/blog/wp-content/uploads/2020/10/Keqiu-Hu-bio.png\" alt width=\"100\" class=\"alignright size-full wp-image-57766\"></a><a href=\"https://www.linkedin.com/in/keqiuhu/\">Keqiu Hu</a> is Engineering Manager at LinkedIn, where he leads a team that works on big data compute orchestration and deep learning. In addition, Keqiu manages system infrastructure engineers with experience in Apache YARN, Kubernetes, and TensorFlow. He is also involved in the development of TonY and Bluepill.</small></p><p><center><small>This blog post was written by <a href=\"https://www.altoros.com/blog/author/carlo/\">Carlo Gutierrez</a>, edited by <a href=\"https://www.altoros.com/blog/author/sophie.turol/\">Sophia Turol</a> and <a href=\"https://www.altoros.com/blog/author/alex/\">Alex Khizhniak</a>.</small></center></p> </div><p class=\"altor-blogpost-banner2\" id=\"altor-711444893\"><!--HubSpot Call-to-Action Code --><span class=\"hs-cta-wrapper\" id=\"hs-cta-wrapper-34e58a75-9e7a-40c6-abea-c974718bd144\"><span class=\"hs-cta-node hs-cta-34e58a75-9e7a-40c6-abea-c974718bd144\" id=\"hs-cta-34e58a75-9e7a-40c6-abea-c974718bd144\"><!--[if lte IE 8]><div id=\"hs-cta-ie-element\"></div><![endif]--><a href=\"https://cta-redirect.hubspot.com/cta/redirect/2950617/34e58a75-9e7a-40c6-abea-c974718bd144\"><img class=\"hs-cta-img\" id=\"hs-cta-img-34e58a75-9e7a-40c6-abea-c974718bd144\" width=\"900\" src=\"https://no-cache.hubspot.com/cta/default/2950617/34e58a75-9e7a-40c6-abea-c974718bd144.png\" alt=\"Interested in how to effectively use the Kuberneres CLI to manage your deployment? Download our kubectl cheat sheet!\"></a></span></span><!-- end HubSpot Call-to-Action Code --></p></div>",
      "contentAsText": "Hadoop is a collection of open-source software for utilizing a computer network to solve problems around massive amounts of data and computation. It provides a framework for distributed storage and big data processing. For the past 10 years, LinkedIn has invested heavily in this technology, becoming one of the largest Hadoop data lake operators in the world. The organization has over 10 Hadoop clusters, the largest consisting of 7,000+ servers with a storage capacity of 400+ PB.The Hadoop footprint at LinkedIn (Image credit)In the past few years, Kubernetes rapidly grew in popularity, and LinkedIn saw the opportunity to use the platform for its artificial intelligence (AI) workloads. However, according to Cong Gu, Senior Software Engineer at LinkedIn, before any adoption could occur, the organization needed to address the gap between the security models of Kubernetes and Hadoop.Cong Gu“To enable LinkedIn AI jobs running on Kubernetes, we first need to tackle the problem of accessing a Hadoop data file system (HDFS) from Kubernetes. However, there is a gap in authentication between the two services.”—Cong Gu, LinkedInDuring KubeCon Europe 2020, Cong Gu along with Abin Shahab and Chen Qiang of LinkedIn revealed how the organization solved these and other issues when integrating Hadoop and Kubernetes.Binding Hadoop and KubernetesHadoop uses Kerberos, a three-party protocol built on symmetric key cryptography that ensures anyone accessing a clusters is who they claim to be. The LinkedIn team introduced the concept of delegation tokens—a two-party authentication method—to avoid the necessity to always authenticate against a Kerberos server.By default, the Hadoop delegation token has a lifespan of a day and can be renewed up to seven days. Meanwhile, Kubernetes authenticates via certificates and does not expose the job owner in any of its public-facing APIs. As a result, it is impossible to verify the authorized user from the pod through the native Kubernetes API, and then employ the username to fetch the Hadoop delegation token for HDFS access.For the integration purposes, LinkedIn created and open-sourced Kube2Hadoop, a project that enables secure HDFS access from Kubernetes. The tool has the following functionality:integrates the Hadoop authentication mechanism that uses delegation tokensrenews delegation tokens to support long-term jobsincorporates lightweight directory access protocol (LDAP) for fine-grained access control, enabling users to proxy as themselves or as headless accountsgenerates GDPR-compliant auditable logs and helps administrators to figure out who is accessing data at what timeAn example of the Kube2Hadoop authentication workflow (Image credit)According to the article by Cong Gu, Abin Shahab, Chen Qiang, and Keqiu Hu of LinkedIn, Kube2Hadoop has three major components:Hadoop Token Service fetches delegation tokens on behalf of a user. It is deployed as a Kubernetes Deployment.Kube2Hadoop Init Container resides in every worker that needs to access HDFS. The component sends a request to Hadoop Token Service for fetching a delegation token.IDDecorator writes authenticated userID as an immutable annotation in each pod.An example of the Kube2Hadoop IDDecorator workflow (Image credit)Read the original article by the LinkedIn team for the workflow and additional details.Issues with network throughput and init containerWhen LinkedIn started to experiment with Kube2Hadoop in production, the team encountered two major problems related to network throughput and launch speeds of init containers.Initially, users running synchronous distributed training on the host network could consistently get a rate of 900 Mb/s.Throughput on a host network (Image credit)Once the team switched over to a pod network for Kube2Hadoop to get pod IP addresses, the users’ network throughput went down to around 140 Mb/s.Throughput on a pod network (Image credit)The second problem were customer complaints about slow launch speeds of init containers. According to Cong, this slow down was intentional to allow the Kubernetes API server to propagate IP addresses.Propagation of IP addresses (Image credit)To resolve both the issues, the LinkedIn team made use of the TokenRequest and TokenReview APIs available in Kubernetes v1.14.“Using tokens instead of IP addresses as authentication, we no longer need to bind ourselves to the pod network. Thus, we can support hosted network jobs. Since we won’t face the IP address propagation issue, we can potentially speed up our init container by a very big margin.” —Cong Gu, LinkedInIntegration challengesWhile running Hadoop on Kubernetes, LinkedIn faced a few challenges related to domain name server (DNS), identity, network, and orchestration. Chen Qiang, Engineering Manager and Data Site Reliability Engineer at LinkedIn, provided an overview of each problem and explained how the team resolved them.According to Chen, each Hadoop worker or HDFS data node communicate with each other using host names. However, by default, there is no global resolvable hostname associated to each. To address the DNS issue, the LinkedIn team made use of Kubernetes StatefulSets with a headless service to provide every pod a global resolvable hostname. Additionally, a resolvable hostname was injected into the main container.Challenges encountered when integrating Hadoop and Kubernetes (Image credit)Next, there are many components in Hadoop that communicate with each other, while the cluster is up and running, but there are no fixed IP addresses for Hadoop master services. By creating a Kubernetes Service for every Hadoop administrator instance, LinkedIn delivers a predefined and structured DNS-resolvable hostname that can be predetermined in Hadoop configuration files.Chen QiangAdditionally, the LinkedIn team encountered an issue around identity related to secure Hadoop clusters with Kerberos enabled. To authenticate, each Hadoop cluster uses a keytab file that includes part of the hostname. However, hostnames have random IP addresses, making it impossible to pregenerate keytabs. To resolve this problem, the team introduced a Keytab Delivery Service (KDS) that uses the authentication mechanism of Kube2Hadoop.Finally, there is a strong dependency between Hadoop components, making the bootstrap order critical. While the bootstrap order can be orchestrated externally, this introduces additional complexity and prolongs deployment. In response, the LinkedIn team introduced built-in dependencies using an init container with Kubernetes service discovery. This way, all pods can be deployed simultaneously, effectively reducing cluster deployment time down to two minutes.“Hadoop on Kubernetes may change the way big data infrastructure runs. A lot of Hadoop-native distributed frameworks are now running natively on Kubernetes.” —Chen Qiang, LinkedInWhat’s next?Moving forward, LinkedIn is looking to expand the effort to run Hadoop on Kubernetes by adding more big data components, such as Spark, Hive, Presto, and Azkaban. The organization is also in the processes of testing long-running Hadoop clusters on Kubernetes in order to replace bare-metal environments.As for Kube2Hadoop, the LinkedIn team is planning to add a chief init container that will be in charge of fetching delegation tokens from Hadoop Token Service and then distributing the tokens to workers. This has the potential to improve scaling, especially with deep learning jobs involving thousands of containers.Fetching and distributing tokens through a chief init container (Image credit)By integrating Hadoop and Kubernetes, LinkedIn can better scale its AI projects. With open-source Kube2Hadoop, organizations are able to combine different Hadoop workloads onto a single resource management platform. This way, companies that have separate online and offline infrastructures can easily leverage their online infrastructure during off-peak hours, effectively utilizing idle resources and potentially reducing millions in hardware cost.Want details? Watch the videos!Abin Shahab and Cong Gu explain how Kube2Hadoop works.Chen Qiang provides an overview of lessons and takeaways LinkedIn encountered in running Hadoop on Kubernetes.Further readingAbout the expertsCong Gu is Software Engineer at LinkedIn’s Big Data Platform team. He joined LinkedIn in 2017 and helps AI engineers by building infrastructure to improve their productivity. Cong has given technical deep dive talks in company-wide settings, as well as at KubeFlow Summit.  Abin Shahab is Staff Engineer at Linkedin’s Big Data Platform (BDP) team. He joined Linkedin in 2017 and leads the Deep Learning infrastructure team in BDP. Abin is a veteran KubeCon speaker.  Chen Qiang is Staff Site Reliability Engineer at LinkedIn focusing on big data infrastructure. He specializes in continuous integration and continuous delivery for big data. Chen is also an expert in big data infrastructure, such as Hadoop HDFS, YARN, HBase, Hive, and Oozie.  Keqiu Hu is Engineering Manager at LinkedIn, where he leads a team that works on big data compute orchestration and deep learning. In addition, Keqiu manages system infrastructure engineers with experience in Apache YARN, Kubernetes, and TensorFlow. He is also involved in the development of TonY and Bluepill.This blog post was written by Carlo Gutierrez, edited by Sophia Turol and Alex Khizhniak. ",
      "publishedDate": "2020-09-30T16:22:49.000Z",
      "description": "The company developed the Kube2Hadoop project to bridge security gaps between Kubernetes and Hadoop. Explore the \"war stories\" and challenges faced.",
      "ogDescription": "Different security modelsHadoop is a collection of open-source software for utilizing a computer network to solve problems around massive amounts of data and computation. It provides a framework f"
    },
    {
      "url": "https://github.com/storax/kubedoom",
      "title": "storax/kubedoom",
      "content": "<div><div><article class=\"markdown-body entry-content container-lg\">\n\n<p>The next level of chaos engineering is here! Kill pods inside your Kubernetes\ncluster by shooting them in Doom!</p>\n<p>This is a fork of the excellent\n<a href=\"https://github.com/gideonred/dockerdoomd\">gideonred/dockerdoomd</a> using a\nslightly modified Doom, forked from <a href=\"https://github.com/gideonred/dockerdoom\">https://github.com/gideonred/dockerdoom</a>,\nwhich was forked from psdoom.</p>\n<p><a href=\"https://github.com/storax/kubedoom/blob/master/assets/doom.jpg\"><img src=\"https://github.com/storax/kubedoom/raw/master/assets/doom.jpg\" alt=\"DOOM\"></a></p>\n<h2><a id=\"user-content-running-locally\" class=\"anchor\" href=\"https://github.com/storax/kubedoom#running-locally\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Running Locally</h2>\n<p>In order to run locally you will need to</p>\n<ol>\n<li>Run the kubedoom container</li>\n<li>Attach a VNC client to the appropriate port (5901)</li>\n</ol>\n<h3><a id=\"user-content-with-docker\" class=\"anchor\" href=\"https://github.com/storax/kubedoom#with-docker\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>With Docker</h3>\n<p>Run <code>storaxdev/kubedoom:0.5.0</code> with docker locally:</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>$ <span class=\"pl-s1\">docker run -p5901:5900 \\</span>\n<span class=\"pl-c1\">  --net=host \\</span>\n<span class=\"pl-c1\">  -v ~/.kube:/root/.kube \\</span>\n<span class=\"pl-c1\">  --rm -it --name kubedoom \\</span>\n<span class=\"pl-c1\">  storaxdev/kubedoom:0.5.0</span></pre></div>\n<h3><a id=\"user-content-with-podman\" class=\"anchor\" href=\"https://github.com/storax/kubedoom#with-podman\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>With Podman</h3>\n<p>Run <code>storaxdev/kubedoom:0.5.0</code> with podman locally:</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>$ <span class=\"pl-s1\">podman run -it -p5901:5900/tcp \\</span>\n<span class=\"pl-c1\">  -v ~/.kube:/tmp/.kube --security-opt label=disable \\</span>\n<span class=\"pl-c1\">  --env &quot;KUBECONFIG=/tmp/.kube/config&quot; --name kubedoom</span>\n<span class=\"pl-c1\">  storaxdev/kubedoom:0.5.0</span></pre></div>\n<h3><a id=\"user-content-attaching-a-vnc-client\" class=\"anchor\" href=\"https://github.com/storax/kubedoom#attaching-a-vnc-client\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Attaching a VNC Client</h3>\n<p>Now start a VNC viewer and connect to <code>localhost:5901</code>. The password is <code>idbehold</code>:</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>$ <span class=\"pl-s1\">vncviewer viewer localhost:5901</span></pre></div>\n<p>You should now see DOOM! Now if you want to get the job done quickly enter the\ncheat <code>idspispopd</code> and walk through the wall on your right. You should be\ngreeted by your pods as little pink monsters. Press <code>CTRL</code> to fire. If the\npistol is not your thing, cheat with <code>idkfa</code> and press <code>5</code> for a nice surprise.\nPause the game with <code>ESC</code>.</p>\n<h3><a id=\"user-content-killing-namespaces\" class=\"anchor\" href=\"https://github.com/storax/kubedoom#killing-namespaces\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Killing namespaces</h3>\n<p>Kubedoom now also supports killing namespaces <a href=\"https://github.com/storax/kubedoom/issues/5\">in case you have too many of\nthem</a>. Simply set the <code>-mode</code> flag\nto <code>namespaces</code>:</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>$ <span class=\"pl-s1\">docker run -p5901:5900 \\</span>\n<span class=\"pl-c1\">  --net=host \\</span>\n<span class=\"pl-c1\">  -v ~/.kube:/root/.kube \\</span>\n<span class=\"pl-c1\">  --rm -it --name kubedoom \\</span>\n<span class=\"pl-c1\">  storaxdev/kubedoom:0.5.0 \\</span>\n<span class=\"pl-c1\">  -mode namespaces</span></pre></div>\n<h3><a id=\"user-content-running-kubedoom-inside-kubernetes\" class=\"anchor\" href=\"https://github.com/storax/kubedoom#running-kubedoom-inside-kubernetes\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Running Kubedoom inside Kubernetes</h3>\n<p>See the example in the <code>/manifest</code> directory. You can quickly test it using\n<a href=\"https://github.com/kubernetes-sigs/kind\">kind</a>. Create a cluster with the\nexample config from this repository:</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>$ <span class=\"pl-s1\">kind create cluster --config kind-config.yaml</span>\n<span class=\"pl-c1\">Creating cluster &quot;kind&quot; ...</span>\n<span class=\"pl-c1\"> \u0013 Ensuring node image (kindest/node:v1.19.1) =&#xFFFD;</span>\n<span class=\"pl-c1\"> \u0013 Preparing nodes =&#xFFFD; =&#xFFFD;</span>\n<span class=\"pl-c1\"> \u0013 Writing configuration =&#xFFFD;</span>\n<span class=\"pl-c1\"> \u0013 Starting control-plane =y\u000f</span>\n<span class=\"pl-c1\"> \u0013 Installing CNI =\f</span>\n<span class=\"pl-c1\"> \u0013 Installing StorageClass =&#xFFFD;</span>\n<span class=\"pl-c1\"> \u0013 Joining worker nodes =&#xFFFD;</span>\n<span class=\"pl-c1\">Set kubectl context to &quot;kind-kind&quot;</span>\n<span class=\"pl-c1\">You can now use your cluster with:</span>\n\n<span class=\"pl-c1\">kubectl cluster-info --context kind-kind</span>\n\n<span class=\"pl-c1\">Not sure what to do next? =\u0005  Check out https://kind.sigs.k8s.io/docs/user/quick-start/</span></pre></div>\n<p>This will spin up a 2 node cluster inside docker, with port 5900 exposed from\nthe worker node. Then run kubedoom inside the cluster by applying the manifest\nprovided in this repository:</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>$ <span class=\"pl-s1\">kubectl apply -f manifest/</span>\n<span class=\"pl-c1\">namespace/kubedoom created</span>\n<span class=\"pl-c1\">deployment.apps/kubedoom created</span>\n<span class=\"pl-c1\">serviceaccount/kubedoom created</span>\n<span class=\"pl-c1\">clusterrolebinding.rbac.authorization.k8s.io/kubedoom created</span></pre></div>\n<p>To connect run:</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>$ <span class=\"pl-s1\">vncviewer viewer localhost:5900</span></pre></div>\n<p>Kubedoom requires a service account with permissions to list all pods and delete\nthem and uses kubectl 1.19.2.</p>\n</article></div></div>",
      "contentAsText": "\n\nThe next level of chaos engineering is here! Kill pods inside your Kubernetes\ncluster by shooting them in Doom!\nThis is a fork of the excellent\ngideonred/dockerdoomd using a\nslightly modified Doom, forked from https://github.com/gideonred/dockerdoom,\nwhich was forked from psdoom.\n\nRunning Locally\nIn order to run locally you will need to\n\nRun the kubedoom container\nAttach a VNC client to the appropriate port (5901)\n\nWith Docker\nRun storaxdev/kubedoom:0.5.0 with docker locally:\n$ docker run -p5901:5900 \\\n  --net=host \\\n  -v ~/.kube:/root/.kube \\\n  --rm -it --name kubedoom \\\n  storaxdev/kubedoom:0.5.0\nWith Podman\nRun storaxdev/kubedoom:0.5.0 with podman locally:\n$ podman run -it -p5901:5900/tcp \\\n  -v ~/.kube:/tmp/.kube --security-opt label=disable \\\n  --env \"KUBECONFIG=/tmp/.kube/config\" --name kubedoom\n  storaxdev/kubedoom:0.5.0\nAttaching a VNC Client\nNow start a VNC viewer and connect to localhost:5901. The password is idbehold:\n$ vncviewer viewer localhost:5901\nYou should now see DOOM! Now if you want to get the job done quickly enter the\ncheat idspispopd and walk through the wall on your right. You should be\ngreeted by your pods as little pink monsters. Press CTRL to fire. If the\npistol is not your thing, cheat with idkfa and press 5 for a nice surprise.\nPause the game with ESC.\nKilling namespaces\nKubedoom now also supports killing namespaces in case you have too many of\nthem. Simply set the -mode flag\nto namespaces:\n$ docker run -p5901:5900 \\\n  --net=host \\\n  -v ~/.kube:/root/.kube \\\n  --rm -it --name kubedoom \\\n  storaxdev/kubedoom:0.5.0 \\\n  -mode namespaces\nRunning Kubedoom inside Kubernetes\nSee the example in the /manifest directory. You can quickly test it using\nkind. Create a cluster with the\nexample config from this repository:\n$ kind create cluster --config kind-config.yaml\nCreating cluster \"kind\" ...\n \u0013 Ensuring node image (kindest/node:v1.19.1) =�\n \u0013 Preparing nodes =� =�\n \u0013 Writing configuration =�\n \u0013 Starting control-plane =y\u000f\n \u0013 Installing CNI =\f\n \u0013 Installing StorageClass =�\n \u0013 Joining worker nodes =�\nSet kubectl context to \"kind-kind\"\nYou can now use your cluster with:\n\nkubectl cluster-info --context kind-kind\n\nNot sure what to do next? =\u0005  Check out https://kind.sigs.k8s.io/docs/user/quick-start/\nThis will spin up a 2 node cluster inside docker, with port 5900 exposed from\nthe worker node. Then run kubedoom inside the cluster by applying the manifest\nprovided in this repository:\n$ kubectl apply -f manifest/\nnamespace/kubedoom created\ndeployment.apps/kubedoom created\nserviceaccount/kubedoom created\nclusterrolebinding.rbac.authorization.k8s.io/kubedoom created\nTo connect run:\n$ vncviewer viewer localhost:5900\nKubedoom requires a service account with permissions to list all pods and delete\nthem and uses kubectl 1.19.2.\n",
      "description": "Kill Kubernetes pods by playing Id's DOOM! Contribute to storax/kubedoom development by creating an account on GitHub.",
      "ogDescription": "Kill Kubernetes pods by playing Id's DOOM! Contribute to storax/kubedoom development by creating an account on GitHub."
    },
    {
      "url": "https://github.com/chris-free/networkpolicy-dns",
      "title": "chris-free/networkpolicy-dns",
      "content": "<div><div><article class=\"markdown-body entry-content container-lg\">\n\n\n<p>Not all software has a perfect security record (<em>cough</em> Wordpress). Wouldn&apos;t it be great to know that your site can&apos;t reach any external services apart from the ones you whitelist? Kubernetes doesn&apos;t any native functionality to restrict outgoing access to a set of domains, only a CIDR range; this is where this controller comes in.</p>\n<h2><a id=\"user-content-design\" class=\"anchor\" href=\"https://github.com/chris-free/networkpolicy-dns#design\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Design</h2>\n<p>This is a very simple Kubernetes controller, net-dns is deployed with RBAC rights to modify NetworkPolicy&apos;s and periodically checks for DNS changes on the whitelisted domains and if necessary updates the NetworkPolicy.</p>\n<p>So simple it uses: ~16mb docker image ~1mb RAM usage</p>\n<h2><a id=\"user-content-alternatives\" class=\"anchor\" href=\"https://github.com/chris-free/networkpolicy-dns#alternatives\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Alternatives</h2>\n<p>This is the simplest solution to the problem without a doubt <g-emoji class=\"g-emoji\">=&#xFFFD;</g-emoji>, however you might need greater control and reassurances. This is likely the case if you want to audit the network data (what is your website even doing?), or if you want to modify the connections (e.g load balancing, why would you even be reading this! <g-emoji class=\"g-emoji\">=\u0015</g-emoji>). If this is the case it is likely you will want something in-between a simple reverse proxy (e.g Tinyproxy or Squid) to a Service Mesh &#xFFFD; la Istio.</p>\n<h2><a id=\"user-content-configuration\" class=\"anchor\" href=\"https://github.com/chris-free/networkpolicy-dns#configuration\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Configuration</h2>\n<p>As you have probably figured, configuration is extremely simple.</p>\n<p>There are 3 values:</p>\n<ul>\n<li>podselector (full k8s 1.18 spec)</li>\n<li>domain list (you figured it out, the domains that are whitelisted)</li>\n<li>interval (period in seconds to check for DNS changes)</li>\n</ul>\n<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: netdns-settings\ndata:\n  settings.yml: |\n    podSelector: # Kubernetes 1.18 spec\n      matchLabels:\n        role: mysql-client\n    domain:\n      - &quot;aws.com&quot;\n      - &quot;chrisfreeman.uk&quot;\n    interval: 60 # seconds\n</code></pre>\n<p>The default path for the configmap is &quot;/configmap/settings.yml&quot;</p>\n<h2><a id=\"user-content-example\" class=\"anchor\" href=\"https://github.com/chris-free/networkpolicy-dns#example\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Example</h2>\n<p>Example resource manifest found in example-resources.yml</p>\n<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: netdns\nrules:\n- apiGroups: [&quot;networking.k8s.io&quot;]\n  resources: [&quot;networkpolicies&quot;]\n  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;, &quot;update&quot;, &quot;create&quot;]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: netdns-pods\nsubjects:\nsubjects:\n- kind: Group\n  name: system:authenticated\n  apiGroup: rbac.authorization.k8s.io\n- kind: Group\n  name: system:unauthenticated\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: netdns\n  apiGroup: rbac.authorization.k8s.io\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: netdns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: netdns\n  template:\n    metadata:\n      labels:\n        app: netdns\n    spec:\n      containers:\n      - name: proxy\n        image: chrisfy/networkpolicy-dns:0.1\n        volumeMounts:\n        - mountPath: /configmap\n          readOnly: true\n          name: settings\n      restartPolicy: Always\n      volumes:\n      - name: settings\n        configMap:\n          name: netdns-settings\n          items:\n            - key: settings.yml\n              path: settings.yml\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: netdns-settings\ndata:\n  settings.yml: |\n    podSelector:\n      matchLabels:\n        role: web-client\n    domain:\n      - &quot;aws.com&quot;\n      - &quot;chrisfreeman.uk&quot;\n    interval: 60\n</code></pre>\n<h2><a id=\"user-content-todo\" class=\"anchor\" href=\"https://github.com/chris-free/networkpolicy-dns#todo\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>TODO</h2>\n\n</article></div></div>",
      "contentAsText": "\n\n\nNot all software has a perfect security record (cough Wordpress). Wouldn't it be great to know that your site can't reach any external services apart from the ones you whitelist? Kubernetes doesn't any native functionality to restrict outgoing access to a set of domains, only a CIDR range; this is where this controller comes in.\nDesign\nThis is a very simple Kubernetes controller, net-dns is deployed with RBAC rights to modify NetworkPolicy's and periodically checks for DNS changes on the whitelisted domains and if necessary updates the NetworkPolicy.\nSo simple it uses: ~16mb docker image ~1mb RAM usage\nAlternatives\nThis is the simplest solution to the problem without a doubt =�, however you might need greater control and reassurances. This is likely the case if you want to audit the network data (what is your website even doing?), or if you want to modify the connections (e.g load balancing, why would you even be reading this! =\u0015). If this is the case it is likely you will want something in-between a simple reverse proxy (e.g Tinyproxy or Squid) to a Service Mesh � la Istio.\nConfiguration\nAs you have probably figured, configuration is extremely simple.\nThere are 3 values:\n\npodselector (full k8s 1.18 spec)\ndomain list (you figured it out, the domains that are whitelisted)\ninterval (period in seconds to check for DNS changes)\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: netdns-settings\ndata:\n  settings.yml: |\n    podSelector: # Kubernetes 1.18 spec\n      matchLabels:\n        role: mysql-client\n    domain:\n      - \"aws.com\"\n      - \"chrisfreeman.uk\"\n    interval: 60 # seconds\n\nThe default path for the configmap is \"/configmap/settings.yml\"\nExample\nExample resource manifest found in example-resources.yml\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: netdns\nrules:\n- apiGroups: [\"networking.k8s.io\"]\n  resources: [\"networkpolicies\"]\n  verbs: [\"get\", \"watch\", \"list\", \"update\", \"create\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: netdns-pods\nsubjects:\nsubjects:\n- kind: Group\n  name: system:authenticated\n  apiGroup: rbac.authorization.k8s.io\n- kind: Group\n  name: system:unauthenticated\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: netdns\n  apiGroup: rbac.authorization.k8s.io\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: netdns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: netdns\n  template:\n    metadata:\n      labels:\n        app: netdns\n    spec:\n      containers:\n      - name: proxy\n        image: chrisfy/networkpolicy-dns:0.1\n        volumeMounts:\n        - mountPath: /configmap\n          readOnly: true\n          name: settings\n      restartPolicy: Always\n      volumes:\n      - name: settings\n        configMap:\n          name: netdns-settings\n          items:\n            - key: settings.yml\n              path: settings.yml\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: netdns-settings\ndata:\n  settings.yml: |\n    podSelector:\n      matchLabels:\n        role: web-client\n    domain:\n      - \"aws.com\"\n      - \"chrisfreeman.uk\"\n    interval: 60\n\nTODO\n\n",
      "description": "Poor mans DNS to NetworkPolicy controller for Kubernetes - chris-free/networkpolicy-dns",
      "ogDescription": "Poor mans DNS to NetworkPolicy controller for Kubernetes - chris-free/networkpolicy-dns"
    },
    {
      "url": "https://github.com/sharadbhat/KubernetesPatterns",
      "title": "sharadbhat/KubernetesPatterns",
      "content": "<div></div>",
      "contentAsText": "",
      "description": "YAML and Golang implementations of common Kubernetes patterns. - sharadbhat/KubernetesPatterns",
      "ogDescription": "YAML and Golang implementations of common Kubernetes patterns. - sharadbhat/KubernetesPatterns"
    },
    {
      "url": "https://github.com/trankchung/kubeswitch",
      "title": "trankchung/kubeswitch",
      "content": "<div><div><article class=\"markdown-body entry-content container-lg\">\n<p>Kubernetes context and namespace switching with style. Inspired by <a href=\"https://github.com/sbstp/kubie\">Kubie</a> with additional features that Kubie lacks.</p>\n<h2><a id=\"user-content-prerequisites\" class=\"anchor\" href=\"https://github.com/trankchung/kubeswitch#prerequisites\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Prerequisites</h2>\n\n\n\n<p>You can download Kubeswitch from <a href=\"https://github.com/trankchung/kubeswitch/releases\">releases</a> page.</p>\n<h2><a id=\"user-content-build\" class=\"anchor\" href=\"https://github.com/trankchung/kubeswitch#build\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Build</h2>\n<p>You can build Kubeswitch by running the following commands.</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ git clone https://github.com/trankchung/kubeswitch\n$ <span class=\"pl-c1\">cd</span> kubeswitch\n$ make\n$ sudo make install</pre></div>\n<h2><a id=\"user-content-shell-completion\" class=\"anchor\" href=\"https://github.com/trankchung/kubeswitch#shell-completion\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Shell Completion</h2>\n<h3><a id=\"user-content-bash\" class=\"anchor\" href=\"https://github.com/trankchung/kubeswitch#bash\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Bash</h3>\n\n<h3><a id=\"user-content-zsh\" class=\"anchor\" href=\"https://github.com/trankchung/kubeswitch#zsh\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>ZSH</h3>\n\n<h3><a id=\"user-content-fish\" class=\"anchor\" href=\"https://github.com/trankchung/kubeswitch#fish\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Fish</h3>\n\n<h2><a id=\"user-content-usage\" class=\"anchor\" href=\"https://github.com/trankchung/kubeswitch#usage\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Usage</h2>\n\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Switching context. Use context or ctx commands.</span>\n$ kubeswitch ctx [ENTER]\n<span class=\"pl-k\">?</span> Select context. / to search:\n    aws-east1\n    aws-west1\n    k3s\n  &#xFFFD; kind\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Switching namespace. Use namespace or ns commands.</span>\n(kind<span class=\"pl-k\">|</span>default) $ kubeswitch ns [ENTER]\n<span class=\"pl-k\">?</span> Select namespace. / to search:\n    argocd\n    default\n  &#xFFFD; jenkins\n    kube-node-lease\n    kube-public\n    kube-system\n(kind<span class=\"pl-k\">|</span>jenkins) $</pre></div>\n\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Switching context. Use context or ctx commands.</span>\n$ kubeswitch ctx [TAB]\naws-east1  aws-west1   k3s  kind\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Switching namespace. Use namespace or ns commands.</span>\n(kind<span class=\"pl-k\">|</span>default) $ kubeswitch ns [TAB]\nargocd  default  jenkins  kube-node-lease  kube-public  kube-system </pre></div>\n\n<p>Using shell prompt integration will greatly help knowing which Kubernetes context and namespace you&apos;re currently interacting with.</p>\n<h2><a id=\"user-content-bash-and-zsh\" class=\"anchor\" href=\"https://github.com/trankchung/kubeswitch#bash-and-zsh\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Bash and ZSH</h2>\n\n<h2><a id=\"user-content-fish-1\" class=\"anchor\" href=\"https://github.com/trankchung/kubeswitch#fish-1\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Fish</h2>\n\n</article></div></div><hr><h4>Page 2</h4><div><div><article class=\"markdown-body entry-content container-lg\">\n<p>Kubernetes context and namespace switching with style. Inspired by <a href=\"https://github.com/sbstp/kubie\">Kubie</a> with additional features that Kubie lacks.</p>\n<h2><a id=\"user-content-prerequisites\" class=\"anchor\" href=\"#prerequisites\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Prerequisites</h2>\n\n\n\n<p>You can download Kubeswitch from <a href=\"https://github.com/trankchung/kubeswitch/releases\">releases</a> page.</p>\n<h2><a id=\"user-content-build\" class=\"anchor\" href=\"#build\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Build</h2>\n<p>You can build Kubeswitch by running the following commands.</p>\n<div class=\"highlight highlight-source-shell\"><pre>$ git clone https://github.com/trankchung/kubeswitch\n$ <span class=\"pl-c1\">cd</span> kubeswitch\n$ make\n$ sudo make install</pre></div>\n<h2><a id=\"user-content-shell-completion\" class=\"anchor\" href=\"#shell-completion\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Shell Completion</h2>\n<h3><a id=\"user-content-bash\" class=\"anchor\" href=\"#bash\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Bash</h3>\n\n<h3><a id=\"user-content-zsh\" class=\"anchor\" href=\"#zsh\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>ZSH</h3>\n\n<h3><a id=\"user-content-fish\" class=\"anchor\" href=\"#fish\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Fish</h3>\n\n<h2><a id=\"user-content-usage\" class=\"anchor\" href=\"#usage\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Usage</h2>\n\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Switching context. Use context or ctx commands.</span>\n$ kubeswitch ctx [ENTER]\n<span class=\"pl-k\">?</span> Select context. / to search:\n    aws-east1\n    aws-west1\n    k3s\n  &#x25B8; kind\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Switching namespace. Use namespace or ns commands.</span>\n(kind<span class=\"pl-k\">|</span>default) $ kubeswitch ns [ENTER]\n<span class=\"pl-k\">?</span> Select namespace. / to search:\n    argocd\n    default\n  &#x25B8; jenkins\n    kube-node-lease\n    kube-public\n    kube-system\n(kind<span class=\"pl-k\">|</span>jenkins) $</pre></div>\n\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Switching context. Use context or ctx commands.</span>\n$ kubeswitch ctx [TAB]\naws-east1  aws-west1   k3s  kind\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Switching namespace. Use namespace or ns commands.</span>\n(kind<span class=\"pl-k\">|</span>default) $ kubeswitch ns [TAB]\nargocd  default  jenkins  kube-node-lease  kube-public  kube-system </pre></div>\n\n<p>Using shell prompt integration will greatly help knowing which Kubernetes context and namespace you&apos;re currently interacting with.</p>\n<h2><a id=\"user-content-bash-and-zsh\" class=\"anchor\" href=\"#bash-and-zsh\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Bash and ZSH</h2>\n\n<h2><a id=\"user-content-fish-1\" class=\"anchor\" href=\"#fish-1\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Fish</h2>\n\n</article></div></div>",
      "contentAsText": "\nKubernetes context and namespace switching with style. Inspired by Kubie with additional features that Kubie lacks.\nPrerequisites\n\n\n\nYou can download Kubeswitch from releases page.\nBuild\nYou can build Kubeswitch by running the following commands.\n$ git clone https://github.com/trankchung/kubeswitch\n$ cd kubeswitch\n$ make\n$ sudo make install\nShell Completion\nBash\n\nZSH\n\nFish\n\nUsage\n\n# Switching context. Use context or ctx commands.\n$ kubeswitch ctx [ENTER]\n? Select context. / to search:\n    aws-east1\n    aws-west1\n    k3s\n  � kind\n\n# Switching namespace. Use namespace or ns commands.\n(kind|default) $ kubeswitch ns [ENTER]\n? Select namespace. / to search:\n    argocd\n    default\n  � jenkins\n    kube-node-lease\n    kube-public\n    kube-system\n(kind|jenkins) $\n\n# Switching context. Use context or ctx commands.\n$ kubeswitch ctx [TAB]\naws-east1  aws-west1   k3s  kind\n\n# Switching namespace. Use namespace or ns commands.\n(kind|default) $ kubeswitch ns [TAB]\nargocd  default  jenkins  kube-node-lease  kube-public  kube-system \n\nUsing shell prompt integration will greatly help knowing which Kubernetes context and namespace you're currently interacting with.\nBash and ZSH\n\nFish\n\nPage 2\nKubernetes context and namespace switching with style. Inspired by Kubie with additional features that Kubie lacks.\nPrerequisites\n\n\n\nYou can download Kubeswitch from releases page.\nBuild\nYou can build Kubeswitch by running the following commands.\n$ git clone https://github.com/trankchung/kubeswitch\n$ cd kubeswitch\n$ make\n$ sudo make install\nShell Completion\nBash\n\nZSH\n\nFish\n\nUsage\n\n# Switching context. Use context or ctx commands.\n$ kubeswitch ctx [ENTER]\n? Select context. / to search:\n    aws-east1\n    aws-west1\n    k3s\n  ▸ kind\n\n# Switching namespace. Use namespace or ns commands.\n(kind|default) $ kubeswitch ns [ENTER]\n? Select namespace. / to search:\n    argocd\n    default\n  ▸ jenkins\n    kube-node-lease\n    kube-public\n    kube-system\n(kind|jenkins) $\n\n# Switching context. Use context or ctx commands.\n$ kubeswitch ctx [TAB]\naws-east1  aws-west1   k3s  kind\n\n# Switching namespace. Use namespace or ns commands.\n(kind|default) $ kubeswitch ns [TAB]\nargocd  default  jenkins  kube-node-lease  kube-public  kube-system \n\nUsing shell prompt integration will greatly help knowing which Kubernetes context and namespace you're currently interacting with.\nBash and ZSH\n\nFish\n\n",
      "description": "Kubernetes context and namspace switcher. Contribute to trankchung/kubeswitch development by creating an account on GitHub.",
      "ogDescription": "Kubernetes context and namspace switcher. Contribute to trankchung/kubeswitch development by creating an account on GitHub."
    },
    {
      "url": "https://medium.com/@ashokponkumar/introducing-konveyor-move2kube-f3b28e78cd22",
      "title": "Introducing Konveyor Move2Kube",
      "content": "<div><article class=\"meteredContent\"><section class=\"dq dr ds dt w du bm s\"></section><div><section class=\"db eb ec cw ed\"><div class=\"n p\"><div class=\"ag ah ai aj ak ee am w\"><p id=\"a22e\" class=\"gb gc eg gd b ge gf gg gh gi gj gk gl gm gn go gp gq gr gs gt gu db fc\">Are you deploying your applications in Cloud Foundry or Docker swarm/compose and would like to try out Kubernetes, but not sure where to start? <a href=\"https://github.com/konveyor/move2kube\" class=\"dg gv\">Konveyor Move2Kube</a> is here to give you a headstart.</p><p id=\"a184\" class=\"gb gc eg gd b ge gw gf gg gh gx gi gj gk gy gl gm gn gz go gp gq ha gr gs gu db fc\">You application is yet to be containerized? Konveyor Move2Kube can help accelerate your journey.</p></div></div></section><section class=\"db eb ec cw ed\"><div class=\"n p\"><div class=\"ag ah ai aj ak ee am w\"><p id=\"22ed\" class=\"gb gc eg gd b ge gw gf gg gh gx gi gj gk gy gl gm gn gz go gp gq ha gr gs gu db fc\">Move2Kube was conceived in IBM Research to help accelrate the replatforming journey to Kubernetes. One of the challenges in moving to Kubernetes is understanding its various concepts and creating the optimal deployment artifacts. It requires a good understanding of Kubernetes concepts, and the right mapping from the source platform. Move2Kube helps solve this problem by using an opinionated approach and automating the process, while involving you for critical information.</p><p id=\"0703\" class=\"gb gc eg gd b ge gw gf gg gh gx gi gj gk gy gl gm gn gz go gp gq ha gr gs gu db fc\">Move2Kube also helps you customize your configurations for the specific cluster type you are planning to deploy. May it be Openshift or any flavor of Kubernetes, Move2Kube looks at the specific features supported by that cluster type, and tailors the artifacts for that cluster type.</p><p id=\"8dcc\" class=\"gb gc eg gd b ge gw gf gg gh gx gi gj gk gy gl gm gn gz go gp gq ha gr gs gu db fc\">When it comes to replatforming your application to Kubernetes there are a few crucial steps:</p><ol class><li id=\"43ad\" class=\"gb gc eg gd b ge gw gf gg gh gx gi gj gk gy gl gm gn gz go gp gq ha gr gs gu hi hj hk fc\">Containerization</li><li id=\"7877\" class=\"gb gc eg gd b ge hl gf gg gh hm gi gj gk hn gl gm gn ho go gp gq hp gr gs gu hi hj hk fc\">Creation of deployment artifacts</li><li id=\"69e6\" class=\"gb gc eg gd b ge hl gf gg gh hm gi gj gk hn gl gm gn ho go gp gq hp gr gs gu hi hj hk fc\">Incorporate best practices in deployment artifacts</li><li id=\"1170\" class=\"gb gc eg gd b ge hl gf gg gh hm gi gj gk hn gl gm gn ho go gp gq hp gr gs gu hi hj hk fc\">Customizing for a specific cluster</li></ol><p id=\"16da\" class=\"gb gc eg gd b ge gw gf gg gh gx gi gj gk gy gl gm gn gz go gp gq ha gr gs gu db fc\">Move2Kube helps users in all the above steps by looking at the source artifacts, runtime artifacts, analysing, correlating them and coming up with a plan for transformation. It then involves you in an interactive journey to create the right artifacts for the application. We will look into how Move2Kube handles the above steps in detail in subsequent related blogs.</p><p id=\"090f\" class=\"gb gc eg gd b ge gw gf gg gh gx gi gj gk gy gl gm gn gz go gp gq ha gr gs gu db fc\">Here is a quick preview of them:</p><p id=\"4e4d\" class=\"gb gc eg gd b ge gw gf gg gh gx gi gj gk gy gl gm gn gz go gp gq ha gr gs gu db fc\"><strong class=\"gd co\">Containerization:</strong></p><p id=\"20dd\" class=\"gb gc eg gd b ge gw gf gg gh gx gi gj gk gy gl gm gn gz go gp gq ha gr gs gu db fc\">Move2Kube can containerize the application using the below techniques.</p><p id=\"f1a7\" class=\"gb gc eg gd b ge gw gf gg gh gx gi gj gk gy gl gm gn gz go gp gq ha gr gs gu db fc\"><strong class=\"gd co\">Creation of deployment artifacts:</strong></p><p id=\"1d47\" class=\"gb gc eg gd b ge gw gf gg gh gx gi gj gk gy gl gm gn gz go gp gq ha gr gs gu db fc\">Move2Kube can create various deployment artifacts that are required for deploying applications to Kubernetes.</p><ol class><li id=\"ce82\" class=\"gb gc eg gd b ge gw gf gg gh gx gi gj gk gy gl gm gn gz go gp gq ha gr gs gu hi hj hk fc\">Deployment (Deploymentconfig, Deployment, ..)</li><li id=\"3afa\" class=\"gb gc eg gd b ge hl gf gg gh hm gi gj gk hn gl gm gn ho go gp gq hp gr gs gu hi hj hk fc\">Service</li><li id=\"444f\" class=\"gb gc eg gd b ge hl gf gg gh hm gi gj gk hn gl gm gn ho go gp gq hp gr gs gu hi hj hk fc\">Ingress (Route, ..)</li><li id=\"64f8\" class=\"gb gc eg gd b ge hl gf gg gh hm gi gj gk hn gl gm gn ho go gp gq hp gr gs gu hi hj hk fc\">NetworkPolicy (based on docker compose network)</li><li id=\"ab2d\" class=\"gb gc eg gd b ge hl gf gg gh hm gi gj gk hn gl gm gn ho go gp gq hp gr gs gu hi hj hk fc\">Storage (PVC, Secret, ConfigMap)</li><li id=\"122d\" class=\"gb gc eg gd b ge hl gf gg gh hm gi gj gk hn gl gm gn ho go gp gq hp gr gs gu hi hj hk fc\">Knative Service</li></ol><p id=\"35d5\" class=\"gb gc eg gd b ge gw gf gg gh gx gi gj gk gy gl gm gn gz go gp gq ha gr gs gu db fc\"><strong class=\"gd co\">Incorporate best practices in deployment artifacts:</strong></p><p id=\"c9e2\" class=\"gb gc eg gd b ge gw gf gg gh gx gi gj gk gy gl gm gn gz go gp gq ha gr gs gu db fc\">Move2Kube can incorporate best practices in the deployment artifacts depending on an opinionated design, like enforcing minimum number of replicas.</p><p id=\"c23f\" class=\"gb gc eg gd b ge gw gf gg gh gx gi gj gk gy gl gm gn gz go gp gq ha gr gs gu db fc\"><strong class=\"gd co\">Customizing for a specific cluster:</strong></p><p id=\"49e6\" class=\"gb gc eg gd b ge gw gf gg gh gx gi gj gk gy gl gm gn gz go gp gq ha gr gs gu db fc\">Move2Kube can customize the artifacts for a specific cluster in an opinionated manner. For example, if the target cluster is a Openshift cluster, it will generate DeploymentConfig instead of Deployment. Also based on the specific version of the api supported by the cluster, Move2Kube can generate the yamls for the specific Kind Group Versions.</p><p id=\"0aac\" class=\"gb gc eg gd b ge gw gf gg gh gx gi gj gk gy gl gm gn gz go gp gq ha gr gs gu db fc\">What more? Move2Kube code is completely extensible, and more features can be easily added to Move2Kube by leveraging the Move2Kube framework.</p></div></div></section><section class=\"db eb ec cw ed\"><div class=\"n p\"><div class=\"ag ah ai aj ak ee am w\"><p id=\"1432\" class=\"gb gc eg gd b ge im gf gg gh in gi gj gk io gl gm gn ip go gp gq iq gr gs gu db fc\">Move2Kube takes as input the source artifacts and outputs the target deployment artifacts.</p><p id=\"2aca\" class=\"gb gc eg gd b ge gw gf gg gh gx gi gj gk gy gl gm gn gz go gp gq ha gr gs gu db fc\">Move2Kube accomplishes the above activities using a 3 step approach of</p><ol class><li id=\"0e0f\" class=\"gb gc eg gd b ge gw gf gg gh gx gi gj gk gy gl gm gn gz go gp gq ha gr gs gu hi hj hk fc\"><em class=\"ir\">Collect</em> : If runtime inspection is required, <code class=\"is it iu iv iw b\">move2kube collect</code> will analyse your runtime environment such as cloud foundry or kubernetes, extract the required metadata and output them as yaml files in <code class=\"is it iu iv iw b\">m2k_collect</code> folder.</li><li id=\"5a0f\" class=\"gb gc eg gd b ge hl gf gg gh hm gi gj gk hn gl gm gn ho go gp gq hp gr gs gu hi hj hk fc\"><em class=\"ir\">Plan</em> : If you need fine granular control over the services detected in the source artifacts and runtime inspection, <code class=\"is it iu iv iw b\">move2kube plan -s &lt;sourcefolder&gt;</code>will allow you to create a <code class=\"is it iu iv iw b\">m2k.plan</code> yaml file, with information about the services it detected and the various ways it can transform it. You can curate this file before proceeding to <code class=\"is it iu iv iw b\">translate</code> step.</li><li id=\"59b0\" class=\"gb gc eg gd b ge hl gf gg gh hm gi gj gk hn gl gm gn ho go gp gq hp gr gs gu hi hj hk fc\"><em class=\"ir\">Translate</em> : <code class=\"is it iu iv iw b\">move2kube translate</code> translates the artifacts into target deployment artifacts based on the <code class=\"is it iu iv iw b\">m2k.plan</code> you have curated.</li></ol><p id=\"657b\" class=\"gb gc eg gd b ge gw gf gg gh gx gi gj gk gy gl gm gn gz go gp gq ha gr gs gu db fc\">At the end of the translate step, you could potentially have the following artifacts:</p><ol class><li id=\"4552\" class=\"gb gc eg gd b ge gw gf gg gh gx gi gj gk gy gl gm gn gz go gp gq ha gr gs gu hi hj hk fc\"><em class=\"ir\">Helm Chart/Kubernetes/Knative yamls</em></li><li id=\"dfa6\" class=\"gb gc eg gd b ge hl gf gg gh hm gi gj gk hn gl gm gn ho go gp gq hp gr gs gu hi hj hk fc\"><em class=\"ir\">Containerization scripts + Docker compose file</em></li><li id=\"68a3\" class=\"gb gc eg gd b ge hl gf gg gh hm gi gj gk hn gl gm gn ho go gp gq hp gr gs gu hi hj hk fc\"><em class=\"ir\">Operator Stub</em></li></ol><p id=\"a028\" class=\"gb gc eg gd b ge gw gf gg gh gx gi gj gk gy gl gm gn gz go gp gq ha gr gs gu db fc\">Depending on the complexity of your application you can either choose to follow the 3 steps, or just go with a much simplified one step process.</p><p id=\"b462\" class=\"gb gc eg gd b ge gw gf gg gh gx gi gj gk gy gl gm gn gz go gp gq ha gr gs gu db fc\">Once you have the <a href=\"https://github.com/konveyor/move2kube/releases\" class=\"dg gv\">move2kube binary</a> in your path, all you need to do is to point Move2Kube to that folder using :</p><blockquote class=\"ix iy iz\"><p id=\"3ac4\" class=\"gb gc ir gd b ge gw gf gg gh gx gi gj gk gy gl gm gn gz go gp gq ha gr gs gu db fc\">move2kube translate -s &lt;PathToSource&gt;</p></blockquote><p id=\"12ec\" class=\"gb gc eg gd b ge gw gf gg gh gx gi gj gk gy gl gm gn gz go gp gq ha gr gs gu db fc\">We are just starting and are looking for feedback as we improve Move2Kube along with our partners at <a href=\"https://github.com/konveyor\" class=\"dg gv\">RedHat Konveyor</a> project. Do join us in the journey by participating in <a href=\"https://github.com/konveyor/move2kube/issues\" class=\"dg gv\">github issues</a>, <a href=\"https://kubernetes.slack.com/archives/CR85S82A2\" class=\"dg gv\">slack</a>, creating <a href=\"https://github.com/konveyor/move2kube/pulls\" class=\"dg gv\">pull requests</a> and star the project if you find the project useful.</p><p id=\"3e92\" class=\"gb gc eg gd b ge gw gf gg gh gx gi gj gk gy gl gm gn gz go gp gq ha gr gs gu db fc\">If you are interested to contribute to the repository as part of <a href=\"https://hacktoberfest.digitalocean.com/\" class=\"dg gv\">Hacktoberfest</a>, <a href=\"https://github.com/konveyor/move2kube/issues?q=is%3Aissue+is%3Aopen+label%3Ahacktoberfest\" class=\"dg gv\">here</a> are some issues (other move2kube repos too have some) that you can fix in short time.</p><p id=\"d4d4\" class=\"gb gc eg gd b ge gw gf gg gh gx gi gj gk gy gl gm gn gz go gp gq ha gr gs gu db fc\">We are working on making Move2Kube even more consumable, stay tuned&amp;</p></div></div></section></div></article></div>",
      "contentAsText": "Are you deploying your applications in Cloud Foundry or Docker swarm/compose and would like to try out Kubernetes, but not sure where to start? Konveyor Move2Kube is here to give you a headstart.You application is yet to be containerized? Konveyor Move2Kube can help accelerate your journey.Move2Kube was conceived in IBM Research to help accelrate the replatforming journey to Kubernetes. One of the challenges in moving to Kubernetes is understanding its various concepts and creating the optimal deployment artifacts. It requires a good understanding of Kubernetes concepts, and the right mapping from the source platform. Move2Kube helps solve this problem by using an opinionated approach and automating the process, while involving you for critical information.Move2Kube also helps you customize your configurations for the specific cluster type you are planning to deploy. May it be Openshift or any flavor of Kubernetes, Move2Kube looks at the specific features supported by that cluster type, and tailors the artifacts for that cluster type.When it comes to replatforming your application to Kubernetes there are a few crucial steps:ContainerizationCreation of deployment artifactsIncorporate best practices in deployment artifactsCustomizing for a specific clusterMove2Kube helps users in all the above steps by looking at the source artifacts, runtime artifacts, analysing, correlating them and coming up with a plan for transformation. It then involves you in an interactive journey to create the right artifacts for the application. We will look into how Move2Kube handles the above steps in detail in subsequent related blogs.Here is a quick preview of them:Containerization:Move2Kube can containerize the application using the below techniques.Creation of deployment artifacts:Move2Kube can create various deployment artifacts that are required for deploying applications to Kubernetes.Deployment (Deploymentconfig, Deployment, ..)ServiceIngress (Route, ..)NetworkPolicy (based on docker compose network)Storage (PVC, Secret, ConfigMap)Knative ServiceIncorporate best practices in deployment artifacts:Move2Kube can incorporate best practices in the deployment artifacts depending on an opinionated design, like enforcing minimum number of replicas.Customizing for a specific cluster:Move2Kube can customize the artifacts for a specific cluster in an opinionated manner. For example, if the target cluster is a Openshift cluster, it will generate DeploymentConfig instead of Deployment. Also based on the specific version of the api supported by the cluster, Move2Kube can generate the yamls for the specific Kind Group Versions.What more? Move2Kube code is completely extensible, and more features can be easily added to Move2Kube by leveraging the Move2Kube framework.Move2Kube takes as input the source artifacts and outputs the target deployment artifacts.Move2Kube accomplishes the above activities using a 3 step approach ofCollect : If runtime inspection is required, move2kube collect will analyse your runtime environment such as cloud foundry or kubernetes, extract the required metadata and output them as yaml files in m2k_collect folder.Plan : If you need fine granular control over the services detected in the source artifacts and runtime inspection, move2kube plan -s <sourcefolder>will allow you to create a m2k.plan yaml file, with information about the services it detected and the various ways it can transform it. You can curate this file before proceeding to translate step.Translate : move2kube translate translates the artifacts into target deployment artifacts based on the m2k.plan you have curated.At the end of the translate step, you could potentially have the following artifacts:Helm Chart/Kubernetes/Knative yamlsContainerization scripts + Docker compose fileOperator StubDepending on the complexity of your application you can either choose to follow the 3 steps, or just go with a much simplified one step process.Once you have the move2kube binary in your path, all you need to do is to point Move2Kube to that folder using :move2kube translate -s <PathToSource>We are just starting and are looking for feedback as we improve Move2Kube along with our partners at RedHat Konveyor project. Do join us in the journey by participating in github issues, slack, creating pull requests and star the project if you find the project useful.If you are interested to contribute to the repository as part of Hacktoberfest, here are some issues (other move2kube repos too have some) that you can fix in short time.We are working on making Move2Kube even more consumable, stay tuned&",
      "publishedDate": "2020-10-02T10:46:22.290Z",
      "description": "Are you deploying your applications in Cloud Foundry or Docker swarm/compose and would like to try out Kubernetes, but not sure where to start? Konveyor Move2Kube is here to give you a headstart…",
      "ogDescription": "Want to deploy your application in Kubernetes, but not sure where to start? Konveyor Move2Kube is here to give you a headstart."
    },
    {
      "url": "https://github.com/didier-durand/knative-on-cloud-kubernetes",
      "title": "didier-durand/knative-on-cloud-kubernetes",
      "content": "<div><div><article class=\"markdown-body entry-content container-lg\">\n<p><a href=\"https://github.com/didier-durand/knative-on-cloud-kubernetes/blob/master/img/knative-k8s-istio-logos.jpg\"><img src=\"https://github.com/didier-durand/knative-on-cloud-kubernetes/raw/master/img/knative-k8s-istio-logos.jpg\"></a></p>\n<p><a href=\"https://github.com/didier-durand/knative-on-cloud-kubernetes/workflows/Deploy%20Knative%20on%20GKE/badge.svg\"><img src=\"https://github.com/didier-durand/knative-on-cloud-kubernetes/workflows/Deploy%20Knative%20on%20GKE/badge.svg\" alt=\"workflow badge\"></a>\n<a href=\"https://github.com/didier-durand/knative-on-cloud-kubernetes/workflows/Deploy%20Knative%20on%20EKS/badge.svg\"><img src=\"https://github.com/didier-durand/knative-on-cloud-kubernetes/workflows/Deploy%20Knative%20on%20EKS/badge.svg\" alt=\"workflow badge\"></a></p>\n<p>This repository can be re-used in subsequent projects as the initial stage of a live test bed to leverage the feaures of\n<a href=\"https://knative.dev/\">Knative project</a> (sponsored by Google) for serverless workloads: it is implemented on Google Cloud Platform (GCP) and Amazon Web Services (AWS)\nvia a fully automated Github workflows (see files <a href=\"https://github.com/didier-durand/knative-on-cloud-kubernetes/blob/master/.github/workflows/gcloud-gke-knative.yml\">gcloud-gke-knative.yml</a>\nand <a href=\"https://github.com/didier-durand/knative-on-cloud-kubernetes/blob/master/.github/workflows/aws-eks-knative.yml\">aws-eks-knative.yml</a>)</p>\n<p>This workflow creates a standard Kubernetes cluster on the cloud (either <a href=\"https://cloud.google.com/kubernetes-engine\">Google Kubernetes Engine - GKE</a>\nor <a href=\"https://aws.amazon.com/eks/\">Amazon EKS</a>). When the cluster is up, the workflow deploys the required Knative components and dependencies. Then,\nit rolls out a couple of Knative mock-up Docker services, validates their proper unitary functioning. Finally, it checks the scalability feature of Knative before termination via deletion\nof the deployed services and of the cluster.</p>\n<p>As per <a href=\"https://knative.dev/docs/\">Knative documentation</a>: &quot;<em>Knative extends Kubernetes to provide a set of middleware components that are\nessential to <strong>build modern, source-centric, and container-based applications that can run anywhere: on premises, in the cloud, or even in a\nthird-party data center</strong>. Each of the components under the Knative project attempt to identify common patterns and codify the best practices\nthat are shared by successful, real-world, Kubernetes-based frameworks and applications.</em>&quot;.</p>\n<p>So, two different containers are deployed on Knative: their proper execution is validated. The <em><a href=\"https://knative.dev/v0.16-docs/serving/autoscaling/autoscale-go/\">autoscale-go</a></em>\nimage allows requests with parameters to consume more or less cpu and  memory check via query specific query parameters while\n<em><a href=\"https://knative.dev/v0.17-docs/serving/samples/hello-world/helloworld-go/\">helloworld-go</a></em> Docker image is limited to a unique and trivial\nresponse. We deploy helloworld-go with official <a href=\"https://github.com/knative/client\">Knative client (kn)</a> with minimal command-line options\nto demonstrate as easy as it can get for the developer: <em>&apos;kn service create &lt;SERVICE_NAME&gt; --image-nname &lt;IMAGE_NAME&gt; --env &lt;ENV_VARS_TO_SET&gt;&apos;</em>.\nBut, a rich set of deployment options is provided as per <a href=\"https://github.com/knative/client/blob/master/docs/cmd/kn_service_create.md\">reference documentation</a>.\nThe autoscale-go service is deployed via a YAML file describing a Knative service.</p>\n<p>The outcome of the recurring executions of the workflow and all the messages produced by those runs can be checked in the <a href=\"https://github.com/didier-durand/knative-on-cloud-kubernetes-private/actions\">Actions tab</a>\nof this repository. Also, the logs of any execution can be downloaded as text files via &quot;download log archive&quot; of the job dashboard for\nfurther analysis. The workflow is executed at least weekly via Github&apos;s cron to make sure that it remains fully operational.</p>\n<p><a href=\"https://cloud.google.com/run\">Google Cloud Run</a> is Google&apos;s own implementation of the Knative stack (<a href=\"https://ahmet.im/blog/cloud-run-is-a-knative/\">with some limitations</a>),\nintegrated with other services of GCP. But, this workflow demonstrates how to implement Knative on a raw Kubernetes cluster when a fully controlled\nimplementation is desired.</p>\n<p>(Current version of this repository is limited to implementation Knative Serving)</p>\n<h2><a id=\"user-content-why-knative-\" class=\"anchor\" href=\"https://github.com/didier-durand/knative-on-cloud-kubernetes#why-knative-\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Why Knative ?</h2>\n<p>As said above, Knative, OSS project sponsored by Google is implemented on GCP as service <a href=\"https://cloud.google.com/run\">Cloud Run</a> and integrated\nwith the other services (logging, monitoring, iam, etc.) of the platform. It is very easy to use: check out &quot;Deploy to Cloud Run&quot; of our other\nrepository collating GitHub-based CI/CD workflows.</p>\n<p><a href=\"https://github.com/didier-durand/knative-on-cloud-kubernetes/blob/master/img/Knative-Serving-architecture.jpg\"><img src=\"https://github.com/didier-durand/knative-on-cloud-kubernetes/raw/master/img/Knative-Serving-architecture.jpg\"></a></p>\n<p>But, if your workloads run elsewhere (on premise, other public cloud, etc.) or if you desire to avoid lock-in with a specific vendor and you still\nwant to enjoy the benefits of serverless: the Serving block (see its archiecture in Figure 1 - source: <a href=\"https://www.researchgate.net/publication/336567672_Towards_Serverless_as_Commodity_a_case_of_Knative\">N. Kaviani &amp; al</a>)\nof Knative delivers horizontal scalability, resiliency and deep observability for your application services by leveraging those same core\ncharacteristics of Kubernetes. But, it abstracts them through a <a href=\"https://github.com/knative/client/blob/master/docs/cmd/kn_service.md\">CLI named &quot;kn&quot;</a>.\nSeveral versions of the same service can be active simultaneously and Knative can organize fractional routing between them to allow incremental\nrollout of new versions of the application.</p>\n<p>Via the setup of <a href=\"https://prometheus.io/\">Prometheus</a>, Knative can deliver exhaustive metrics, accessible through <a href=\"https://grafana.com/\">Grafana</a>.\nAdditionally, Knative implements <a href=\"https://www.fluentd.org/\">fluentd</a>to collect logs, make them accessible via <a href=\"https://www.elastic.co/kibana\">Kibana</a>\nor centralize them in a global logger like <a href=\"https://cloud.google.com/logging\">Google Cloud Logging</a>. Also, traces of dialog between the service\nand its clients can also be collected via the setup of either <a href=\"https://zipkin.io/\">Zipkin</a> or <a href=\"https://www.jaegertracing.io/\">Jaeger</a>. So, added\nto the use of <a href=\"https://istio.io/\">Istio</a> in the core implementation of Knative, the full spectrum of observability is thoroughly covered\nto allow high QoS at scale.</p>\n<p>Knative is solely focused on those serverless workloads, which are &quot;autonomous&quot;: they rely on single and independent containers to deliver their\nservice.  Clearly, it doesn&apos;t cover the full spectrum of large-scale and sophisticated applications relying on additional services in\nother containers to run properly. But, for well-designed applications, a fair amount of their services can usually be implemented through Knative\nto obtain optimal efficiency: they will benefit from the advantages (again scalability, resiliency, etc.) of the core underlying Kubernetes\ninfrastructure, used by the other parts of the application, without the need to develop the sophisticated configuration required to be a good K8s\ncitizen.</p>\n<h2><a id=\"user-content-autoscaling-with-knative\" class=\"anchor\" href=\"https://github.com/didier-durand/knative-on-cloud-kubernetes#autoscaling-with-knative\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Autoscaling with Knative</h2>\n<p>Section &quot;Scalability test&quot; below demonstrates the automated scaling delivered as a core feature of Knative: the  autoscaing algorithm is detailed\non <a href=\"https://knative.dev/v0.15-docs/serving/samples/autoscale-go/#algorithm\">Knative Autoscaling page</a> and <a href=\"https://github.com/knative/serving/blob/master/docs/scaling/SYSTEM.md\">Knative Serving Autoscaling System</a>.\nThe initial <em>&apos;kubectl get pods -n default&apos;</em>\ndemonstrates that the workload containers for autoscale-go was scaled down to zero due to initial inactivity.</p>\n<p>After 90s of http traffic maintaining 350 requests (as per <a href=\"https://github.com/rakyll/hey\">hey</a> command line) in parallel, the final <em>&apos;kubectl get pods -n default&apos;</em> shows that\n5 autoscale-go additional pods (for a total of 6) autoscale-go pods were launched by Knative to sustain the demand because autoscale-go can\nbe parametrized to generate resource-demanding workloads (here biggest prime number under 10&apos;000 and 5 MB memory consumption).</p>\n<p>In comparison, 200k requests to helloworld-go are then executed again with hey. The throughput gets much higher (1&apos;420 qps) because helloworld-go\nresponses don&apos;t require much resources at all.  They last 390 seconds in total with an average throughput\nof 1&apos;280 requests per second. For that purpose, as per final <em>&apos;kubectl get pods -n default&apos;</em>, Knative didn&apos;t scale up at all: a single pod is sufficient.</p>\n<p><strong>NB</strong>: for the histogram of response times, hey measures the full round-trip time between the Github CI/CD platform on Microsoft Azure datacenters\nand Google GCP datacenters because hey is run on GitHub while Knative is hosted on GKE.</p>\n<h2><a id=\"user-content-steps-of-github-workflow\" class=\"anchor\" href=\"https://github.com/didier-durand/knative-on-cloud-kubernetes#steps-of-github-workflow\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Steps of Github workflow</h2>\n<ol>\n<li>checkout the project from git repository in CI/CD runner provided by Github</li>\n<li>setup the gcloud SDK with proper credentials to interact with GCP services</li>\n<li>get gcloud and info version (if needed for debugging)</li>\n<li>cleanup existing GKE cluster of previous run (if any)</li>\n<li>create fresh GKE cluster with default add-ons for this run. The additional parameters allow cluster autoscaling between 1 and 10 nodes of instance type <a href=\"https://cloud.google.com/compute/docs/machine-types\">n1-standard-4</a>.</li>\n<li><em>&apos;gcloud container clusters get-credentials&apos;</em> updates kubeconfig file with appropriate credentials and endpoint information to make kubectl usable with the newly created cluster.</li>\n<li>install Istio operator (an implementation of <a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/\">standard K8s Custom Resource Definition (CRD)feature</a>. This install is done via istioctl, whose nightly build is downloaded on the fly to proceed. We check - via kubectl + grep - the presence of expected services and pods.</li>\n<li>Install the K8s CRD for Knative Serving feature block</li>\n<li>Install the core components for Knative Serving feature block</li>\n<li>Install the Istio controller for Knative Serving feature block</li>\n<li>Check if full setup of GKE + Istio + Knative is working properly. We check - via kubectl + grep - the presence of expected services and pods.</li>\n<li>Install a test Go workload coming from <a href=\"https://knative.dev/docs/serving/getting-started-knative-app/\">Knative tutorial</a></li>\n<li>Deploy the tutum/hello-world image as a Knative workload and use an http request to validate its proper functioning</li>\n<li>Delete the cluster and all its resources created for this execution</li>\n</ol>\n<h2><a id=\"user-content-setup-for-forks\" class=\"anchor\" href=\"https://github.com/didier-durand/knative-on-cloud-kubernetes#setup-for-forks\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Setup for forks</h2>\n<p>To fork and run this project in your own Github account, you only need to:</p>\n<ul>\n<li><strong>GCP</strong>: Create a test project in your GCP account and define it as a Github secret named ${{ secrets.GCP_PROJECT }} in workflow YAML. Define a service account in GCP IAM with Project Owner role (to make security definitions simpler), download its secret key and define the value of a Github\nsecret named ${{ secrets.GCP_SA_KEY }} with the downloaded json.</li>\n<li><strong>AWS</strong>: Define a user in AWS IAM with full admin rights (to make security definitions simpler), download its access key and secret key to define 2 secrets ${{ secrets.AWS_ACCESS_KEY_ID }} and ${{ secrets.AWS_SECRET_KEY }}. Finally, define the region that you want to work in as ${{ secrets.AWS_REGION }}</li>\n</ul>\n<h2><a id=\"user-content-scalability-test\" class=\"anchor\" href=\"https://github.com/didier-durand/knative-on-cloud-kubernetes#scalability-test\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Scalability test</h2>\n<p>See above for some comments</p>\n<h3><a id=\"user-content-results-with-autoscale-go\" class=\"anchor\" href=\"https://github.com/didier-durand/knative-on-cloud-kubernetes#results-with-autoscale-go\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>results with autoscale-go</h3>\n<pre><code> run hey:\n\nSummary:\n  Total:\t90.1358 secs\n  Slowest:\t3.1309 secs\n  Fastest:\t0.1320 secs\n  Average:\t0.1411 secs\n  Requests/sec:\t353.9438\n  \n  Total data:\t3189679 bytes\n  Size/request:\t99 bytes\n\nResponse time histogram:\n  0.132 [1]\t|\n  0.432 [31852]\t|&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;\n  0.732 [0]\t|\n  1.032 [0]\t|\n  1.332 [0]\t|\n  1.631 [0]\t|\n  1.931 [0]\t|\n  2.231 [0]\t|\n  2.531 [0]\t|\n  2.831 [13]\t|\n  3.131 [37]\t|\n\n\nLatency distribution:\n  10% in 0.1336 secs\n  25% in 0.1352 secs\n  50% in 0.1364 secs\n  75% in 0.1373 secs\n  90% in 0.1390 secs\n  95% in 0.1407 secs\n  99% in 0.1506 secs\n\nDetails (average, fastest, slowest):\n  DNS+dialup:\t0.0001 secs, 0.1320 secs, 3.1309 secs\n  DNS-lookup:\t0.0000 secs, 0.0000 secs, 0.0016 secs\n  req write:\t0.0000 secs, 0.0000 secs, 0.0007 secs\n  resp wait:\t0.1410 secs, 0.1320 secs, 3.0980 secs\n  resp read:\t0.0001 secs, 0.0000 secs, 0.0015 secs\n\nStatus code distribution:\n  [200]\t31903 responses\n\n\n\nget pods [after]: \nNAME                                                READY   STATUS        RESTARTS   AGE\nautoscale-go-t2zm7-deployment-6446d97d5b-8mxwt      2/2     Running       0          90s\nautoscale-go-t2zm7-deployment-6446d97d5b-d2f46      2/2     Running       0          87s\nautoscale-go-t2zm7-deployment-6446d97d5b-dkgnd      2/2     Running       0          89s\nautoscale-go-t2zm7-deployment-6446d97d5b-lvm6q      2/2     Running       0          89s\nautoscale-go-t2zm7-deployment-6446d97d5b-njlnd      2/2     Running       0          87s\nautoscale-go-t2zm7-deployment-6446d97d5b-xg89r      2/2     Running       0          89s\nhelloworld-go-rzznz-1-deployment-5b64869744-pq7hz   1/2     Terminating   0          4m58s\n</code></pre>\n<h3><a id=\"user-content-results-with-helloworld-go\" class=\"anchor\" href=\"https://github.com/didier-durand/knative-on-cloud-kubernetes#results-with-helloworld-go\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>results with helloworld-go</h3>\n<pre><code>run hey:\n\nSummary:\n  Total:\t140.7448 secs\n  Slowest:\t0.1092 secs\n  Fastest:\t0.0307 secs\n  Average:\t0.0350 secs\n  Requests/sec:\t1421.0119\n  \n  Total data:\t4000000 bytes\n  Size/request:\t20 bytes\n\nResponse time histogram:\n  0.031 [1]\t|\n  0.039 [189442]\t|&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;&#xFFFD;\n  0.046 [9240]\t|&#xFFFD;&#xFFFD;\n  0.054 [1023]\t|\n  0.062 [157]\t|\n  0.070 [58]\t|\n  0.078 [21]\t|\n  0.086 [14]\t|\n  0.094 [3]\t|\n  0.101 [6]\t|\n  0.109 [35]\t|\n\n\nLatency distribution:\n  10% in 0.0322 secs\n  25% in 0.0339 secs\n  50% in 0.0348 secs\n  75% in 0.0355 secs\n  90% in 0.0370 secs\n  95% in 0.0387 secs\n  99% in 0.0445 secs\n\nDetails (average, fastest, slowest):\n  DNS+dialup:\t0.0000 secs, 0.0307 secs, 0.1092 secs\n  DNS-lookup:\t0.0000 secs, 0.0000 secs, 0.0020 secs\n  req write:\t0.0000 secs, 0.0000 secs, 0.0027 secs\n  resp wait:\t0.0349 secs, 0.0307 secs, 0.0848 secs\n  resp read:\t0.0000 secs, 0.0000 secs, 0.0052 secs\n\nStatus code distribution:\n  [200]\t200000 responses\n\n\n\nget pods [after]: \nNAME                                                READY   STATUS    RESTARTS   AGE\nhelloworld-go-rzznz-1-deployment-5b64869744-pq7hz   2/2     Running   0          3m28s\n</code></pre>\n</article></div></div><hr><h4>Page 2</h4><div><div><article class=\"markdown-body entry-content container-lg\">\n<p><a href=\"https://github.com/didier-durand/knative-on-cloud-kubernetes/blob/master/img/knative-k8s-istio-logos.jpg\"><img src=\"https://github.com/didier-durand/knative-on-cloud-kubernetes/raw/master/img/knative-k8s-istio-logos.jpg\"></a></p>\n<p><a href=\"https://github.com/didier-durand/knative-on-cloud-kubernetes/workflows/Deploy%20Knative%20on%20GKE/badge.svg\"><img src=\"https://github.com/didier-durand/knative-on-cloud-kubernetes/workflows/Deploy%20Knative%20on%20GKE/badge.svg\" alt=\"workflow badge\"></a>\n<a href=\"https://github.com/didier-durand/knative-on-cloud-kubernetes/workflows/Deploy%20Knative%20on%20EKS/badge.svg\"><img src=\"https://github.com/didier-durand/knative-on-cloud-kubernetes/workflows/Deploy%20Knative%20on%20EKS/badge.svg\" alt=\"workflow badge\"></a></p>\n<p>This repository can be re-used in subsequent projects as the initial stage of a live test bed to leverage the feaures of\n<a href=\"https://knative.dev/\">Knative project</a> (sponsored by Google) for serverless workloads: it is implemented on Google Cloud Platform (GCP) and Amazon Web Services (AWS)\nvia a fully automated Github workflows (see files <a href=\"https://github.com/didier-durand/knative-on-cloud-kubernetes/blob/master/.github/workflows/gcloud-gke-knative.yml\">gcloud-gke-knative.yml</a>\nand <a href=\"https://github.com/didier-durand/knative-on-cloud-kubernetes/blob/master/.github/workflows/aws-eks-knative.yml\">aws-eks-knative.yml</a>)</p>\n<p>This workflow creates a standard Kubernetes cluster on the cloud (either <a href=\"https://cloud.google.com/kubernetes-engine\">Google Kubernetes Engine - GKE</a>\nor <a href=\"https://aws.amazon.com/eks/\">Amazon EKS</a>). When the cluster is up, the workflow deploys the required Knative components and dependencies. Then,\nit rolls out a couple of Knative mock-up Docker services, validates their proper unitary functioning. Finally, it checks the scalability feature of Knative before termination via deletion\nof the deployed services and of the cluster.</p>\n<p>As per <a href=\"https://knative.dev/docs/\">Knative documentation</a>: &quot;<em>Knative extends Kubernetes to provide a set of middleware components that are\nessential to <strong>build modern, source-centric, and container-based applications that can run anywhere: on premises, in the cloud, or even in a\nthird-party data center</strong>. Each of the components under the Knative project attempt to identify common patterns and codify the best practices\nthat are shared by successful, real-world, Kubernetes-based frameworks and applications.</em>&quot;.</p>\n<p>So, two different containers are deployed on Knative: their proper execution is validated. The <em><a href=\"https://knative.dev/v0.16-docs/serving/autoscaling/autoscale-go/\">autoscale-go</a></em>\nimage allows requests with parameters to consume more or less cpu and  memory check via query specific query parameters while\n<em><a href=\"https://knative.dev/v0.17-docs/serving/samples/hello-world/helloworld-go/\">helloworld-go</a></em> Docker image is limited to a unique and trivial\nresponse. We deploy helloworld-go with official <a href=\"https://github.com/knative/client\">Knative client (kn)</a> with minimal command-line options\nto demonstrate as easy as it can get for the developer: <em>&apos;kn service create &lt;SERVICE_NAME&gt; --image-nname &lt;IMAGE_NAME&gt; --env &lt;ENV_VARS_TO_SET&gt;&apos;</em>.\nBut, a rich set of deployment options is provided as per <a href=\"https://github.com/knative/client/blob/master/docs/cmd/kn_service_create.md\">reference documentation</a>.\nThe autoscale-go service is deployed via a YAML file describing a Knative service.</p>\n<p>The outcome of the recurring executions of the workflow and all the messages produced by those runs can be checked in the <a href=\"https://github.com/didier-durand/knative-on-cloud-kubernetes-private/actions\">Actions tab</a>\nof this repository. Also, the logs of any execution can be downloaded as text files via &quot;download log archive&quot; of the job dashboard for\nfurther analysis. The workflow is executed at least weekly via Github&apos;s cron to make sure that it remains fully operational.</p>\n<p><a href=\"https://cloud.google.com/run\">Google Cloud Run</a> is Google&apos;s own implementation of the Knative stack (<a href=\"https://ahmet.im/blog/cloud-run-is-a-knative/\">with some limitations</a>),\nintegrated with other services of GCP. But, this workflow demonstrates how to implement Knative on a raw Kubernetes cluster when a fully controlled\nimplementation is desired.</p>\n<p>(Current version of this repository is limited to implementation Knative Serving)</p>\n<h2><a id=\"user-content-why-knative-\" class=\"anchor\" href=\"#why-knative-\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Why Knative ?</h2>\n<p>As said above, Knative, OSS project sponsored by Google is implemented on GCP as service <a href=\"https://cloud.google.com/run\">Cloud Run</a> and integrated\nwith the other services (logging, monitoring, iam, etc.) of the platform. It is very easy to use: check out &quot;Deploy to Cloud Run&quot; of our other\nrepository collating GitHub-based CI/CD workflows.</p>\n<p><a href=\"/didier-durand/knative-on-cloud-kubernetes/blob/44fce0c96038a91c332074937895b5ebd570b1b8/img/Knative-Serving-architecture.jpg\"><img src=\"/didier-durand/knative-on-cloud-kubernetes/raw/44fce0c96038a91c332074937895b5ebd570b1b8/img/Knative-Serving-architecture.jpg\"></a></p>\n<p>But, if your workloads run elsewhere (on premise, other public cloud, etc.) or if you desire to avoid lock-in with a specific vendor and you still\nwant to enjoy the benefits of serverless: the Serving block (see its archiecture in Figure 1 - source: <a href=\"https://www.researchgate.net/publication/336567672_Towards_Serverless_as_Commodity_a_case_of_Knative\">N. Kaviani &amp; al</a>)\nof Knative delivers horizontal scalability, resiliency and deep observability for your application services by leveraging those same core\ncharacteristics of Kubernetes. But, it abstracts them through a <a href=\"https://github.com/knative/client/blob/master/docs/cmd/kn_service.md\">CLI named &quot;kn&quot;</a>.\nSeveral versions of the same service can be active simultaneously and Knative can organize fractional routing between them to allow incremental\nrollout of new versions of the application.</p>\n<p>Via the setup of <a href=\"https://prometheus.io/\">Prometheus</a>, Knative can deliver exhaustive metrics, accessible through <a href=\"https://grafana.com/\">Grafana</a>.\nAdditionally, Knative implements <a href=\"https://www.fluentd.org/\">fluentd</a>to collect logs, make them accessible via <a href=\"https://www.elastic.co/kibana\">Kibana</a>\nor centralize them in a global logger like <a href=\"https://cloud.google.com/logging\">Google Cloud Logging</a>. Also, traces of dialog between the service\nand its clients can also be collected via the setup of either <a href=\"https://zipkin.io/\">Zipkin</a> or <a href=\"https://www.jaegertracing.io/\">Jaeger</a>. So, added\nto the use of <a href=\"https://istio.io/\">Istio</a> in the core implementation of Knative, the full spectrum of observability is thoroughly covered\nto allow high QoS at scale.</p>\n<p>Knative is solely focused on those serverless workloads, which are &quot;autonomous&quot;: they rely on single and independent containers to deliver their\nservice.  Clearly, it doesn&apos;t cover the full spectrum of large-scale and sophisticated applications relying on additional services in\nother containers to run properly. But, for well-designed applications, a fair amount of their services can usually be implemented through Knative\nto obtain optimal efficiency: they will benefit from the advantages (again scalability, resiliency, etc.) of the core underlying Kubernetes\ninfrastructure, used by the other parts of the application, without the need to develop the sophisticated configuration required to be a good K8s\ncitizen.</p>\n<h2><a id=\"user-content-autoscaling-with-knative\" class=\"anchor\" href=\"#autoscaling-with-knative\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Autoscaling with Knative</h2>\n<p>Section &quot;Scalability test&quot; below demonstrates the automated scaling delivered as a core feature of Knative: the  autoscaing algorithm is detailed\non <a href=\"https://knative.dev/v0.15-docs/serving/samples/autoscale-go/#algorithm\">Knative Autoscaling page</a> and <a href=\"https://github.com/knative/serving/blob/master/docs/scaling/SYSTEM.md\">Knative Serving Autoscaling System</a>.\nThe initial <em>&apos;kubectl get pods -n default&apos;</em>\ndemonstrates that the workload containers for autoscale-go was scaled down to zero due to initial inactivity.</p>\n<p>After 90s of http traffic maintaining 350 requests (as per <a href=\"https://github.com/rakyll/hey\">hey</a> command line) in parallel, the final <em>&apos;kubectl get pods -n default&apos;</em> shows that\n5 autoscale-go additional pods (for a total of 6) autoscale-go pods were launched by Knative to sustain the demand because autoscale-go can\nbe parametrized to generate resource-demanding workloads (here biggest prime number under 10&apos;000 and 5 MB memory consumption).</p>\n<p>In comparison, 200k requests to helloworld-go are then executed again with hey. The throughput gets much higher (1&apos;420 qps) because helloworld-go\nresponses don&apos;t require much resources at all.  They last 390 seconds in total with an average throughput\nof 1&apos;280 requests per second. For that purpose, as per final <em>&apos;kubectl get pods -n default&apos;</em>, Knative didn&apos;t scale up at all: a single pod is sufficient.</p>\n<p><strong>NB</strong>: for the histogram of response times, hey measures the full round-trip time between the Github CI/CD platform on Microsoft Azure datacenters\nand Google GCP datacenters because hey is run on GitHub while Knative is hosted on GKE.</p>\n<h2><a id=\"user-content-steps-of-github-workflow\" class=\"anchor\" href=\"#steps-of-github-workflow\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Steps of Github workflow</h2>\n<ol>\n<li>checkout the project from git repository in CI/CD runner provided by Github</li>\n<li>setup the gcloud SDK with proper credentials to interact with GCP services</li>\n<li>get gcloud and info version (if needed for debugging)</li>\n<li>cleanup existing GKE cluster of previous run (if any)</li>\n<li>create fresh GKE cluster with default add-ons for this run. The additional parameters allow cluster autoscaling between 1 and 10 nodes of instance type <a href=\"https://cloud.google.com/compute/docs/machine-types\">n1-standard-4</a>.</li>\n<li><em>&apos;gcloud container clusters get-credentials&apos;</em> updates kubeconfig file with appropriate credentials and endpoint information to make kubectl usable with the newly created cluster.</li>\n<li>install Istio operator (an implementation of <a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/\">standard K8s Custom Resource Definition (CRD)feature</a>. This install is done via istioctl, whose nightly build is downloaded on the fly to proceed. We check - via kubectl + grep - the presence of expected services and pods.</li>\n<li>Install the K8s CRD for Knative Serving feature block</li>\n<li>Install the core components for Knative Serving feature block</li>\n<li>Install the Istio controller for Knative Serving feature block</li>\n<li>Check if full setup of GKE + Istio + Knative is working properly. We check - via kubectl + grep - the presence of expected services and pods.</li>\n<li>Install a test Go workload coming from <a href=\"https://knative.dev/docs/serving/getting-started-knative-app/\">Knative tutorial</a></li>\n<li>Deploy the tutum/hello-world image as a Knative workload and use an http request to validate its proper functioning</li>\n<li>Delete the cluster and all its resources created for this execution</li>\n</ol>\n<h2><a id=\"user-content-setup-for-forks\" class=\"anchor\" href=\"#setup-for-forks\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Setup for forks</h2>\n<p>To fork and run this project in your own Github account, you only need to:</p>\n<ul>\n<li><strong>GCP</strong>: Create a test project in your GCP account and define it as a Github secret named ${{ secrets.GCP_PROJECT }} in workflow YAML. Define a service account in GCP IAM with Project Owner role (to make security definitions simpler), download its secret key and define the value of a Github\nsecret named ${{ secrets.GCP_SA_KEY }} with the downloaded json.</li>\n<li><strong>AWS</strong>: Define a user in AWS IAM with full admin rights (to make security definitions simpler), download its access key and secret key to define 2 secrets ${{ secrets.AWS_ACCESS_KEY_ID }} and ${{ secrets.AWS_SECRET_KEY }}. Finally, define the region that you want to work in as ${{ secrets.AWS_REGION }}</li>\n</ul>\n<h2><a id=\"user-content-scalability-test\" class=\"anchor\" href=\"#scalability-test\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Scalability test</h2>\n<p>See above for some comments</p>\n<h3><a id=\"user-content-results-with-autoscale-go\" class=\"anchor\" href=\"#results-with-autoscale-go\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>results with autoscale-go</h3>\n<pre><code> run hey:\n\nSummary:\n  Total:\t90.1358 secs\n  Slowest:\t3.1309 secs\n  Fastest:\t0.1320 secs\n  Average:\t0.1411 secs\n  Requests/sec:\t353.9438\n  \n  Total data:\t3189679 bytes\n  Size/request:\t99 bytes\n\nResponse time histogram:\n  0.132 [1]\t|\n  0.432 [31852]\t|&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;\n  0.732 [0]\t|\n  1.032 [0]\t|\n  1.332 [0]\t|\n  1.631 [0]\t|\n  1.931 [0]\t|\n  2.231 [0]\t|\n  2.531 [0]\t|\n  2.831 [13]\t|\n  3.131 [37]\t|\n\n\nLatency distribution:\n  10% in 0.1336 secs\n  25% in 0.1352 secs\n  50% in 0.1364 secs\n  75% in 0.1373 secs\n  90% in 0.1390 secs\n  95% in 0.1407 secs\n  99% in 0.1506 secs\n\nDetails (average, fastest, slowest):\n  DNS+dialup:\t0.0001 secs, 0.1320 secs, 3.1309 secs\n  DNS-lookup:\t0.0000 secs, 0.0000 secs, 0.0016 secs\n  req write:\t0.0000 secs, 0.0000 secs, 0.0007 secs\n  resp wait:\t0.1410 secs, 0.1320 secs, 3.0980 secs\n  resp read:\t0.0001 secs, 0.0000 secs, 0.0015 secs\n\nStatus code distribution:\n  [200]\t31903 responses\n\n\n\nget pods [after]: \nNAME                                                READY   STATUS        RESTARTS   AGE\nautoscale-go-t2zm7-deployment-6446d97d5b-8mxwt      2/2     Running       0          90s\nautoscale-go-t2zm7-deployment-6446d97d5b-d2f46      2/2     Running       0          87s\nautoscale-go-t2zm7-deployment-6446d97d5b-dkgnd      2/2     Running       0          89s\nautoscale-go-t2zm7-deployment-6446d97d5b-lvm6q      2/2     Running       0          89s\nautoscale-go-t2zm7-deployment-6446d97d5b-njlnd      2/2     Running       0          87s\nautoscale-go-t2zm7-deployment-6446d97d5b-xg89r      2/2     Running       0          89s\nhelloworld-go-rzznz-1-deployment-5b64869744-pq7hz   1/2     Terminating   0          4m58s\n</code></pre>\n<h3><a id=\"user-content-results-with-helloworld-go\" class=\"anchor\" href=\"#results-with-helloworld-go\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>results with helloworld-go</h3>\n<pre><code>run hey:\n\nSummary:\n  Total:\t140.7448 secs\n  Slowest:\t0.1092 secs\n  Fastest:\t0.0307 secs\n  Average:\t0.0350 secs\n  Requests/sec:\t1421.0119\n  \n  Total data:\t4000000 bytes\n  Size/request:\t20 bytes\n\nResponse time histogram:\n  0.031 [1]\t|\n  0.039 [189442]\t|&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;&#x25A0;\n  0.046 [9240]\t|&#x25A0;&#x25A0;\n  0.054 [1023]\t|\n  0.062 [157]\t|\n  0.070 [58]\t|\n  0.078 [21]\t|\n  0.086 [14]\t|\n  0.094 [3]\t|\n  0.101 [6]\t|\n  0.109 [35]\t|\n\n\nLatency distribution:\n  10% in 0.0322 secs\n  25% in 0.0339 secs\n  50% in 0.0348 secs\n  75% in 0.0355 secs\n  90% in 0.0370 secs\n  95% in 0.0387 secs\n  99% in 0.0445 secs\n\nDetails (average, fastest, slowest):\n  DNS+dialup:\t0.0000 secs, 0.0307 secs, 0.1092 secs\n  DNS-lookup:\t0.0000 secs, 0.0000 secs, 0.0020 secs\n  req write:\t0.0000 secs, 0.0000 secs, 0.0027 secs\n  resp wait:\t0.0349 secs, 0.0307 secs, 0.0848 secs\n  resp read:\t0.0000 secs, 0.0000 secs, 0.0052 secs\n\nStatus code distribution:\n  [200]\t200000 responses\n\n\n\nget pods [after]: \nNAME                                                READY   STATUS    RESTARTS   AGE\nhelloworld-go-rzznz-1-deployment-5b64869744-pq7hz   2/2     Running   0          3m28s\n</code></pre>\n</article></div></div>",
      "contentAsText": "\n\n\n\nThis repository can be re-used in subsequent projects as the initial stage of a live test bed to leverage the feaures of\nKnative project (sponsored by Google) for serverless workloads: it is implemented on Google Cloud Platform (GCP) and Amazon Web Services (AWS)\nvia a fully automated Github workflows (see files gcloud-gke-knative.yml\nand aws-eks-knative.yml)\nThis workflow creates a standard Kubernetes cluster on the cloud (either Google Kubernetes Engine - GKE\nor Amazon EKS). When the cluster is up, the workflow deploys the required Knative components and dependencies. Then,\nit rolls out a couple of Knative mock-up Docker services, validates their proper unitary functioning. Finally, it checks the scalability feature of Knative before termination via deletion\nof the deployed services and of the cluster.\nAs per Knative documentation: \"Knative extends Kubernetes to provide a set of middleware components that are\nessential to build modern, source-centric, and container-based applications that can run anywhere: on premises, in the cloud, or even in a\nthird-party data center. Each of the components under the Knative project attempt to identify common patterns and codify the best practices\nthat are shared by successful, real-world, Kubernetes-based frameworks and applications.\".\nSo, two different containers are deployed on Knative: their proper execution is validated. The autoscale-go\nimage allows requests with parameters to consume more or less cpu and  memory check via query specific query parameters while\nhelloworld-go Docker image is limited to a unique and trivial\nresponse. We deploy helloworld-go with official Knative client (kn) with minimal command-line options\nto demonstrate as easy as it can get for the developer: 'kn service create <SERVICE_NAME> --image-nname <IMAGE_NAME> --env <ENV_VARS_TO_SET>'.\nBut, a rich set of deployment options is provided as per reference documentation.\nThe autoscale-go service is deployed via a YAML file describing a Knative service.\nThe outcome of the recurring executions of the workflow and all the messages produced by those runs can be checked in the Actions tab\nof this repository. Also, the logs of any execution can be downloaded as text files via \"download log archive\" of the job dashboard for\nfurther analysis. The workflow is executed at least weekly via Github's cron to make sure that it remains fully operational.\nGoogle Cloud Run is Google's own implementation of the Knative stack (with some limitations),\nintegrated with other services of GCP. But, this workflow demonstrates how to implement Knative on a raw Kubernetes cluster when a fully controlled\nimplementation is desired.\n(Current version of this repository is limited to implementation Knative Serving)\nWhy Knative ?\nAs said above, Knative, OSS project sponsored by Google is implemented on GCP as service Cloud Run and integrated\nwith the other services (logging, monitoring, iam, etc.) of the platform. It is very easy to use: check out \"Deploy to Cloud Run\" of our other\nrepository collating GitHub-based CI/CD workflows.\n\nBut, if your workloads run elsewhere (on premise, other public cloud, etc.) or if you desire to avoid lock-in with a specific vendor and you still\nwant to enjoy the benefits of serverless: the Serving block (see its archiecture in Figure 1 - source: N. Kaviani & al)\nof Knative delivers horizontal scalability, resiliency and deep observability for your application services by leveraging those same core\ncharacteristics of Kubernetes. But, it abstracts them through a CLI named \"kn\".\nSeveral versions of the same service can be active simultaneously and Knative can organize fractional routing between them to allow incremental\nrollout of new versions of the application.\nVia the setup of Prometheus, Knative can deliver exhaustive metrics, accessible through Grafana.\nAdditionally, Knative implements fluentdto collect logs, make them accessible via Kibana\nor centralize them in a global logger like Google Cloud Logging. Also, traces of dialog between the service\nand its clients can also be collected via the setup of either Zipkin or Jaeger. So, added\nto the use of Istio in the core implementation of Knative, the full spectrum of observability is thoroughly covered\nto allow high QoS at scale.\nKnative is solely focused on those serverless workloads, which are \"autonomous\": they rely on single and independent containers to deliver their\nservice.  Clearly, it doesn't cover the full spectrum of large-scale and sophisticated applications relying on additional services in\nother containers to run properly. But, for well-designed applications, a fair amount of their services can usually be implemented through Knative\nto obtain optimal efficiency: they will benefit from the advantages (again scalability, resiliency, etc.) of the core underlying Kubernetes\ninfrastructure, used by the other parts of the application, without the need to develop the sophisticated configuration required to be a good K8s\ncitizen.\nAutoscaling with Knative\nSection \"Scalability test\" below demonstrates the automated scaling delivered as a core feature of Knative: the  autoscaing algorithm is detailed\non Knative Autoscaling page and Knative Serving Autoscaling System.\nThe initial 'kubectl get pods -n default'\ndemonstrates that the workload containers for autoscale-go was scaled down to zero due to initial inactivity.\nAfter 90s of http traffic maintaining 350 requests (as per hey command line) in parallel, the final 'kubectl get pods -n default' shows that\n5 autoscale-go additional pods (for a total of 6) autoscale-go pods were launched by Knative to sustain the demand because autoscale-go can\nbe parametrized to generate resource-demanding workloads (here biggest prime number under 10'000 and 5 MB memory consumption).\nIn comparison, 200k requests to helloworld-go are then executed again with hey. The throughput gets much higher (1'420 qps) because helloworld-go\nresponses don't require much resources at all.  They last 390 seconds in total with an average throughput\nof 1'280 requests per second. For that purpose, as per final 'kubectl get pods -n default', Knative didn't scale up at all: a single pod is sufficient.\nNB: for the histogram of response times, hey measures the full round-trip time between the Github CI/CD platform on Microsoft Azure datacenters\nand Google GCP datacenters because hey is run on GitHub while Knative is hosted on GKE.\nSteps of Github workflow\n\ncheckout the project from git repository in CI/CD runner provided by Github\nsetup the gcloud SDK with proper credentials to interact with GCP services\nget gcloud and info version (if needed for debugging)\ncleanup existing GKE cluster of previous run (if any)\ncreate fresh GKE cluster with default add-ons for this run. The additional parameters allow cluster autoscaling between 1 and 10 nodes of instance type n1-standard-4.\n'gcloud container clusters get-credentials' updates kubeconfig file with appropriate credentials and endpoint information to make kubectl usable with the newly created cluster.\ninstall Istio operator (an implementation of standard K8s Custom Resource Definition (CRD)feature. This install is done via istioctl, whose nightly build is downloaded on the fly to proceed. We check - via kubectl + grep - the presence of expected services and pods.\nInstall the K8s CRD for Knative Serving feature block\nInstall the core components for Knative Serving feature block\nInstall the Istio controller for Knative Serving feature block\nCheck if full setup of GKE + Istio + Knative is working properly. We check - via kubectl + grep - the presence of expected services and pods.\nInstall a test Go workload coming from Knative tutorial\nDeploy the tutum/hello-world image as a Knative workload and use an http request to validate its proper functioning\nDelete the cluster and all its resources created for this execution\n\nSetup for forks\nTo fork and run this project in your own Github account, you only need to:\n\nGCP: Create a test project in your GCP account and define it as a Github secret named ${{ secrets.GCP_PROJECT }} in workflow YAML. Define a service account in GCP IAM with Project Owner role (to make security definitions simpler), download its secret key and define the value of a Github\nsecret named ${{ secrets.GCP_SA_KEY }} with the downloaded json.\nAWS: Define a user in AWS IAM with full admin rights (to make security definitions simpler), download its access key and secret key to define 2 secrets ${{ secrets.AWS_ACCESS_KEY_ID }} and ${{ secrets.AWS_SECRET_KEY }}. Finally, define the region that you want to work in as ${{ secrets.AWS_REGION }}\n\nScalability test\nSee above for some comments\nresults with autoscale-go\n run hey:\n\nSummary:\n  Total:\t90.1358 secs\n  Slowest:\t3.1309 secs\n  Fastest:\t0.1320 secs\n  Average:\t0.1411 secs\n  Requests/sec:\t353.9438\n  \n  Total data:\t3189679 bytes\n  Size/request:\t99 bytes\n\nResponse time histogram:\n  0.132 [1]\t|\n  0.432 [31852]\t|����������������������������������������\n  0.732 [0]\t|\n  1.032 [0]\t|\n  1.332 [0]\t|\n  1.631 [0]\t|\n  1.931 [0]\t|\n  2.231 [0]\t|\n  2.531 [0]\t|\n  2.831 [13]\t|\n  3.131 [37]\t|\n\n\nLatency distribution:\n  10% in 0.1336 secs\n  25% in 0.1352 secs\n  50% in 0.1364 secs\n  75% in 0.1373 secs\n  90% in 0.1390 secs\n  95% in 0.1407 secs\n  99% in 0.1506 secs\n\nDetails (average, fastest, slowest):\n  DNS+dialup:\t0.0001 secs, 0.1320 secs, 3.1309 secs\n  DNS-lookup:\t0.0000 secs, 0.0000 secs, 0.0016 secs\n  req write:\t0.0000 secs, 0.0000 secs, 0.0007 secs\n  resp wait:\t0.1410 secs, 0.1320 secs, 3.0980 secs\n  resp read:\t0.0001 secs, 0.0000 secs, 0.0015 secs\n\nStatus code distribution:\n  [200]\t31903 responses\n\n\n\nget pods [after]: \nNAME                                                READY   STATUS        RESTARTS   AGE\nautoscale-go-t2zm7-deployment-6446d97d5b-8mxwt      2/2     Running       0          90s\nautoscale-go-t2zm7-deployment-6446d97d5b-d2f46      2/2     Running       0          87s\nautoscale-go-t2zm7-deployment-6446d97d5b-dkgnd      2/2     Running       0          89s\nautoscale-go-t2zm7-deployment-6446d97d5b-lvm6q      2/2     Running       0          89s\nautoscale-go-t2zm7-deployment-6446d97d5b-njlnd      2/2     Running       0          87s\nautoscale-go-t2zm7-deployment-6446d97d5b-xg89r      2/2     Running       0          89s\nhelloworld-go-rzznz-1-deployment-5b64869744-pq7hz   1/2     Terminating   0          4m58s\n\nresults with helloworld-go\nrun hey:\n\nSummary:\n  Total:\t140.7448 secs\n  Slowest:\t0.1092 secs\n  Fastest:\t0.0307 secs\n  Average:\t0.0350 secs\n  Requests/sec:\t1421.0119\n  \n  Total data:\t4000000 bytes\n  Size/request:\t20 bytes\n\nResponse time histogram:\n  0.031 [1]\t|\n  0.039 [189442]\t|����������������������������������������\n  0.046 [9240]\t|��\n  0.054 [1023]\t|\n  0.062 [157]\t|\n  0.070 [58]\t|\n  0.078 [21]\t|\n  0.086 [14]\t|\n  0.094 [3]\t|\n  0.101 [6]\t|\n  0.109 [35]\t|\n\n\nLatency distribution:\n  10% in 0.0322 secs\n  25% in 0.0339 secs\n  50% in 0.0348 secs\n  75% in 0.0355 secs\n  90% in 0.0370 secs\n  95% in 0.0387 secs\n  99% in 0.0445 secs\n\nDetails (average, fastest, slowest):\n  DNS+dialup:\t0.0000 secs, 0.0307 secs, 0.1092 secs\n  DNS-lookup:\t0.0000 secs, 0.0000 secs, 0.0020 secs\n  req write:\t0.0000 secs, 0.0000 secs, 0.0027 secs\n  resp wait:\t0.0349 secs, 0.0307 secs, 0.0848 secs\n  resp read:\t0.0000 secs, 0.0000 secs, 0.0052 secs\n\nStatus code distribution:\n  [200]\t200000 responses\n\n\n\nget pods [after]: \nNAME                                                READY   STATUS    RESTARTS   AGE\nhelloworld-go-rzznz-1-deployment-5b64869744-pq7hz   2/2     Running   0          3m28s\n\nPage 2\n\n\n\nThis repository can be re-used in subsequent projects as the initial stage of a live test bed to leverage the feaures of\nKnative project (sponsored by Google) for serverless workloads: it is implemented on Google Cloud Platform (GCP) and Amazon Web Services (AWS)\nvia a fully automated Github workflows (see files gcloud-gke-knative.yml\nand aws-eks-knative.yml)\nThis workflow creates a standard Kubernetes cluster on the cloud (either Google Kubernetes Engine - GKE\nor Amazon EKS). When the cluster is up, the workflow deploys the required Knative components and dependencies. Then,\nit rolls out a couple of Knative mock-up Docker services, validates their proper unitary functioning. Finally, it checks the scalability feature of Knative before termination via deletion\nof the deployed services and of the cluster.\nAs per Knative documentation: \"Knative extends Kubernetes to provide a set of middleware components that are\nessential to build modern, source-centric, and container-based applications that can run anywhere: on premises, in the cloud, or even in a\nthird-party data center. Each of the components under the Knative project attempt to identify common patterns and codify the best practices\nthat are shared by successful, real-world, Kubernetes-based frameworks and applications.\".\nSo, two different containers are deployed on Knative: their proper execution is validated. The autoscale-go\nimage allows requests with parameters to consume more or less cpu and  memory check via query specific query parameters while\nhelloworld-go Docker image is limited to a unique and trivial\nresponse. We deploy helloworld-go with official Knative client (kn) with minimal command-line options\nto demonstrate as easy as it can get for the developer: 'kn service create <SERVICE_NAME> --image-nname <IMAGE_NAME> --env <ENV_VARS_TO_SET>'.\nBut, a rich set of deployment options is provided as per reference documentation.\nThe autoscale-go service is deployed via a YAML file describing a Knative service.\nThe outcome of the recurring executions of the workflow and all the messages produced by those runs can be checked in the Actions tab\nof this repository. Also, the logs of any execution can be downloaded as text files via \"download log archive\" of the job dashboard for\nfurther analysis. The workflow is executed at least weekly via Github's cron to make sure that it remains fully operational.\nGoogle Cloud Run is Google's own implementation of the Knative stack (with some limitations),\nintegrated with other services of GCP. But, this workflow demonstrates how to implement Knative on a raw Kubernetes cluster when a fully controlled\nimplementation is desired.\n(Current version of this repository is limited to implementation Knative Serving)\nWhy Knative ?\nAs said above, Knative, OSS project sponsored by Google is implemented on GCP as service Cloud Run and integrated\nwith the other services (logging, monitoring, iam, etc.) of the platform. It is very easy to use: check out \"Deploy to Cloud Run\" of our other\nrepository collating GitHub-based CI/CD workflows.\n\nBut, if your workloads run elsewhere (on premise, other public cloud, etc.) or if you desire to avoid lock-in with a specific vendor and you still\nwant to enjoy the benefits of serverless: the Serving block (see its archiecture in Figure 1 - source: N. Kaviani & al)\nof Knative delivers horizontal scalability, resiliency and deep observability for your application services by leveraging those same core\ncharacteristics of Kubernetes. But, it abstracts them through a CLI named \"kn\".\nSeveral versions of the same service can be active simultaneously and Knative can organize fractional routing between them to allow incremental\nrollout of new versions of the application.\nVia the setup of Prometheus, Knative can deliver exhaustive metrics, accessible through Grafana.\nAdditionally, Knative implements fluentdto collect logs, make them accessible via Kibana\nor centralize them in a global logger like Google Cloud Logging. Also, traces of dialog between the service\nand its clients can also be collected via the setup of either Zipkin or Jaeger. So, added\nto the use of Istio in the core implementation of Knative, the full spectrum of observability is thoroughly covered\nto allow high QoS at scale.\nKnative is solely focused on those serverless workloads, which are \"autonomous\": they rely on single and independent containers to deliver their\nservice.  Clearly, it doesn't cover the full spectrum of large-scale and sophisticated applications relying on additional services in\nother containers to run properly. But, for well-designed applications, a fair amount of their services can usually be implemented through Knative\nto obtain optimal efficiency: they will benefit from the advantages (again scalability, resiliency, etc.) of the core underlying Kubernetes\ninfrastructure, used by the other parts of the application, without the need to develop the sophisticated configuration required to be a good K8s\ncitizen.\nAutoscaling with Knative\nSection \"Scalability test\" below demonstrates the automated scaling delivered as a core feature of Knative: the  autoscaing algorithm is detailed\non Knative Autoscaling page and Knative Serving Autoscaling System.\nThe initial 'kubectl get pods -n default'\ndemonstrates that the workload containers for autoscale-go was scaled down to zero due to initial inactivity.\nAfter 90s of http traffic maintaining 350 requests (as per hey command line) in parallel, the final 'kubectl get pods -n default' shows that\n5 autoscale-go additional pods (for a total of 6) autoscale-go pods were launched by Knative to sustain the demand because autoscale-go can\nbe parametrized to generate resource-demanding workloads (here biggest prime number under 10'000 and 5 MB memory consumption).\nIn comparison, 200k requests to helloworld-go are then executed again with hey. The throughput gets much higher (1'420 qps) because helloworld-go\nresponses don't require much resources at all.  They last 390 seconds in total with an average throughput\nof 1'280 requests per second. For that purpose, as per final 'kubectl get pods -n default', Knative didn't scale up at all: a single pod is sufficient.\nNB: for the histogram of response times, hey measures the full round-trip time between the Github CI/CD platform on Microsoft Azure datacenters\nand Google GCP datacenters because hey is run on GitHub while Knative is hosted on GKE.\nSteps of Github workflow\n\ncheckout the project from git repository in CI/CD runner provided by Github\nsetup the gcloud SDK with proper credentials to interact with GCP services\nget gcloud and info version (if needed for debugging)\ncleanup existing GKE cluster of previous run (if any)\ncreate fresh GKE cluster with default add-ons for this run. The additional parameters allow cluster autoscaling between 1 and 10 nodes of instance type n1-standard-4.\n'gcloud container clusters get-credentials' updates kubeconfig file with appropriate credentials and endpoint information to make kubectl usable with the newly created cluster.\ninstall Istio operator (an implementation of standard K8s Custom Resource Definition (CRD)feature. This install is done via istioctl, whose nightly build is downloaded on the fly to proceed. We check - via kubectl + grep - the presence of expected services and pods.\nInstall the K8s CRD for Knative Serving feature block\nInstall the core components for Knative Serving feature block\nInstall the Istio controller for Knative Serving feature block\nCheck if full setup of GKE + Istio + Knative is working properly. We check - via kubectl + grep - the presence of expected services and pods.\nInstall a test Go workload coming from Knative tutorial\nDeploy the tutum/hello-world image as a Knative workload and use an http request to validate its proper functioning\nDelete the cluster and all its resources created for this execution\n\nSetup for forks\nTo fork and run this project in your own Github account, you only need to:\n\nGCP: Create a test project in your GCP account and define it as a Github secret named ${{ secrets.GCP_PROJECT }} in workflow YAML. Define a service account in GCP IAM with Project Owner role (to make security definitions simpler), download its secret key and define the value of a Github\nsecret named ${{ secrets.GCP_SA_KEY }} with the downloaded json.\nAWS: Define a user in AWS IAM with full admin rights (to make security definitions simpler), download its access key and secret key to define 2 secrets ${{ secrets.AWS_ACCESS_KEY_ID }} and ${{ secrets.AWS_SECRET_KEY }}. Finally, define the region that you want to work in as ${{ secrets.AWS_REGION }}\n\nScalability test\nSee above for some comments\nresults with autoscale-go\n run hey:\n\nSummary:\n  Total:\t90.1358 secs\n  Slowest:\t3.1309 secs\n  Fastest:\t0.1320 secs\n  Average:\t0.1411 secs\n  Requests/sec:\t353.9438\n  \n  Total data:\t3189679 bytes\n  Size/request:\t99 bytes\n\nResponse time histogram:\n  0.132 [1]\t|\n  0.432 [31852]\t|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\n  0.732 [0]\t|\n  1.032 [0]\t|\n  1.332 [0]\t|\n  1.631 [0]\t|\n  1.931 [0]\t|\n  2.231 [0]\t|\n  2.531 [0]\t|\n  2.831 [13]\t|\n  3.131 [37]\t|\n\n\nLatency distribution:\n  10% in 0.1336 secs\n  25% in 0.1352 secs\n  50% in 0.1364 secs\n  75% in 0.1373 secs\n  90% in 0.1390 secs\n  95% in 0.1407 secs\n  99% in 0.1506 secs\n\nDetails (average, fastest, slowest):\n  DNS+dialup:\t0.0001 secs, 0.1320 secs, 3.1309 secs\n  DNS-lookup:\t0.0000 secs, 0.0000 secs, 0.0016 secs\n  req write:\t0.0000 secs, 0.0000 secs, 0.0007 secs\n  resp wait:\t0.1410 secs, 0.1320 secs, 3.0980 secs\n  resp read:\t0.0001 secs, 0.0000 secs, 0.0015 secs\n\nStatus code distribution:\n  [200]\t31903 responses\n\n\n\nget pods [after]: \nNAME                                                READY   STATUS        RESTARTS   AGE\nautoscale-go-t2zm7-deployment-6446d97d5b-8mxwt      2/2     Running       0          90s\nautoscale-go-t2zm7-deployment-6446d97d5b-d2f46      2/2     Running       0          87s\nautoscale-go-t2zm7-deployment-6446d97d5b-dkgnd      2/2     Running       0          89s\nautoscale-go-t2zm7-deployment-6446d97d5b-lvm6q      2/2     Running       0          89s\nautoscale-go-t2zm7-deployment-6446d97d5b-njlnd      2/2     Running       0          87s\nautoscale-go-t2zm7-deployment-6446d97d5b-xg89r      2/2     Running       0          89s\nhelloworld-go-rzznz-1-deployment-5b64869744-pq7hz   1/2     Terminating   0          4m58s\n\nresults with helloworld-go\nrun hey:\n\nSummary:\n  Total:\t140.7448 secs\n  Slowest:\t0.1092 secs\n  Fastest:\t0.0307 secs\n  Average:\t0.0350 secs\n  Requests/sec:\t1421.0119\n  \n  Total data:\t4000000 bytes\n  Size/request:\t20 bytes\n\nResponse time histogram:\n  0.031 [1]\t|\n  0.039 [189442]\t|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■\n  0.046 [9240]\t|■■\n  0.054 [1023]\t|\n  0.062 [157]\t|\n  0.070 [58]\t|\n  0.078 [21]\t|\n  0.086 [14]\t|\n  0.094 [3]\t|\n  0.101 [6]\t|\n  0.109 [35]\t|\n\n\nLatency distribution:\n  10% in 0.0322 secs\n  25% in 0.0339 secs\n  50% in 0.0348 secs\n  75% in 0.0355 secs\n  90% in 0.0370 secs\n  95% in 0.0387 secs\n  99% in 0.0445 secs\n\nDetails (average, fastest, slowest):\n  DNS+dialup:\t0.0000 secs, 0.0307 secs, 0.1092 secs\n  DNS-lookup:\t0.0000 secs, 0.0000 secs, 0.0020 secs\n  req write:\t0.0000 secs, 0.0000 secs, 0.0027 secs\n  resp wait:\t0.0349 secs, 0.0307 secs, 0.0848 secs\n  resp read:\t0.0000 secs, 0.0000 secs, 0.0052 secs\n\nStatus code distribution:\n  [200]\t200000 responses\n\n\n\nget pods [after]: \nNAME                                                READY   STATUS    RESTARTS   AGE\nhelloworld-go-rzznz-1-deployment-5b64869744-pq7hz   2/2     Running   0          3m28s\n\n",
      "description": "Knative on cloud Kubernetes. Test autoscaling for Serving block. - didier-durand/knative-on-cloud-kubernetes",
      "ogDescription": "Knative on cloud Kubernetes. Test autoscaling for Serving block. - didier-durand/knative-on-cloud-kubernetes"
    },
    {
      "url": "https://github.com/gtsystem/lightkube",
      "title": "gtsystem/lightkube",
      "content": "<div><div><article class=\"markdown-body entry-content container-lg\">\n<p><a href=\"https://camo.githubusercontent.com/08b9c0d2d0757b45b3389ebefb087b164d00a1296ff1c0769586001e80660d17/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f776f726b666c6f772f7374617475732f677473797374656d2f6c696768746b7562652f507974686f6e2532307061636b616765\"><img src=\"https://camo.githubusercontent.com/08b9c0d2d0757b45b3389ebefb087b164d00a1296ff1c0769586001e80660d17/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f776f726b666c6f772f7374617475732f677473797374656d2f6c696768746b7562652f507974686f6e2532307061636b616765\" alt></a>\n<a href=\"https://coveralls.io/github/gtsystem/lightkube?branch=master\"><img src=\"https://camo.githubusercontent.com/bea0310eae0e0fe20bdaf4be570d210197068da84572b3f35eda89fefc366830/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f677473797374656d2f6c696768746b7562652f62616467652e7376673f6272616e63683d6d6173746572\" alt=\"Coverage Status\"></a>\n<a href=\"https://pypi.python.org/pypi/lightkube\"><img src=\"https://camo.githubusercontent.com/fb0993854e8e1c64c04df857e9b8cedafe723ef96bf0f3dcb4c7236422e96688/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f6c696768746b7562652e737667\" alt=\"pypi supported versions\"></a></p>\n<p>Modern lightweight kubernetes module for python</p>\n<p><strong>NOTICE:</strong> This project is still under development and not suitable for production usage.</p>\n<h2><a id=\"user-content-highlights\" class=\"anchor\" href=\"https://github.com/gtsystem/lightkube#highlights\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Highlights</h2>\n<ul>\n<li><em>Simple</em> interface shared across all kubernetes APIs.</li>\n<li>Extensive <em>type hints</em> to avoid common mistakes and to support autocompletion.</li>\n<li>Models and resources generated from the swagger specifications using standard dataclasses.</li>\n<li>Support for async/await</li>\n<li>Support for installing a specific version of the kubernetes models (1.15 to 1.19)</li>\n<li>Lazy instantiation of inner models.</li>\n<li>Fast startup and small memory footprint as only needed models and resources can be imported.</li>\n<li>Automatic handling of pagination when listing resources.</li>\n<li>Customizable handling of errors when watching resources.</li>\n</ul>\n<p>This module is powered by <a href=\"https://github.com/encode/httpx/tree/master/httpx\">httpx</a>.</p>\n<h2><a id=\"user-content-installation\" class=\"anchor\" href=\"https://github.com/gtsystem/lightkube#installation\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Installation</h2>\n<p>This module requires python &gt;= 3.6</p>\n<pre><code>pip install lightkube\n</code></pre>\n<h2><a id=\"user-content-usage\" class=\"anchor\" href=\"https://github.com/gtsystem/lightkube#usage\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Usage</h2>\n<p>Read a pod</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-s1\">lightkube</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">Client</span>\n<span class=\"pl-k\">from</span> <span class=\"pl-s1\">lightkube</span>.<span class=\"pl-s1\">resources</span>.<span class=\"pl-s1\">core_v1</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">Pod</span>\n\n<span class=\"pl-s1\">client</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Client</span>()\n<span class=\"pl-s1\">pod</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">client</span>.<span class=\"pl-en\">get</span>(<span class=\"pl-v\">Pod</span>, <span class=\"pl-s1\">name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">&quot;my-pod&quot;</span>, <span class=\"pl-s1\">namespace</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">&quot;default&quot;</span>)\n<span class=\"pl-en\">print</span>(<span class=\"pl-s1\">pod</span>.<span class=\"pl-s1\">namespace</span>.<span class=\"pl-s1\">uid</span>)</pre></div>\n<p>List nodes</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-s1\">lightkube</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">Client</span>\n<span class=\"pl-k\">from</span> <span class=\"pl-s1\">lightkube</span>.<span class=\"pl-s1\">resources</span>.<span class=\"pl-s1\">core_v1</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">Node</span>\n\n<span class=\"pl-s1\">client</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Client</span>()\n<span class=\"pl-k\">for</span> <span class=\"pl-s1\">node</span> <span class=\"pl-c1\">in</span> <span class=\"pl-s1\">client</span>.<span class=\"pl-en\">list</span>(<span class=\"pl-v\">Node</span>):\n    <span class=\"pl-en\">print</span>(<span class=\"pl-s1\">node</span>.<span class=\"pl-s1\">metadata</span>.<span class=\"pl-s1\">name</span>)</pre></div>\n<p>Watch deployments</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-s1\">lightkube</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">Client</span>\n<span class=\"pl-k\">from</span> <span class=\"pl-s1\">lightkube</span>.<span class=\"pl-s1\">resources</span>.<span class=\"pl-s1\">apps_v1</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">Deployment</span>\n\n<span class=\"pl-s1\">client</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Client</span>()\n<span class=\"pl-k\">for</span> <span class=\"pl-s1\">op</span>, <span class=\"pl-s1\">dep</span> <span class=\"pl-c1\">in</span> <span class=\"pl-s1\">client</span>.<span class=\"pl-en\">watch</span>(<span class=\"pl-v\">Deployment</span>, <span class=\"pl-s1\">namespace</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">&quot;default&quot;</span>):\n    <span class=\"pl-en\">print</span>(<span class=\"pl-s\">f&quot;<span class=\"pl-s1\"><span class=\"pl-kos\">{</span><span class=\"pl-s1\">dep</span>.<span class=\"pl-s1\">namespace</span>.<span class=\"pl-s1\">name</span><span class=\"pl-kos\">}</span></span> <span class=\"pl-s1\"><span class=\"pl-kos\">{</span><span class=\"pl-s1\">dep</span>.<span class=\"pl-s1\">spec</span>.<span class=\"pl-s1\">replicas</span><span class=\"pl-kos\">}</span></span>&quot;</span>)</pre></div>\n<p>Create a config map</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-s1\">lightkube</span>.<span class=\"pl-s1\">resources</span>.<span class=\"pl-s1\">core_v1</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ConfigMap</span>\n<span class=\"pl-k\">from</span> <span class=\"pl-s1\">lightkube</span>.<span class=\"pl-s1\">models</span>.<span class=\"pl-s1\">meta_v1</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ObjectMeta</span>\n\n<span class=\"pl-s1\">config</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ConfigMap</span>(\n    <span class=\"pl-s1\">metadata</span><span class=\"pl-c1\">=</span><span class=\"pl-v\">ObjectMeta</span>(<span class=\"pl-s1\">name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">&apos;my-config&apos;</span>, <span class=\"pl-s1\">namespace</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">&apos;default&apos;</span>),\n    <span class=\"pl-s1\">data</span><span class=\"pl-c1\">=</span>{<span class=\"pl-s\">&apos;key1&apos;</span>: <span class=\"pl-s\">&apos;value1&apos;</span>, <span class=\"pl-s\">&apos;key2&apos;</span>: <span class=\"pl-s\">&apos;value2&apos;</span>}\n)\n\n<span class=\"pl-s1\">client</span>.<span class=\"pl-en\">create</span>(<span class=\"pl-s1\">config</span>)</pre></div>\n<p>Replace the previous config with a different content</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-s1\">config</span>.<span class=\"pl-s1\">data</span>[<span class=\"pl-s\">&apos;key1&apos;</span>] <span class=\"pl-c1\">=</span> <span class=\"pl-s\">&apos;new value&apos;</span>\n<span class=\"pl-s1\">client</span>.<span class=\"pl-en\">replace</span>(<span class=\"pl-s1\">config</span>)</pre></div>\n<p>Patch an existing config</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-s1\">patch</span> <span class=\"pl-c1\">=</span> {<span class=\"pl-s\">&apos;metadata&apos;</span>: {<span class=\"pl-s\">&apos;labels&apos;</span>: {<span class=\"pl-s\">&apos;app&apos;</span>: <span class=\"pl-s\">&apos;xyz&apos;</span>}}}\n<span class=\"pl-s1\">client</span>.<span class=\"pl-en\">patch</span>(<span class=\"pl-v\">ConfigMap</span>, <span class=\"pl-s1\">name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">&apos;my-config&apos;</span>, <span class=\"pl-s1\">namespace</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">&apos;default&apos;</span>, <span class=\"pl-s1\">obj</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">patch</span>)</pre></div>\n<p>Delete a namespaced resource</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-s1\">client</span>.<span class=\"pl-en\">delete</span>(<span class=\"pl-v\">ConfigMap</span>, <span class=\"pl-s1\">name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">&apos;my-config&apos;</span>, <span class=\"pl-s1\">namespace</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">&apos;default&apos;</span>)</pre></div>\n<p>Scale a deployment</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-s1\">lightkube</span>.<span class=\"pl-s1\">resources</span>.<span class=\"pl-s1\">apps_v1</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">Deployment</span>\n<span class=\"pl-k\">from</span> <span class=\"pl-s1\">lightkube</span>.<span class=\"pl-s1\">models</span>.<span class=\"pl-s1\">meta_v1</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ObjectMeta</span>\n<span class=\"pl-k\">from</span> <span class=\"pl-s1\">lightkube</span>.<span class=\"pl-s1\">models</span>.<span class=\"pl-s1\">autoscaling_v1</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ScaleSpec</span>\n\n<span class=\"pl-s1\">obj</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Deployment</span>.<span class=\"pl-v\">Scale</span>(\n    <span class=\"pl-s1\">metadata</span><span class=\"pl-c1\">=</span><span class=\"pl-v\">ObjectMeta</span>(<span class=\"pl-s1\">name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">&apos;metrics-server&apos;</span>, <span class=\"pl-s1\">namespace</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">&apos;kube-system&apos;</span>),\n    <span class=\"pl-s1\">spec</span><span class=\"pl-c1\">=</span><span class=\"pl-v\">ScaleSpec</span>(<span class=\"pl-s1\">replicas</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">1</span>)\n)\n<span class=\"pl-s1\">client</span>.<span class=\"pl-en\">replace</span>(<span class=\"pl-s1\">obj</span>, <span class=\"pl-s\">&apos;metrics-server&apos;</span>, <span class=\"pl-s1\">namespace</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">&apos;kube-system&apos;</span>)</pre></div>\n<h2><a id=\"user-content-unsupported-features\" class=\"anchor\" href=\"https://github.com/gtsystem/lightkube#unsupported-features\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Unsupported features</h2>\n<p>The following features are not supported at the moment:</p>\n<ul>\n<li>Special subresources like <code>log</code>, <code>attach</code>, <code>exec</code>, <code>portforward</code> and <code>proxy</code>.</li>\n<li><code>auth-provider</code> authentication method is not supported. The supported\nauthentication methods are <code>token</code>, <code>username</code> + <code>password</code> and <code>exec</code>.</li>\n</ul>\n</article></div></div><hr><h4>Page 2</h4><div><div><article class=\"markdown-body entry-content container-lg\">\n<p><a href=\"https://camo.githubusercontent.com/08b9c0d2d0757b45b3389ebefb087b164d00a1296ff1c0769586001e80660d17/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f776f726b666c6f772f7374617475732f677473797374656d2f6c696768746b7562652f507974686f6e2532307061636b616765\"><img src=\"https://camo.githubusercontent.com/08b9c0d2d0757b45b3389ebefb087b164d00a1296ff1c0769586001e80660d17/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f776f726b666c6f772f7374617475732f677473797374656d2f6c696768746b7562652f507974686f6e2532307061636b616765\" alt></a>\n<a href=\"https://coveralls.io/github/gtsystem/lightkube?branch=master\"><img src=\"https://camo.githubusercontent.com/bea0310eae0e0fe20bdaf4be570d210197068da84572b3f35eda89fefc366830/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f677473797374656d2f6c696768746b7562652f62616467652e7376673f6272616e63683d6d6173746572\" alt=\"Coverage Status\"></a>\n<a href=\"https://pypi.python.org/pypi/lightkube\"><img src=\"https://camo.githubusercontent.com/fb0993854e8e1c64c04df857e9b8cedafe723ef96bf0f3dcb4c7236422e96688/68747470733a2f2f696d672e736869656c64732e696f2f707970692f707976657273696f6e732f6c696768746b7562652e737667\" alt=\"pypi supported versions\"></a></p>\n<p>Modern lightweight kubernetes module for python</p>\n<p><strong>NOTICE:</strong> This project is still under development and not suitable for production usage.</p>\n<h2><a id=\"user-content-highlights\" class=\"anchor\" href=\"#highlights\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Highlights</h2>\n<ul>\n<li><em>Simple</em> interface shared across all kubernetes APIs.</li>\n<li>Extensive <em>type hints</em> to avoid common mistakes and to support autocompletion.</li>\n<li>Models and resources generated from the swagger specifications using standard dataclasses.</li>\n<li>Support for async/await</li>\n<li>Support for installing a specific version of the kubernetes models (1.15 to 1.19)</li>\n<li>Lazy instantiation of inner models.</li>\n<li>Fast startup and small memory footprint as only needed models and resources can be imported.</li>\n<li>Automatic handling of pagination when listing resources.</li>\n<li>Customizable handling of errors when watching resources.</li>\n</ul>\n<p>This module is powered by <a href=\"https://github.com/encode/httpx/tree/master/httpx\">httpx</a>.</p>\n<h2><a id=\"user-content-installation\" class=\"anchor\" href=\"#installation\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Installation</h2>\n<p>This module requires python &gt;= 3.6</p>\n<pre><code>pip install lightkube\n</code></pre>\n<h2><a id=\"user-content-usage\" class=\"anchor\" href=\"#usage\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Usage</h2>\n<p>Read a pod</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-s1\">lightkube</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">Client</span>\n<span class=\"pl-k\">from</span> <span class=\"pl-s1\">lightkube</span>.<span class=\"pl-s1\">resources</span>.<span class=\"pl-s1\">core_v1</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">Pod</span>\n\n<span class=\"pl-s1\">client</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Client</span>()\n<span class=\"pl-s1\">pod</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">client</span>.<span class=\"pl-en\">get</span>(<span class=\"pl-v\">Pod</span>, <span class=\"pl-s1\">name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">&quot;my-pod&quot;</span>, <span class=\"pl-s1\">namespace</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">&quot;default&quot;</span>)\n<span class=\"pl-en\">print</span>(<span class=\"pl-s1\">pod</span>.<span class=\"pl-s1\">namespace</span>.<span class=\"pl-s1\">uid</span>)</pre></div>\n<p>List nodes</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-s1\">lightkube</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">Client</span>\n<span class=\"pl-k\">from</span> <span class=\"pl-s1\">lightkube</span>.<span class=\"pl-s1\">resources</span>.<span class=\"pl-s1\">core_v1</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">Node</span>\n\n<span class=\"pl-s1\">client</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Client</span>()\n<span class=\"pl-k\">for</span> <span class=\"pl-s1\">node</span> <span class=\"pl-c1\">in</span> <span class=\"pl-s1\">client</span>.<span class=\"pl-en\">list</span>(<span class=\"pl-v\">Node</span>):\n    <span class=\"pl-en\">print</span>(<span class=\"pl-s1\">node</span>.<span class=\"pl-s1\">metadata</span>.<span class=\"pl-s1\">name</span>)</pre></div>\n<p>Watch deployments</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-s1\">lightkube</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">Client</span>\n<span class=\"pl-k\">from</span> <span class=\"pl-s1\">lightkube</span>.<span class=\"pl-s1\">resources</span>.<span class=\"pl-s1\">apps_v1</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">Deployment</span>\n\n<span class=\"pl-s1\">client</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Client</span>()\n<span class=\"pl-k\">for</span> <span class=\"pl-s1\">op</span>, <span class=\"pl-s1\">dep</span> <span class=\"pl-c1\">in</span> <span class=\"pl-s1\">client</span>.<span class=\"pl-en\">watch</span>(<span class=\"pl-v\">Deployment</span>, <span class=\"pl-s1\">namespace</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">&quot;default&quot;</span>):\n    <span class=\"pl-en\">print</span>(<span class=\"pl-s\">f&quot;<span class=\"pl-s1\"><span class=\"pl-kos\">{</span><span class=\"pl-s1\">dep</span>.<span class=\"pl-s1\">namespace</span>.<span class=\"pl-s1\">name</span><span class=\"pl-kos\">}</span></span> <span class=\"pl-s1\"><span class=\"pl-kos\">{</span><span class=\"pl-s1\">dep</span>.<span class=\"pl-s1\">spec</span>.<span class=\"pl-s1\">replicas</span><span class=\"pl-kos\">}</span></span>&quot;</span>)</pre></div>\n<p>Create a config map</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-s1\">lightkube</span>.<span class=\"pl-s1\">resources</span>.<span class=\"pl-s1\">core_v1</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ConfigMap</span>\n<span class=\"pl-k\">from</span> <span class=\"pl-s1\">lightkube</span>.<span class=\"pl-s1\">models</span>.<span class=\"pl-s1\">meta_v1</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ObjectMeta</span>\n\n<span class=\"pl-s1\">config</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">ConfigMap</span>(\n    <span class=\"pl-s1\">metadata</span><span class=\"pl-c1\">=</span><span class=\"pl-v\">ObjectMeta</span>(<span class=\"pl-s1\">name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">&apos;my-config&apos;</span>, <span class=\"pl-s1\">namespace</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">&apos;default&apos;</span>),\n    <span class=\"pl-s1\">data</span><span class=\"pl-c1\">=</span>{<span class=\"pl-s\">&apos;key1&apos;</span>: <span class=\"pl-s\">&apos;value1&apos;</span>, <span class=\"pl-s\">&apos;key2&apos;</span>: <span class=\"pl-s\">&apos;value2&apos;</span>}\n)\n\n<span class=\"pl-s1\">client</span>.<span class=\"pl-en\">create</span>(<span class=\"pl-s1\">config</span>)</pre></div>\n<p>Replace the previous config with a different content</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-s1\">config</span>.<span class=\"pl-s1\">data</span>[<span class=\"pl-s\">&apos;key1&apos;</span>] <span class=\"pl-c1\">=</span> <span class=\"pl-s\">&apos;new value&apos;</span>\n<span class=\"pl-s1\">client</span>.<span class=\"pl-en\">replace</span>(<span class=\"pl-s1\">config</span>)</pre></div>\n<p>Patch an existing config</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-s1\">patch</span> <span class=\"pl-c1\">=</span> {<span class=\"pl-s\">&apos;metadata&apos;</span>: {<span class=\"pl-s\">&apos;labels&apos;</span>: {<span class=\"pl-s\">&apos;app&apos;</span>: <span class=\"pl-s\">&apos;xyz&apos;</span>}}}\n<span class=\"pl-s1\">client</span>.<span class=\"pl-en\">patch</span>(<span class=\"pl-v\">ConfigMap</span>, <span class=\"pl-s1\">name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">&apos;my-config&apos;</span>, <span class=\"pl-s1\">namespace</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">&apos;default&apos;</span>, <span class=\"pl-s1\">obj</span><span class=\"pl-c1\">=</span><span class=\"pl-s1\">patch</span>)</pre></div>\n<p>Delete a namespaced resource</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-s1\">client</span>.<span class=\"pl-en\">delete</span>(<span class=\"pl-v\">ConfigMap</span>, <span class=\"pl-s1\">name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">&apos;my-config&apos;</span>, <span class=\"pl-s1\">namespace</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">&apos;default&apos;</span>)</pre></div>\n<p>Scale a deployment</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-s1\">lightkube</span>.<span class=\"pl-s1\">resources</span>.<span class=\"pl-s1\">apps_v1</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">Deployment</span>\n<span class=\"pl-k\">from</span> <span class=\"pl-s1\">lightkube</span>.<span class=\"pl-s1\">models</span>.<span class=\"pl-s1\">meta_v1</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ObjectMeta</span>\n<span class=\"pl-k\">from</span> <span class=\"pl-s1\">lightkube</span>.<span class=\"pl-s1\">models</span>.<span class=\"pl-s1\">autoscaling_v1</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">ScaleSpec</span>\n\n<span class=\"pl-s1\">obj</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">Deployment</span>.<span class=\"pl-v\">Scale</span>(\n    <span class=\"pl-s1\">metadata</span><span class=\"pl-c1\">=</span><span class=\"pl-v\">ObjectMeta</span>(<span class=\"pl-s1\">name</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">&apos;metrics-server&apos;</span>, <span class=\"pl-s1\">namespace</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">&apos;kube-system&apos;</span>),\n    <span class=\"pl-s1\">spec</span><span class=\"pl-c1\">=</span><span class=\"pl-v\">ScaleSpec</span>(<span class=\"pl-s1\">replicas</span><span class=\"pl-c1\">=</span><span class=\"pl-c1\">1</span>)\n)\n<span class=\"pl-s1\">client</span>.<span class=\"pl-en\">replace</span>(<span class=\"pl-s1\">obj</span>, <span class=\"pl-s\">&apos;metrics-server&apos;</span>, <span class=\"pl-s1\">namespace</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">&apos;kube-system&apos;</span>)</pre></div>\n<h2><a id=\"user-content-unsupported-features\" class=\"anchor\" href=\"#unsupported-features\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Unsupported features</h2>\n<p>The following features are not supported at the moment:</p>\n<ul>\n<li>Special subresources like <code>log</code>, <code>attach</code>, <code>exec</code>, <code>portforward</code> and <code>proxy</code>.</li>\n<li><code>auth-provider</code> authentication method is not supported. The supported\nauthentication methods are <code>token</code>, <code>username</code> + <code>password</code> and <code>exec</code>.</li>\n</ul>\n</article></div></div>",
      "contentAsText": "\n\n\n\nModern lightweight kubernetes module for python\nNOTICE: This project is still under development and not suitable for production usage.\nHighlights\n\nSimple interface shared across all kubernetes APIs.\nExtensive type hints to avoid common mistakes and to support autocompletion.\nModels and resources generated from the swagger specifications using standard dataclasses.\nSupport for async/await\nSupport for installing a specific version of the kubernetes models (1.15 to 1.19)\nLazy instantiation of inner models.\nFast startup and small memory footprint as only needed models and resources can be imported.\nAutomatic handling of pagination when listing resources.\nCustomizable handling of errors when watching resources.\n\nThis module is powered by httpx.\nInstallation\nThis module requires python >= 3.6\npip install lightkube\n\nUsage\nRead a pod\nfrom lightkube import Client\nfrom lightkube.resources.core_v1 import Pod\n\nclient = Client()\npod = client.get(Pod, name=\"my-pod\", namespace=\"default\")\nprint(pod.namespace.uid)\nList nodes\nfrom lightkube import Client\nfrom lightkube.resources.core_v1 import Node\n\nclient = Client()\nfor node in client.list(Node):\n    print(node.metadata.name)\nWatch deployments\nfrom lightkube import Client\nfrom lightkube.resources.apps_v1 import Deployment\n\nclient = Client()\nfor op, dep in client.watch(Deployment, namespace=\"default\"):\n    print(f\"{dep.namespace.name} {dep.spec.replicas}\")\nCreate a config map\nfrom lightkube.resources.core_v1 import ConfigMap\nfrom lightkube.models.meta_v1 import ObjectMeta\n\nconfig = ConfigMap(\n    metadata=ObjectMeta(name='my-config', namespace='default'),\n    data={'key1': 'value1', 'key2': 'value2'}\n)\n\nclient.create(config)\nReplace the previous config with a different content\nconfig.data['key1'] = 'new value'\nclient.replace(config)\nPatch an existing config\npatch = {'metadata': {'labels': {'app': 'xyz'}}}\nclient.patch(ConfigMap, name='my-config', namespace='default', obj=patch)\nDelete a namespaced resource\nclient.delete(ConfigMap, name='my-config', namespace='default')\nScale a deployment\nfrom lightkube.resources.apps_v1 import Deployment\nfrom lightkube.models.meta_v1 import ObjectMeta\nfrom lightkube.models.autoscaling_v1 import ScaleSpec\n\nobj = Deployment.Scale(\n    metadata=ObjectMeta(name='metrics-server', namespace='kube-system'),\n    spec=ScaleSpec(replicas=1)\n)\nclient.replace(obj, 'metrics-server', namespace='kube-system')\nUnsupported features\nThe following features are not supported at the moment:\n\nSpecial subresources like log, attach, exec, portforward and proxy.\nauth-provider authentication method is not supported. The supported\nauthentication methods are token, username + password and exec.\n\nPage 2\n\n\n\nModern lightweight kubernetes module for python\nNOTICE: This project is still under development and not suitable for production usage.\nHighlights\n\nSimple interface shared across all kubernetes APIs.\nExtensive type hints to avoid common mistakes and to support autocompletion.\nModels and resources generated from the swagger specifications using standard dataclasses.\nSupport for async/await\nSupport for installing a specific version of the kubernetes models (1.15 to 1.19)\nLazy instantiation of inner models.\nFast startup and small memory footprint as only needed models and resources can be imported.\nAutomatic handling of pagination when listing resources.\nCustomizable handling of errors when watching resources.\n\nThis module is powered by httpx.\nInstallation\nThis module requires python >= 3.6\npip install lightkube\n\nUsage\nRead a pod\nfrom lightkube import Client\nfrom lightkube.resources.core_v1 import Pod\n\nclient = Client()\npod = client.get(Pod, name=\"my-pod\", namespace=\"default\")\nprint(pod.namespace.uid)\nList nodes\nfrom lightkube import Client\nfrom lightkube.resources.core_v1 import Node\n\nclient = Client()\nfor node in client.list(Node):\n    print(node.metadata.name)\nWatch deployments\nfrom lightkube import Client\nfrom lightkube.resources.apps_v1 import Deployment\n\nclient = Client()\nfor op, dep in client.watch(Deployment, namespace=\"default\"):\n    print(f\"{dep.namespace.name} {dep.spec.replicas}\")\nCreate a config map\nfrom lightkube.resources.core_v1 import ConfigMap\nfrom lightkube.models.meta_v1 import ObjectMeta\n\nconfig = ConfigMap(\n    metadata=ObjectMeta(name='my-config', namespace='default'),\n    data={'key1': 'value1', 'key2': 'value2'}\n)\n\nclient.create(config)\nReplace the previous config with a different content\nconfig.data['key1'] = 'new value'\nclient.replace(config)\nPatch an existing config\npatch = {'metadata': {'labels': {'app': 'xyz'}}}\nclient.patch(ConfigMap, name='my-config', namespace='default', obj=patch)\nDelete a namespaced resource\nclient.delete(ConfigMap, name='my-config', namespace='default')\nScale a deployment\nfrom lightkube.resources.apps_v1 import Deployment\nfrom lightkube.models.meta_v1 import ObjectMeta\nfrom lightkube.models.autoscaling_v1 import ScaleSpec\n\nobj = Deployment.Scale(\n    metadata=ObjectMeta(name='metrics-server', namespace='kube-system'),\n    spec=ScaleSpec(replicas=1)\n)\nclient.replace(obj, 'metrics-server', namespace='kube-system')\nUnsupported features\nThe following features are not supported at the moment:\n\nSpecial subresources like log, attach, exec, portforward and proxy.\nauth-provider authentication method is not supported. The supported\nauthentication methods are token, username + password and exec.\n\n",
      "description": "Modern lightweight kubernetes module for python. Contribute to gtsystem/lightkube development by creating an account on GitHub.",
      "ogDescription": "Modern lightweight kubernetes module for python. Contribute to gtsystem/lightkube development by creating an account on GitHub."
    },
    {
      "url": "https://www.getcortexapp.com/post/understanding-kubernetes-services-ingress-networking",
      "title": "Cortex",
      "content": "<div class=\"article w-richtext\"><p>In the previous<a href=\"https://www.getcortexapp.com/post/understanding-kubernetes\"> article</a>, we looked into the basics of Kubernetes and setting up and running Kubernetes in a local machine. There we had briefly discussed Kubernetes objects called Services. Services are Kubernetes resources that enable network access to Pods. In this article, we will look deeply into the concepts of Kubernetes Services and its different types. We will also look into Kubernetes Ingress, which is not a service but is another way of routing traffic to your services and your cluster.</p><h3><strong>Kubernetes Services</strong></h3><p>As we know, a Kubernetes cluster consists of a set of node machines, running containerized applications inside objects named <em>Pods</em>. The pods are grouped based on the type of service they provide into various groups. Pods must be able to accept connections in some way, from your cluster or from outside your cluster.</p><p>In the case of external access, we know that pods inside the cluster are present inside an internal pod network and cannot be accessed by the node\u0019s IP address. A user should be able to communicate with the application using the IP address of the node.&#xFFFD;</p><p>In the case of internal communication, we know that each pod in the system is assigned with its own unique IP known as Pod IP. But these IPs are not static, as we know the pods can go down any time and new pods are created all the time in a cluster. So we cannot rely on these IPs for Internal communication.</p><p>So we need something that is consistent so that things outside or inside the cluster might be able to access it persistently. A <strong>Service</strong> is a Kubernetes object that acts as an endpoint for enabling the communication between various components within and outside the application. In other words, a service is a stable address for pods. The three important Service types in Kubernetes are:</p><ol><li>ClusterIP</li><li>NodePort</li><li>LoadBalancer</li></ol><h4><strong>ClusterIP</strong></h4><p>A full-stack web application typically is made up of different kinds of pods hosting different parts of the application. It may have a set of pods running a backend server, a set of pods running the front-end web server and a set of pods running a database, and so on. All these sets of pods need to communicate with each other. As we discussed, we can\u0019t depend on the IP addresses of pods, since they are not static.</p><p>ClusterIP is a Kubernetes service type that is used to group pods together and provide a single interface to access them. For example, an incoming request by another service will be forwarded to one of the pods in the ClusterIP randomly.</p><p>Now let\u0019s look at an example. Before creating the ClusterIP service we can start by creating a simple pod based on a definition file.</p><p><em>front-end-pod-definition.yml</em></p><blockquote>apiVersion: v1kind: Podmetadata: name: myapp-pod labels: &#xFFFD; app: myapp &#xFFFD; type: front-endspec: containers: &#xFFFD; - name: nginx-container<p> &#xFFFD; &#xFFFD; image: nginx</p></blockquote><p>As we can see our pod is simply a container that has the Nginx web server behind it. We have added labels <em>app</em> and <em>type</em>. Pod will be grouped into the type front-end. Next, we need to run the create command to create the pod.</p><blockquote>kubectl create -f frontend-pod-definition.yml</blockquote><p>Let&apos;s look at the ClusterIP service definition:</p><p><em>fe-clusterip-service-definition.yml</em></p><blockquote>apiVersion: v1kind: Servicemetadata: name: front-end-servicespec: type: ClusterIP selector: &#xFFFD; app: myapp &#xFFFD; type: front-end ports: &#xFFFD; - targetPort: 80<p> &#xFFFD; &#xFFFD; port: 80</p></blockquote><p>The Service definition has type as ClusterIP (it&apos;s not mandatory, as by default services are of kind ClusterIP). We can see that we have used the selector to link the service to a set of pods. Under ports, we have a target port and port.&#xFFFD;</p><p>The <strong>target port</strong> is the port where the front-end service is exposed which in this case is 80 and the <strong>port</strong> is where the ClusterIP service is exposed which is also 80.</p><p>Now we can create the service by the create command.</p><blockquote>kubectl create -f clusterip-service-definition.yml</blockquote><p>Let\u0019s look at the service created</p><figure class=\"w-richtext-align-center w-richtext-figure-type-image\"><div><img src=\"https://assets.website-files.com/5ec5ad145468d2d2ff78b364/5f6bc5225587b6be7c95e156_i9voFNRyRthFr7g2dH40vaqeQNw7srlhOe4BlZKsWJBlBZrw1lQdb6hmio0_vxDbohpPx_qdCNZHTV3KcsXhPb1fNYLCrN-QHIlusoMKlOhSmvW3ov44pDqbL377PWbZQT6hb3g.png\" alt></div></figure><p>We can see that in addition to the default Kubernetes ClusterIP a new ClusterIP of the name <em>front-end-service</em> is created with an IP address. The name of the service can be used by other pods to access it.</p><h4><strong>NodePort</strong></h4><p>NodePort is a Kubernetes service type that listens on a port on the node and forward requests on that port to a pod on the node. Let&apos;s look at an example.&#xFFFD;</p><ul><li>We have a node with IP address <em>10.1.3.4</em>.&#xFFFD;</li><li>The internal pod network of the node is in the range 10.244.0.0</li><li>The pod itself has an IP of 10.244.0.2.&#xFFFD;</li><li>The actual web server is running on port 80 in the pod.&#xFFFD;</li></ul><p>Essentially, we want to forward requests coming to 10.1.3.4 to the pod.</p><p>When we create a NodePort service, the service is assigned a high port on all nodes. When a request comes in for <em>node:port</em>, it will act as a built-in load balancer and send the request to one of the pods at random.</p><p>Let\u0019s create a NodePort service to forward the incoming request to the node to port 80 of the pod. Let\u0019s start by creating a service definition:</p><p>nodeport-service-definition.yml</p><blockquote>apiVersion: v1kind: Servicemetadata: name: myapp-servicespec: type: NodePort selector: &#xFFFD; app: myapp &#xFFFD; type: front-end ports: &#xFFFD; - targetPort: 80 &#xFFFD; &#xFFFD; port: 80<p> &#xFFFD; &#xFFFD; nodePort: 32593</p></blockquote><p>We can see three values in the ports section.&#xFFFD;</p><p><strong>targetPort</strong>: The port on the pod where the actual web server is running, that is 80 in this case. Service forwards the requests to the target port. If no ports are provided in the spec, it will default to 80</p><p><strong>port</strong>: Like all Kubernetes objects, the Service is a virtual server inside the node. Inside the cluster, it will have its own IP address. The \u0018port\u0019 is the port exposed to the NodePort service itself. This value is mandatory.</p><p><strong>nodePort: </strong>The port on the node which is used to access the web server externally. These ports can only be in a valid range from 30000 to 32767. This is not a mandatory field, if it is not provided a free port from the range is selected.</p><p>Now we can create the service by the command,</p><blockquote>kubectl create -f nodeport-service-definition.yml</blockquote><p>Let&apos;s check if the service is created.</p><figure class=\"w-richtext-align-center w-richtext-figure-type-image\"><div><img src=\"https://assets.website-files.com/5ec5ad145468d2d2ff78b364/5f6bc522c1dac43f794be000_AIDxqYyNHBc2JxSPheW8ML0VJVtHeUW6PN6kSD0u-LShhTA4_2oZTcPs9N7nYvjIcJg_--8EYDoPyFO-xqmjPavONwZo9Uee4JKd8Jan3nNJg1M8xPSOCEJsoQabVJuFqT4VLXM.png\" alt></div></figure><p>Let&apos;s try to access the service using the IP of the node</p><p>Since I am using Minikube, the IP of the node is different from the local IP of the system. To get that value, type the command below&#xFFFD; in the terminal</p><blockquote>minikube ip</blockquote><p>Let&apos;s use curl to access the app using the NodePort in this IP</p><blockquote>curl 192.168.99.101:32593</blockquote><figure class=\"w-richtext-align-center w-richtext-figure-type-image\"><div><img src=\"https://assets.website-files.com/5ec5ad145468d2d2ff78b364/5f6bc5224d5a3d39180e7560_Xphy_GvMOUKXeCtxr8yqhjNzsjqB9Ud_3LsLCtTmoooC6Q8oZmRHWvm2NxoWczFNJG2CMlNy9_Rl-ApmBicvvTffdy-V_ip1pluzFWqDl3_5SSWHaMMv_hbiewo8ZC9X6C7dHC8.png\" alt></div></figure><p>Great! We got a response from the pod.</p><h4><strong>LoadBalancer</strong></h4><p>Using nodePort we were able to expose our web app to the internet. However, there\u0019s a problem - multiple instances of the web app can be deployed across multiple nodes in our cluster. To access this web app, we\u0019d need to provide both a node IP and the node port to the user. In real life, it\u0019s difficult to determine which node IP and node port should be provided to the user, manually. Instead, we need to have a load balancer to expose our web app to the internet.</p><p>A LoadBalancer is a service that provides (as you may have guessed) a load balancer for our application, in supported cloud providers. The service becomes accessible through a provided load balancer service. Most cloud providers like AWS, GCP, Azure offer this functionality. Once you create a service of type LoadBalancer, cloud providers will create a load balancer in the backend and generate a public IP address. This public IP can be used to access our web app from the public internet.</p><p>This is the standard way to directly expose a service to the Internet. It is similar to the NodePort where all the traffic on the port we specify will be forwarded to the service. Almost all kinds of traffic like HTTP, TCP, UDP, Websockets, gRPC etc can be sent to this service.</p><p>Let&apos;s look at an example definition file:</p><blockquote>apiVersion: v1kind: Servicemetadata: name: my-servicespec: selector: &#xFFFD; app: myapp type: LoadBalancer ports: &#xFFFD; - nodePort: 31000 &#xFFFD; &#xFFFD; port: 80<p> &#xFFFD; &#xFFFD; targetPort: 9376</p></blockquote><p>We can see that this is almost the same as a NodePort definition file.</p><p>Let&apos;s create the service with create command</p><blockquote>kubectl create -f load-balancer-service-definition.yml</blockquote><p>Now let&apos;s look at the service that got created using the command.&#xFFFD;</p><blockquote>kubectl get services</blockquote><figure class=\"w-richtext-align-center w-richtext-figure-type-image\"><div><img src=\"https://assets.website-files.com/5ec5ad145468d2d2ff78b364/5f6bc522e9914240281c4d1a_I27mI6NbO5gl6KqJFVQ0zJTr8t7r5aV43JKZEGNtfUkG_XXarnt1_RavpLnhhL66aDErRKDQemO728-Uog59_xwdOhUGwbuUU9c6gncjaDP43Aq71OVTM13Oq6GzRCao3bnrlB0.png\" alt></div></figure><p>You can see that since I am using Minikube the value of the external IP is shown as &lt;pending&gt;. However, in an actual cloud setup, the IP will be generated and can be used to access the application. This is the IP that can be used by our users to access our web app from the internet.</p><h3><strong>Ingress Networking</strong></h3><p>We have seen in the Kubernetes services sections on how to expose our application to the outside world using the NodePort and LoadBalancer. If we only have to have a single service port we can use NodePort. In the case of multiple instances of the same service, we have to use the LoadBalancer.&#xFFFD;</p><p>But what if we have to add one more service to our node and access it from another URL. In this case, we will have to add another load balancer to our cluster. This means that each service exposed with a LoadBalancer will get its own IP address and we will have to pay for each of these load balancers which can be quite expensive.</p><p>An Ingress is used when we have multiple services on our cluster and we want the user request routed to the service based on their path. Consider an example, I have two services foo and bar in our cluster. When we type<a href=\"http://www.example.com/foo\"> www.example.com/foo</a> we should be routed to the foo service and <a href=\"http://www.example.com/bar\">www.example.com/bar</a> should be routed to bar service. These routings will be performed by an Ingress. Unlike NodePort or LoadBalancer, Ingress is not actually a type of service. Instead, it is an entry point that sits in front of multiple services in the cluster. It can be defined as a collection of routing rules that govern how external users access services running inside a Kubernetes cluster.</p><p>Ingress is most useful if you want to expose multiple services under the same IP address, and these services all use the same L7 protocol (typically HTTP). You only pay for one load balancer if you are using the native GCP integration, and because Ingress is \u001csmart\u001d you can get a lot of features out of the box (like SSL, Auth, Routing, etc)</p><p>Ingress can be considered as the best way to expose multiple services under the same IP. Also, we should only pay for a single load balancer.</p><p>Let&apos;s see how Ingress works. Before we implement the Ingress we need to deploy a supported reverse proxy or load balancing solution like Nginx, Haproxy, or Trafik. Then we need to specify a set of rules to configure the Ingress. The solution we deploy is called an ingress controller and the set of rules that we configure are called as ingress resources. Ingress resources are created using definition files like the ones we used to create pods and deployments.</p><h4><strong>Ingress Controller</strong></h4><p>The Ingress controller is not a part of the Kubernetes cluster by default. So we cannot simply create an Ingress resource and expect it to work. There are a number of solutions available for Ingress. A few of them being GCE which is Google\u0019s layers of HTTP load balancer Nginx, Contour, Haproxy Traefik, and Istio. Out of this, GCE and Nginx are currently being supported and maintained by the Kubernetes project.</p><p>The Ingress Controller is not just another load balancer or a reverse proxy service. They have additional components that monitor the Kubernetes cluster for new definitions and Ingress resources and configure the service accordingly.</p><p>We will be looking at Nginx as an example. Nginx controllers can be deployed just like another deployment into Kubernetes. Here is a sample definition file:</p><blockquote>apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx-ingress-controllerspec: replicas: 1 selector: &#xFFFD; matchLabels: &#xFFFD; &#xFFFD; name: nginx-ingress template: &#xFFFD; metadata: &#xFFFD; &#xFFFD; labels: &#xFFFD; &#xFFFD; &#xFFFD; name: nginx-ingress &#xFFFD; spec: &#xFFFD; &#xFFFD; containers: &#xFFFD; &#xFFFD; &#xFFFD; - name: nginx-ingress-controller &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0 &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; args: &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; - /nginx-ingress-controller &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; env: &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; - name: POD_NAME &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; valueFrom: &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; fieldRef: &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; fieldPath: metadata.name &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; - name: POD_NAMESPACE &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; valueFrom: &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; fieldRef: &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; fieldPath: metadata.namespace &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; ports: &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; - name: http &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; containerPort: 80 &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; - name: https<p> &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; containerPort: 443</p></blockquote><p>We can see that our deployment is named nginx-ingress-controller and has one replica. It contains a pod definition labeled as nginx-ingress. In the spec, we can see that we are using a special build of Nginx specifically built to be used as an ingress controller. This image has its own set of requirements. The first argument is the location of the Nginx program. Next, we have to pass a config map that stores the configurations of Nginx like keep-alive threshold, SSL settings, etc.</p><p>Config map definition sample:</p><blockquote>kind: ConfigMapapiVersion: v1metadata: name: nginx-configurationdata: map-hash-bucket-size: &quot;128&quot;<p> ssl-protocols: SSLv2</p></blockquote><p>We have also passed the environment variables carrying the pod\u0019s name and the namespace it is deployed to. The service requires these values to read the configuration data of the pod.</p><p>Finally, we have specified the ports used by the ingress controller 80 and 443.</p><p>Next, let&apos;s expose the controller to the outside world using a service. Let&apos;s create a service of the kind NodePort:</p><blockquote>apiVersion: v1kind: Servicemetadata: name: myapp-servicespec: type: NodePort selector: &#xFFFD; name: nginx-ingress ports: &#xFFFD; - targetPort: 80 &#xFFFD; &#xFFFD; port: 80 &#xFFFD; &#xFFFD; protocol: tcp &#xFFFD; &#xFFFD; name: http &#xFFFD; - targetPort: 443 &#xFFFD; &#xFFFD; port: 443 &#xFFFD; &#xFFFD; protocol: tcp<p> &#xFFFD; &#xFFFD; name: http</p></blockquote><p>Now that our controller is ready, let&apos;s look into the rules required to configure the Ingress.</p><h4><strong>Ingress Resource</strong></h4><p>An Ingress Resource is a set of rules and configurations applied to the Ingress controller. The rules can be specified to forward all incoming traffic to a single application or route the traffic to different applications. So in our example, when a user hits the \u0018foo\u0019 URL, then route them to the foo application or if the user hits the \u0018bar\u0019 URL, then route them to the bar application. Similarly, the request should be forwarded based on the domain name .</p><p>We can create an Ingress resource with a Kubernetes definition file. Let&apos;s look at some examples.</p><p>First, let&apos;s create a resource to route the incoming request based on the path:</p><blockquote>apiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-foo-barspec:&#xFFFD; rules: &#xFFFD; - http:&#xFFFD; &#xFFFD; &#xFFFD; paths:&#xFFFD; &#xFFFD; &#xFFFD; - path: /foo&#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; backend:&#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; serviceName: foo-service&#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; servicePort: 80&#xFFFD; &#xFFFD; &#xFFFD; - path: /bar&#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; backend:&#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; serviceName: bar-service<p>&#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; servicePort: 80</p></blockquote><p>We have specified an object of kind Ingress named ingress-foo-bar. And in the spec, we have defined two rules. The first rule will check if our request URL is of the foo type or bar type and serves the foo-service or bar service accordingly to the user. The backend part of the rule must contain the service name and a service port.</p><p>Now let&apos;s look at an example where the routing is based on the domain name:</p><blockquote>apiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-foo-barspec:&#xFFFD; rules:&#xFFFD; - host: foo.example.com&#xFFFD; &#xFFFD; http: &#xFFFD; &#xFFFD; paths: &#xFFFD; &#xFFFD; - backend: &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; serviceName: foo-service &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; servicePort: 80 &#xFFFD; - host: bar.example.com &#xFFFD; &#xFFFD; http: &#xFFFD; &#xFFFD; &#xFFFD; paths: &#xFFFD; &#xFFFD; &#xFFFD; - backend: &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; serviceName: bar-service<p> &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; &#xFFFD; servicePort: 80</p></blockquote><p>Here we can see instead of paths in the rule we have hosts which are domain names. Also, we can see that splitting traffic by URL, has just one rule with two path specifications and splitting traffic by hostname have two rules with one path in each rule.</p><p>If the users type a URL that is not specified in the rule, they will be forwarded to a default URL that can be configured.</p><h4><strong>Wrapping Up</strong></h4><p>In this article, we have gone through the various Kubernetes Services and Ingress based on very simple examples. It should be noted that in real-life use cases can get more complicated and the Service solutions should be selected based on the requirements and complexity of the applications. Hope this article was helpful to you. Do check out the official documentation to find more examples and scenarios.</p></div>",
      "contentAsText": "In the previous article, we looked into the basics of Kubernetes and setting up and running Kubernetes in a local machine. There we had briefly discussed Kubernetes objects called Services. Services are Kubernetes resources that enable network access to Pods. In this article, we will look deeply into the concepts of Kubernetes Services and its different types. We will also look into Kubernetes Ingress, which is not a service but is another way of routing traffic to your services and your cluster.Kubernetes ServicesAs we know, a Kubernetes cluster consists of a set of node machines, running containerized applications inside objects named Pods. The pods are grouped based on the type of service they provide into various groups. Pods must be able to accept connections in some way, from your cluster or from outside your cluster.In the case of external access, we know that pods inside the cluster are present inside an internal pod network and cannot be accessed by the node\u0019s IP address. A user should be able to communicate with the application using the IP address of the node.�In the case of internal communication, we know that each pod in the system is assigned with its own unique IP known as Pod IP. But these IPs are not static, as we know the pods can go down any time and new pods are created all the time in a cluster. So we cannot rely on these IPs for Internal communication.So we need something that is consistent so that things outside or inside the cluster might be able to access it persistently. A Service is a Kubernetes object that acts as an endpoint for enabling the communication between various components within and outside the application. In other words, a service is a stable address for pods. The three important Service types in Kubernetes are:ClusterIPNodePortLoadBalancerClusterIPA full-stack web application typically is made up of different kinds of pods hosting different parts of the application. It may have a set of pods running a backend server, a set of pods running the front-end web server and a set of pods running a database, and so on. All these sets of pods need to communicate with each other. As we discussed, we can\u0019t depend on the IP addresses of pods, since they are not static.ClusterIP is a Kubernetes service type that is used to group pods together and provide a single interface to access them. For example, an incoming request by another service will be forwarded to one of the pods in the ClusterIP randomly.Now let\u0019s look at an example. Before creating the ClusterIP service we can start by creating a simple pod based on a definition file.front-end-pod-definition.ymlapiVersion: v1kind: Podmetadata: name: myapp-pod labels: � app: myapp � type: front-endspec: containers: � - name: nginx-container � � image: nginxAs we can see our pod is simply a container that has the Nginx web server behind it. We have added labels app and type. Pod will be grouped into the type front-end. Next, we need to run the create command to create the pod.kubectl create -f frontend-pod-definition.ymlLet's look at the ClusterIP service definition:fe-clusterip-service-definition.ymlapiVersion: v1kind: Servicemetadata: name: front-end-servicespec: type: ClusterIP selector: � app: myapp � type: front-end ports: � - targetPort: 80 � � port: 80The Service definition has type as ClusterIP (it's not mandatory, as by default services are of kind ClusterIP). We can see that we have used the selector to link the service to a set of pods. Under ports, we have a target port and port.�The target port is the port where the front-end service is exposed which in this case is 80 and the port is where the ClusterIP service is exposed which is also 80.Now we can create the service by the create command.kubectl create -f clusterip-service-definition.ymlLet\u0019s look at the service createdWe can see that in addition to the default Kubernetes ClusterIP a new ClusterIP of the name front-end-service is created with an IP address. The name of the service can be used by other pods to access it.NodePortNodePort is a Kubernetes service type that listens on a port on the node and forward requests on that port to a pod on the node. Let's look at an example.�We have a node with IP address 10.1.3.4.�The internal pod network of the node is in the range 10.244.0.0The pod itself has an IP of 10.244.0.2.�The actual web server is running on port 80 in the pod.�Essentially, we want to forward requests coming to 10.1.3.4 to the pod.When we create a NodePort service, the service is assigned a high port on all nodes. When a request comes in for node:port, it will act as a built-in load balancer and send the request to one of the pods at random.Let\u0019s create a NodePort service to forward the incoming request to the node to port 80 of the pod. Let\u0019s start by creating a service definition:nodeport-service-definition.ymlapiVersion: v1kind: Servicemetadata: name: myapp-servicespec: type: NodePort selector: � app: myapp � type: front-end ports: � - targetPort: 80 � � port: 80 � � nodePort: 32593We can see three values in the ports section.�targetPort: The port on the pod where the actual web server is running, that is 80 in this case. Service forwards the requests to the target port. If no ports are provided in the spec, it will default to 80port: Like all Kubernetes objects, the Service is a virtual server inside the node. Inside the cluster, it will have its own IP address. The \u0018port\u0019 is the port exposed to the NodePort service itself. This value is mandatory.nodePort: The port on the node which is used to access the web server externally. These ports can only be in a valid range from 30000 to 32767. This is not a mandatory field, if it is not provided a free port from the range is selected.Now we can create the service by the command,kubectl create -f nodeport-service-definition.ymlLet's check if the service is created.Let's try to access the service using the IP of the nodeSince I am using Minikube, the IP of the node is different from the local IP of the system. To get that value, type the command below� in the terminalminikube ipLet's use curl to access the app using the NodePort in this IPcurl 192.168.99.101:32593Great! We got a response from the pod.LoadBalancerUsing nodePort we were able to expose our web app to the internet. However, there\u0019s a problem - multiple instances of the web app can be deployed across multiple nodes in our cluster. To access this web app, we\u0019d need to provide both a node IP and the node port to the user. In real life, it\u0019s difficult to determine which node IP and node port should be provided to the user, manually. Instead, we need to have a load balancer to expose our web app to the internet.A LoadBalancer is a service that provides (as you may have guessed) a load balancer for our application, in supported cloud providers. The service becomes accessible through a provided load balancer service. Most cloud providers like AWS, GCP, Azure offer this functionality. Once you create a service of type LoadBalancer, cloud providers will create a load balancer in the backend and generate a public IP address. This public IP can be used to access our web app from the public internet.This is the standard way to directly expose a service to the Internet. It is similar to the NodePort where all the traffic on the port we specify will be forwarded to the service. Almost all kinds of traffic like HTTP, TCP, UDP, Websockets, gRPC etc can be sent to this service.Let's look at an example definition file:apiVersion: v1kind: Servicemetadata: name: my-servicespec: selector: � app: myapp type: LoadBalancer ports: � - nodePort: 31000 � � port: 80 � � targetPort: 9376We can see that this is almost the same as a NodePort definition file.Let's create the service with create commandkubectl create -f load-balancer-service-definition.ymlNow let's look at the service that got created using the command.�kubectl get servicesYou can see that since I am using Minikube the value of the external IP is shown as <pending>. However, in an actual cloud setup, the IP will be generated and can be used to access the application. This is the IP that can be used by our users to access our web app from the internet.Ingress NetworkingWe have seen in the Kubernetes services sections on how to expose our application to the outside world using the NodePort and LoadBalancer. If we only have to have a single service port we can use NodePort. In the case of multiple instances of the same service, we have to use the LoadBalancer.�But what if we have to add one more service to our node and access it from another URL. In this case, we will have to add another load balancer to our cluster. This means that each service exposed with a LoadBalancer will get its own IP address and we will have to pay for each of these load balancers which can be quite expensive.An Ingress is used when we have multiple services on our cluster and we want the user request routed to the service based on their path. Consider an example, I have two services foo and bar in our cluster. When we type www.example.com/foo we should be routed to the foo service and www.example.com/bar should be routed to bar service. These routings will be performed by an Ingress. Unlike NodePort or LoadBalancer, Ingress is not actually a type of service. Instead, it is an entry point that sits in front of multiple services in the cluster. It can be defined as a collection of routing rules that govern how external users access services running inside a Kubernetes cluster.Ingress is most useful if you want to expose multiple services under the same IP address, and these services all use the same L7 protocol (typically HTTP). You only pay for one load balancer if you are using the native GCP integration, and because Ingress is \u001csmart\u001d you can get a lot of features out of the box (like SSL, Auth, Routing, etc)Ingress can be considered as the best way to expose multiple services under the same IP. Also, we should only pay for a single load balancer.Let's see how Ingress works. Before we implement the Ingress we need to deploy a supported reverse proxy or load balancing solution like Nginx, Haproxy, or Trafik. Then we need to specify a set of rules to configure the Ingress. The solution we deploy is called an ingress controller and the set of rules that we configure are called as ingress resources. Ingress resources are created using definition files like the ones we used to create pods and deployments.Ingress ControllerThe Ingress controller is not a part of the Kubernetes cluster by default. So we cannot simply create an Ingress resource and expect it to work. There are a number of solutions available for Ingress. A few of them being GCE which is Google\u0019s layers of HTTP load balancer Nginx, Contour, Haproxy Traefik, and Istio. Out of this, GCE and Nginx are currently being supported and maintained by the Kubernetes project.The Ingress Controller is not just another load balancer or a reverse proxy service. They have additional components that monitor the Kubernetes cluster for new definitions and Ingress resources and configure the service accordingly.We will be looking at Nginx as an example. Nginx controllers can be deployed just like another deployment into Kubernetes. Here is a sample definition file:apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx-ingress-controllerspec: replicas: 1 selector: � matchLabels: � � name: nginx-ingress template: � metadata: � � labels: � � � name: nginx-ingress � spec: � � containers: � � � - name: nginx-ingress-controller � � � � image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0 � � � � args: � � � � � - /nginx-ingress-controller � � � � env: � � � � � - name: POD_NAME � � � � � � valueFrom: � � � � � � � fieldRef: � � � � � � � � fieldPath: metadata.name � � � � � - name: POD_NAMESPACE � � � � � � valueFrom: � � � � � � � fieldRef: � � � � � � � � fieldPath: metadata.namespace � � � � ports: � � � � � - name: http � � � � � � containerPort: 80 � � � � � - name: https � � � � � � containerPort: 443We can see that our deployment is named nginx-ingress-controller and has one replica. It contains a pod definition labeled as nginx-ingress. In the spec, we can see that we are using a special build of Nginx specifically built to be used as an ingress controller. This image has its own set of requirements. The first argument is the location of the Nginx program. Next, we have to pass a config map that stores the configurations of Nginx like keep-alive threshold, SSL settings, etc.Config map definition sample:kind: ConfigMapapiVersion: v1metadata: name: nginx-configurationdata: map-hash-bucket-size: \"128\" ssl-protocols: SSLv2We have also passed the environment variables carrying the pod\u0019s name and the namespace it is deployed to. The service requires these values to read the configuration data of the pod.Finally, we have specified the ports used by the ingress controller 80 and 443.Next, let's expose the controller to the outside world using a service. Let's create a service of the kind NodePort:apiVersion: v1kind: Servicemetadata: name: myapp-servicespec: type: NodePort selector: � name: nginx-ingress ports: � - targetPort: 80 � � port: 80 � � protocol: tcp � � name: http � - targetPort: 443 � � port: 443 � � protocol: tcp � � name: httpNow that our controller is ready, let's look into the rules required to configure the Ingress.Ingress ResourceAn Ingress Resource is a set of rules and configurations applied to the Ingress controller. The rules can be specified to forward all incoming traffic to a single application or route the traffic to different applications. So in our example, when a user hits the \u0018foo\u0019 URL, then route them to the foo application or if the user hits the \u0018bar\u0019 URL, then route them to the bar application. Similarly, the request should be forwarded based on the domain name .We can create an Ingress resource with a Kubernetes definition file. Let's look at some examples.First, let's create a resource to route the incoming request based on the path:apiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-foo-barspec:� rules: � - http:� � � paths:� � � - path: /foo� � � � backend:� � � � � serviceName: foo-service� � � � � servicePort: 80� � � - path: /bar� � � � backend:� � � � � serviceName: bar-service� � � � � servicePort: 80We have specified an object of kind Ingress named ingress-foo-bar. And in the spec, we have defined two rules. The first rule will check if our request URL is of the foo type or bar type and serves the foo-service or bar service accordingly to the user. The backend part of the rule must contain the service name and a service port.Now let's look at an example where the routing is based on the domain name:apiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-foo-barspec:� rules:� - host: foo.example.com� � http: � � paths: � � - backend: � � � � serviceName: foo-service � � � � servicePort: 80 � - host: bar.example.com � � http: � � � paths: � � � - backend: � � � � � serviceName: bar-service � � � � � servicePort: 80Here we can see instead of paths in the rule we have hosts which are domain names. Also, we can see that splitting traffic by URL, has just one rule with two path specifications and splitting traffic by hostname have two rules with one path in each rule.If the users type a URL that is not specified in the rule, they will be forwarded to a default URL that can be configured.Wrapping UpIn this article, we have gone through the various Kubernetes Services and Ingress based on very simple examples. It should be noted that in real-life use cases can get more complicated and the Service solutions should be selected based on the requirements and complexity of the applications. Hope this article was helpful to you. Do check out the official documentation to find more examples and scenarios."
    },
    {
      "url": "https://www.learncloudnative.com/blog/2020-09-26-init-containers/",
      "title": "Kubernetes Init Containers",
      "content": "<article class=\"prose prose-sm sm:prose lg:prose-lg xl:prose-2xl\"><p>Init containers allow you to separate your application from the initialization logic and provide a way to run the initialization tasks such as setting up permissions, database schemas, or seeding data for the main application, etc. The init containers may also include any tools or binaries that you don&apos;t want to have in your primary container image due to security reasons.</p><p>The init containers are executed in a sequence before your primary or application containers start. On the other hand, any application containers have a non-deterministic startup order, so you can&apos;t use them for the initialization type of work.</p><p>The figure below shows the execution flow of the init containers and the application containers.</p><p><span class=\"gatsby-resp-image-wrapper\"> <a class=\"gatsby-resp-image-link\" href=\"/static/9d90a818ebe0c8029d1b58facf7e414a/9f82e/init-containers.png\"> <span class=\"gatsby-resp-image-background-image\"></span> <img class=\"gatsby-resp-image-image\" alt=\"Init Containers\" src=\"https://d33wubrfki0l68.cloudfront.net/9eec2180e1c0fbe463738e707c7a23e64f161f5c/efbec/static/9d90a818ebe0c8029d1b58facf7e414a/9f82e/init-containers.png\" srcset=\"https://d33wubrfki0l68.cloudfront.net/974b7854c6dd364ca80b766e83b5fcc160ed52a8/85291/static/9d90a818ebe0c8029d1b58facf7e414a/2eb24/init-containers.png 215w, https://d33wubrfki0l68.cloudfront.net/b19eb655fd61ef64249f1a32972ca729f67eb88a/5ea0c/static/9d90a818ebe0c8029d1b58facf7e414a/05ed2/init-containers.png 430w, https://d33wubrfki0l68.cloudfront.net/9eec2180e1c0fbe463738e707c7a23e64f161f5c/efbec/static/9d90a818ebe0c8029d1b58facf7e414a/9f82e/init-containers.png 820w\" sizes=\"(max-width: 820px) 100vw, 820px\"> </a> </span></p><p>The application containers will wait for the init containers to complete successfully before starting. If the init containers fail, the Pod is restarted (assuming we didn&apos;t set the restart policy to <code class=\"language-text\">RestartNever</code>), which causes the init containers to run again. When designing your init containers, make sure they are idempotent, to run multiple times without issues. For example, if you&apos;re seeding the database, check if it already contains the records before re-inserting them again.</p><p>Since init containers are part of the same Pod, they share the volumes, network, security settings, and resource limits, just like any other container in the Pod. </p><p>Let&apos;s look at an example where we use an init container to clone a GitHub repository to a shared volume between all containers. The Github repo contains a single <code class=\"language-text\">index.html</code>. Once the repo is cloned and the init container has executed, the primary container running the Nginx server can use <code class=\"language-text\">index.html</code> from the shared volume and serve it.</p><p>You define the init containers under the <code class=\"language-text\">spec</code> using the <code class=\"language-text\">initContainers</code> field, while you define the application containers under the <code class=\"language-text\">containers</code> field. We define an <code class=\"language-text\">emptyDir</code> volume and mount it into both the init and application container. When the init container starts, it will run the <code class=\"language-text\">git clone</code> command and clone the repository into the <code class=\"language-text\">/usr/share/nginx/html</code> folder. This folder is the default folder Nginx serves the HTML pages from, so when the application container starts, we will be able to access the HTML page we cloned. </p><div class=\"gatsby-highlight\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">apiVersion</span><span class=\"token punctuation\">:</span> v1\n<span class=\"token key atrule\">kind</span><span class=\"token punctuation\">:</span> Pod\n<span class=\"token key atrule\">metadata</span><span class=\"token punctuation\">:</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> website\n<span class=\"token key atrule\">spec</span><span class=\"token punctuation\">:</span> <span class=\"token key atrule\">initContainers</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> clone<span class=\"token punctuation\">-</span>repo <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> alpine/git <span class=\"token key atrule\">command</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">-</span> git <span class=\"token punctuation\">-</span> clone <span class=\"token punctuation\">-</span> <span class=\"token punctuation\">-</span><span class=\"token punctuation\">-</span>progress <span class=\"token punctuation\">-</span> https<span class=\"token punctuation\">:</span>//github.com/peterj/simple<span class=\"token punctuation\">-</span>http<span class=\"token punctuation\">-</span>page.git <span class=\"token punctuation\">-</span> /usr/share/nginx/html <span class=\"token key atrule\">volumeMounts</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> web <span class=\"token key atrule\">mountPath</span><span class=\"token punctuation\">:</span> <span class=\"token string\">&quot;/usr/share/nginx/html&quot;</span> <span class=\"token key atrule\">containers</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> nginx <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> nginx <span class=\"token key atrule\">ports</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> http <span class=\"token key atrule\">containerPort</span><span class=\"token punctuation\">:</span> <span class=\"token number\">80</span> <span class=\"token key atrule\">volumeMounts</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> web <span class=\"token key atrule\">mountPath</span><span class=\"token punctuation\">:</span> <span class=\"token string\">&quot;/usr/share/nginx/html&quot;</span> <span class=\"token key atrule\">volumes</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> web <span class=\"token key atrule\">emptyDir</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">{</span><span class=\"token punctuation\">}</span></code></pre></div><p>Save the above YAML to <code class=\"language-text\">init-container.yaml</code> and create the Pod using <code class=\"language-text\">kubectl apply -f init-container.yaml</code>.</p><p>If you run <code class=\"language-text\">kubectl get pods</code> right after the above command, you should see the status of the init container:</p><div class=\"gatsby-highlight\"><pre class=\"language-bash\"><code class=\"language-bash\">$ kubectl get po\nNAME READY STATUS RESTARTS AGE\nwebsite <span class=\"token number\">0</span>/1 Init:0/1 <span class=\"token number\">0</span>          1s</code></pre></div><p>The number <code class=\"language-text\">0/1</code> indicates a total of 1 init containers, and 0 containers have been completed so far. In case the init container fails, the status changes to <code class=\"language-text\">Init:Error</code> or <code class=\"language-text\">Init:CrashLoopBackOff</code> if the container fails repeatedly.</p><p>You can also look at the events using the <code class=\"language-text\">describe</code> command to see what happened:</p><div class=\"gatsby-highlight\"><pre class=\"language-text\"><code class=\"language-text\">Normal  Scheduled  19s   default-scheduler  Successfully assigned default/website to minikube\nNormal  Pulling    18s   kubelet, minikube  Pulling image &quot;alpine/git&quot;\nNormal  Pulled     17s   kubelet, minikube  Successfully pulled image &quot;alpine/git&quot;\nNormal  Created    17s   kubelet, minikube  Created container clone-repo\nNormal  Started    16s   kubelet, minikube  Started container clone-repo\nNormal  Pulling    15s   kubelet, minikube  Pulling image &quot;nginx&quot;\nNormal  Pulled     13s   kubelet, minikube  Successfully pulled image &quot;nginx&quot;\nNormal  Created    13s   kubelet, minikube  Created container nginx\nNormal  Started    13s   kubelet, minikube  Started container nginx</code></pre></div><p>You will notice as soon as Kubernetes schedules the Pod, the first Docker image is pulled (<code class=\"language-text\">alpine/git</code>), and the init container (<code class=\"language-text\">clone-repo</code>) is created and started. Once that&apos;s completed (the container cloned the repo) the main application container (<code class=\"language-text\">nginx</code>) starts.</p><p>Additionally, you can also use the <code class=\"language-text\">logs</code> command to get the logs from the init container by specifying the container name using the <code class=\"language-text\">-c</code> flag:</p><div class=\"gatsby-highlight\"><pre class=\"language-bash\"><code class=\"language-bash\">$ kubectl logs website -c clone-repo\nCloning into <span class=\"token string\">&apos;/usr/share/nginx/html&apos;</span><span class=\"token punctuation\">..</span>.\nremote: Enumerating objects: <span class=\"token number\">6</span>, done.\nremote: Counting objects: <span class=\"token number\">100</span>% <span class=\"token punctuation\">(</span><span class=\"token number\">6</span>/6<span class=\"token punctuation\">)</span>, done.\nremote: Compressing objects: <span class=\"token number\">100</span>% <span class=\"token punctuation\">(</span><span class=\"token number\">4</span>/4<span class=\"token punctuation\">)</span>, done.\nremote: Total <span class=\"token number\">6</span> <span class=\"token punctuation\">(</span>delta <span class=\"token number\">0</span><span class=\"token punctuation\">)</span>, reused <span class=\"token number\">0</span> <span class=\"token punctuation\">(</span>delta <span class=\"token number\">0</span><span class=\"token punctuation\">)</span>, pack-reused <span class=\"token number\">0</span>\nReceiving objects: <span class=\"token number\">100</span>% <span class=\"token punctuation\">(</span><span class=\"token number\">6</span>/6<span class=\"token punctuation\">)</span>, done.</code></pre></div><p>Finally, to actually see the static HTML page can use <code class=\"language-text\">port-forward</code> to forward the local port to the port <code class=\"language-text\">80</code> on the container:</p><div class=\"gatsby-highlight\"><pre class=\"language-bash\"><code class=\"language-bash\">$ kubectl port-forward pod/website <span class=\"token number\">8000</span>:80\nForwarding from <span class=\"token number\">127.0</span>.0.1:8000 -<span class=\"token operator\">&gt;</span> <span class=\"token number\">80</span>\nForwarding from <span class=\"token punctuation\">[</span>::1<span class=\"token punctuation\">]</span>:8000 -<span class=\"token operator\">&gt;</span> <span class=\"token number\">80</span></code></pre></div><p>You can now open your browser at <code class=\"language-text\">http://localhost:8000</code> to open the static page as shown in figure below.</p><p><span class=\"gatsby-resp-image-wrapper\"> <a class=\"gatsby-resp-image-link\" href=\"/static/9c0a3a9db42df7251937675e882b29f9/01e7c/simple-http-page.png\"> <span class=\"gatsby-resp-image-background-image\"></span> <img class=\"gatsby-resp-image-image\" alt=\"Static HTML From Github Repo\" src=\"https://d33wubrfki0l68.cloudfront.net/eb837020adf8929b21c1216ccba09d735bfe4686/ef2be/static/9c0a3a9db42df7251937675e882b29f9/01e7c/simple-http-page.png\" srcset=\"https://d33wubrfki0l68.cloudfront.net/2178c0862319f9f33792560202123e4702a50d89/8edeb/static/9c0a3a9db42df7251937675e882b29f9/2eb24/simple-http-page.png 215w, https://d33wubrfki0l68.cloudfront.net/187c313f77573848e4ade2d37023d3376329ec92/2b16d/static/9c0a3a9db42df7251937675e882b29f9/05ed2/simple-http-page.png 430w, https://d33wubrfki0l68.cloudfront.net/eb837020adf8929b21c1216ccba09d735bfe4686/ef2be/static/9c0a3a9db42df7251937675e882b29f9/01e7c/simple-http-page.png 512w\" sizes=\"(max-width: 512px) 100vw, 512px\"> </a> </span></p><p>Lastly, delete the Pod by running <code class=\"language-text\">kubectl delete po website</code>.</p></article>",
      "contentAsText": "Init containers allow you to separate your application from the initialization logic and provide a way to run the initialization tasks such as setting up permissions, database schemas, or seeding data for the main application, etc. The init containers may also include any tools or binaries that you don't want to have in your primary container image due to security reasons.The init containers are executed in a sequence before your primary or application containers start. On the other hand, any application containers have a non-deterministic startup order, so you can't use them for the initialization type of work.The figure below shows the execution flow of the init containers and the application containers.     The application containers will wait for the init containers to complete successfully before starting. If the init containers fail, the Pod is restarted (assuming we didn't set the restart policy to RestartNever), which causes the init containers to run again. When designing your init containers, make sure they are idempotent, to run multiple times without issues. For example, if you're seeding the database, check if it already contains the records before re-inserting them again.Since init containers are part of the same Pod, they share the volumes, network, security settings, and resource limits, just like any other container in the Pod. Let's look at an example where we use an init container to clone a GitHub repository to a shared volume between all containers. The Github repo contains a single index.html. Once the repo is cloned and the init container has executed, the primary container running the Nginx server can use index.html from the shared volume and serve it.You define the init containers under the spec using the initContainers field, while you define the application containers under the containers field. We define an emptyDir volume and mount it into both the init and application container. When the init container starts, it will run the git clone command and clone the repository into the /usr/share/nginx/html folder. This folder is the default folder Nginx serves the HTML pages from, so when the application container starts, we will be able to access the HTML page we cloned. apiVersion: v1\nkind: Pod\nmetadata: name: website\nspec: initContainers: - name: clone-repo image: alpine/git command: - git - clone - --progress - https://github.com/peterj/simple-http-page.git - /usr/share/nginx/html volumeMounts: - name: web mountPath: \"/usr/share/nginx/html\" containers: - name: nginx image: nginx ports: - name: http containerPort: 80 volumeMounts: - name: web mountPath: \"/usr/share/nginx/html\" volumes: - name: web emptyDir: {}Save the above YAML to init-container.yaml and create the Pod using kubectl apply -f init-container.yaml.If you run kubectl get pods right after the above command, you should see the status of the init container:$ kubectl get po\nNAME READY STATUS RESTARTS AGE\nwebsite 0/1 Init:0/1 0          1sThe number 0/1 indicates a total of 1 init containers, and 0 containers have been completed so far. In case the init container fails, the status changes to Init:Error or Init:CrashLoopBackOff if the container fails repeatedly.You can also look at the events using the describe command to see what happened:Normal  Scheduled  19s   default-scheduler  Successfully assigned default/website to minikube\nNormal  Pulling    18s   kubelet, minikube  Pulling image \"alpine/git\"\nNormal  Pulled     17s   kubelet, minikube  Successfully pulled image \"alpine/git\"\nNormal  Created    17s   kubelet, minikube  Created container clone-repo\nNormal  Started    16s   kubelet, minikube  Started container clone-repo\nNormal  Pulling    15s   kubelet, minikube  Pulling image \"nginx\"\nNormal  Pulled     13s   kubelet, minikube  Successfully pulled image \"nginx\"\nNormal  Created    13s   kubelet, minikube  Created container nginx\nNormal  Started    13s   kubelet, minikube  Started container nginxYou will notice as soon as Kubernetes schedules the Pod, the first Docker image is pulled (alpine/git), and the init container (clone-repo) is created and started. Once that's completed (the container cloned the repo) the main application container (nginx) starts.Additionally, you can also use the logs command to get the logs from the init container by specifying the container name using the -c flag:$ kubectl logs website -c clone-repo\nCloning into '/usr/share/nginx/html'...\nremote: Enumerating objects: 6, done.\nremote: Counting objects: 100% (6/6), done.\nremote: Compressing objects: 100% (4/4), done.\nremote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 0\nReceiving objects: 100% (6/6), done.Finally, to actually see the static HTML page can use port-forward to forward the local port to the port 80 on the container:$ kubectl port-forward pod/website 8000:80\nForwarding from 127.0.0.1:8000 -> 80\nForwarding from [::1]:8000 -> 80You can now open your browser at http://localhost:8000 to open the static page as shown in figure below.     Lastly, delete the Pod by running kubectl delete po website.",
      "publishedDate": "2020-09-25T16:00:00.000Z",
      "description": "Init containers allow you to separate your application from the initialization logic and provide a way to run the initialization tasks such as setting up permissions, database schemas, or seeding data for the main application, etc. The init containers may also include any tools or binaries that you don't want to have in your primary container image due to security reasons.",
      "ogDescription": "Init containers allow you to separate your application from the initialization logic and provide a way to run the initialization tasks such as setting up permissions, database schemas, or seeding data for the main application, etc. The init containers may also include any tools or binaries that you don't want to have in your primary container image due to security reasons."
    },
    {
      "url": "https://github.com/kubenav/kubenav",
      "title": "kubenav/kubenav",
      "content": "<div><div><article class=\"markdown-body entry-content container-lg\"><div>\n  <a href=\"https://github.com/kubenav/kubenav/blob/master/utils/assets/github-logo.png\"><img src=\"https://github.com/kubenav/kubenav/raw/master/utils/assets/github-logo.png\" width=\"200\"></a>\n  <br><br>\n<p><strong>kubenav</strong> is the navigator for your <strong>Kubernetes</strong> clusters right in your pocket. kubenav is a <strong>mobile, desktop and web</strong> app to manage Kubernetes clusters and to get an overview of the status of your resources.</p>\n  <p>\n    <a href=\"https://apps.apple.com/us/app/kubenav/id1494512160\"><img src=\"https://github.com/kubenav/kubenav/raw/master/utils/assets/app-store-badge.png\"></a>\n    <a href=\"https://play.google.com/store/apps/details?id=io.kubenav.kubenav\"><img src=\"https://github.com/kubenav/kubenav/raw/master/utils/assets/google-play-badge.png\"></a>\n    <a href=\"https://github.com/kubenav/kubenav/releases\"><img src=\"https://github.com/kubenav/kubenav/raw/master/utils/assets/desktop-badge.png\"></a>\n  </p>\n  <a href=\"https://github.com/kubenav/kubenav/blob/master/utils/assets/github-screenshot.png\"><img src=\"https://github.com/kubenav/kubenav/raw/master/utils/assets/github-screenshot.png\" width=\"100%\"></a>\n</div>\n<p>kubenav is a mobile, desktop and web app to manage Kubernetes clusters. The app provides an overview of all resources in a Kubernetes clusters, including current status information for workloads. The details view for resources provides additional information. It is possible to view logs and events or to get a shell into a container. You can also edit and delete resources or scale your workloads within the app.</p>\n<p>The app is developed using <a href=\"https://ionicframework.com/\">Ionic Framework</a> and <a href=\"https://capacitor.ionicframework.com/\">Capacitor</a>. The frontend part of the app is implemented using TypeScript and React functional components. The backend part uses <a href=\"https://github.com/golang/go/wiki/Mobile\">Go mobile</a> for communication with the Kubernetes API server and Cloud Providers. So it is possible to achieve nearly 100% code sharing between the mobile and desktop implementation of kubenav.</p>\n<h2><a id=\"user-content-features\" class=\"anchor\" href=\"https://github.com/kubenav/kubenav#features\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Features</h2>\n<ul>\n<li><strong>Available for mobile, desktop and web:</strong> kubenav provides the same experience for mobile, desktop and web, with nearly 100% code sharing.</li>\n<li><strong>Manage Resources:</strong> All major resources like Deployments, StatefulSets, DaemonSets, Pods, etc. are supported.</li>\n<li><strong>Custom Resource Definitions:</strong> View all Custom Resource Definitions and mange Custom Resources.</li>\n<li><strong>Modify Resources:</strong> Edit and delete all available resources or scale your Deployments, StatefulSets, DaemonSets.</li>\n<li><strong>Filter and Search:</strong> Filter the resources by Namespace and find them by there name.</li>\n<li><strong>Status Information:</strong> Fast overview of the status of workloads and detailed information including Events.</li>\n<li><strong>Resource Usage:</strong> View the requests, limits and current usage of Pods and Containers.</li>\n<li><strong>Logs:</strong> View the logs of a container or stream the logs in realtime.</li>\n<li><strong>Terminal:</strong> Get a shell into a container, right from your phone.</li>\n<li><strong>Manage multiple Clusters:</strong> Add multiple clusters via <code>kubeconfig</code> or your prefered Cloud Provider, including Google, AWS and Azure.</li>\n<li><strong>Port-Forwarding:</strong> Create a port-forwarding connection to one of your Pods and open the served page in your browser.</li>\n<li><strong>Prometheus Integration:</strong> kubenav allows you to view your Prometheus metrics directly in the dashboard via the Prometheus plugin.</li>\n</ul>\n<h2><a id=\"user-content-usage\" class=\"anchor\" href=\"https://github.com/kubenav/kubenav#usage\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Usage</h2>\n<p>The mobile version of kubenav can be downloaded from the <a href=\"https://apps.apple.com/us/app/kubenav/id1494512160\">App Store</a> or <a href=\"https://play.google.com/store/apps/details?id=io.kubenav.kubenav\">Google Play</a>. The desktop version for macOS, Linux and Windows can be downloaded from the <a href=\"https://github.com/kubenav/kubenav/releases\">release page</a>.</p>\n<p>For testing new features and faster feedback, we provide a <strong>beta version</strong> via <a href=\"https://testflight.apple.com/join/RQUFGkHi\">Apple Testflight</a> and <a href=\"https://play.google.com/apps/testing/io.kubenav.kubenav\">Google Play</a>. For the desktop version we are running <strong>nightly builds</strong>, where the binaries for each platform are uploaded as artifacts in the <a href=\"https://github.com/kubenav/kubenav/actions?query=workflow%3ABuild+event%3Aschedule\">GitHub Action</a>.</p>\n<p>If your want to run kubenav in your Kubernetes cluster, simply run the following commands and open <a href=\"http://localhost:14122/\">localhost:14122</a> in your browser:</p>\n<div class=\"highlight highlight-source-shell\"><pre>kubectl apply --kustomize github.com/kubenav/deploy/kustomize\nkubectl port-forward --namespace kubenav svc/kubenav 14122</pre></div>\n<p>For further information, please view our documentation at <a href=\"https://docs.kubenav.io/\">docs.kubenav.io</a>.</p>\n</article></div></div>",
      "contentAsText": "\n  \n  \nkubenav is the navigator for your Kubernetes clusters right in your pocket. kubenav is a mobile, desktop and web app to manage Kubernetes clusters and to get an overview of the status of your resources.\n  \n    \n    \n    \n  \n  \n\nkubenav is a mobile, desktop and web app to manage Kubernetes clusters. The app provides an overview of all resources in a Kubernetes clusters, including current status information for workloads. The details view for resources provides additional information. It is possible to view logs and events or to get a shell into a container. You can also edit and delete resources or scale your workloads within the app.\nThe app is developed using Ionic Framework and Capacitor. The frontend part of the app is implemented using TypeScript and React functional components. The backend part uses Go mobile for communication with the Kubernetes API server and Cloud Providers. So it is possible to achieve nearly 100% code sharing between the mobile and desktop implementation of kubenav.\nFeatures\n\nAvailable for mobile, desktop and web: kubenav provides the same experience for mobile, desktop and web, with nearly 100% code sharing.\nManage Resources: All major resources like Deployments, StatefulSets, DaemonSets, Pods, etc. are supported.\nCustom Resource Definitions: View all Custom Resource Definitions and mange Custom Resources.\nModify Resources: Edit and delete all available resources or scale your Deployments, StatefulSets, DaemonSets.\nFilter and Search: Filter the resources by Namespace and find them by there name.\nStatus Information: Fast overview of the status of workloads and detailed information including Events.\nResource Usage: View the requests, limits and current usage of Pods and Containers.\nLogs: View the logs of a container or stream the logs in realtime.\nTerminal: Get a shell into a container, right from your phone.\nManage multiple Clusters: Add multiple clusters via kubeconfig or your prefered Cloud Provider, including Google, AWS and Azure.\nPort-Forwarding: Create a port-forwarding connection to one of your Pods and open the served page in your browser.\nPrometheus Integration: kubenav allows you to view your Prometheus metrics directly in the dashboard via the Prometheus plugin.\n\nUsage\nThe mobile version of kubenav can be downloaded from the App Store or Google Play. The desktop version for macOS, Linux and Windows can be downloaded from the release page.\nFor testing new features and faster feedback, we provide a beta version via Apple Testflight and Google Play. For the desktop version we are running nightly builds, where the binaries for each platform are uploaded as artifacts in the GitHub Action.\nIf your want to run kubenav in your Kubernetes cluster, simply run the following commands and open localhost:14122 in your browser:\nkubectl apply --kustomize github.com/kubenav/deploy/kustomize\nkubectl port-forward --namespace kubenav svc/kubenav 14122\nFor further information, please view our documentation at docs.kubenav.io.\n",
      "description": "kubenav is the navigator for your Kubernetes clusters right in your pocket. - kubenav/kubenav",
      "ogDescription": "kubenav is the navigator for your Kubernetes clusters right in your pocket. - kubenav/kubenav"
    },
    {
      "url": "https://github.com/didier-durand/microservices-on-cloud-kubernetes",
      "title": "didier-durand/microservices-on-cloud-kubernetes",
      "content": "<div><div><article class=\"markdown-body entry-content container-lg\"><p><a href=\"https://github.com/didier-durand/microservices-on-cloud-kubernetes/blob/master/img/header-logos.jpg\"><img src=\"https://github.com/didier-durand/microservices-on-cloud-kubernetes/raw/master/img/header-logos.jpg\"></a></p>\n\n<p><a href=\"https://github.com/didier-durand/microservices-on-cloud-kubernetes/workflows/Microservices%20on%20GKE/badge.svg\"><img src=\"https://github.com/didier-durand/microservices-on-cloud-kubernetes/workflows/Microservices%20on%20GKE/badge.svg\" alt=\"workflow badge\"></a>\n<a href=\"https://opensource.org/licenses/Apache-2.0\"><img src=\"https://camo.githubusercontent.com/2a2157c971b7ae1deb8eb095799440551c33dcf61ea3d965d86b496a5a65df55/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d417061636865253230322e302d626c75652e737667\" alt=\"License\"></a></p>\n<p>The purpose of this repository is to provide the fully automated setup of a nice-looking (see <a href=\"https://github.com/GoogleCloudPlatform/microservices-demo#screenshots\">screenshots</a>)\nshowcase / testbed for a cloud-native (<a href=\"https://docs.microsoft.com/en-us/dotnet/architecture/cloud-native/definition\">precisely defined</a> application\nby Microsoft) on a cloud-hosted Kubernetes cluster (here <a href=\"https://cloud.google.com/kubernetes-engine\">GKE by Google Cloud</a>) based on an interesting <a href=\"https://www.redhat.com/en/topics/microservices/what-is-a-service-mesh\">service mesh</a>.\nSo, additionally, the setup will install tooling (coming from the <a href=\"https://github.com/cncf/trailmap\">CNCF Cloud Trail Map</a> for many of them) to make the application and its service mesh observable and manageable.</p>\n<p>This application, licensed under Apache terms (same terms for all components used in this worklfow - So, allowing free reuse) is the <a href=\"https://github.com/GoogleCloudPlatform/microservices-demo\">&quot;Online Boutique&quot;</a>\n(formerly known as Hipster Shop - developed by a Google team but not an official product of them). It is composed of 10 polyglot microservices behind a nice-looking web frontend calling them to serve client requests.\nA load-generator - part of the package - will generate traffic while the application is running to make use of tools (Prometheus, OpenTelemetry,\netc.) more attractive.</p>\n<p>Another goal of this repository is to help people exploring the cloud-native architecture: when you fork it, you rapidly get a working cluster with a somewhat\n&quot;real-life&quot; application and decent tooling to experiment with, without the need for a long trial-and-error process starting with infrastructure to set it\nup from scratch. It makes it much faster to grasp the philosophy of the distributed architecture proposed by Kubernetes.</p>\n<p>So, happy forking for your own use! (see <a href=\"https://github.com/didier-durand/microservices-on-cloud-kubernetes#setup-for-forks\">Setup section</a> for all\ntechnical details) And come back regularly or get notified by following this repository: we will add additional tools in subsequent updates.</p>\n<p>We implement here a Github workflow (<a href=\"https://github.com/didier-durand/microservices-on-cloud-kubernetes/blob/master/.github/workflows/microservices-on-gke.yml\">microservices-on-gke.yml</a> &amp; <a href=\"https://github.com/didier-durand/microservices-on-cloud-kubernetes/blob/master/sh\">shells in sh directory</a> - see our <a href=\"https://github.com/didier-durand/gcp-workflows-on-github\">other repository</a> for other workflows)\nwhich allows to automatically deploy a fresh cluster on GKE and to deploy the application on it whenever needed via a <a href=\"https://github.blog/changelog/2020-07-06-github-actions-manual-triggers-with-workflow_dispatch/\">single click</a>.\nOn our side, this same workflow is also started automatically on a recurring basis (at least weekly) via Github&apos;s cron facility (included in workflow yaml)\nto make sure that the deployment remains fully operational as underlying GKE infrastructure and implemented components evolve. You can access logs\nof previous runs in the <a href=\"https://github.com/didier-durand/microservices-on-cloud-kubernetes/blob/master/actions\">Actions Tab</a>.</p>\n<h2><a id=\"user-content-access-to-application--load-generator\" class=\"anchor\" href=\"https://github.com/didier-durand/microservices-on-cloud-kubernetes#access-to-application--load-generator\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Access to application &amp; load generator</h2>\n<p>On successful completion of the workflow, the Online Boutique is accessible from anywhere on the Internet at the public IP address\n(dynamically created and published by GKE) displayed in the final lines of workflow execution step &quot;Deploy application on GKE&quot;. Indeed, it is the IP\naddress of the <a href=\"https://kubernetes.io/docs/concepts/services-networking/service/\">K8s service</a> &apos;frontend-external&apos; defined by the deployment. Hence,\nyou can also get it at any time via <em>&apos;kubectl get service &apos;frontend-external&apos;&apos;</em> provided that you went through proper setup as described below.</p>\n<p>To check the activity of the load generator, you can at any time run <em>&apos;kubectl logs -l app=loadgenerator -c main&apos;</em>\nYou should get something like the following describing how many requests were already triggered:</p>\n<pre><code>kubectl logs -l app=loadgenerator -c main\n GET /product/66VCHSJNUP                                          600     0(0.00%)      77      34    1048  |      41    0.10    0.00\n GET /product/6E92ZMYYFZ                                          563     0(0.00%)      77      34    1763  |      41    0.00    0.00\n GET /product/9SIQT8TOJO                                          593     0(0.00%)      73      34    1013  |      41    0.30    0.00\n GET /product/L9ECAV7KIM                                          631     0(0.00%)      82      34    1349  |      42    0.20    0.00\n GET /product/LS4PSXUNUM                                          608     0(0.00%)      83      34     896  |      42    0.20    0.00\n GET /product/OLJCESPC7Z                                          623     0(0.00%)      69      34    1079  |      41    0.10    0.00\n POST /setCurrency                                                808     0(0.00%)      82      44    1089  |      51    0.20    0.00\n--------------------------------------------------------------------------------------------------------------------------------------------\n Aggregated                                                      9517     0(0.00%)                                       1.80    0.00\n</code></pre>\n<p>If you want to easily inject more traffic, you can additionally use the <a href=\"https://github.com/rakyll/hey\">hey</a> or <a href=\"https://github.com/fortio/fortio\">fortio</a>\nutilities as we did in our <a href=\"https://github.com/didier-durand/knative-on-cloud-kubernetes\">Knative project</a> : see correponding <a href=\"https://github.com/didier-durand/knative-on-cloud-kubernetes/blob/master/.github/workflows/gcloud-gke-knative.yml\">workflow script</a>.</p>\n<h2><a id=\"user-content-access-to-deployed-tools--dashboards\" class=\"anchor\" href=\"https://github.com/didier-durand/microservices-on-cloud-kubernetes#access-to-deployed-tools--dashboards\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Access to deployed tools &amp; dashboards</h2>\n<p>You have first to implement the requirements of the <a href=\"https://github.com/didier-durand/microservices-on-cloud-kubernetes#setup-for-forks\">Setup section</a>\nbefore trying to access the dashboards.</p>\n<p>To keep things simple, we access all tools and dashboards via the proxy functions available in Kubernetes: either directly via <em>&apos;kubectl proxy&apos;</em>\nor indirectly via <em>&apos;istioctl dashboard xxx&apos;</em>. Only limited additional definitions are then required: it&apos;s just fine for a demo and initial tests.\nOf course, the laptop running the proxies must be authentified to gcloud via SDK with proper credentials giving rights to cluster administration.</p>\n<p><strong><strong>Available dashboards:</strong></strong></p>\n<p>(click on pictures to enlarge them - also use the hyperlinks provided with each dashboard description to have a good overview of the\nfeatures of each tool from its official documentation)</p>\n<p><a href=\"https://github.com/didier-durand/microservices-on-cloud-kubernetes/blob/master/img/screenshots/k8s_ui.jpg\"><img src=\"https://github.com/didier-durand/microservices-on-cloud-kubernetes/raw/master/img/screenshots/k8s_ui.jpg\"></a></p>\n<ol>\n<li><strong>Standard K8s UI</strong>: our workflow deploys first <a href=\"https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/\">this standard Kubernetes dashboard</a>\nas a tool that should anyway be included in any installation. It gives a good overview of the deployed cluster with static (configuration) and dynamic (metrics) information\nabout the active objects. When <em>&apos;kubectl proxy&apos;</em> is active, the dashboard is available at <a href=\"http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/\">this url</a>.\nThe security check at login is most easily satisfied by selection the login option of config file (see Prereqs to obtain it in Setup section).</li>\n</ol>\n<p><a href=\"https://github.com/didier-durand/microservices-on-cloud-kubernetes/blob/master/img/screenshots/polaris-dashboard.jpg\"><img src=\"https://github.com/didier-durand/microservices-on-cloud-kubernetes/raw/master/img/screenshots/polaris-dashboard.jpg\"></a></p>\n<ol>\n<li><strong>Polaris dashboard</strong>: <a href=\"https://github.com/FairwindsOps/polaris\">Polaris</a> is an interesting tool, even in its OSS version (the paid-for version\nprovides more checks) used here. After installation, it <a href=\"https://www.fairwinds.com/polaris\">scans the definitions</a> of various kinds of objects and applies sanity checking rules to\nvalidate their proper configuration. For example, in the case of Online Boutique, it will warn that containers have no resource constraints (cpu,\nmemory, etc.) imposed on them or that their security credentials are too wide compared to what they do. The hyperlinks provided on the unsatisfactory\nchecks document the reason(s) of the alert as well as the possible remedies to apply. So, a quite useful tool to incrementally increase the quality of the\nconfiguration of a given cluster: new versions of yaml object manifests can be gradually deployed to minimize the issue notifications.</li>\n</ol>\n<p><a href=\"https://github.com/didier-durand/microservices-on-cloud-kubernetes/blob/master/img/screenshots/kiali-dashboard.jpg\"><img src=\"https://github.com/didier-durand/microservices-on-cloud-kubernetes/raw/master/img/screenshots/kiali-dashboard.jpg\"></a></p>\n<ol>\n<li><strong>Kiali Dashboard</strong>: <a href=\"https://kiali.io/\">Kiali</a> claims itself the <em>&quot;Service mesh management for Istio&quot;</em>. it allows an interactive discovery of\nthe defined relations between the services. Then, it provides <a href=\"https://kiali.io/documentation/latest/features/\">detailed insights and metrics</a> about the\nhealth of those services and the requests between them.  The live traffic animation in the UI is extremely useful: it allows to spot very quickly\nwhere the activity happens to focus on those hot spots during root cause analysis for an issue. You can also go back in time with the replay\nfeature to see the traffic and the interactions that happened in the past to understand why and how you reached current situation. This dashboard\nis accessed via <em>&apos;istioctl dashboard kiali&apos;</em> that will open the corresponding UI into your web browser.</li>\n</ol>\n<p><a href=\"https://github.com/didier-durand/microservices-on-cloud-kubernetes/blob/master/img/screenshots/grafana-dashboard.jpg\"><img src=\"https://github.com/didier-durand/microservices-on-cloud-kubernetes/raw/master/img/screenshots/grafana-dashboard.jpg\"></a></p>\n<ol>\n<li><strong>Grafana dashboard</strong>: <a href=\"https://grafana.com/oss/grafana/\">Grafana</a> which <em>&apos;allows you to query, visualize, alert on metrics and logs, no\nmatter where they are stored&apos;</em> provides very nice charts about the activity of the cluster as the whole (usual metrics about resource consumption:\ncpu, memory, etc. for nodes. But, more specifically in this context it provides interesting additional <a href=\"https://istio.io/latest/docs/tasks/observability/metrics/using-istio-dashboard/\">dashboards specific to the Istio service mesh</a>\nrelated to the traffic between the pods. Those dashboards are accessed via <em>&apos;istioctl dashboard grafana&apos;</em> that will open the corresponding UI into\nyour web browser.</li>\n</ol>\n<p><a href=\"https://github.com/didier-durand/microservices-on-cloud-kubernetes/blob/master/img/screenshots/jaeger-dashboard.jpg\"><img src=\"https://github.com/didier-durand/microservices-on-cloud-kubernetes/raw/master/img/screenshots/jaeger-dashboard.jpg\"></a></p>\n<ol>\n<li><strong>Jaeger dashboard</strong>: the Open Boutique is instrumented via <a href=\"https://opencensus.io/\">OpenCensus</a>, now merged into\n<a href=\"https://opentelemetry.io/\">OpenTelemetry</a>, component of the CNCF Cloud Trail Map. Jaeger - in CNCF Cloud Trail Map - is the tracing backend\nimplemented here. It centralizes and ingests the distributed traces produced by the various microservices. So, the Jaeger dashboard will allow the\ndetailed examination of those aggregated distributed traces also known as <a href=\"https://opentracing.io/docs/overview/spans/#what-is-a-span\">&quot;spans&quot;</a>. This\ndashboard is accessed via <em>&apos;istioctl dashboard jaeger&apos;</em> that will open the corresponding UI into your web browser.</li>\n</ol>\n<p><a href=\"https://github.com/didier-durand/microservices-on-cloud-kubernetes/blob/master/img/screenshots/prometheus-dashboard.jpg\"><img src=\"https://github.com/didier-durand/microservices-on-cloud-kubernetes/raw/master/img/screenshots/prometheus-dashboard.jpg\"></a></p>\n<ol>\n<li><strong>Prometheus dashboard</strong>: <a href=\"https://prometheus.io/\">Prometheus</a> (also member of CNCF Cloud Trail Map) is the cornerstone component for metrics\ncollection. The collected data is used by Grafana, Kiali &amp; Jaeger for their specific purposes. The Prometheus dashboard can be used as the &quot;source\nof truth&quot;: for example, it can be used to verify if some metrics claimed as missing by a downstream component using this value is really collected\nor not and to compare the graph produced by Prometheus itself to the graph produced downstream user to spot potential discrepancies. This dashboard\nis accessed via <em>&apos;istioctl dashboard prometheus&apos;</em> that will open the corresponding UI into your web browser.</li>\n</ol>\n<p><a href=\"https://github.com/didier-durand/microservices-on-cloud-kubernetes/blob/master/img/screenshots/envoy-dashboard.jpg\"><img src=\"https://github.com/didier-durand/microservices-on-cloud-kubernetes/raw/master/img/screenshots/envoy-dashboard.jpg\"></a></p>\n<ol>\n<li><strong>Envoy dashboard</strong>: an improved version of <a href=\"https://www.envoyproxy.io/docs/envoy/latest/intro/what_is_envoy\">Envoy Proxy</a>, also part of the CNCF Cloud Trail Map,  is the <a href=\"https://istio.io/latest/docs/concepts/what-is-istio/#why-use-istio\">sidecar\ncontainer</a> used by Istio. Envoy provides a (somewhat rough) dashboard to go into the\nnitty-gritty details of a given pod: it is usually used for low-level introspection into the traffic between pods in unexpected situations. It is\nmore a debugging tool at very low-level: most cluster administrators shouldn&apos;t need to use it. This dashboard is accessed via <em>&apos;istioctl\ndashboard envoy podname[.namespace]&apos;</em>: it will open the corresponding UI for the chosen pod sidecar into your web browser.</li>\n</ol>\n<h2><a id=\"user-content-workflow-steps\" class=\"anchor\" href=\"https://github.com/didier-durand/microservices-on-cloud-kubernetes#workflow-steps\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Workflow steps</h2>\n<p>The workflow has following steps:</p>\n<ol>\n<li>setup of workflow parameters via environment variables</li>\n<li>checkout of project artefacts</li>\n<li>Setup of gcloud SDK for authentication to Google Cloud through secrets defined in repository via setup.(subsequent steps in shell script)</li>\n<li>create cluster\n6, import cluster config &amp; credentials for kubectl &amp; istioctl</li>\n<li>deploy standard K8s dashboard and check its proper deployment</li>\n<li>deploy Polaris dashboard and check its proper deployment</li>\n<li>deploy Istio and check its proper deployment</li>\n<li>deploy Istio-based addons and check their proper deployment</li>\n<li>label application namespace to ensure automatic sidecar injection (see <a href=\"https://istio.io/latest/docs/ops/deployment/architecture/\">Istio architecture</a> uses an improved version of Envoy for sidecars) in microservice pods</li>\n<li>deploy proper application manifests for Istio</li>\n<li>deploy Online Boutique and check its proper deployment</li>\n<li>obtain application public IP address and check accessibility from GitHub CI/CD</li>\n<li>validate expected activity of the traffic generator for Online Boutique</li>\n<li>check proper service mesh config for the microservices via istioctl</li>\n</ol>\n<p>Application can now be accessed as described <a href=\"https://github.com/didier-durand/microservices-on-cloud-kubernetes#access-to-application--load-generator\">above</a></p>\n<h2><a id=\"user-content-application-features--service-mesh\" class=\"anchor\" href=\"https://github.com/didier-durand/microservices-on-cloud-kubernetes#application-features--service-mesh\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Application features &amp; service mesh</h2>\n<p><a href=\"https://github.com/didier-durand/microservices-on-cloud-kubernetes/blob/master/img/application/online-boutique-architecture.png\"><img src=\"https://github.com/didier-durand/microservices-on-cloud-kubernetes/raw/master/img/application/online-boutique-architecture.png\"></a></p>\n<p><ins>application service mesh<ins></ins></ins></p>\n<p>This demo application contains an interesting service mesh to give some substance to demos and tests: its schema is given above. This mesh\nwhich is thoroughly used by a traffic generator - also part of the demo package - which generates constant solid traffic to make the implementation\nof monitoring tools.</p>\n<p>Interesting points of Online Boutique:</p>\n<ol>\n<li>\n<p><strong>Multi-language:</strong> the microservices corresponding to the various application features were on purpose written on purpose by the authors in\nnumerous languages (Go, NodeJS, Java, C#, Python) to demonstrate a key strength of container-based applications: many microservices collaborate in\na &quot;polyglot&quot; environment where each team (or individual) can program in its language of choice while ad hoc frameworks for each language make sure\nthat all Kubernetes standards (probes, etc.) and architecture can be easily respected with minimal effort to obtain a coherent and compliant global\nsystem, manageable by the standard palette of Kubernetes tools. This polyglot aspect is reinforced by the mixed use of http and gRpc, which are both\nunderstood by the monitoring tools.</p>\n</li>\n<li>\n<p><strong>Service Mesh:</strong> the application graph shows relationships between the various services and the front-end. Indeed, the application\nis made of13 pods. This high-level of granularity is the accepted Kubernetes pattern for application architecture: it brings numerous advantages like continuous\ndelivery, exhaustive unit testing, higher resilience, optimal scalability, etc. But, it also requires the use of a thorough set of tools to\nmaximize the observability of the system. If &quot;divide and conquer&quot; is the motto of cloud-native architectures, the motto of their operations is probably\n&quot;observe to sustain&quot;: when working with Kubernetes application, one feels very quickly the need for (very) solid tools monitoring automatically\nthe myriad of objects (services, pods, ingress, volumes, etc.) composing the system.</p>\n</li>\n<li>\n<p><strong>GCP Tooling:</strong> the application is instrumented for <a href=\"https://en.wikipedia.org/wiki/Stackdriver\">Stackdriver (profiling, logging, debugging)</a>. So,\nthe source code of this application provides the right guidance to see how to code in order to obtain the right leverage on tools directly available\nfrom the GCP service portfolio.</p>\n</li>\n</ol>\n<h2><a id=\"user-content-setup-for-forks\" class=\"anchor\" href=\"https://github.com/didier-durand/microservices-on-cloud-kubernetes#setup-for-forks\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Setup for forks</h2>\n<p>To start with, you need a Google Cloud account with a project in it where the GKE APIs have been enabled. Obtain the id of your project from\nGCP dashboard. Additionally, you need to create in this project a service account and give it proper GKE credentials: right to create, administer\nand delete a cluster. Save its private key in json format.</p>\n<p>Then, fork our repository and define the required <a href=\"https://docs.github.com/en/actions/reference/encrypted-secrets\">Github Secrets</a> in your forked\nrepository:</p>\n<ol>\n<li>your GCP project id will be {{ secrets.GCP_PROJECT }}</li>\n<li>The private key of your service account in json format will be ${{ secrets.GCP_SA_KEY }}</li>\n</ol>\n<p>To easily launch the workflow, you can launch it with the <a href=\"https://github.blog/changelog/2020-07-06-github-actions-manual-triggers-with-workflow_dispatch/\">manual dispatch feature of Github</a> that you can see as a launch button in the Action tab of your project for\nthe &quot;Deploy Online Boutique&quot; workflow. Similarly, you can stop it via similar button in &quot;Terminate Online Boutique&quot; workflow.</p>\n<p>When the deployment workflow completes successfully, you should be able to access the Online Boutique from anywhere on the Internet at the pubic IP\naddress displayed in the final lines of step &quot;Deploy application on GKE&quot; (or via</p>\n<p>To get access to the cluster via kubectl and to the dashboards via istioctl, you need to install on your machine the gcloud SDK, connect to GCP\nwith your userid (having at least same credentials as service account above). Then, use <em>&apos;gcloud container clusters get-credentials  --zone  --project=&apos;</em>\nwith your own values. It will prepare and install on your machine the proper config and credentials files - usually located in ~/.kube - to give\nyou access to your cluster via kubectl and istioctl.</p>\n<p>Finally, you should <a href=\"https://kubernetes.io/docs/tasks/tools/install-kubectl/\">install kubectl</a> and\n<a href=\"https://istio.io/latest/docs/setup/install/istioctl/\">install istioctl</a> if not present on your laptop yet.</p>\n</article></div></div>",
      "contentAsText": "\n\n\n\nThe purpose of this repository is to provide the fully automated setup of a nice-looking (see screenshots)\nshowcase / testbed for a cloud-native (precisely defined application\nby Microsoft) on a cloud-hosted Kubernetes cluster (here GKE by Google Cloud) based on an interesting service mesh.\nSo, additionally, the setup will install tooling (coming from the CNCF Cloud Trail Map for many of them) to make the application and its service mesh observable and manageable.\nThis application, licensed under Apache terms (same terms for all components used in this worklfow - So, allowing free reuse) is the \"Online Boutique\"\n(formerly known as Hipster Shop - developed by a Google team but not an official product of them). It is composed of 10 polyglot microservices behind a nice-looking web frontend calling them to serve client requests.\nA load-generator - part of the package - will generate traffic while the application is running to make use of tools (Prometheus, OpenTelemetry,\netc.) more attractive.\nAnother goal of this repository is to help people exploring the cloud-native architecture: when you fork it, you rapidly get a working cluster with a somewhat\n\"real-life\" application and decent tooling to experiment with, without the need for a long trial-and-error process starting with infrastructure to set it\nup from scratch. It makes it much faster to grasp the philosophy of the distributed architecture proposed by Kubernetes.\nSo, happy forking for your own use! (see Setup section for all\ntechnical details) And come back regularly or get notified by following this repository: we will add additional tools in subsequent updates.\nWe implement here a Github workflow (microservices-on-gke.yml & shells in sh directory - see our other repository for other workflows)\nwhich allows to automatically deploy a fresh cluster on GKE and to deploy the application on it whenever needed via a single click.\nOn our side, this same workflow is also started automatically on a recurring basis (at least weekly) via Github's cron facility (included in workflow yaml)\nto make sure that the deployment remains fully operational as underlying GKE infrastructure and implemented components evolve. You can access logs\nof previous runs in the Actions Tab.\nAccess to application & load generator\nOn successful completion of the workflow, the Online Boutique is accessible from anywhere on the Internet at the public IP address\n(dynamically created and published by GKE) displayed in the final lines of workflow execution step \"Deploy application on GKE\". Indeed, it is the IP\naddress of the K8s service 'frontend-external' defined by the deployment. Hence,\nyou can also get it at any time via 'kubectl get service 'frontend-external'' provided that you went through proper setup as described below.\nTo check the activity of the load generator, you can at any time run 'kubectl logs -l app=loadgenerator -c main'\nYou should get something like the following describing how many requests were already triggered:\nkubectl logs -l app=loadgenerator -c main\n GET /product/66VCHSJNUP                                          600     0(0.00%)      77      34    1048  |      41    0.10    0.00\n GET /product/6E92ZMYYFZ                                          563     0(0.00%)      77      34    1763  |      41    0.00    0.00\n GET /product/9SIQT8TOJO                                          593     0(0.00%)      73      34    1013  |      41    0.30    0.00\n GET /product/L9ECAV7KIM                                          631     0(0.00%)      82      34    1349  |      42    0.20    0.00\n GET /product/LS4PSXUNUM                                          608     0(0.00%)      83      34     896  |      42    0.20    0.00\n GET /product/OLJCESPC7Z                                          623     0(0.00%)      69      34    1079  |      41    0.10    0.00\n POST /setCurrency                                                808     0(0.00%)      82      44    1089  |      51    0.20    0.00\n--------------------------------------------------------------------------------------------------------------------------------------------\n Aggregated                                                      9517     0(0.00%)                                       1.80    0.00\n\nIf you want to easily inject more traffic, you can additionally use the hey or fortio\nutilities as we did in our Knative project : see correponding workflow script.\nAccess to deployed tools & dashboards\nYou have first to implement the requirements of the Setup section\nbefore trying to access the dashboards.\nTo keep things simple, we access all tools and dashboards via the proxy functions available in Kubernetes: either directly via 'kubectl proxy'\nor indirectly via 'istioctl dashboard xxx'. Only limited additional definitions are then required: it's just fine for a demo and initial tests.\nOf course, the laptop running the proxies must be authentified to gcloud via SDK with proper credentials giving rights to cluster administration.\nAvailable dashboards:\n(click on pictures to enlarge them - also use the hyperlinks provided with each dashboard description to have a good overview of the\nfeatures of each tool from its official documentation)\n\n\nStandard K8s UI: our workflow deploys first this standard Kubernetes dashboard\nas a tool that should anyway be included in any installation. It gives a good overview of the deployed cluster with static (configuration) and dynamic (metrics) information\nabout the active objects. When 'kubectl proxy' is active, the dashboard is available at this url.\nThe security check at login is most easily satisfied by selection the login option of config file (see Prereqs to obtain it in Setup section).\n\n\n\nPolaris dashboard: Polaris is an interesting tool, even in its OSS version (the paid-for version\nprovides more checks) used here. After installation, it scans the definitions of various kinds of objects and applies sanity checking rules to\nvalidate their proper configuration. For example, in the case of Online Boutique, it will warn that containers have no resource constraints (cpu,\nmemory, etc.) imposed on them or that their security credentials are too wide compared to what they do. The hyperlinks provided on the unsatisfactory\nchecks document the reason(s) of the alert as well as the possible remedies to apply. So, a quite useful tool to incrementally increase the quality of the\nconfiguration of a given cluster: new versions of yaml object manifests can be gradually deployed to minimize the issue notifications.\n\n\n\nKiali Dashboard: Kiali claims itself the \"Service mesh management for Istio\". it allows an interactive discovery of\nthe defined relations between the services. Then, it provides detailed insights and metrics about the\nhealth of those services and the requests between them.  The live traffic animation in the UI is extremely useful: it allows to spot very quickly\nwhere the activity happens to focus on those hot spots during root cause analysis for an issue. You can also go back in time with the replay\nfeature to see the traffic and the interactions that happened in the past to understand why and how you reached current situation. This dashboard\nis accessed via 'istioctl dashboard kiali' that will open the corresponding UI into your web browser.\n\n\n\nGrafana dashboard: Grafana which 'allows you to query, visualize, alert on metrics and logs, no\nmatter where they are stored' provides very nice charts about the activity of the cluster as the whole (usual metrics about resource consumption:\ncpu, memory, etc. for nodes. But, more specifically in this context it provides interesting additional dashboards specific to the Istio service mesh\nrelated to the traffic between the pods. Those dashboards are accessed via 'istioctl dashboard grafana' that will open the corresponding UI into\nyour web browser.\n\n\n\nJaeger dashboard: the Open Boutique is instrumented via OpenCensus, now merged into\nOpenTelemetry, component of the CNCF Cloud Trail Map. Jaeger - in CNCF Cloud Trail Map - is the tracing backend\nimplemented here. It centralizes and ingests the distributed traces produced by the various microservices. So, the Jaeger dashboard will allow the\ndetailed examination of those aggregated distributed traces also known as \"spans\". This\ndashboard is accessed via 'istioctl dashboard jaeger' that will open the corresponding UI into your web browser.\n\n\n\nPrometheus dashboard: Prometheus (also member of CNCF Cloud Trail Map) is the cornerstone component for metrics\ncollection. The collected data is used by Grafana, Kiali & Jaeger for their specific purposes. The Prometheus dashboard can be used as the \"source\nof truth\": for example, it can be used to verify if some metrics claimed as missing by a downstream component using this value is really collected\nor not and to compare the graph produced by Prometheus itself to the graph produced downstream user to spot potential discrepancies. This dashboard\nis accessed via 'istioctl dashboard prometheus' that will open the corresponding UI into your web browser.\n\n\n\nEnvoy dashboard: an improved version of Envoy Proxy, also part of the CNCF Cloud Trail Map,  is the sidecar\ncontainer used by Istio. Envoy provides a (somewhat rough) dashboard to go into the\nnitty-gritty details of a given pod: it is usually used for low-level introspection into the traffic between pods in unexpected situations. It is\nmore a debugging tool at very low-level: most cluster administrators shouldn't need to use it. This dashboard is accessed via 'istioctl\ndashboard envoy podname[.namespace]': it will open the corresponding UI for the chosen pod sidecar into your web browser.\n\nWorkflow steps\nThe workflow has following steps:\n\nsetup of workflow parameters via environment variables\ncheckout of project artefacts\nSetup of gcloud SDK for authentication to Google Cloud through secrets defined in repository via setup.(subsequent steps in shell script)\ncreate cluster\n6, import cluster config & credentials for kubectl & istioctl\ndeploy standard K8s dashboard and check its proper deployment\ndeploy Polaris dashboard and check its proper deployment\ndeploy Istio and check its proper deployment\ndeploy Istio-based addons and check their proper deployment\nlabel application namespace to ensure automatic sidecar injection (see Istio architecture uses an improved version of Envoy for sidecars) in microservice pods\ndeploy proper application manifests for Istio\ndeploy Online Boutique and check its proper deployment\nobtain application public IP address and check accessibility from GitHub CI/CD\nvalidate expected activity of the traffic generator for Online Boutique\ncheck proper service mesh config for the microservices via istioctl\n\nApplication can now be accessed as described above\nApplication features & service mesh\n\napplication service mesh\nThis demo application contains an interesting service mesh to give some substance to demos and tests: its schema is given above. This mesh\nwhich is thoroughly used by a traffic generator - also part of the demo package - which generates constant solid traffic to make the implementation\nof monitoring tools.\nInteresting points of Online Boutique:\n\n\nMulti-language: the microservices corresponding to the various application features were on purpose written on purpose by the authors in\nnumerous languages (Go, NodeJS, Java, C#, Python) to demonstrate a key strength of container-based applications: many microservices collaborate in\na \"polyglot\" environment where each team (or individual) can program in its language of choice while ad hoc frameworks for each language make sure\nthat all Kubernetes standards (probes, etc.) and architecture can be easily respected with minimal effort to obtain a coherent and compliant global\nsystem, manageable by the standard palette of Kubernetes tools. This polyglot aspect is reinforced by the mixed use of http and gRpc, which are both\nunderstood by the monitoring tools.\n\n\nService Mesh: the application graph shows relationships between the various services and the front-end. Indeed, the application\nis made of13 pods. This high-level of granularity is the accepted Kubernetes pattern for application architecture: it brings numerous advantages like continuous\ndelivery, exhaustive unit testing, higher resilience, optimal scalability, etc. But, it also requires the use of a thorough set of tools to\nmaximize the observability of the system. If \"divide and conquer\" is the motto of cloud-native architectures, the motto of their operations is probably\n\"observe to sustain\": when working with Kubernetes application, one feels very quickly the need for (very) solid tools monitoring automatically\nthe myriad of objects (services, pods, ingress, volumes, etc.) composing the system.\n\n\nGCP Tooling: the application is instrumented for Stackdriver (profiling, logging, debugging). So,\nthe source code of this application provides the right guidance to see how to code in order to obtain the right leverage on tools directly available\nfrom the GCP service portfolio.\n\n\nSetup for forks\nTo start with, you need a Google Cloud account with a project in it where the GKE APIs have been enabled. Obtain the id of your project from\nGCP dashboard. Additionally, you need to create in this project a service account and give it proper GKE credentials: right to create, administer\nand delete a cluster. Save its private key in json format.\nThen, fork our repository and define the required Github Secrets in your forked\nrepository:\n\nyour GCP project id will be {{ secrets.GCP_PROJECT }}\nThe private key of your service account in json format will be ${{ secrets.GCP_SA_KEY }}\n\nTo easily launch the workflow, you can launch it with the manual dispatch feature of Github that you can see as a launch button in the Action tab of your project for\nthe \"Deploy Online Boutique\" workflow. Similarly, you can stop it via similar button in \"Terminate Online Boutique\" workflow.\nWhen the deployment workflow completes successfully, you should be able to access the Online Boutique from anywhere on the Internet at the pubic IP\naddress displayed in the final lines of step \"Deploy application on GKE\" (or via\nTo get access to the cluster via kubectl and to the dashboards via istioctl, you need to install on your machine the gcloud SDK, connect to GCP\nwith your userid (having at least same credentials as service account above). Then, use 'gcloud container clusters get-credentials  --zone  --project='\nwith your own values. It will prepare and install on your machine the proper config and credentials files - usually located in ~/.kube - to give\nyou access to your cluster via kubectl and istioctl.\nFinally, you should install kubectl and\ninstall istioctl if not present on your laptop yet.\n",
      "description": "Microservices demo application on cloud-hosted Kubernetes cluster - didier-durand/microservices-on-cloud-kubernetes",
      "ogDescription": "Microservices demo application on cloud-hosted Kubernetes cluster - didier-durand/microservices-on-cloud-kubernetes"
    },
    {
      "url": "https://tansanrao.com/guide-storage-ingress-webui-k8s/",
      "title": "Guide: Storage, Ingress and Web UIs for Kubernetes Cluster",
      "content": "<div class=\"pos-relative js-post-content\"> <p>Hello everybody, tansanrao here! This is a follow up on the previous guide about how to <a href=\"https://tansanrao.com/kubernetes-ha-cluster-with-kubeadm/\">setup an HA Kubernetes Cluster</a>. We will be dealing with Distributed Block Storage using Longhorn, Ingress Controller using Traefik, A Few useful Middleware Examples for Traefik and deploying the Kubernetes Dashboard.</p><h2 id=\"storage-longhorn-distributed-block-storage-for-kubernetes\">Storage: Longhorn distributed block storage for Kubernetes</h2><h3 id=\"what-is-longhorn\">What is Longhorn?</h3><p>Longhorn is a lightweight, reliable and easy-to-use distributed block storage system for Kubernetes.</p><p>Longhorn is free, open source software. Originally developed by Rancher Labs, it is now being developed as a sandbox project of the Cloud Native Computing Foundation.</p><h3 id=\"installing-longhorn\">Installing Longhorn</h3><p>Install Longhorn by applying the following manifest with <code>kubectl</code></p><pre><code class=\"language-bash\">kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/longhorn.yaml\n</code></pre><p>To monitor the progress of the installation, you can use this</p><pre><code class=\"language-bash\">kubectl get pods \\\n--namespace longhorn-system \\\n--watch\n</code></pre><p>To check for successful deployment, see that all the pods show a ready and running state like so</p><pre><code class=\"language-bash\">NAME                                        READY     STATUS    RESTARTS   AGE\ncsi-attacher-6fdc77c485-8wlpg               1/1       Running   0          9d\ncsi-attacher-6fdc77c485-psqlr               1/1       Running   0          9d\ncsi-attacher-6fdc77c485-wkn69               1/1       Running   0          9d\ncsi-provisioner-78f7db7d6d-rj9pr            1/1       Running   0          9d\ncsi-provisioner-78f7db7d6d-sgm6w            1/1       Running   0          9d\ncsi-provisioner-78f7db7d6d-vnjww            1/1       Running   0          9d\nengine-image-ei-6e2b0e32-2p9nk              1/1       Running   0          9d\nengine-image-ei-6e2b0e32-s8ggt              1/1       Running   0          9d\nengine-image-ei-6e2b0e32-wgkj5              1/1       Running   0          9d\nlonghorn-csi-plugin-g8r4b                   2/2       Running   0          9d\nlonghorn-csi-plugin-kbxrl                   2/2       Running   0          9d\nlonghorn-csi-plugin-wv6sb                   2/2       Running   0          9d\nlonghorn-driver-deployer-788984b49c-zzk7b   1/1       Running   0          9d\nlonghorn-manager-nr5rs                      1/1       Running   0          9d\nlonghorn-manager-rd4k5                      1/1       Running   0          9d\nlonghorn-manager-snb9t                      1/1       Running   0          9d\nlonghorn-ui-67b9b6887f-n7x9q                1/1       Running   0          9d\n</code></pre><h2 id=\"dashboard-kubernetes-web-ui\">Dashboard: Kubernetes Web UI</h2><pre><code class=\"language-bash\">kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml\n</code></pre><p>To protect your cluster data, Dashboard deploys with a minimal RBAC configuration by default. Currently, Dashboard only supports logging in with a Bearer Token. To create a token for this demo, create a ServiceAccount called <code>admin-user</code>.</p><pre><code class=\"language-bash\">cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: admin-user\n  namespace: kubernetes-dashboard\nEOF\n</code></pre><p>Next we create a ClusterRoleBinding between the ClusterRole <code>cluster-admin</code> and the ServiceAccount <code>admin-user</code></p><pre><code class=\"language-bash\">cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: admin-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: admin-user\n  namespace: kubernetes-dashboard\nEOF\n</code></pre><p>Next we need to fetch a Bearer Token,</p><p>For Bash, do this:</p><pre><code class=\"language-bash\">kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk &apos;{print $1}&apos;)\n\n</code></pre><p>For PowerShell, do this:</p><pre><code class=\"language-bash\">kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | sls admin-user | ForEach-Object { $_ -Split &apos;\\s+&apos; } | Select -First 1)\n\n</code></pre><p>It should output something like this</p><pre><code class=\"language-bash\">Name:         admin-user-token-v57nw\nNamespace:    kubernetes-dashboard\nLabels:       &lt;none&gt;\nAnnotations:  kubernetes.io/service-account.name: admin-user\n              kubernetes.io/service-account.uid: 0303243c-4040-4a58-8a47-849ee9ba79c1\n\nType:  kubernetes.io/service-account-token\n\nData\n====\nca.crt:     1066 bytes\nnamespace:  20 bytes\ntoken:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLXY1N253Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwMzAzMjQzYy00MDQwLTRhNTgtOGE0Ny04NDllZTliYTc5YzEiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.Z2JrQlitASVwWbc-s6deLRFVk5DWD3P_vjUFXsqVSY10pbjFLG4njoZwh8p3tLxnX_VBsr7_6bwxhWSYChp9hwxznemD5x5HLtjb16kI9Z7yFWLtohzkTwuFbqmQaMoget_nYcQBUC5fDmBHRfFvNKePh_vSSb2h_aYXa8GV5AcfPQpY7r461itme1EXHQJqv-SN-zUnguDguCTjD80pFZ_CmnSE1z9QdMHPB8hoB4V68gtswR1VLa6mSYdgPwCHauuOobojALSaMc3RH7MmFUumAgguhqAkX3Omqd3rJbYOMRuMjhANqd08piDC3aIabINX6gP5-Tuuw2svnV6NYQ\n</code></pre><p>Now copy the token and store it somewhere safe, we will need it in a minute.</p><h3 id=\"accessing-the-web-ui\">Accessing the Web UI</h3><p>We create a proxy to our cluster using <code>kubectl</code></p><pre><code class=\"language-bash\">kubectl proxy\n</code></pre><p><code>kubectl</code> will make Dashboard available at <a href=\"http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/\">http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/</a>.</p><p>The UI can only be accessed from the machine where the command is executed.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://tansanrao.com/content/images/2020/09/Screenshot-2020-09-25-at-8.48.07-PM.png\" class=\"kg-image\" alt=\"Web UI Login\"><figcaption>Web UI Login</figcaption></figure><p>Enter the token from the previous step here to login.</p><h2 id=\"ingress-traefik-proxy-as-ingress-controller\">Ingress: Traefik Proxy as Ingress Controller</h2><p>We will be setting up Traefik with Let\u0019s Encrypt with optional Cloudflare DNS Challenge Configuration</p><h3 id=\"installing-traefik-via-helm-chart\">Installing Traefik via Helm Chart</h3><p>Make sure you have Helm installed, then add the Traefik repo</p><pre><code class=\"language-bash\">helm repo add traefik https://helm.traefik.io/traefik\n</code></pre><p>Update Helm Repos</p><pre><code class=\"language-bash\">helm repo update\n</code></pre><p>Create a <code>traefik-values.yaml</code> file</p><pre><code class=\"language-bash\">nano traefik-values.yaml\n</code></pre><p>Paste the following into the file</p><pre><code class=\"language-yaml\">additionalArguments: - &quot;<a href=\"/cdn-cgi/l/email-protection\" class=\"__cf_email__\">[email&#xA0;protected]</a>&quot;\n  - &quot;--certificatesresolvers.http-le.acme.storage=/data/acme.json&quot;\n  - &quot;--certificatesresolvers.http-le.acme.caserver=https://acme-v02.api.letsencrypt.org/directory&quot;\n  - &quot;--certificatesResolvers.http-le.acme.httpchallenge=true&quot;\n  - &quot;--certificatesResolvers.http-le.acme.httpchallenge.entrypoint=web&quot;\n  - &quot;--api.insecure=true&quot;\n  - &quot;--accesslog=true&quot;\n  - &quot;--log.level=WARN&quot;\n  - &quot;--serversTransport.insecureSkipVerify=true&quot;\n</code></pre><p>Change the email to match your valid email address for Let\u0019s Encrypt. That will setup an ACME certResolver named <code>http-le</code> which will use the <code>web</code> entry point to perform an <code>HTTP-01 challenge</code> and issue certs.</p><p>To use Cloudflare DNS via certResolver named <code>dns-le</code> to perform a <code>DNS-01 challenge</code> you can use the values below.</p><pre><code class=\"language-yaml\">additionalArguments: - &quot;<a href=\"/cdn-cgi/l/email-protection\" class=\"__cf_email__\">[email&#xA0;protected]</a>&quot;\n  - &quot;--certificatesresolvers.dns-le.acme.storage=/data/acme.json&quot;\n  - &quot;--certificatesresolvers.dns-le.acme.caserver=https://acme-v02.api.letsencrypt.org/directory&quot;\n  - &quot;--certificatesResolvers.dns-le.acme.dnschallenge=true&quot;\n  - &quot;--certificatesResolvers.dns-le.acme.dnschallenge.provider=cloudflare&quot;\n  - &quot;--api.insecure=true&quot;\n  - &quot;--accesslog=true&quot;\n  - &quot;--log.level=WARN&quot;\n  - &quot;--serversTransport.insecureSkipVerify=true&quot;\nenv:\n  - name: CF_DNS_API_TOKEN\n    valueFrom:\n      secretKeyRef:\n        name: cloudflare\n        key: dns-token\n</code></pre><p>In the Cloudflare version of the values file, we are loading the Cloudflare API token from a secret, let\u0019s go ahead and create one.</p><p>In your Cloudflare Dashboard, navigate to your domain page. On the right, you\u0019ll see a section called API. Click on get API Token</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://tansanrao.com/content/images/2020/09/Screenshot-2020-09-26-at-12.03.27-PM.png\" class=\"kg-image\" alt=\"Get API Token\"><figcaption>Get API Token</figcaption></figure><p>Then on the API Tokens Tab, click on Create Token</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://tansanrao.com/content/images/2020/09/Screenshot-2020-09-26-at-12.05.43-PM.png\" class=\"kg-image\" alt=\"Api Tokens\"><figcaption>Api Tokens</figcaption></figure><p>Use the Edit Zone DNS Template and add Edit permissions for Zone.Zone and Zone.DNS, then under Zone Resources, Include the zone for the domain you plan on using, in my case, I\u0019ve chosen to grant access to all zones.</p><figure class=\"kg-card kg-image-card kg-card-hascaption\"><img src=\"https://tansanrao.com/content/images/2020/09/Screenshot-2020-09-26-at-12.07.21-PM.png\" class=\"kg-image\" alt=\"Token Config\"><figcaption>Token Config</figcaption></figure><p>Save the Token as it\u0019ll only be shown once.</p><p>Next we create a secret for the same using <code>kubectl</code></p><pre><code class=\"language-bash\">kubectl create secret generic cloudflare \\\n  --from-literal=dns-token=&apos;&lt;cloudflare_token_here&gt;&apos;\n</code></pre><p>Now we are ready to install Traefik using Helm</p><pre><code class=\"language-bash\">helm install traefik traefik/traefik --values=traefik-values.yaml\n</code></pre><p>Check for successful installation by running</p><pre><code class=\"language-bash\">kubectl get pods\n</code></pre><p>Traefik should be visible with Ready Pods and a status of Running. The dashboard can be accessed by port forwarding through <code>kubectl</code></p><pre><code class=\"language-bash\">kubectl port-forward $(kubectl get pods --selector &quot;app.kubernetes.io/name=traefik&quot; --output=name) 9000:9000\n</code></pre><p>The dashboard will be available at <a href=\"http://localhost:9000/dashboard\">http://localhost:9000/dashboard</a>.</p><h2 id=\"basic-auth-middleware-for-traefik\">Basic Auth Middleware for Traefik</h2><p>Create a file named <code>auth-middleware.yaml</code> and paste the following into it</p><pre><code class=\"language-bash\"># Declaring the user list\napiVersion: traefik.containo.us/v1alpha1\nkind: Middleware\nmetadata:\n  name: admin-auth\n  namespace: default\nspec:\n  basicAuth:\n    secret: authsecret\n\n---\n# Note: in a kubernetes secret the string (e.g. generated by htpasswd) must be base64-encoded first.\n# To create an encoded user:password pair, the following command can be used:\n# htpasswd -nb user password | openssl base64\n\napiVersion: v1\nkind: Secret\nmetadata:\n  name: authsecret\n  namespace: default\n\ndata:\n  users: \n    &lt;base64_encoded_string&gt;\n\n</code></pre><p>Replace the line at the end with a base64-encoded string from <code>htpasswd</code> generated using this command</p><pre><code class=\"language-bash\">htpasswd -nb user password | openssl base64\n</code></pre><p>Replace user and password with the username and password that you would like to use.</p><p>Apply the file using <code>kubectl</code></p><pre><code class=\"language-bash\">kubectl apply -f auth-middleware.yaml\n</code></pre><h3 id=\"cors-middleware\">CORS Middleware</h3><p>Create a file named <code>cors-middleware.yaml</code> and paste the following</p><pre><code class=\"language-bash\">apiVersion: traefik.containo.us/v1alpha1\nkind: Middleware\nmetadata:\n  name: cors-allow-all\n  namespace: default\nspec:\n  headers:\n    accessControlAllowCredentials: true\n    accessControlAllowMethods:\n      - &quot;*&quot;\n    accessControlAllowOriginList:\n      - &quot;*&quot;\n    accessControlMaxAge: 100\n    addVaryHeader: true\n</code></pre><p>It grants all origins permission to use all methods and credentials. Apply it with <code>kubectl</code></p><pre><code class=\"language-bash\">kubectl apply -f cors-middleware.yaml\n</code></pre><h3 id=\"svc-wss-middleware\">SVC-WSS Middleware</h3><p>Create a file named <code>wss-middleware.yaml</code> and paste the following</p><pre><code class=\"language-bash\">apiVersion: traefik.containo.us/v1alpha1\nkind: Middleware\nmetadata:\n  name: svc-wss-headers\n  namespace: default\nspec:\n  headers:\n    customRequestHeaders:\n      X-Forwarded-Proto: &quot;https&quot;\n\n</code></pre><p>It adds https in the Forwarded-Photo header to force https WebSocketSecure connections. It is needed only sometimes to get Longhorn UI to play nicely with https in Traefik.</p><p>Apply it with <code>kubectl</code></p><pre><code class=\"language-bash\">kubectl apply -f wss-middleware.yaml\n</code></pre><h2 id=\"longhorn-ui-router-using-ingressroutes\">Longhorn UI Router using IngressRoutes</h2><p>Create a file named <code>ingress-longhorn.yaml</code> and add the following to it</p><pre><code class=\"language-bash\">apiVersion: traefik.containo.us/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: longhorn-ui-ingress\n  namespace: longhorn-system\nspec:\n  entryPoints:\n    - websecure\n  routes:\n    - match: Host(`longhorn.k8s.example.com`)\n      kind: Rule\n      services:\n        - name: longhorn-frontend\n          port: 80\n          namespace: longhorn-system\n      middlewares:\n        - name: cors-allow-all\n          namespace: default\n        - name: admin-auth\n          namespace: default\n        - name: svc-wss-headers\n          namespace: default\n  tls:\n    certResolver: http-le\n\n</code></pre><p>Here we are creating an IngressRoute that matches the Hostname <code>longhorn.k8s.example.com</code> substitute it for whatever you are going to use. We are also chaining the 3 middleware we defined above to add core, was and basic auth to the route.</p><p>Apply it with <code>kubectl</code></p><pre><code class=\"language-bash\">kubectl apply -f ingress-longhorn.yaml\n</code></pre><p>Done, the UI should be up on the host url specified in the ingress route.</p><p>For any doubts, suggestions or issues, leave a comment below and subscribe to the newsletter for the latest posts delivered straight to your inbox! Follow me on <a href=\"https://twitter.com/tansanrao\">Twitter</a> &amp; <a href=\"https://instagram.com/tansanrao\">Instagram</a> for behind the scenes and updates.</p>\n<section class=\"m-tags in-post\"> </section>\n</div>",
      "contentAsText": " Hello everybody, tansanrao here! This is a follow up on the previous guide about how to setup an HA Kubernetes Cluster. We will be dealing with Distributed Block Storage using Longhorn, Ingress Controller using Traefik, A Few useful Middleware Examples for Traefik and deploying the Kubernetes Dashboard.Storage: Longhorn distributed block storage for KubernetesWhat is Longhorn?Longhorn is a lightweight, reliable and easy-to-use distributed block storage system for Kubernetes.Longhorn is free, open source software. Originally developed by Rancher Labs, it is now being developed as a sandbox project of the Cloud Native Computing Foundation.Installing LonghornInstall Longhorn by applying the following manifest with kubectlkubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/longhorn.yaml\nTo monitor the progress of the installation, you can use thiskubectl get pods \\\n--namespace longhorn-system \\\n--watch\nTo check for successful deployment, see that all the pods show a ready and running state like soNAME                                        READY     STATUS    RESTARTS   AGE\ncsi-attacher-6fdc77c485-8wlpg               1/1       Running   0          9d\ncsi-attacher-6fdc77c485-psqlr               1/1       Running   0          9d\ncsi-attacher-6fdc77c485-wkn69               1/1       Running   0          9d\ncsi-provisioner-78f7db7d6d-rj9pr            1/1       Running   0          9d\ncsi-provisioner-78f7db7d6d-sgm6w            1/1       Running   0          9d\ncsi-provisioner-78f7db7d6d-vnjww            1/1       Running   0          9d\nengine-image-ei-6e2b0e32-2p9nk              1/1       Running   0          9d\nengine-image-ei-6e2b0e32-s8ggt              1/1       Running   0          9d\nengine-image-ei-6e2b0e32-wgkj5              1/1       Running   0          9d\nlonghorn-csi-plugin-g8r4b                   2/2       Running   0          9d\nlonghorn-csi-plugin-kbxrl                   2/2       Running   0          9d\nlonghorn-csi-plugin-wv6sb                   2/2       Running   0          9d\nlonghorn-driver-deployer-788984b49c-zzk7b   1/1       Running   0          9d\nlonghorn-manager-nr5rs                      1/1       Running   0          9d\nlonghorn-manager-rd4k5                      1/1       Running   0          9d\nlonghorn-manager-snb9t                      1/1       Running   0          9d\nlonghorn-ui-67b9b6887f-n7x9q                1/1       Running   0          9d\nDashboard: Kubernetes Web UIkubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml\nTo protect your cluster data, Dashboard deploys with a minimal RBAC configuration by default. Currently, Dashboard only supports logging in with a Bearer Token. To create a token for this demo, create a ServiceAccount called admin-user.cat <<EOF | kubectl apply -f -\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: admin-user\n  namespace: kubernetes-dashboard\nEOF\nNext we create a ClusterRoleBinding between the ClusterRole cluster-admin and the ServiceAccount admin-usercat <<EOF | kubectl apply -f -\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: admin-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: admin-user\n  namespace: kubernetes-dashboard\nEOF\nNext we need to fetch a Bearer Token,For Bash, do this:kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}')\n\nFor PowerShell, do this:kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | sls admin-user | ForEach-Object { $_ -Split '\\s+' } | Select -First 1)\n\nIt should output something like thisName:         admin-user-token-v57nw\nNamespace:    kubernetes-dashboard\nLabels:       <none>\nAnnotations:  kubernetes.io/service-account.name: admin-user\n              kubernetes.io/service-account.uid: 0303243c-4040-4a58-8a47-849ee9ba79c1\n\nType:  kubernetes.io/service-account-token\n\nData\n====\nca.crt:     1066 bytes\nnamespace:  20 bytes\ntoken:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLXY1N253Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwMzAzMjQzYy00MDQwLTRhNTgtOGE0Ny04NDllZTliYTc5YzEiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.Z2JrQlitASVwWbc-s6deLRFVk5DWD3P_vjUFXsqVSY10pbjFLG4njoZwh8p3tLxnX_VBsr7_6bwxhWSYChp9hwxznemD5x5HLtjb16kI9Z7yFWLtohzkTwuFbqmQaMoget_nYcQBUC5fDmBHRfFvNKePh_vSSb2h_aYXa8GV5AcfPQpY7r461itme1EXHQJqv-SN-zUnguDguCTjD80pFZ_CmnSE1z9QdMHPB8hoB4V68gtswR1VLa6mSYdgPwCHauuOobojALSaMc3RH7MmFUumAgguhqAkX3Omqd3rJbYOMRuMjhANqd08piDC3aIabINX6gP5-Tuuw2svnV6NYQ\nNow copy the token and store it somewhere safe, we will need it in a minute.Accessing the Web UIWe create a proxy to our cluster using kubectlkubectl proxy\nkubectl will make Dashboard available at http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/.The UI can only be accessed from the machine where the command is executed.Web UI LoginEnter the token from the previous step here to login.Ingress: Traefik Proxy as Ingress ControllerWe will be setting up Traefik with Let\u0019s Encrypt with optional Cloudflare DNS Challenge ConfigurationInstalling Traefik via Helm ChartMake sure you have Helm installed, then add the Traefik repohelm repo add traefik https://helm.traefik.io/traefik\nUpdate Helm Reposhelm repo update\nCreate a traefik-values.yaml filenano traefik-values.yaml\nPaste the following into the fileadditionalArguments: - \"[email protected]\"\n  - \"--certificatesresolvers.http-le.acme.storage=/data/acme.json\"\n  - \"--certificatesresolvers.http-le.acme.caserver=https://acme-v02.api.letsencrypt.org/directory\"\n  - \"--certificatesResolvers.http-le.acme.httpchallenge=true\"\n  - \"--certificatesResolvers.http-le.acme.httpchallenge.entrypoint=web\"\n  - \"--api.insecure=true\"\n  - \"--accesslog=true\"\n  - \"--log.level=WARN\"\n  - \"--serversTransport.insecureSkipVerify=true\"\nChange the email to match your valid email address for Let\u0019s Encrypt. That will setup an ACME certResolver named http-le which will use the web entry point to perform an HTTP-01 challenge and issue certs.To use Cloudflare DNS via certResolver named dns-le to perform a DNS-01 challenge you can use the values below.additionalArguments: - \"[email protected]\"\n  - \"--certificatesresolvers.dns-le.acme.storage=/data/acme.json\"\n  - \"--certificatesresolvers.dns-le.acme.caserver=https://acme-v02.api.letsencrypt.org/directory\"\n  - \"--certificatesResolvers.dns-le.acme.dnschallenge=true\"\n  - \"--certificatesResolvers.dns-le.acme.dnschallenge.provider=cloudflare\"\n  - \"--api.insecure=true\"\n  - \"--accesslog=true\"\n  - \"--log.level=WARN\"\n  - \"--serversTransport.insecureSkipVerify=true\"\nenv:\n  - name: CF_DNS_API_TOKEN\n    valueFrom:\n      secretKeyRef:\n        name: cloudflare\n        key: dns-token\nIn the Cloudflare version of the values file, we are loading the Cloudflare API token from a secret, let\u0019s go ahead and create one.In your Cloudflare Dashboard, navigate to your domain page. On the right, you\u0019ll see a section called API. Click on get API TokenGet API TokenThen on the API Tokens Tab, click on Create TokenApi TokensUse the Edit Zone DNS Template and add Edit permissions for Zone.Zone and Zone.DNS, then under Zone Resources, Include the zone for the domain you plan on using, in my case, I\u0019ve chosen to grant access to all zones.Token ConfigSave the Token as it\u0019ll only be shown once.Next we create a secret for the same using kubectlkubectl create secret generic cloudflare \\\n  --from-literal=dns-token='<cloudflare_token_here>'\nNow we are ready to install Traefik using Helmhelm install traefik traefik/traefik --values=traefik-values.yaml\nCheck for successful installation by runningkubectl get pods\nTraefik should be visible with Ready Pods and a status of Running. The dashboard can be accessed by port forwarding through kubectlkubectl port-forward $(kubectl get pods --selector \"app.kubernetes.io/name=traefik\" --output=name) 9000:9000\nThe dashboard will be available at http://localhost:9000/dashboard.Basic Auth Middleware for TraefikCreate a file named auth-middleware.yaml and paste the following into it# Declaring the user list\napiVersion: traefik.containo.us/v1alpha1\nkind: Middleware\nmetadata:\n  name: admin-auth\n  namespace: default\nspec:\n  basicAuth:\n    secret: authsecret\n\n---\n# Note: in a kubernetes secret the string (e.g. generated by htpasswd) must be base64-encoded first.\n# To create an encoded user:password pair, the following command can be used:\n# htpasswd -nb user password | openssl base64\n\napiVersion: v1\nkind: Secret\nmetadata:\n  name: authsecret\n  namespace: default\n\ndata:\n  users: \n    <base64_encoded_string>\n\nReplace the line at the end with a base64-encoded string from htpasswd generated using this commandhtpasswd -nb user password | openssl base64\nReplace user and password with the username and password that you would like to use.Apply the file using kubectlkubectl apply -f auth-middleware.yaml\nCORS MiddlewareCreate a file named cors-middleware.yaml and paste the followingapiVersion: traefik.containo.us/v1alpha1\nkind: Middleware\nmetadata:\n  name: cors-allow-all\n  namespace: default\nspec:\n  headers:\n    accessControlAllowCredentials: true\n    accessControlAllowMethods:\n      - \"*\"\n    accessControlAllowOriginList:\n      - \"*\"\n    accessControlMaxAge: 100\n    addVaryHeader: true\nIt grants all origins permission to use all methods and credentials. Apply it with kubectlkubectl apply -f cors-middleware.yaml\nSVC-WSS MiddlewareCreate a file named wss-middleware.yaml and paste the followingapiVersion: traefik.containo.us/v1alpha1\nkind: Middleware\nmetadata:\n  name: svc-wss-headers\n  namespace: default\nspec:\n  headers:\n    customRequestHeaders:\n      X-Forwarded-Proto: \"https\"\n\nIt adds https in the Forwarded-Photo header to force https WebSocketSecure connections. It is needed only sometimes to get Longhorn UI to play nicely with https in Traefik.Apply it with kubectlkubectl apply -f wss-middleware.yaml\nLonghorn UI Router using IngressRoutesCreate a file named ingress-longhorn.yaml and add the following to itapiVersion: traefik.containo.us/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: longhorn-ui-ingress\n  namespace: longhorn-system\nspec:\n  entryPoints:\n    - websecure\n  routes:\n    - match: Host(`longhorn.k8s.example.com`)\n      kind: Rule\n      services:\n        - name: longhorn-frontend\n          port: 80\n          namespace: longhorn-system\n      middlewares:\n        - name: cors-allow-all\n          namespace: default\n        - name: admin-auth\n          namespace: default\n        - name: svc-wss-headers\n          namespace: default\n  tls:\n    certResolver: http-le\n\nHere we are creating an IngressRoute that matches the Hostname longhorn.k8s.example.com substitute it for whatever you are going to use. We are also chaining the 3 middleware we defined above to add core, was and basic auth to the route.Apply it with kubectlkubectl apply -f ingress-longhorn.yaml\nDone, the UI should be up on the host url specified in the ingress route.For any doubts, suggestions or issues, leave a comment below and subscribe to the newsletter for the latest posts delivered straight to your inbox! Follow me on Twitter & Instagram for behind the scenes and updates.\n \n",
      "publishedDate": "2020-09-29T04:30:00.000Z",
      "description": "A Getting Started Guide to setup Persistent Volumes, Ingress and Web UIs for a Kubernetes Cluster",
      "ogDescription": "A Getting Started Guide to setup Persistent Volumes, Ingress and Web UIs for a Kubernetes Cluster"
    },
    {
      "url": "https://www.magalix.com/blog/kubernetes-cost-optimization-101",
      "title": "Kubernetes Cost Optimization 101",
      "content": "<div id=\"hs_cos_wrapper_post_body\" class=\"hs_cos_wrapper\"><p>Over the past two years at Magalix, we have focused on building our system, introducing new features, and scaling our infrastructure and microservices. During this time, we had a look at our Kubernetes clusters utilization and found it to be very low. We were paying for resources we didn\u0019t use, so we started a cost-saving practice to increase cluster utilization, use the resources we already had and pay less to run our cluster.</p> <p>In this article, I will discuss the top five techniques we used to better utilize our Kubernetes clusters on the cloud and eliminate wasted resources, thus saving money. In the end, we were able to cut our monthly bill by more than 50%!</p>\n<h2>1. Applying Workload Right-Sizing</h2>\n<p>Kubernetes manages and schedules pods are based on container resource specs:</p>\n<ul>\n<li><u><strong>Resource Requests</strong></u>: Kubernetes scheduler is used to place containers on the right node which has enough capacity</li>\n<li><u><strong>Resource Limits</strong></u>: Containers are NOT allowed to use more than their resource limit</li>\n</ul>\n<p>Resources requests and limits are container-scooped specs, while multi-container pods define separate resource specs for each container:</p>\n<pre><code class=\"language-yaml line-numbers\">apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: magalix\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 1\n            memory: 1Gi\n</code></pre>\n<p>Kubernetes schedules pods based on resource requests and other restrictions without impairing availability. The scheduler uses CPU and memory resource requests to schedule the workloads in the right nodes, control which pod works on which node and if multiple pods can schedule together on a single node.</p>\n<p>Every node type has its own allocatable CPU and memory capacities. Assigning high/unneeded CPU or memory resource requests can end up running underutilized pods on each node, which leads to underutilized nodes.</p>\n<p>In this section, we compared resource requests, limited against actual usage and changed the resource request to something closer to the actual utilization while adding a little safety margin.</p>\n<h2>2. Choosing the Right Worker Nodes</h2>\n<p>Every Kubernetes cluster has its own special workload utilization. Some clusters use memory more than CPU (e.g: database and caching workloads), while others use CPU more than memory (e.g: user-interactive and batch-processing workloads)</p>\n<p>Cloud providers such as GCP and AWS offer various node types that you can choose from.</p>\n<p>Choosing the wrong node size for your cluster can end up costing you. For instance, choosing high CPU-to-memory ratio nodes for workloads that use memory extensively can starve for memory easily and trigger auto node scale-up, wasting more CPUs that we don\u0019t need.</p>\n<p>Calculating the right ratio of CPU-to-memory isn\u0019t easy; you will need to monitor and know your workloads well.</p>\n<p>For example, GCP offers general purpose, compute-optimized, memory-optimized with various CPU and memory count and ratios:</p>\n<div><img src=\"https://www.magalix.com/hubfs/Google%20Drive%20Integration/Kubernetes%20Cost%20Optimization%20101-Sep-22-2020-05-16-57-23-AM.png\" alt=\"Kubernetes Cost Optimization 101\"></div> <p>Just keep in mind that 1 vCPU is way more expensive than 1GB memory. I have enough memory in the clusters I manage so I try to make sure that when there is a pending pod, this pod is pending on CPUs (which is expensive) so the autoscaler triggers a scale-up for the new node.</p>\n<p>To see the cost difference between CPU and memory, let us look at the GCP N2 machine price. GCP gives you the freedom to choose a custom machine type:</p>\n<pre><code class=\"language-yaml\"> (# vCPU x 1vCPU price) + (# GB memory x 1GB memory price)\n</code></pre>\n<p>It\u0019s clear here that the 1vCPU costs 7.44 times more than the cost of 1GB.</p>\n<div><img src=\"https://www.magalix.com/hubfs/Google%20Drive%20Integration/Kubernetes%20Cost%20Optimization%20101-2.png\" alt=\"Kubernetes Cost Optimization 101\"></div>\n<p>You can run multiple worker nodes with different node types and sizes, and control which workloads to run on which node pool using Kubernetes taints and toleration.</p> <h4><span>Already working in production with Kubernetes? Want to know more about kubernetes application patterns?</span></h4>\n<p><span>=G=G</span></p>\n<p><span><span class=\"hs-cta-wrapper\" id=\"hs-cta-wrapper-60591d93-9dc8-40ae-8685-02f6b03c25b8\"><span class=\"hs-cta-node hs-cta-60591d93-9dc8-40ae-8685-02f6b03c25b8\" id=\"hs-cta-60591d93-9dc8-40ae-8685-02f6b03c25b8\"><a href=\"https://cta-redirect.hubspot.com/cta/redirect/3487587/60591d93-9dc8-40ae-8685-02f6b03c25b8\"><img class=\"hs-cta-img\" id=\"hs-cta-img-60591d93-9dc8-40ae-8685-02f6b03c25b8\" src=\"https://no-cache.hubspot.com/cta/default/3487587/60591d93-9dc8-40ae-8685-02f6b03c25b8.png\" alt=\"Download Kubernetes Application Patterns E-Book\"></a></span></span></span></p> <h2>3. Autoscaling Workloads</h2>\n<p>Autoscaling is great because it helps to scale up/down your workloads and shut down nodes to save you money while you sleep.</p>\n<p>Many cases can benefit from autoscaling:</p>\n<ul>\n<li>Variable load web applications: A good example would be a web application which receives variable traffic through the day: traffic increases during certain hours of the day and decreases during the night.<p>Kubernetes comes with the Horizontal Pod Autoscaler (HPA) that can scale workload replicas based on their CPU and memory utilization ratio to the resource request. Kubernetes will keep monitoring the target resource in <code>scaleTargetRef</code><span> and will scale up/down the replica count to keep </span><code>targetCPUUtilizationPercentage</code><span> around 75%. </span></p><br>In this example, the deployment <code>frontend</code><span> will be scaled up to 20 replicas during the high load, and 4 replicas during the low load.</span>\n<pre><code class=\"language-yaml line-numbers\">apiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n&#xA0; name: frontend\n&#xA0; namespace: magalix\nspec:\n&#xA0; maxReplicas: 20\n&#xA0; minReplicas: 4\n&#xA0; scaleTargetRef:\n&#xA0; &#xA0; apiVersion: apps/v1\n&#xA0; &#xA0; kind: Deployment\n&#xA0; &#xA0; Name: frontend\n&#xA0; targetCPUUtilizationPercentage: 75\n</code></pre>\n</li>\n<li>Event-driven workers: background workers that need to be started in multiple replicas when there are messages in a Kafka topic or a message queue. It can be scaled to zero when there are no messages.<p>Compared to HPA, there is a more advanced Kubernetes Event-driven Autoscaling (<a href=\"https://github.com/kedacore/keda\">KEDA</a><span>) that can integrate with Prometheus/PosqreSQL/Kafka/Redis and many more to scale based on more advanced metrics from multiple data sources.</span></p><p>In this example, we installed this KEDA ScaledObject custom resource definition to scale the worker deployment <code>eventer</code><span> replicas. When the Kafka consumer lag changes, it can scale to 0 when there are no messages to consume and can scale up 1 replica for every 10,000 lagged messages up to 8 when the lag is more than 80,000:</span>\n</p><pre><code class=\"language-yaml line-numbers\">apiVersion: keda.k8s.io/v1alpha1\nkind: ScaledObject\nmetadata:\n&#xA0; labels:\n&#xA0; &#xA0; deploymentName: eventer\n&#xA0; name: eventer\n&#xA0; namespace: magalix\nspec:\n&#xA0; cooldownPeriod: 10\n&#xA0; maxReplicaCount: 8\n&#xA0; minReplicaCount: 0\n&#xA0; pollingInterval: 15\n&#xA0; scaleTargetRef:\n&#xA0; &#xA0; deploymentName: eventer\n&#xA0; triggers:\n&#xA0; - metadata:\n&#xA0; &#xA0; &#xA0; metricName: eventer\n&#xA0; &#xA0; &#xA0; query: sum(kafka_consumergroup_lag{consumergroup=&quot;eventer-group&quot;})\n&#xA0; &#xA0; &#xA0; serverAddress: http://prometheus-server.monitoring.svc.cluster.local\n&#xA0; &#xA0; &#xA0; threshold: &quot;10000&quot;\n&#xA0; &#xA0; type: prometheus\n</code></pre>\n</li>\n</ul>\n<h2>4. Autoscaling Worker Nodes</h2>\n<p>After scaling workloads, you will notice the number of running pods is low, but this won\u0019t save you money unless we configure auto-scaling the worker nodes</p>\n<p>Some Could providers provide node autoscaling out of the box on some node pools. <a href=\"https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler\">Cluster Autoscaler</a> can help you manage worker node autoscaling.</p>\n<p>GCP: Kubernetes Engine &#xFFFD; Cluster &#xFFFD; Node Pool</p>\n<div><img src=\"https://www.magalix.com/hubfs/Google%20Drive%20Integration/Kubernetes%20Cost%20Optimization%20101-1.png\" alt=\"Kubernetes Cost Optimization 101\"></div>\n<p>AWS: EKS &#xFFFD; Cluster &#xFFFD; Node Group</p>\n<div><img src=\"https://www.magalix.com/hubfs/Google%20Drive%20Integration/Kubernetes%20Cost%20Optimization%20101-Sep-22-2020-05-16-57-06-AM.png\" alt=\"Kubernetes Cost Optimization 101\"></div>\n<p>Azure: Kubernetes services &#xFFFD; Node pools &#xFFFD; Scale &#xFFFD; Automatic</p>\n<div><img src=\"https://www.magalix.com/hubfs/Google%20Drive%20Integration/Kubernetes%20Cost%20Optimization%20101-4.png\" alt=\"Kubernetes Cost Optimization 101\"></div>\n<p>The result of scaling workload and worker nodes together can end with the node count trends:</p>\n<div><img src=\"https://www.magalix.com/hubfs/Google%20Drive%20Integration/Kubernetes%20Cost%20Optimization%20101.png\" alt=\"Kubernetes Cost Optimization 101\"></div>\n<h2>5. Purchasing Commitment/Saving Plans</h2>\n<p>Running a Kubernetes service is relatively cheap. What\u0019s most expensive is the worker nodes compute cost.</p>\n<ul>\n<li><strong>GCP</strong> offers \u001cCommitment Plans\u001d on a certain number of vCPUs, memory, GPUs, and local SSDs for 1 or 3 years. This can save up to 57% of the compute cost</li>\n<li><strong>AWS</strong> offers \u001cCompute Saving Plans\u001d to commit to using a certain amount of money on compute every month, as well as \u001cReserved Instances\u001d to commit to using a certain type of machine, both for 1 or 3 years with a possible savings of up to 60%</li>\n<li><strong>Azure </strong>offers \u001cAzure Reserved VM Instances\u001d such as AWS with a possible savings of up to 60%</li>\n</ul>\n<h2>Conclusion</h2>\n<p>As we read in this article, there are multiple factors and considerations when trying to reduce your cluster cost. Going through the whole process can give you huge savings. We have managed to reduce our cluster daily cost by 56%- and you can do the same!</p>\n<div><img src=\"https://www.magalix.com/hubfs/Google%20Drive%20Integration/Kubernetes%20Cost%20Optimization%20101-3.png\" alt=\"Kubernetes Cost Optimization 101\"></div> <div> <p><span>=G=G</span></p>\n<p><span><span class=\"hs-cta-wrapper\" id=\"hs-cta-wrapper-2c65165a-51ca-4979-aa1b-2526964ce343\"><span class=\"hs-cta-node hs-cta-2c65165a-51ca-4979-aa1b-2526964ce343\" id=\"hs-cta-2c65165a-51ca-4979-aa1b-2526964ce343\"><a href=\"https://cta-redirect.hubspot.com/cta/redirect/3487587/2c65165a-51ca-4979-aa1b-2526964ce343\"><img class=\"hs-cta-img\" id=\"hs-cta-img-2c65165a-51ca-4979-aa1b-2526964ce343\" src=\"https://no-cache.hubspot.com/cta/default/3487587/2c65165a-51ca-4979-aa1b-2526964ce343.png\" alt=\"Learn More\"></a></span></span></span></p> </div></div>",
      "contentAsText": "Over the past two years at Magalix, we have focused on building our system, introducing new features, and scaling our infrastructure and microservices. During this time, we had a look at our Kubernetes clusters utilization and found it to be very low. We were paying for resources we didn\u0019t use, so we started a cost-saving practice to increase cluster utilization, use the resources we already had and pay less to run our cluster. In this article, I will discuss the top five techniques we used to better utilize our Kubernetes clusters on the cloud and eliminate wasted resources, thus saving money. In the end, we were able to cut our monthly bill by more than 50%!\n1. Applying Workload Right-Sizing\nKubernetes manages and schedules pods are based on container resource specs:\n\nResource Requests: Kubernetes scheduler is used to place containers on the right node which has enough capacity\nResource Limits: Containers are NOT allowed to use more than their resource limit\n\nResources requests and limits are container-scooped specs, while multi-container pods define separate resource specs for each container:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: magalix\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 1\n            memory: 1Gi\n\nKubernetes schedules pods based on resource requests and other restrictions without impairing availability. The scheduler uses CPU and memory resource requests to schedule the workloads in the right nodes, control which pod works on which node and if multiple pods can schedule together on a single node.\nEvery node type has its own allocatable CPU and memory capacities. Assigning high/unneeded CPU or memory resource requests can end up running underutilized pods on each node, which leads to underutilized nodes.\nIn this section, we compared resource requests, limited against actual usage and changed the resource request to something closer to the actual utilization while adding a little safety margin.\n2. Choosing the Right Worker Nodes\nEvery Kubernetes cluster has its own special workload utilization. Some clusters use memory more than CPU (e.g: database and caching workloads), while others use CPU more than memory (e.g: user-interactive and batch-processing workloads)\nCloud providers such as GCP and AWS offer various node types that you can choose from.\nChoosing the wrong node size for your cluster can end up costing you. For instance, choosing high CPU-to-memory ratio nodes for workloads that use memory extensively can starve for memory easily and trigger auto node scale-up, wasting more CPUs that we don\u0019t need.\nCalculating the right ratio of CPU-to-memory isn\u0019t easy; you will need to monitor and know your workloads well.\nFor example, GCP offers general purpose, compute-optimized, memory-optimized with various CPU and memory count and ratios:\n Just keep in mind that 1 vCPU is way more expensive than 1GB memory. I have enough memory in the clusters I manage so I try to make sure that when there is a pending pod, this pod is pending on CPUs (which is expensive) so the autoscaler triggers a scale-up for the new node.\nTo see the cost difference between CPU and memory, let us look at the GCP N2 machine price. GCP gives you the freedom to choose a custom machine type:\n (# vCPU x 1vCPU price) + (# GB memory x 1GB memory price)\n\nIt\u0019s clear here that the 1vCPU costs 7.44 times more than the cost of 1GB.\n\nYou can run multiple worker nodes with different node types and sizes, and control which workloads to run on which node pool using Kubernetes taints and toleration. Already working in production with Kubernetes? Want to know more about kubernetes application patterns?\n=G=G\n 3. Autoscaling Workloads\nAutoscaling is great because it helps to scale up/down your workloads and shut down nodes to save you money while you sleep.\nMany cases can benefit from autoscaling:\n\nVariable load web applications: A good example would be a web application which receives variable traffic through the day: traffic increases during certain hours of the day and decreases during the night.Kubernetes comes with the Horizontal Pod Autoscaler (HPA) that can scale workload replicas based on their CPU and memory utilization ratio to the resource request. Kubernetes will keep monitoring the target resource in scaleTargetRef and will scale up/down the replica count to keep targetCPUUtilizationPercentage around 75%. In this example, the deployment frontend will be scaled up to 20 replicas during the high load, and 4 replicas during the low load.\napiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: frontend\n  namespace: magalix\nspec:\n  maxReplicas: 20\n  minReplicas: 4\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    Name: frontend\n  targetCPUUtilizationPercentage: 75\n\n\nEvent-driven workers: background workers that need to be started in multiple replicas when there are messages in a Kafka topic or a message queue. It can be scaled to zero when there are no messages.Compared to HPA, there is a more advanced Kubernetes Event-driven Autoscaling (KEDA) that can integrate with Prometheus/PosqreSQL/Kafka/Redis and many more to scale based on more advanced metrics from multiple data sources.In this example, we installed this KEDA ScaledObject custom resource definition to scale the worker deployment eventer replicas. When the Kafka consumer lag changes, it can scale to 0 when there are no messages to consume and can scale up 1 replica for every 10,000 lagged messages up to 8 when the lag is more than 80,000:\napiVersion: keda.k8s.io/v1alpha1\nkind: ScaledObject\nmetadata:\n  labels:\n    deploymentName: eventer\n  name: eventer\n  namespace: magalix\nspec:\n  cooldownPeriod: 10\n  maxReplicaCount: 8\n  minReplicaCount: 0\n  pollingInterval: 15\n  scaleTargetRef:\n    deploymentName: eventer\n  triggers:\n  - metadata:\n      metricName: eventer\n      query: sum(kafka_consumergroup_lag{consumergroup=\"eventer-group\"})\n      serverAddress: http://prometheus-server.monitoring.svc.cluster.local\n      threshold: \"10000\"\n    type: prometheus\n\n\n\n4. Autoscaling Worker Nodes\nAfter scaling workloads, you will notice the number of running pods is low, but this won\u0019t save you money unless we configure auto-scaling the worker nodes\nSome Could providers provide node autoscaling out of the box on some node pools. Cluster Autoscaler can help you manage worker node autoscaling.\nGCP: Kubernetes Engine � Cluster � Node Pool\n\nAWS: EKS � Cluster � Node Group\n\nAzure: Kubernetes services � Node pools � Scale � Automatic\n\nThe result of scaling workload and worker nodes together can end with the node count trends:\n\n5. Purchasing Commitment/Saving Plans\nRunning a Kubernetes service is relatively cheap. What\u0019s most expensive is the worker nodes compute cost.\n\nGCP offers \u001cCommitment Plans\u001d on a certain number of vCPUs, memory, GPUs, and local SSDs for 1 or 3 years. This can save up to 57% of the compute cost\nAWS offers \u001cCompute Saving Plans\u001d to commit to using a certain amount of money on compute every month, as well as \u001cReserved Instances\u001d to commit to using a certain type of machine, both for 1 or 3 years with a possible savings of up to 60%\nAzure offers \u001cAzure Reserved VM Instances\u001d such as AWS with a possible savings of up to 60%\n\nConclusion\nAs we read in this article, there are multiple factors and considerations when trying to reduce your cluster cost. Going through the whole process can give you huge savings. We have managed to reduce our cluster daily cost by 56%- and you can do the same!\n  =G=G\n ",
      "description": "there are multiple factors and considerations when trying to reduce your cluster cost. Going through the whole process can give you huge savings.",
      "ogDescription": "there are multiple factors and considerations when trying to reduce your cluster cost. Going through the whole process can give you huge savings."
    },
    {
      "url": "https://itnext.io/copy-files-to-from-pods-in-java-using-fabric8-kubernetes-client-485e73335cb6",
      "title": "Copy Files to/from Pods in Java using Fabric8 Kubernetes Client",
      "content": "<div><article><section class=\"cx cy cz da aj db dc s\"></section><div><section class=\"dh di dj dk dl\"><div class=\"n p\"><div class=\"ab ac ae af ag dm ai aj\"><figure class=\"gp gq gr gs gt gu cz da paragraph-image\"><img alt=\"Image for post\" class=\"t u v gx aj\" src=\"https://miro.medium.com/max/512/1*-58gIpXpeckgKtH8jJPUfA.png\" width=\"256\"><figcaption class=\"hi hj db cz da hk hl cg b ew ci fz\"><a href=\"https://github.com/fabric8io/kubernetes-client\" class=\"cm hm\">Fabric8 Kubernetes Client</a></figcaption></figure><p id=\"1059\" class=\"hn ho dp hp b hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ik dh em\">There are times when we want to copy some files to our application pod running inside Kubernetes or download some file from within an application pod to our local system. You can see various articles on internet about <code class=\"hc il im in io b\">kubectl cp</code>, but this article is focused on doing all that programmatically in Java using <a href=\"https://github.com/fabric8io/kubernetes-client\" class=\"cm hm\">Fabric8 Kubernetes Client</a></p><h2 id=\"be37\" class=\"ip\"><strong class=\"az\">Getting the Client:</strong></h2><p id=\"4158\" class=\"hn ho dp hp b hq jl hs ht hu jm hw hx hy jn ia ib ic jo ie if ig jp ii ij ik dh em\">You can find the client on <a href=\"https://search.maven.org/search?q=g:io.fabric8%20a:kubernetes-client\" class=\"cm hm\">maven central</a> as usual. You can add it as a dependency in your <a href=\"https://maven.apache.org/\" class=\"cm hm\">maven</a> project:</p><figure class=\"gp gq gr gs gt gu\"><figcaption class=\"hi hj db cz da hk hl cg b ew ci fz\">Fabric8 Kubernetes Client dependency in pom.xml</figcaption></figure><p id=\"83d5\" class=\"hn ho dp hp b hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ik dh em\">If you\u0019re using <a href=\"https://gradle.org/\" class=\"cm hm\">gradle</a>, you might want to use this:</p><figure class=\"gp gq gr gs gt gu\"><figcaption class=\"hi hj db cz da hk hl cg b ew ci fz\">Fabric8 Kubernetes Client dependency via gradle groovy DSL</figcaption></figure><p id=\"83a7\" class=\"hn ho dp hp b hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ik dh em\">Once added as a dependency, you can start using the client. File copy operations involve compressing/decompressing files/directories. Fabric8 Kubernetes Client includes commons-codec and commons-compress as optional dependencies. So you would need to add them to your project in order to make use of these functionalities:</p><figure class=\"gp gq gr gs gt gu\"><figcaption class=\"hi hj db cz da hk hl cg b ew ci fz\">Adding commons-coded and commons-compress optional dependencies to classpath</figcaption></figure><h2 id=\"7360\" class=\"ip\"><strong class=\"az\">Copying file from Pod to your local (Download):</strong></h2><p id=\"d451\" class=\"hn ho dp hp b hq jl hs ht hu jm hw hx hy jn ia ib ic jo ie if ig jp ii ij ik dh em\">.Let\u0019s assume that I have a pod running in Kubernetes like this:</p><figure class=\"gp gq gr gs gt gu\"><figcaption class=\"hi hj db cz da hk hl cg b ew ci fz\">A simple <a href=\"https://quarkus.io/\" class=\"cm hm\">Quarkus</a> application pod running</figcaption></figure><p id=\"1af4\" class=\"hn ho dp hp b hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ik dh em\">I want to copy this file <code class=\"hc il im in io b\">/deployments/quarkus-1.0.0-runner.jar</code> to my local machine programmatically using Kubernetes Client. Here is how I would do it:</p><figure class=\"gp gq gr gs gt gu\"><figcaption class=\"hi hj db cz da hk hl cg b ew ci fz\">Downloading a file from Pod to your machine. See code <a href=\"https://github.com/rohanKanojia/kubernetes-client-demo/blob/master/src/main/java/io/fabric8/DownloadFileFromPod.java\" class=\"cm hm\">DownloadFileFromPod.java</a></figcaption></figure><h2 id=\"7484\" class=\"ip\"><strong class=\"az\">Copying a file from your local to Pod (Upload ):</strong></h2><p id=\"e0ed\" class=\"hn ho dp hp b hq jl hs ht hu jm hw hx hy jn ia ib ic jo ie if ig jp ii ij ik dh em\">Let\u0019s say you have a file locally in your system in path <code class=\"hc il im in io b\">/home/rohaan/work/k8s-resource-yamls/jobExample.yml</code> and you want to copy this file to your pod at path <code class=\"hc il im in io b\">/tmp/jobExample.yml</code> . Here is how you would do it:</p><figure class=\"gp gq gr gs gt gu\"><figcaption class=\"hi hj db cz da hk hl cg b ew ci fz\">Uploading a file to Pod. See <a href=\"https://github.com/rohanKanojia/kubernetes-client-demo/blob/master/src/main/java/io/fabric8/UploadFileToPod.java\" class=\"cm hm\">UploadFileToPod.java</a></figcaption></figure><h2 id=\"3c98\" class=\"ip\">Reading a file from Pod via InputStream(Download):</h2><p id=\"472d\" class=\"hn ho dp hp b hq jl hs ht hu jm hw hx hy jn ia ib ic jo ie if ig jp ii ij ik dh em\">Let\u0019s try to read the same file we copied in our previous example of uploading file to pod but using java streams. Here is how you can do it:</p><figure class=\"gp gq gr gs gt gu\"><figcaption class=\"hi hj db cz da hk hl cg b ew ci fz\">Reading a file from Pod using InputStream. See <a href=\"https://github.com/rohanKanojia/kubernetes-client-demo/blob/master/src/main/java/io/fabric8/ReadFileAsStream.java\" class=\"cm hm\">ReadFileAsStream.java</a></figcaption></figure><h2 id=\"f2bf\" class=\"ip\"><strong class=\"az\">Copying a directory from your local to Pod (Upload):</strong></h2><p id=\"4361\" class=\"hn ho dp hp b hq jl hs ht hu jm hw hx hy jn ia ib ic jo ie if ig jp ii ij ik dh em\">I have a small directory that contains these two text files:</p><figure class=\"gp gq gr gs gt gu\"></figure><p id=\"4ce7\" class=\"hn ho dp hp b hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ik dh em\">I want to copy this folder <code class=\"hc il im in io b\">test-dir</code>into my Pod. Here is how I would do it:</p><figure class=\"gp gq gr gs gt gu\"><figcaption class=\"hi hj db cz da hk hl cg b ew ci fz\">Uploading a directory to Kubernetes Pod. See <a href=\"https://github.com/rohanKanojia/kubernetes-client-demo/blob/master/src/main/java/io/fabric8/UploadDirectoryToPod.java\" class=\"cm hm\">UploadDirectoryToPod.java</a></figcaption></figure><h2 id=\"f765\" class=\"ip\">Specify container while copying to/from Multi Container Pods:</h2><p id=\"d37f\" class=\"hn ho dp hp b hq jl hs ht hu jm hw hx hy jn ia ib ic jo ie if ig jp ii ij ik dh em\">You can also specify which container to choose using <code class=\"hc il im in io b\">inContainer(String containerName)</code> DSL method which filters the container for you. It is applicable to both uploading and downloading files to/from pods. Here is an example of choosing container while downloading a file from Pod:</p><figure class=\"gp gq gr gs gt gu\"><figcaption class=\"hi hj db cz da hk hl cg b ew ci fz\">Copying a file from Pod with multiple containers. See <a href=\"https://github.com/rohanKanojia/kubernetes-client-demo/blob/master/src/main/java/io/fabric8/DownloadFileFromMultiContainerPod.java\" class=\"cm hm\">DownloadFileFromMultiContainerPod.java</a></figcaption></figure><p id=\"db66\" class=\"hn ho dp hp b hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ik dh em\">This concludes my blog today. I hope it was useful. Thanks a lot for taking time to read. You can find above code shared in gists in my Github repository:</p><h2 id=\"1c0f\" class=\"ip\">Join Us:</h2><p id=\"2fbc\" class=\"hn ho dp hp b hq jl hs ht hu jm hw hx hy jn ia ib ic jo ie if ig jp ii ij ik dh em\">Do you like Fabric8 Kubernetes Client and want to get involved in development of project. Feel free to get involved by methods:</p><p id=\"831e\" class=\"hn ho dp hp b hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ik dh em\">Orignally published on <a href=\"https://r0haan.wordpress.com/2020/09/25/copy-files-to-from-pods-in-java-using-fabric8-kubernetes-client/\" class=\"cm hm\">Wordpress</a></p></div></div></section></div></article></div>",
      "contentAsText": "Fabric8 Kubernetes ClientThere are times when we want to copy some files to our application pod running inside Kubernetes or download some file from within an application pod to our local system. You can see various articles on internet about kubectl cp, but this article is focused on doing all that programmatically in Java using Fabric8 Kubernetes ClientGetting the Client:You can find the client on maven central as usual. You can add it as a dependency in your maven project:Fabric8 Kubernetes Client dependency in pom.xmlIf you\u0019re using gradle, you might want to use this:Fabric8 Kubernetes Client dependency via gradle groovy DSLOnce added as a dependency, you can start using the client. File copy operations involve compressing/decompressing files/directories. Fabric8 Kubernetes Client includes commons-codec and commons-compress as optional dependencies. So you would need to add them to your project in order to make use of these functionalities:Adding commons-coded and commons-compress optional dependencies to classpathCopying file from Pod to your local (Download):.Let\u0019s assume that I have a pod running in Kubernetes like this:A simple Quarkus application pod runningI want to copy this file /deployments/quarkus-1.0.0-runner.jar to my local machine programmatically using Kubernetes Client. Here is how I would do it:Downloading a file from Pod to your machine. See code DownloadFileFromPod.javaCopying a file from your local to Pod (Upload ):Let\u0019s say you have a file locally in your system in path /home/rohaan/work/k8s-resource-yamls/jobExample.yml and you want to copy this file to your pod at path /tmp/jobExample.yml . Here is how you would do it:Uploading a file to Pod. See UploadFileToPod.javaReading a file from Pod via InputStream(Download):Let\u0019s try to read the same file we copied in our previous example of uploading file to pod but using java streams. Here is how you can do it:Reading a file from Pod using InputStream. See ReadFileAsStream.javaCopying a directory from your local to Pod (Upload):I have a small directory that contains these two text files:I want to copy this folder test-dirinto my Pod. Here is how I would do it:Uploading a directory to Kubernetes Pod. See UploadDirectoryToPod.javaSpecify container while copying to/from Multi Container Pods:You can also specify which container to choose using inContainer(String containerName) DSL method which filters the container for you. It is applicable to both uploading and downloading files to/from pods. Here is an example of choosing container while downloading a file from Pod:Copying a file from Pod with multiple containers. See DownloadFileFromMultiContainerPod.javaThis concludes my blog today. I hope it was useful. Thanks a lot for taking time to read. You can find above code shared in gists in my Github repository:Join Us:Do you like Fabric8 Kubernetes Client and want to get involved in development of project. Feel free to get involved by methods:Orignally published on Wordpress",
      "publishedDate": "2020-09-25T13:47:45.216Z",
      "description": "There are times when we want to copy some files to our application pod running inside Kubernetes or download some file from within an application pod to our local system. You can see various articles…",
      "ogDescription": "There are times when we want to copy some files to our application pod running inside Kubernetes or download some file from within an…"
    },
    {
      "url": "https://cloudowski.com/articles/how-to-modify-containers-wihtout-rebuilding/",
      "title": "How to modify containers without rebuilding their image",
      "content": "<article class=\"page\"> <meta> <meta> <meta> <p class=\"page__inner-wrap\"> <header> <p class=\"page__meta\"><i class=\"far fa-clock\"></i> 6 minute read\n</p> </header> <section class=\"page__content\"> <p>Containers are a beautiful piece of technology that ease the development of modern applications and also the maintenance of modern environments. One thing that draws many people to them is how they reduce the time required to set up a service, or a whole environment, with everything included. It is possible mainly because there are so many container images available and ready to use. You will probably need to build your own container images with your applications, but many containers in your environment will use prebuilt images prepared by someone else. It\u0019s especially worth considering for software that is provided by the software vendor or a trusted group of developers like it has been done in the case of \u001cofficial\u001d images published on Docker Hub. In both cases, it makes your life easier by letting someone else take care of updates, packaging new versions, and making sure it works.\nBut what if you want to change something in those images? Maybe it\u0019s a minor change or something bigger that is specific for your particular usage of the service. The first instinct may tell you to rebuild that image. This, however, brings some overhead - these images will have to be published, rebuilt when new upstream versions are published, and you lose most of the benefits that come with those prebuilt versions.</p><p>\nThere is an alternative to that - actually, I found four of them which I will describe below. These solutions will allow you to keep all the benefits and adjust the behavior of running containers in a seamless way.</p></section></p> <h2 id=\"method-1---init-containers\">Method 1 - init-containers</h2> <p><a href=\"https://kubernetes.io/docs/concepts/workloads/pods/init-containers/\">Init-containers</a> were created to provide additional functionality to the main container (or containers) defined in a Pod. They are executed before the main container and can use a different container image. In case of any failure, they will prevent the main container from starting. All logs can be easily retrieved and troubleshooting is fairly simple - they are fetched just like any other container defined in a Pod by providing its name. This methods is quiote popular among services such as databases to initialize and configure them based on configuration parameters.</p> <h4 id=\"example\">Example</h4> <p>The following example uses a dedicated empty volume for storing data initialized by an init-container. In this specific case, it\u0019s just a simple \u001cecho\u001d command, but in a real-world scenario, this can be a script that does something more complex.</p> <div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"na\">apiVersion</span><span class=\"pi\">:</span> <span class=\"s\">apps/v1</span>\n<span class=\"na\">kind</span><span class=\"pi\">:</span> <span class=\"s\">Deployment</span>\n<span class=\"na\">metadata</span><span class=\"pi\">:</span> <span class=\"na\">labels</span><span class=\"pi\">:</span> <span class=\"na\">app</span><span class=\"pi\">:</span> <span class=\"s\">nginx</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">nginx-init</span>\n<span class=\"na\">spec</span><span class=\"pi\">:</span> <span class=\"na\">selector</span><span class=\"pi\">:</span> <span class=\"na\">matchLabels</span><span class=\"pi\">:</span> <span class=\"na\">app</span><span class=\"pi\">:</span> <span class=\"s\">nginx</span> <span class=\"na\">template</span><span class=\"pi\">:</span> <span class=\"na\">metadata</span><span class=\"pi\">:</span> <span class=\"na\">labels</span><span class=\"pi\">:</span> <span class=\"na\">app</span><span class=\"pi\">:</span> <span class=\"s\">nginx</span> <span class=\"na\">spec</span><span class=\"pi\">:</span> <span class=\"na\">initContainers</span><span class=\"pi\">:</span> <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">prepare-webpage</span> <span class=\"na\">image</span><span class=\"pi\">:</span> <span class=\"s\">busybox:1.28</span> <span class=\"na\">command</span><span class=\"pi\">:</span> <span class=\"pi\">[</span><span class=\"s2\">&quot;</span><span class=\"s\">sh&quot;</span><span class=\"pi\">,</span> <span class=\"s2\">&quot;</span><span class=\"s\">-c&quot;</span><span class=\"pi\">]</span> <span class=\"na\">args</span><span class=\"pi\">:</span> <span class=\"pi\">[</span> <span class=\"s2\">&quot;</span><span class=\"s\">set</span><span class=\"nv\"> </span><span class=\"s\">-x;</span> <span class=\"s\">echo</span><span class=\"nv\"> </span><span class=\"s\">&apos;&lt;h2&gt;Page</span><span class=\"nv\"> </span><span class=\"s\">prepared</span><span class=\"nv\"> </span><span class=\"s\">by</span><span class=\"nv\"> </span><span class=\"s\">an</span><span class=\"nv\"> </span><span class=\"s\">init</span><span class=\"nv\"> </span><span class=\"s\">container&lt;/h2&gt;&apos;</span><span class=\"nv\"> </span><span class=\"s\">&gt;</span><span class=\"nv\"> </span><span class=\"s\">/web/index.html;</span> <span class=\"s\">echo</span><span class=\"nv\"> </span><span class=\"s\">&apos;Init</span><span class=\"nv\"> </span><span class=\"s\">finished</span><span class=\"nv\"> </span><span class=\"s\">successfully&apos;</span> <span class=\"s\">&quot;</span><span class=\"pi\">,</span> <span class=\"pi\">]</span> <span class=\"na\">volumeMounts</span><span class=\"pi\">:</span> <span class=\"pi\">-</span> <span class=\"na\">mountPath</span><span class=\"pi\">:</span> <span class=\"s\">/web</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">web</span> <span class=\"na\">containers</span><span class=\"pi\">:</span> <span class=\"pi\">-</span> <span class=\"na\">image</span><span class=\"pi\">:</span> <span class=\"s\">nginx:1.19</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">nginx</span> <span class=\"na\">volumeMounts</span><span class=\"pi\">:</span> <span class=\"pi\">-</span> <span class=\"na\">mountPath</span><span class=\"pi\">:</span> <span class=\"s\">/usr/share/nginx/html/</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">web</span> <span class=\"na\">ports</span><span class=\"pi\">:</span> <span class=\"pi\">-</span> <span class=\"na\">containerPort</span><span class=\"pi\">:</span> <span class=\"s\">80</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">http</span> <span class=\"na\">volumes</span><span class=\"pi\">:</span> <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">web</span> <span class=\"na\">emptyDir</span><span class=\"pi\">:</span> <span class=\"pi\">{}</span>\n</code></pre></div></div> <h2 id=\"method-2---post-start-hook\">Method 2 - post-start hook</h2> <p>A Post-start <a href=\"https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/\">hook</a> can be used to execute some action just after the main container starts. It can be either a script executed in the same context as the container or an HTTP request that is executed against a defined endpoint. In most cases, it would probably be a shell script. Pod stays in the <em>ContainerCreating</em> state until this script ends. It can be tricky to debug since there are no logs available. There are more <a href=\"https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#hook-delivery-guarantees\">caveats</a> and this should be used only for simple, non-invasive actions. The best feature of this method is that the script is executed when the service in the main container starts and can be used to interact with the service (e.g. by executing some API requests). With a proper readinessProbe configuration, this can give a nice way of initializing the application before any requests are allowed.</p> <h4 id=\"example-1\">Example</h4> <p>In the following example a post-start hook executes the <code class=\"highlighter-rouge\">echo</code> command, but again - this can be anything that uses the same set of files available on the container filesystem in order to perform some sort of initialization.</p> <div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"na\">apiVersion</span><span class=\"pi\">:</span> <span class=\"s\">apps/v1</span>\n<span class=\"na\">kind</span><span class=\"pi\">:</span> <span class=\"s\">Deployment</span>\n<span class=\"na\">metadata</span><span class=\"pi\">:</span> <span class=\"na\">labels</span><span class=\"pi\">:</span> <span class=\"na\">app</span><span class=\"pi\">:</span> <span class=\"s\">nginx</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">nginx-hook</span>\n<span class=\"na\">spec</span><span class=\"pi\">:</span> <span class=\"na\">selector</span><span class=\"pi\">:</span> <span class=\"na\">matchLabels</span><span class=\"pi\">:</span> <span class=\"na\">app</span><span class=\"pi\">:</span> <span class=\"s\">nginx</span> <span class=\"na\">template</span><span class=\"pi\">:</span> <span class=\"na\">metadata</span><span class=\"pi\">:</span> <span class=\"na\">labels</span><span class=\"pi\">:</span> <span class=\"na\">app</span><span class=\"pi\">:</span> <span class=\"s\">nginx</span> <span class=\"na\">spec</span><span class=\"pi\">:</span> <span class=\"na\">containers</span><span class=\"pi\">:</span> <span class=\"pi\">-</span> <span class=\"na\">image</span><span class=\"pi\">:</span> <span class=\"s\">nginx:1.19</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">nginx</span> <span class=\"na\">ports</span><span class=\"pi\">:</span> <span class=\"pi\">-</span> <span class=\"na\">containerPort</span><span class=\"pi\">:</span> <span class=\"s\">80</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">http</span> <span class=\"na\">lifecycle</span><span class=\"pi\">:</span> <span class=\"na\">postStart</span><span class=\"pi\">:</span> <span class=\"na\">exec</span><span class=\"pi\">:</span> <span class=\"na\">command</span><span class=\"pi\">:</span> <span class=\"pi\">[</span> <span class=\"s2\">&quot;</span><span class=\"s\">sh&quot;</span><span class=\"pi\">,</span> <span class=\"s2\">&quot;</span><span class=\"s\">-c&quot;</span><span class=\"pi\">,</span> <span class=\"s2\">&quot;</span><span class=\"s\">sleep</span><span class=\"nv\"> </span><span class=\"s\">5;set</span><span class=\"nv\"> </span><span class=\"s\">-x;</span><span class=\"nv\"> </span><span class=\"s\">echo</span><span class=\"nv\"> </span><span class=\"s\">&apos;&lt;h2&gt;Page</span><span class=\"nv\"> </span><span class=\"s\">prepared</span><span class=\"nv\"> </span><span class=\"s\">by</span><span class=\"nv\"> </span><span class=\"s\">a</span><span class=\"nv\"> </span><span class=\"s\">PostStart</span><span class=\"nv\"> </span><span class=\"s\">hook&lt;/h2&gt;&apos;</span><span class=\"nv\"> </span><span class=\"s\">&gt;</span><span class=\"nv\"> </span><span class=\"s\">/usr/share/nginx/html/index.html&quot;</span><span class=\"pi\">,</span> <span class=\"pi\">]</span>\n</code></pre></div></div> <p>This method leverages the concept of the Pod where multiple containers run at the same time sharing IPC and network kernel namespaces. It\u0019s been widely used in the Kubernetes ecosystem by projects such as Istio, Consul Connect, and many others. The assumption here is that all containers run simultaneously which makes it a little bit tricky to use a sidecar container to modify the behaviour of the main container. But it\u0019s doable and it can be used to interact with the running application or a service. I\u0019ve been using this feature with the <a href=\"https://github.com/jenkinsci/helm-charts/tree/main/charts/jenkins\">Jenkins helm chart</a> where there\u0019s a sidecar container responsible for reading ConfigMap objects with Configuration-as-Code config entries.</p> <h4 id=\"example-2\">Example</h4> <p>Nothing new here, just the \u001cecho\u001d command with a little caveat - since sidecar containers must obey <code class=\"highlighter-rouge\">restartPolicy</code> setting, they must run after they finish their actions and thus it uses a simple <code class=\"highlighter-rouge\">while</code> infinite loop. In more advanced cases this would be rather some small daemon (or a loop that checks some state) that runs like a service.</p> <div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>\n<span class=\"na\">apiVersion</span><span class=\"pi\">:</span> <span class=\"s\">apps/v1</span>\n<span class=\"na\">kind</span><span class=\"pi\">:</span> <span class=\"s\">Deployment</span>\n<span class=\"na\">metadata</span><span class=\"pi\">:</span> <span class=\"na\">labels</span><span class=\"pi\">:</span> <span class=\"na\">app</span><span class=\"pi\">:</span> <span class=\"s\">nginx</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">nginx-sidecar</span>\n<span class=\"na\">spec</span><span class=\"pi\">:</span> <span class=\"na\">selector</span><span class=\"pi\">:</span> <span class=\"na\">matchLabels</span><span class=\"pi\">:</span> <span class=\"na\">app</span><span class=\"pi\">:</span> <span class=\"s\">nginx</span> <span class=\"na\">template</span><span class=\"pi\">:</span> <span class=\"na\">metadata</span><span class=\"pi\">:</span> <span class=\"na\">labels</span><span class=\"pi\">:</span> <span class=\"na\">app</span><span class=\"pi\">:</span> <span class=\"s\">nginx</span> <span class=\"na\">spec</span><span class=\"pi\">:</span> <span class=\"na\">containers</span><span class=\"pi\">:</span> <span class=\"pi\">-</span> <span class=\"na\">image</span><span class=\"pi\">:</span> <span class=\"s\">nginx:1.19</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">nginx</span> <span class=\"na\">volumeMounts</span><span class=\"pi\">:</span> <span class=\"pi\">-</span> <span class=\"na\">mountPath</span><span class=\"pi\">:</span> <span class=\"s\">/usr/share/nginx/html/</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">web</span> <span class=\"na\">ports</span><span class=\"pi\">:</span> <span class=\"pi\">-</span> <span class=\"na\">containerPort</span><span class=\"pi\">:</span> <span class=\"s\">80</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">http</span> <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">prepare-webpage</span> <span class=\"na\">image</span><span class=\"pi\">:</span> <span class=\"s\">busybox:1.28</span> <span class=\"na\">command</span><span class=\"pi\">:</span> <span class=\"pi\">[</span><span class=\"s2\">&quot;</span><span class=\"s\">sh&quot;</span><span class=\"pi\">,</span> <span class=\"s2\">&quot;</span><span class=\"s\">-c&quot;</span><span class=\"pi\">]</span> <span class=\"na\">args</span><span class=\"pi\">:</span> <span class=\"pi\">[</span> <span class=\"s2\">&quot;</span><span class=\"s\">set</span><span class=\"nv\"> </span><span class=\"s\">-x;</span> <span class=\"s\">echo</span><span class=\"nv\"> </span><span class=\"s\">&apos;&lt;h2&gt;Page</span><span class=\"nv\"> </span><span class=\"s\">prepared</span><span class=\"nv\"> </span><span class=\"s\">by</span><span class=\"nv\"> </span><span class=\"s\">a</span><span class=\"nv\"> </span><span class=\"s\">sidecar</span><span class=\"nv\"> </span><span class=\"s\">container&lt;/h2&gt;&apos;</span><span class=\"nv\"> </span><span class=\"s\">&gt;</span><span class=\"nv\"> </span><span class=\"s\">/web/index.html;</span> <span class=\"s\">while</span><span class=\"nv\"> </span><span class=\"s\">:;do</span><span class=\"nv\"> </span><span class=\"s\">sleep</span><span class=\"nv\"> </span><span class=\"s\">9999;done</span> <span class=\"s\">&quot;</span><span class=\"pi\">,</span> <span class=\"pi\">]</span> <span class=\"na\">volumeMounts</span><span class=\"pi\">:</span> <span class=\"pi\">-</span> <span class=\"na\">mountPath</span><span class=\"pi\">:</span> <span class=\"s\">/web</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">web</span> <span class=\"na\">volumes</span><span class=\"pi\">:</span> <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">web</span> <span class=\"na\">emptyDir</span><span class=\"pi\">:</span> <span class=\"pi\">{}</span>\n</code></pre></div></div> <h2 id=\"method-4---entrypoint\">Method 4 - entrypoint</h2> <p>The last method uses the same container image and is similar to the Post-start hook except it runs before the main app or service. As you probably know in every container image there is an <code class=\"highlighter-rouge\">ENTRYPOINT</code> command defined (explicitly or <a href=\"https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact\">implicitly</a>) and we can leverage it to execute some arbitrary scripts. It is often used by many official images and in this method we will just prepend our own script to modify the behavior of the main container. In more advanced scenarios you could actually provide a modified version of the original entrypoint file.</p> <h4 id=\"example-3\">Example</h4> <p>This method is a little bit more complex and involves creating a ConfigMap with a script content that is executed before the main entrypoint. Our script for modifying nginx entrypoint is embedded in the following ConfigMap</p> <div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"na\">apiVersion</span><span class=\"pi\">:</span> <span class=\"s\">v1</span>\n<span class=\"na\">kind</span><span class=\"pi\">:</span> <span class=\"s\">ConfigMap</span>\n<span class=\"na\">metadata</span><span class=\"pi\">:</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">scripts</span>\n<span class=\"na\">data</span><span class=\"pi\">:</span> <span class=\"s\">prestart-script.sh</span><span class=\"pi\">:</span> <span class=\"pi\">|-</span> <span class=\"no\">#!/usr/bin/env bash</span> <span class=\"no\">echo &apos;&lt;h2&gt;Page prepared by a script executed before entrypoint container&lt;/h2&gt;&apos; &gt; /usr/share/nginx/html/index.html</span> <span class=\"no\">exec /docker-entrypoint.sh nginx -g &quot;daemon off;&quot; # it&apos;s &quot;ENTRYPOINT CMD&quot; extracted from the main container image definition</span>\n</code></pre></div></div> <p>One thing that is very important is the last line with <code class=\"highlighter-rouge\">exec</code>. It executes the original entrypoint script and must match it exactly as it is defined in the Dockerfile. In this case it requires additional arguments that are defined in the <a href=\"https://github.com/nginxinc/docker-nginx/blob/1.19.2/stable/buster/Dockerfile#L110\">CMD</a>.</p> <p>Now let\u0019s define the Deployment object</p> <div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"na\">apiVersion</span><span class=\"pi\">:</span> <span class=\"s\">apps/v1</span>\n<span class=\"na\">kind</span><span class=\"pi\">:</span> <span class=\"s\">Deployment</span>\n<span class=\"na\">metadata</span><span class=\"pi\">:</span> <span class=\"na\">labels</span><span class=\"pi\">:</span> <span class=\"na\">app</span><span class=\"pi\">:</span> <span class=\"s\">nginx</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">nginx-script</span>\n<span class=\"na\">spec</span><span class=\"pi\">:</span> <span class=\"na\">selector</span><span class=\"pi\">:</span> <span class=\"na\">matchLabels</span><span class=\"pi\">:</span> <span class=\"na\">app</span><span class=\"pi\">:</span> <span class=\"s\">nginx</span> <span class=\"na\">template</span><span class=\"pi\">:</span> <span class=\"na\">metadata</span><span class=\"pi\">:</span> <span class=\"na\">labels</span><span class=\"pi\">:</span> <span class=\"na\">app</span><span class=\"pi\">:</span> <span class=\"s\">nginx</span> <span class=\"na\">spec</span><span class=\"pi\">:</span> <span class=\"na\">containers</span><span class=\"pi\">:</span> <span class=\"pi\">-</span> <span class=\"na\">image</span><span class=\"pi\">:</span> <span class=\"s\">nginx:1.19</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">nginx</span> <span class=\"na\">command</span><span class=\"pi\">:</span> <span class=\"pi\">[</span><span class=\"s2\">&quot;</span><span class=\"s\">bash&quot;</span><span class=\"pi\">,</span> <span class=\"s2\">&quot;</span><span class=\"s\">-c&quot;</span><span class=\"pi\">,</span> <span class=\"s2\">&quot;</span><span class=\"s\">/scripts/prestart-script.sh&quot;</span><span class=\"pi\">]</span> <span class=\"na\">ports</span><span class=\"pi\">:</span> <span class=\"pi\">-</span> <span class=\"na\">containerPort</span><span class=\"pi\">:</span> <span class=\"s\">80</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">http</span> <span class=\"na\">volumeMounts</span><span class=\"pi\">:</span> <span class=\"pi\">-</span> <span class=\"na\">mountPath</span><span class=\"pi\">:</span> <span class=\"s\">/scripts</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">scripts</span> <span class=\"na\">volumes</span><span class=\"pi\">:</span> <span class=\"pi\">-</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">scripts</span> <span class=\"na\">configMap</span><span class=\"pi\">:</span> <span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">scripts</span> <span class=\"na\">defaultMode</span><span class=\"pi\">:</span> <span class=\"s\">0755</span> <span class=\"c1\"># &lt;- this is important</span>\n</code></pre></div></div> <p>That is pretty straightforward - we override the entrypoint with <code class=\"highlighter-rouge\">command</code> and we also must make sure our script is mounted with proper permissions (thus <code class=\"highlighter-rouge\">defaultMode</code> needs to be defined).</p> <h2 id=\"comparison-table\">Comparison table</h2> <p>Here\u0019s the table that summarizes the differences between the aforementioned methods:</p> <table> <thead> <tr> <th>&#xFFFD;</th> <th>Init-containers</th> <th>Post-start hook</th> <th>Sidecar container</th> <th>Entrypoint</th> </tr> </thead> <tbody> <tr> <td>Can connect to the main process</td> <td>L</td> <td>\u0005</td> <td>\u0005</td> <td>L</td> </tr> <tr> <td>Can use a different image</td> <td>\u0005</td> <td>L</td> <td>\u0005</td> <td>\u0005</td> </tr> <tr> <td>Easy to debug</td> <td>\u0005</td> <td>L</td> <td>\u0005</td> <td>\u0005</td> </tr> <tr> <td>Stops the main container when fails</td> <td>\u0005</td> <td>\u0005</td> <td>L</td> <td>\u0005</td> </tr> </tbody>\n</table> <p>Containers are about reusability and often it\u0019s much easier to make small adjustments without rebuilding the whole container image and take over the responsibility of publishing and maintaining it. It\u0019s just an implementation of the <a href=\"https://en.wikipedia.org/wiki/KISS_principle\">KISS principle</a>.</p> <footer class=\"page__meta\"> <p class=\"page__taxonomy\"> <strong><i class=\"fas fa-fw fa-tags\"></i> Tags: </strong> <span> <a href=\"/tags/#containers\" class=\"page__taxonomy-item\">containers</a><span class=\"sep\">, </span> <a href=\"/tags/#docker\" class=\"page__taxonomy-item\">docker</a><span class=\"sep\">, </span> <a href=\"/tags/#kubernetes\" class=\"page__taxonomy-item\">kubernetes</a> </span> </p> <p class=\"page__taxonomy\"> <strong><i class=\"fas fa-fw fa-folder-open\"></i> Categories: </strong> <span> <a href=\"/categories/#articles\" class=\"page__taxonomy-item\">articles</a> </span> </p> <p class=\"page__date\"><strong><i class=\"fas fa-fw fa-calendar-alt\"></i> Updated:</strong> <time>September 26, 2020</time></p> </footer> <section class=\"page__share\"> <a href=\"https://twitter.com/intent/tweet?via=tomasz_cholewa&amp;text=How+to+modify+containers+without+rebuilding+their+image%20http%3A%2F%2Fcloudowski.com%2Farticles%2Fhow-to-modify-containers-wihtout-rebuilding%2F\" class=\"btn btn--twitter\"><i class=\"fab fa-fw fa-twitter\"></i><span> Twitter</span></a> <a href=\"https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Fcloudowski.com%2Farticles%2Fhow-to-modify-containers-wihtout-rebuilding%2F\" class=\"btn btn--facebook\"><i class=\"fab fa-fw fa-facebook\"></i><span> Facebook</span></a> <a href=\"https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3A%2F%2Fcloudowski.com%2Farticles%2Fhow-to-modify-containers-wihtout-rebuilding%2F\" class=\"btn btn--linkedin\"><i class=\"fab fa-fw fa-linkedin\"></i><span> LinkedIn</span></a>\n</section> <h4 class=\"page__comments-title\">Leave a comment</h4> </article>",
      "contentAsText": "       6 minute read\n   Containers are a beautiful piece of technology that ease the development of modern applications and also the maintenance of modern environments. One thing that draws many people to them is how they reduce the time required to set up a service, or a whole environment, with everything included. It is possible mainly because there are so many container images available and ready to use. You will probably need to build your own container images with your applications, but many containers in your environment will use prebuilt images prepared by someone else. It\u0019s especially worth considering for software that is provided by the software vendor or a trusted group of developers like it has been done in the case of \u001cofficial\u001d images published on Docker Hub. In both cases, it makes your life easier by letting someone else take care of updates, packaging new versions, and making sure it works.\nBut what if you want to change something in those images? Maybe it\u0019s a minor change or something bigger that is specific for your particular usage of the service. The first instinct may tell you to rebuild that image. This, however, brings some overhead - these images will have to be published, rebuilt when new upstream versions are published, and you lose most of the benefits that come with those prebuilt versions.\nThere is an alternative to that - actually, I found four of them which I will describe below. These solutions will allow you to keep all the benefits and adjust the behavior of running containers in a seamless way. Method 1 - init-containers Init-containers were created to provide additional functionality to the main container (or containers) defined in a Pod. They are executed before the main container and can use a different container image. In case of any failure, they will prevent the main container from starting. All logs can be easily retrieved and troubleshooting is fairly simple - they are fetched just like any other container defined in a Pod by providing its name. This methods is quiote popular among services such as databases to initialize and configure them based on configuration parameters. Example The following example uses a dedicated empty volume for storing data initialized by an init-container. In this specific case, it\u0019s just a simple \u001cecho\u001d command, but in a real-world scenario, this can be a script that does something more complex. apiVersion: apps/v1\nkind: Deployment\nmetadata: labels: app: nginx name: nginx-init\nspec: selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: initContainers: - name: prepare-webpage image: busybox:1.28 command: [\"sh\", \"-c\"] args: [ \"set -x; echo '<h2>Page prepared by an init container</h2>' > /web/index.html; echo 'Init finished successfully' \", ] volumeMounts: - mountPath: /web name: web containers: - image: nginx:1.19 name: nginx volumeMounts: - mountPath: /usr/share/nginx/html/ name: web ports: - containerPort: 80 name: http volumes: - name: web emptyDir: {}\n Method 2 - post-start hook A Post-start hook can be used to execute some action just after the main container starts. It can be either a script executed in the same context as the container or an HTTP request that is executed against a defined endpoint. In most cases, it would probably be a shell script. Pod stays in the ContainerCreating state until this script ends. It can be tricky to debug since there are no logs available. There are more caveats and this should be used only for simple, non-invasive actions. The best feature of this method is that the script is executed when the service in the main container starts and can be used to interact with the service (e.g. by executing some API requests). With a proper readinessProbe configuration, this can give a nice way of initializing the application before any requests are allowed. Example In the following example a post-start hook executes the echo command, but again - this can be anything that uses the same set of files available on the container filesystem in order to perform some sort of initialization. apiVersion: apps/v1\nkind: Deployment\nmetadata: labels: app: nginx name: nginx-hook\nspec: selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - image: nginx:1.19 name: nginx ports: - containerPort: 80 name: http lifecycle: postStart: exec: command: [ \"sh\", \"-c\", \"sleep 5;set -x; echo '<h2>Page prepared by a PostStart hook</h2>' > /usr/share/nginx/html/index.html\", ]\n This method leverages the concept of the Pod where multiple containers run at the same time sharing IPC and network kernel namespaces. It\u0019s been widely used in the Kubernetes ecosystem by projects such as Istio, Consul Connect, and many others. The assumption here is that all containers run simultaneously which makes it a little bit tricky to use a sidecar container to modify the behaviour of the main container. But it\u0019s doable and it can be used to interact with the running application or a service. I\u0019ve been using this feature with the Jenkins helm chart where there\u0019s a sidecar container responsible for reading ConfigMap objects with Configuration-as-Code config entries. Example Nothing new here, just the \u001cecho\u001d command with a little caveat - since sidecar containers must obey restartPolicy setting, they must run after they finish their actions and thus it uses a simple while infinite loop. In more advanced cases this would be rather some small daemon (or a loop that checks some state) that runs like a service. \napiVersion: apps/v1\nkind: Deployment\nmetadata: labels: app: nginx name: nginx-sidecar\nspec: selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - image: nginx:1.19 name: nginx volumeMounts: - mountPath: /usr/share/nginx/html/ name: web ports: - containerPort: 80 name: http - name: prepare-webpage image: busybox:1.28 command: [\"sh\", \"-c\"] args: [ \"set -x; echo '<h2>Page prepared by a sidecar container</h2>' > /web/index.html; while :;do sleep 9999;done \", ] volumeMounts: - mountPath: /web name: web volumes: - name: web emptyDir: {}\n Method 4 - entrypoint The last method uses the same container image and is similar to the Post-start hook except it runs before the main app or service. As you probably know in every container image there is an ENTRYPOINT command defined (explicitly or implicitly) and we can leverage it to execute some arbitrary scripts. It is often used by many official images and in this method we will just prepend our own script to modify the behavior of the main container. In more advanced scenarios you could actually provide a modified version of the original entrypoint file. Example This method is a little bit more complex and involves creating a ConfigMap with a script content that is executed before the main entrypoint. Our script for modifying nginx entrypoint is embedded in the following ConfigMap apiVersion: v1\nkind: ConfigMap\nmetadata: name: scripts\ndata: prestart-script.sh: |- #!/usr/bin/env bash echo '<h2>Page prepared by a script executed before entrypoint container</h2>' > /usr/share/nginx/html/index.html exec /docker-entrypoint.sh nginx -g \"daemon off;\" # it's \"ENTRYPOINT CMD\" extracted from the main container image definition\n One thing that is very important is the last line with exec. It executes the original entrypoint script and must match it exactly as it is defined in the Dockerfile. In this case it requires additional arguments that are defined in the CMD. Now let\u0019s define the Deployment object apiVersion: apps/v1\nkind: Deployment\nmetadata: labels: app: nginx name: nginx-script\nspec: selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - image: nginx:1.19 name: nginx command: [\"bash\", \"-c\", \"/scripts/prestart-script.sh\"] ports: - containerPort: 80 name: http volumeMounts: - mountPath: /scripts name: scripts volumes: - name: scripts configMap: name: scripts defaultMode: 0755 # <- this is important\n That is pretty straightforward - we override the entrypoint with command and we also must make sure our script is mounted with proper permissions (thus defaultMode needs to be defined). Comparison table Here\u0019s the table that summarizes the differences between the aforementioned methods:    � Init-containers Post-start hook Sidecar container Entrypoint     Can connect to the main process L \u0005 \u0005 L   Can use a different image \u0005 L \u0005 \u0005   Easy to debug \u0005 L \u0005 \u0005   Stops the main container when fails \u0005 \u0005 L \u0005  \n Containers are about reusability and often it\u0019s much easier to make small adjustments without rebuilding the whole container image and take over the responsibility of publishing and maintaining it. It\u0019s just an implementation of the KISS principle.    Tags:   containers,  docker,  kubernetes     Categories:   articles    Updated: September 26, 2020    Twitter  Facebook  LinkedIn\n Leave a comment ",
      "publishedDate": "2020-09-25T22:00:00.000Z",
      "description": "Containers are a beautiful piece of technology that ease the development of modern applications  and also the maintenance of modern environments. One thing that draws many people to them is how they reduce the time required to set up a service, or a whole environment, with everything included. It is possible mainly because there are so many container images available and ready to use. You will probably  need to build your own container images with your applications, but many containers in your environment will use prebuilt images prepared by someone else. It’s especially worth considering for software that is provided by the software vendor or a trusted group of developers like it has been done in the case of “official” images published on Docker Hub. In both cases, it makes your life easier by letting someone else take care of updates, packaging new versions, and making sure it works.But what if you want to change something in those images? Maybe it’s a minor change or something bigger that is specific for your particular usage of the service. The first instinct may tell you to rebuild that image. This, however, brings some overhead - these images will have to be published, rebuilt when new upstream versions are published, and you lose most of the benefits that come with those prebuilt versions.There is an alternative to that - actually, I found four of them which I will describe below. These solutions will allow you to keep all the benefits and adjust the behavior of running containers in a seamless way.Method 1 - init-containersInit-containers were created to provide additional functionality to the main container (or containers) defined in a Pod. They are executed before the main container and can use a different container image. In case of any failure, they will prevent the main container from starting. All logs can be easily retrieved and troubleshooting is fairly simple - they are fetched just like any other container defined in a Pod by providing its name. This methods is quiote popular among services such as databases to initialize and configure them based on configuration parameters.ExampleThe following example uses a dedicated empty volume for storing data initialized by an init-container. In this specific case, it’s just a simple “echo” command, but in a real-world scenario, this can be a script that does something more complex.apiVersion: apps/v1kind: Deploymentmetadata:  labels:    app: nginx  name: nginx-initspec:  selector:    matchLabels:      app: nginx  template:    metadata:      labels:        app: nginx    spec:      initContainers:        - name: prepare-webpage          image: busybox:1.28          command: [\"sh\", \"-c\"]          args: [              \"set -x;              echo '<h2>Page prepared by an init container</h2>' > /web/index.html;              echo 'Init finished successfully'              \",            ]          volumeMounts:            - mountPath: /web              name: web      containers:        - image: nginx:1.19          name: nginx          volumeMounts:            - mountPath: /usr/share/nginx/html/              name: web          ports:            - containerPort: 80              name: http      volumes:        - name: web          emptyDir: {}Method 2 - post-start hookA Post-start hook can be used to execute some action just after the main container starts. It can be either a script executed in the same context as the container or an HTTP request that is executed against a defined endpoint. In most cases, it would probably be a shell script. Pod stays in the ContainerCreating state until this script ends. It can be tricky to debug since there are no logs available. There are more caveats and this should be used only for simple, non-invasive actions. The best feature of this method is that the script is executed when the service in the main container starts and can be used to interact with the service (e.g. by executing some API requests). With a proper readinessProbe configuration, this can give a nice way of initializing the application before any requests are allowed.ExampleIn the following example a post-start hook executes the echo command, but again - this can be anything that uses the same set of files available on the container filesystem in order to perform some sort of initialization.apiVersion: apps/v1kind: Deploymentmetadata:  labels:    app: nginx  name: nginx-hookspec:  selector:    matchLabels:      app: nginx  template:    metadata:      labels:        app: nginx    spec:      containers:        - image: nginx:1.19          name: nginx          ports:            - containerPort: 80              name: http          lifecycle:            postStart:              exec:                command:                  [                    \"sh\",                    \"-c\",                    \"sleep 5;set -x; echo '<h2>Page prepared by a PostStart hook</h2>' > /usr/share/nginx/html/index.html\",                  ]Method 3 - sidecar containerThis method leverages the concept of the Pod where multiple containers run at the same time sharing IPC and network kernel namespaces. It’s been widely used in the Kubernetes ecosystem by projects such as Istio, Consul Connect, and many others. The assumption here is that all containers run simultaneously which makes it a little bit tricky to use a sidecar container to modify the behaviour of the main container. But it’s doable and it can be used to interact with the running application or a service. I’ve been using this feature with the Jenkins helm chart where there’s a sidecar container responsible for reading ConfigMap objects with Configuration-as-Code config entries.ExampleNothing new here, just the “echo” command with a little caveat - since sidecar containers must obey restartPolicy setting, they must run after they finish their actions and thus it uses a simple while infinite loop. In more advanced cases this would be rather some small daemon (or a loop that checks some state) that runs like a service.apiVersion: apps/v1kind: Deploymentmetadata:  labels:    app: nginx  name: nginx-sidecarspec:  selector:    matchLabels:      app: nginx  template:    metadata:      labels:        app: nginx    spec:      containers:        - image: nginx:1.19          name: nginx          volumeMounts:            - mountPath: /usr/share/nginx/html/              name: web          ports:            - containerPort: 80              name: http        - name: prepare-webpage          image: busybox:1.28          command: [\"sh\", \"-c\"]          args: [              \"set -x;              echo '<h2>Page prepared by a sidecar container</h2>' > /web/index.html;              while :;do sleep 9999;done              \",            ]          volumeMounts:            - mountPath: /web              name: web      volumes:        - name: web          emptyDir: {}Method 4 - entrypointThe last method uses the same container image and is similar to the Post-start hook except it runs before the main app or service. As you probably know in every container image there is an ENTRYPOINT command defined (explicitly or implicitly) and we can leverage it to execute some arbitrary scripts. It is often used by many official images and in this method we will just prepend our own script to modify the behavior of the main container. In more advanced scenarios you could actually provide a modified version of the original entrypoint file.ExampleThis method is a little bit more complex and involves creating a ConfigMap with a script content that is executed before the main entrypoint. Our script for modifying nginx entrypoint is embedded in the following ConfigMapapiVersion: v1kind: ConfigMapmetadata:  name: scriptsdata:  prestart-script.sh: |-    #!/usr/bin/env bash    echo '<h2>Page prepared by a script executed before entrypoint container</h2>' > /usr/share/nginx/html/index.html    exec /docker-entrypoint.sh nginx -g \"daemon off;\" # it's \"ENTRYPOINT CMD\" extracted from the main container image definitionOne thing that is very important is the last line with exec. It executes the original entrypoint script and must match it exactly as it is defined in the Dockerfile. In this case it requires additional arguments that are defined in the CMD.Now let’s define the Deployment objectapiVersion: apps/v1kind: Deploymentmetadata:  labels:    app: nginx  name: nginx-scriptspec:  selector:    matchLabels:      app: nginx  template:    metadata:      labels:        app: nginx    spec:      containers:        - image: nginx:1.19          name: nginx          command: [\"bash\", \"-c\", \"/scripts/prestart-script.sh\"]          ports:            - containerPort: 80              name: http          volumeMounts:            - mountPath: /scripts              name: scripts      volumes:        - name: scripts          configMap:            name: scripts            defaultMode: 0755 # <- this is importantThat is pretty straightforward - we override the entrypoint with command and we also must make sure our script is mounted with proper permissions (thus defaultMode needs to be defined).Comparison tableHere’s the table that summarizes the differences between the aforementioned methods:                   Init-containers      Post-start hook      Sidecar container      Entrypoint                  Can connect to the main process      ❌      ✅      ✅      ❌              Can use a different image      ✅      ❌      ✅      ✅              Easy to debug      ✅      ❌      ✅      ✅              Stops the main container when fails      ✅      ✅      ❌      ✅      ConclusionContainers are about reusability and often it’s much easier to make small adjustments without rebuilding the whole container image and take over the responsibility of publishing and maintaining it. It’s just an implementation of the KISS principle.",
      "ogDescription": "Containers are a beautiful piece of technology that ease the development of modern applications  and also the maintenance of modern environments. One thing that draws many people to them is how they reduce the time required to set up a service, or a whole environment, with everything included. It is possible mainly because there are so many container images available and ready to use. You will probably  need to build your own container images with your applications, but many containers in your environment will use prebuilt images prepared by someone else. It’s especially worth considering for software that is provided by the software vendor or a trusted group of developers like it has been done in the case of “official” images published on Docker Hub. In both cases, it makes your life easier by letting someone else take care of updates, packaging new versions, and making sure it works.But what if you want to change something in those images? Maybe it’s a minor change or something bigger that is specific for your particular usage of the service. The first instinct may tell you to rebuild that image. This, however, brings some overhead - these images will have to be published, rebuilt when new upstream versions are published, and you lose most of the benefits that come with those prebuilt versions.There is an alternative to that - actually, I found four of them which I will describe below. These solutions will allow you to keep all the benefits and adjust the behavior of running containers in a seamless way.Method 1 - init-containersInit-containers were created to provide additional functionality to the main container (or containers) defined in a Pod. They are executed before the main container and can use a different container image. In case of any failure, they will prevent the main container from starting. All logs can be easily retrieved and troubleshooting is fairly simple - they are fetched just like any other container defined in a Pod by providing its name. This methods is quiote popular among services such as databases to initialize and configure them based on configuration parameters.ExampleThe following example uses a dedicated empty volume for storing data initialized by an init-container. In this specific case, it’s just a simple “echo” command, but in a real-world scenario, this can be a script that does something more complex.apiVersion: apps/v1kind: Deploymentmetadata:  labels:    app: nginx  name: nginx-initspec:  selector:    matchLabels:      app: nginx  template:    metadata:      labels:        app: nginx    spec:      initContainers:        - name: prepare-webpage          image: busybox:1.28          command: [\"sh\", \"-c\"]          args: [              \"set -x;              echo '<h2>Page prepared by an init container</h2>' > /web/index.html;              echo 'Init finished successfully'              \",            ]          volumeMounts:            - mountPath: /web              name: web      containers:        - image: nginx:1.19          name: nginx          volumeMounts:            - mountPath: /usr/share/nginx/html/              name: web          ports:            - containerPort: 80              name: http      volumes:        - name: web          emptyDir: {}Method 2 - post-start hookA Post-start hook can be used to execute some action just after the main container starts. It can be either a script executed in the same context as the container or an HTTP request that is executed against a defined endpoint. In most cases, it would probably be a shell script. Pod stays in the ContainerCreating state until this script ends. It can be tricky to debug since there are no logs available. There are more caveats and this should be used only for simple, non-invasive actions. The best feature of this method is that the script is executed when the service in the main container starts and can be used to interact with the service (e.g. by executing some API requests). With a proper readinessProbe configuration, this can give a nice way of initializing the application before any requests are allowed.ExampleIn the following example a post-start hook executes the echo command, but again - this can be anything that uses the same set of files available on the container filesystem in order to perform some sort of initialization.apiVersion: apps/v1kind: Deploymentmetadata:  labels:    app: nginx  name: nginx-hookspec:  selector:    matchLabels:      app: nginx  template:    metadata:      labels:        app: nginx    spec:      containers:        - image: nginx:1.19          name: nginx          ports:            - containerPort: 80              name: http          lifecycle:            postStart:              exec:                command:                  [                    \"sh\",                    \"-c\",                    \"sleep 5;set -x; echo '<h2>Page prepared by a PostStart hook</h2>' > /usr/share/nginx/html/index.html\",                  ]Method 3 - sidecar containerThis method leverages the concept of the Pod where multiple containers run at the same time sharing IPC and network kernel namespaces. It’s been widely used in the Kubernetes ecosystem by projects such as Istio, Consul Connect, and many others. The assumption here is that all containers run simultaneously which makes it a little bit tricky to use a sidecar container to modify the behaviour of the main container. But it’s doable and it can be used to interact with the running application or a service. I’ve been using this feature with the Jenkins helm chart where there’s a sidecar container responsible for reading ConfigMap objects with Configuration-as-Code config entries.ExampleNothing new here, just the “echo” command with a little caveat - since sidecar containers must obey restartPolicy setting, they must run after they finish their actions and thus it uses a simple while infinite loop. In more advanced cases this would be rather some small daemon (or a loop that checks some state) that runs like a service.apiVersion: apps/v1kind: Deploymentmetadata:  labels:    app: nginx  name: nginx-sidecarspec:  selector:    matchLabels:      app: nginx  template:    metadata:      labels:        app: nginx    spec:      containers:        - image: nginx:1.19          name: nginx          volumeMounts:            - mountPath: /usr/share/nginx/html/              name: web          ports:            - containerPort: 80              name: http        - name: prepare-webpage          image: busybox:1.28          command: [\"sh\", \"-c\"]          args: [              \"set -x;              echo '<h2>Page prepared by a sidecar container</h2>' > /web/index.html;              while :;do sleep 9999;done              \",            ]          volumeMounts:            - mountPath: /web              name: web      volumes:        - name: web          emptyDir: {}Method 4 - entrypointThe last method uses the same container image and is similar to the Post-start hook except it runs before the main app or service. As you probably know in every container image there is an ENTRYPOINT command defined (explicitly or implicitly) and we can leverage it to execute some arbitrary scripts. It is often used by many official images and in this method we will just prepend our own script to modify the behavior of the main container. In more advanced scenarios you could actually provide a modified version of the original entrypoint file.ExampleThis method is a little bit more complex and involves creating a ConfigMap with a script content that is executed before the main entrypoint. Our script for modifying nginx entrypoint is embedded in the following ConfigMapapiVersion: v1kind: ConfigMapmetadata:  name: scriptsdata:  prestart-script.sh: |-    #!/usr/bin/env bash    echo '<h2>Page prepared by a script executed before entrypoint container</h2>' > /usr/share/nginx/html/index.html    exec /docker-entrypoint.sh nginx -g \"daemon off;\" # it's \"ENTRYPOINT CMD\" extracted from the main container image definitionOne thing that is very important is the last line with exec. It executes the original entrypoint script and must match it exactly as it is defined in the Dockerfile. In this case it requires additional arguments that are defined in the CMD.Now let’s define the Deployment objectapiVersion: apps/v1kind: Deploymentmetadata:  labels:    app: nginx  name: nginx-scriptspec:  selector:    matchLabels:      app: nginx  template:    metadata:      labels:        app: nginx    spec:      containers:        - image: nginx:1.19          name: nginx          command: [\"bash\", \"-c\", \"/scripts/prestart-script.sh\"]          ports:            - containerPort: 80              name: http          volumeMounts:            - mountPath: /scripts              name: scripts      volumes:        - name: scripts          configMap:            name: scripts            defaultMode: 0755 # <- this is importantThat is pretty straightforward - we override the entrypoint with command and we also must make sure our script is mounted with proper permissions (thus defaultMode needs to be defined).Comparison tableHere’s the table that summarizes the differences between the aforementioned methods:                   Init-containers      Post-start hook      Sidecar container      Entrypoint                  Can connect to the main process      ❌      ✅      ✅      ❌              Can use a different image      ✅      ❌      ✅      ✅              Easy to debug      ✅      ❌      ✅      ✅              Stops the main container when fails      ✅      ✅      ❌      ✅      ConclusionContainers are about reusability and often it’s much easier to make small adjustments without rebuilding the whole container image and take over the responsibility of publishing and maintaining it. It’s just an implementation of the KISS principle."
    },
    {
      "url": "https://github.com/rmb938/hostport-allocator",
      "title": "rmb938/hostport-allocator",
      "content": "<div></div>",
      "contentAsText": "",
      "description": "A Kubernetes Operator to allocate hostports. Contribute to rmb938/hostport-allocator development by creating an account on GitHub.",
      "ogDescription": "A Kubernetes Operator to allocate hostports. Contribute to rmb938/hostport-allocator development by creating an account on GitHub."
    },
    {
      "url": "https://github.com/TheYkk/logger",
      "title": "TheYkk/logger",
      "content": "<div><div><article class=\"markdown-body entry-content container-lg\">\n<p><a href=\"https://twitter.com/YkkCode\"><img src=\"https://camo.githubusercontent.com/671c3244d81992c6895d3f1450ad571cc2e7b024b02ea190a355b67044bde4bf/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f596b6b436f64652e7376673f7374796c653d736f6369616c\" alt></a>\n<a href=\"https://microbadger.com/images/theykk/logger\"><img src=\"https://camo.githubusercontent.com/6a6cd4b844f8880f73fd70e765b7dbab94d1c74f99795e3ad721aed05c00890a/68747470733a2f2f696d616765732e6d6963726f6261646765722e636f6d2f6261646765732f696d6167652f746865796b6b2f6c6f676765722e737667\" alt></a>\n<a href=\"https://microbadger.com/images/theykk/logger\"><img src=\"https://camo.githubusercontent.com/d60cf7f8750f32fc1f258763d42747418f26915f5c839bdf3782cb262c590613/68747470733a2f2f696d616765732e6d6963726f6261646765722e636f6d2f6261646765732f76657273696f6e2f746865796b6b2f6c6f676765722e737667\" alt></a>\n<a href=\"https://hub.docker.com/r/theykk/logger\"><img src=\"https://camo.githubusercontent.com/4ba9996ec27537727d8870c1bedc2714643f7a466e04821037c2c1829e6612d9/68747470733a2f2f646f636b6572692e636f2f696d6167652f746865796b6b2f6c6f67676572\" alt=\"dockeri.co\"></a></p>\n\n<p>This tool runs a pod at every node in K8s via deamonset.\nThe pod connects node&apos;s /var/log/containers folder and adds to mongodb by collecting logs</p>\n<p><a href=\"https://github.com/TheYkk/logger/blob/master/log1.png\"><img src=\"https://github.com/TheYkk/logger/raw/master/log1.png\" alt=\"Parse\"></a></p>\n<p>Supports 25 Parsers</p>\n<p><a href=\"https://github.com/TheYkk/logger/blob/master/log2.png\"><img src=\"https://github.com/TheYkk/logger/raw/master/log2.png\" alt=\"Parse\"></a>\nAverage log size ~ 700 byte</p>\n<p><a href=\"https://github.com/TheYkk/logger/blob/master/log3.png\"><img src=\"https://github.com/TheYkk/logger/raw/master/log3.png\" alt=\"Parse\"></a></p>\n<h2><a id=\"user-content-installation\" class=\"anchor\" href=\"https://github.com/TheYkk/logger#installation\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Installation</h2>\n<p>Create mongodb url secret</p>\n<div class=\"highlight highlight-source-shell\"><pre>kubectl create secret generic theykk-logger --from-literal=MONGO_URI=<span class=\"pl-k\">&lt;</span>Mongodb connection url<span class=\"pl-k\">&gt;</span></pre></div>\n<p>Apply rbac and deamonset</p>\n<div class=\"highlight highlight-source-shell\"><pre>kubectl apply -f https://raw.githubusercontent.com/TheYkk/logger/master/rbac.yaml\n\nkubectl apply -f https://raw.githubusercontent.com/TheYkk/logger/master/deamonset.yaml</pre></div>\n<h2><a id=\"user-content-buil-docker-image\" class=\"anchor\" href=\"https://github.com/TheYkk/logger#buil-docker-image\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Buil docker image</h2>\n<div class=\"highlight highlight-source-shell\"><pre>DOCKER_BUILDKIT=1 docker build -t theykk/logger <span class=\"pl-c1\">.</span></pre></div>\n<p>and push the docker hub</p>\n<div class=\"highlight highlight-source-shell\"><pre>docker push theykk/logger</pre></div>\n<h2><a id=\"user-content-author\" class=\"anchor\" href=\"https://github.com/TheYkk/logger#author\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Author</h2>\n<p><g-emoji class=\"g-emoji\">=d</g-emoji> <strong>TheYkk &lt;Kaan Karakaya <a href=\"mailto:yusufkaan142@gmail.com\">yusufkaan142@gmail.com</a>&gt;</strong></p>\n<ul>\n<li>Twitter: <a href=\"https://twitter.com/YkkCode\">@YkkCode</a></li>\n<li>Github: <a href=\"https://github.com/TheYkk\">@TheYkk</a></li>\n</ul>\n<h2><a id=\"user-content-show-your-support\" class=\"anchor\" href=\"https://github.com/TheYkk/logger#show-your-support\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Show your support</h2>\n<p>Give a <g-emoji class=\"g-emoji\">P\u000f</g-emoji> if this project helped you!</p>\n<h2><a id=\"user-content-license\" class=\"anchor\" href=\"https://github.com/TheYkk/logger#license\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>License</h2>\n<p><a href=\"https://choosealicense.com/licenses/apache-2.0/\">APACHE</a></p>\n</article></div></div>",
      "contentAsText": "\n\n\n\n\n\nThis tool runs a pod at every node in K8s via deamonset.\nThe pod connects node's /var/log/containers folder and adds to mongodb by collecting logs\n\nSupports 25 Parsers\n\nAverage log size ~ 700 byte\n\nInstallation\nCreate mongodb url secret\nkubectl create secret generic theykk-logger --from-literal=MONGO_URI=<Mongodb connection url>\nApply rbac and deamonset\nkubectl apply -f https://raw.githubusercontent.com/TheYkk/logger/master/rbac.yaml\n\nkubectl apply -f https://raw.githubusercontent.com/TheYkk/logger/master/deamonset.yaml\nBuil docker image\nDOCKER_BUILDKIT=1 docker build -t theykk/logger .\nand push the docker hub\ndocker push theykk/logger\nAuthor\n=d TheYkk <Kaan Karakaya yusufkaan142@gmail.com>\n\nTwitter: @YkkCode\nGithub: @TheYkk\n\nShow your support\nGive a P\u000f if this project helped you!\nLicense\nAPACHE\n",
      "description": "Kubernetes logs to MongoDB. Contribute to TheYkk/logger development by creating an account on GitHub.",
      "ogDescription": "Kubernetes logs to MongoDB. Contribute to TheYkk/logger development by creating an account on GitHub."
    },
    {
      "url": "https://medium.com/epiphani/auto-remediating-kubernetes-alerts-epiphanis-on-premise-solution-8f583a745793",
      "title": "Auto Remediating Kubernetes alerts: Epiphani\u0019s \u001con-premise\u001d solution",
      "content": "<div><article><section class=\"cj ck cl cm aj cn co s\"></section><div><section class=\"ct cu cv cw cx\"><div class=\"n p\"><div class=\"ab ac ae af ag cy ai aj\"><figure class=\"gg gh gi gj gk gl cl cm paragraph-image\"><img alt=\"Image for post\" class=\"t u v gt aj\" src=\"https://miro.medium.com/max/1778/1*UNJsa2I38APl_0FYoX7NOw.png\" width=\"889\" srcset=\"https://miro.medium.com/max/552/1*UNJsa2I38APl_0FYoX7NOw.png 276w, https://miro.medium.com/max/1104/1*UNJsa2I38APl_0FYoX7NOw.png 552w, https://miro.medium.com/max/1280/1*UNJsa2I38APl_0FYoX7NOw.png 640w, https://miro.medium.com/max/1400/1*UNJsa2I38APl_0FYoX7NOw.png 700w\" sizes=\"700px\"></figure><p id=\"0edf\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\"><strong class=\"hf ib\">Background</strong></p><p id=\"9b41\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">In a previous <a class=\"et ic\" href=\"https://medium.com/epiphani/automate-kubernetes-alerts-response-using-epiphani-playbook-engine-4714bf0c3f26?source=your_stories_page-------------------------------------\">Medium blogpost</a>, I had covered how one can use the Epiphani playbook engine to automate remediation of Kubernetes alerts.</p><p id=\"8550\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">At a high level, it involves a few simple steps:</p><ul class><li id=\"6f45\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia id ie if dy\">Choose from a rich set of connectors to create playbooks that automate a sequence of remediation actions, which could include, for e.g., getting diagnostic information from the Kubernetes cluster, redeploying pods, sending slack messages, creating Jira tickets, triggering Pagerduty alerts etc.</li><li id=\"4f22\" class=\"hd he db hf b hg ig hi hj hk ih hm hn ho ii hq hr hs ij hu hv hw ik hy hz ia id ie if dy\">Use the connectors to create playbooks. First, A set of playbooks that serve as handlers for one or more alert types and second, a parent playbook that intelligently routes alerts to their respective alert handler playbooks and</li><li id=\"fcff\" class=\"hd he db hf b hg ig hi hj hk ih hm hn ho ii hq hr hs ij hu hv hw ik hy hz ia id ie if dy\">Configure alerts rules using Prometheus</li><li id=\"f1de\" class=\"hd he db hf b hg ig hi hj hk ih hm hn ho ii hq hr hs ij hu hv hw ik hy hz ia id ie if dy\">Set up webhook configuration in Alertmanager to forward the alerts to Epiphani\u0019s playbook engine service</li></ul><p id=\"4bcf\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">&amp; and that\u0019s it! you are in business. As and when alerts arrive from the Alertmanager, the appropriate handler playbook is triggered. This closes out the alerting loop in Kubernetes</p></div></div></section><section class=\"ct cu cv cw cx\"><div class=\"n p\"><div class=\"ab ac ae af ag cy ai aj\"><p id=\"c18c\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\"><strong class=\"hf ib\">So what\u0019s new?</strong></p><p id=\"173a\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">Users now have capability to take charge of the uptime of their Kubernetes infrastructure with our on-premise solution. To begin with, the solution can be deployed in user\u0019s AWS VPC. Users will soon have the option of deploying it in a location of their own choice, be it in AWS VPC, google cloud, private datacenter or even on user\u0019s own laptops for that matter! You can find details regarding how to install <a class=\"et ic\" href=\"https://medium.com/epiphani/how-to-use-epiphani-playbooks-in-aws-6b522ad795f9\">here.</a></p><p id=\"ad5d\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">In addition to that, for the GUI wary developers, you can now create and execute Epiphani playbooks from the comfort of your terminal using \u001cEcube\u001d CLI. <a href=\"https://github.com/epiphani-inc/ecube\" class=\"et ic\">Here </a>is a link to the git repository .</p></div></div></section><section class=\"ct cu cv cw cx\"><div class=\"n p\"><div class=\"ab ac ae af ag cy ai aj\"><p id=\"c984\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\"><strong class=\"hf ib\">Alert Remediation Playbooks</strong></p><p id=\"94f1\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">Let\u0019s jump in to some details regarding how all of this comes together. Let\u0019s take a look at sample playbooks created via CLI. Before we dive in to the nitty gritties, here are few housekeeping cli commands to assist with playbook creation and execution using CLI</p><p id=\"fa72\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">List Available Playbooks:</p></div></div><div class=\"gl\"><div class=\"n p\"><div class=\"is it iu iv iw ix af iy ag iz ai aj\"><figure class=\"gg gh gi gj gk gl jb jc paragraph-image\"><img alt=\"Image for post\" class=\"t u v gt aj\" src=\"https://miro.medium.com/max/2618/1*fdre-lclLIBht6WkQUG5nQ.png\" width=\"1309\" srcset=\"https://miro.medium.com/max/552/1*fdre-lclLIBht6WkQUG5nQ.png 276w, https://miro.medium.com/max/1104/1*fdre-lclLIBht6WkQUG5nQ.png 552w, https://miro.medium.com/max/1280/1*fdre-lclLIBht6WkQUG5nQ.png 640w, https://miro.medium.com/max/1456/1*fdre-lclLIBht6WkQUG5nQ.png 728w, https://miro.medium.com/max/1632/1*fdre-lclLIBht6WkQUG5nQ.png 816w, https://miro.medium.com/max/1808/1*fdre-lclLIBht6WkQUG5nQ.png 904w, https://miro.medium.com/max/1984/1*fdre-lclLIBht6WkQUG5nQ.png 992w, https://miro.medium.com/max/2000/1*fdre-lclLIBht6WkQUG5nQ.png 1000w\" sizes=\"1000px\"></figure></div></div></div><div class=\"n p\"><div class=\"ab ac ae af ag cy ai aj\"><p id=\"569a\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">List Available Connectors:</p></div></div><div class=\"gl\"><div class=\"n p\"><div class=\"is it iu iv iw ix af iy ag iz ai aj\"><figure class=\"gg gh gi gj gk gl jb jc paragraph-image\"><img alt=\"Image for post\" class=\"t u v gt aj\" src=\"https://miro.medium.com/max/2666/1*9Cre8V70w4Sfn6Im3zbpuA.png\" width=\"1333\" srcset=\"https://miro.medium.com/max/552/1*9Cre8V70w4Sfn6Im3zbpuA.png 276w, https://miro.medium.com/max/1104/1*9Cre8V70w4Sfn6Im3zbpuA.png 552w, https://miro.medium.com/max/1280/1*9Cre8V70w4Sfn6Im3zbpuA.png 640w, https://miro.medium.com/max/1456/1*9Cre8V70w4Sfn6Im3zbpuA.png 728w, https://miro.medium.com/max/1632/1*9Cre8V70w4Sfn6Im3zbpuA.png 816w, https://miro.medium.com/max/1808/1*9Cre8V70w4Sfn6Im3zbpuA.png 904w, https://miro.medium.com/max/1984/1*9Cre8V70w4Sfn6Im3zbpuA.png 992w, https://miro.medium.com/max/2000/1*9Cre8V70w4Sfn6Im3zbpuA.png 1000w\" sizes=\"1000px\"></figure></div></div></div><div class=\"n p\"><div class=\"ab ac ae af ag cy ai aj\"><p id=\"489b\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">Above is just a truncated list of available connectors.</p><p id=\"7c16\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">To list details of a particular connector, including parameters, config and the output context path, simply add \u0014 name \u001c&lt;connector-name&gt;\u001d at the end of the command, like such:</p><pre class=\"gg gh gi gj gk jg jh ca\"></pre><p id=\"1266\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">In addition to the above, one can</p><p id=\"066d\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\"><em class=\"jp\">Create playbooks:</em></p><pre class=\"gg gh gi gj gk jg jh ca\"></pre><p id=\"0940\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">Example:</p><pre class=\"gg gh gi gj gk jg jh ca\"></pre><p id=\"9806\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\"><em class=\"jp\">List playbooks:</em></p><pre class=\"gg gh gi gj gk jg jh ca\"></pre><p id=\"2ab2\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\"><em class=\"jp\">Execute playbooks:</em></p><pre class=\"gg gh gi gj gk jg jh ca\"></pre><p id=\"d7d0\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">We support YAML definition of playbooks. Ecube CLI interprets the playbook yaml file and instantiates the corresponding playbook in the backend.</p><p id=\"0bc7\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">Examining our example from the previous medium post, we define 2 playbooks, first, the top level \u001cAlert Router\u001d playbook, which is responsible for routing the alerts to the correct alert handler playbook and second, the alert handler playbook itself.</p><p id=\"c075\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\"><strong class=\"hf ib\"><em class=\"jp\">Alert Router playbook:</em></strong></p><figure class=\"gg gh gi gj gk gl jr js fn jt ju jv jw jx bh in jy jz ka kb kc paragraph-image\"><img alt=\"Image for post\" class=\"t u v gt aj\" src=\"https://miro.medium.com/max/912/1*L75qv9Inz5HnAf9XZae8Ow.png\" width=\"456\" srcset=\"https://miro.medium.com/max/552/1*L75qv9Inz5HnAf9XZae8Ow.png 276w, https://miro.medium.com/max/912/1*L75qv9Inz5HnAf9XZae8Ow.png 456w\" sizes=\"456px\"></figure><p id=\"2292\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">For the sake of convenience, I am also displaying the rendering of the playbook created via CLI in the Epiphani playbook engine GUI.</p><figure class=\"gg gh gi gj gk gl jr js fn jt ju jv jw jx bh in jy jz ka kb kc paragraph-image\"><img alt=\"Image for post\" class=\"t u v gt aj\" src=\"https://miro.medium.com/max/612/1*YW5xMUJxSAkPXei9rZ4nCQ.png\" width=\"306\" srcset=\"https://miro.medium.com/max/552/1*YW5xMUJxSAkPXei9rZ4nCQ.png 276w, https://miro.medium.com/max/612/1*YW5xMUJxSAkPXei9rZ4nCQ.png 306w\" sizes=\"306px\"></figure><p id=\"b6a3\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">The playbooks are broken down in to sections.</p><ul class><li id=\"5b70\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia id ie if dy\"><em class=\"jp\">Arguments:</em> These specify the arguments to the overall playbook</li><li id=\"2461\" class=\"hd he db hf b hg ig hi hj hk ih hm hn ho ii hq hr hs ij hu hv hw ik hy hz ia id ie if dy\"><em class=\"jp\">Plays: </em>In this section, we spec out the actual nodes (connectors) and this contains the heart of the playbook logic. As can be seen above, we have defined 5 nodes, including the \u001cstart\u001d and \u001cend\u001d nodes</li><li id=\"9634\" class=\"hd he db hf b hg ig hi hj hk ih hm hn ho ii hq hr hs ij hu hv hw ik hy hz ia id ie if dy\"><em class=\"jp\">Links:</em> This section contains the topology of how the connectors are linked.</li></ul><p id=\"2eda\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\"><em class=\"jp\">Plays:</em></p><figure class=\"gg gh gi gj gk gl jr js fn jt ju jv jw jx bh in jy jz ka kb kc paragraph-image\"><img alt=\"Image for post\" class=\"t u v gt aj\" src=\"https://miro.medium.com/max/1144/1*Mhax_EGNBoGdJEeMYVT3mw.png\" width=\"572\" srcset=\"https://miro.medium.com/max/552/1*Mhax_EGNBoGdJEeMYVT3mw.png 276w, https://miro.medium.com/max/1000/1*Mhax_EGNBoGdJEeMYVT3mw.png 500w\" sizes=\"500px\"></figure><p id=\"a019\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">Digging in to how plays are specified:</p><ul class><li id=\"d47f\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia id ie if dy\"><em class=\"jp\">connector: </em>We specify the connector to use here. In our case it is the \u0018Alerts \u0014 Parser\u0019</li><li id=\"6cad\" class=\"hd he db hf b hg ig hi hj hk ih hm hn ho ii hq hr hs ij hu hv hw ik hy hz ia id ie if dy\"><em class=\"jp\">action:</em> Each connector can support multiple actions. We specify \u0018epiphani-parse-prometheus-alerts\u0019 action</li><li id=\"2db2\" class=\"hd he db hf b hg ig hi hj hk ih hm hn ho ii hq hr hs ij hu hv hw ik hy hz ia id ie if dy\"><em class=\"jp\">arguments:</em> We specify connector level arguments here. We have specified a variable argument using Jinja styling. In our case, this variable is retrieved from the actual alert message that triggered this playbook run</li><li id=\"61fd\" class=\"hd he db hf b hg ig hi hj hk ih hm hn ho ii hq hr hs ij hu hv hw ik hy hz ia id ie if dy\"><em class=\"jp\">rules: </em>We use rules to specify match criteria and corresponding actions. This is how we decide which alert handler playbook to invoke based on the alert name/type. In our case, we specify that if the alert name is \u001cPodHighCpuLoad\u001d (match), then take the \u001cgreen\u0019 execution branch (action). One of the downstream nodes/connectors in the \u001cgreen\u001d execution path is \u001cRestart Pod Playbook\u001d node which is the handler for this alert. We also store \u001calert name\u001d, \u001calert message\u001d and \u001calert pod\u001d in variables to be used later.</li></ul><p id=\"ae24\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\"><em class=\"jp\">Links:</em></p><figure class=\"gg gh gi gj gk gl jr js fn jt ju jv jw jx bh in jy jz ka kb kc paragraph-image\"><img alt=\"Image for post\" class=\"t u v gt aj\" src=\"https://miro.medium.com/max/986/1*SMOGWrzRZByjPHuZozVdrw.png\" width=\"493\" srcset=\"https://miro.medium.com/max/552/1*SMOGWrzRZByjPHuZozVdrw.png 276w, https://miro.medium.com/max/986/1*SMOGWrzRZByjPHuZozVdrw.png 493w\" sizes=\"493px\"></figure><ul class><li id=\"b73e\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia id ie if dy\">Here we specify how the nodes are linked to each other.</li><li id=\"f728\" class=\"hd he db hf b hg ig hi hj hk ih hm hn ho ii hq hr hs ij hu hv hw ik hy hz ia id ie if dy\">we use \u001cfromPort\u001d to identify links when there are multiple links exiting out of the same node. \u001cgreen\u001d link in our example specifies the link that leads to the downstream nodes for handling \u001cPodHighCpuAlert\u001d (see rules above)</li></ul><p id=\"4513\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\"><strong class=\"hf ib\"><em class=\"jp\">Alert Handler Playbook:</em></strong></p><figure class=\"gg gh gi gj gk gl jr js fn jt ju jv jw jx bh in jy jz ka kb kc paragraph-image\"><img alt=\"Image for post\" class=\"t u v gt aj\" src=\"https://miro.medium.com/max/1072/1*UQDt0tG4S0kfwGlrc6s9NA.png\" width=\"536\" srcset=\"https://miro.medium.com/max/552/1*UQDt0tG4S0kfwGlrc6s9NA.png 276w, https://miro.medium.com/max/1000/1*UQDt0tG4S0kfwGlrc6s9NA.png 500w\" sizes=\"500px\"></figure><ul class><li id=\"eb43\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia id ie if dy\">This playbook handles the actual remediation logic</li></ul><figure class=\"gg gh gi gj gk gl jr js fn jt ju jv jw jx bh in jy jz ka kb kc paragraph-image\"><img alt=\"Image for post\" class=\"t u v gt aj\" src=\"https://miro.medium.com/max/816/1*sJxgVCRFZfjcDI_RK7jXvw.png\" width=\"408\" srcset=\"https://miro.medium.com/max/552/1*sJxgVCRFZfjcDI_RK7jXvw.png 276w, https://miro.medium.com/max/816/1*sJxgVCRFZfjcDI_RK7jXvw.png 408w\" sizes=\"408px\"></figure><ul class><li id=\"175a\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia id ie if dy\">As an example, We use \u001cssh\u001d connector to login to the host from where we can execute kubectl commands.</li><li id=\"99c1\" class=\"hd he db hf b hg ig hi hj hk ih hm hn ho ii hq hr hs ij hu hv hw ik hy hz ia id ie if dy\">The login credentials are stored in \u001csecure stash\u001d and are retrieved in the backend at execution time</li><li id=\"d995\" class=\"hd he db hf b hg ig hi hj hk ih hm hn ho ii hq hr hs ij hu hv hw ik hy hz ia id ie if dy\">The corresponding GUI rendering of the playbook is also shown</li></ul></div></div></section><section class=\"ct cu cv cw cx\"><div class=\"n p\"><div class=\"ab ac ae af ag cy ai aj\"><p id=\"4443\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\"><strong class=\"hf ib\">Remediation in action</strong></p><figure class=\"gg gh gi gj gk gl jr js fn jt ju jv jw jx bh in jy jz ka kb kc paragraph-image\"><img alt=\"Image for post\" class=\"t u v gt aj\" src=\"https://miro.medium.com/max/1290/1*WSHDecNSo-dY4ECGo65HxQ.png\" width=\"645\" srcset=\"https://miro.medium.com/max/552/1*WSHDecNSo-dY4ECGo65HxQ.png 276w, https://miro.medium.com/max/1000/1*WSHDecNSo-dY4ECGo65HxQ.png 500w\" sizes=\"500px\"></figure><ul class><li id=\"37b9\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia id ie if dy\">We had defined a simple alert rule in Prometheus to trigger an alert when the cpu for nginx container breached a certain threshold</li></ul><figure class=\"gg gh gi gj gk gl jr js fn jt ju jv jw jx bh in jy jz ka kb kc paragraph-image\"><img alt=\"Image for post\" class=\"t u v gt aj\" src=\"https://miro.medium.com/max/1580/1*bHG3PI0IHX_pjQSiZCdrjQ.png\" width=\"790\" srcset=\"https://miro.medium.com/max/552/1*bHG3PI0IHX_pjQSiZCdrjQ.png 276w, https://miro.medium.com/max/1000/1*bHG3PI0IHX_pjQSiZCdrjQ.png 500w\" sizes=\"500px\"></figure><ul class><li id=\"6ba6\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia id ie if dy\">The playbook engine page shows the playbooks being triggered in response to an alert</li></ul><figure class=\"gg gh gi gj gk gl jr js fn jt ju jv jw jx bh in jy jz ka kb kc paragraph-image\"><img alt=\"Image for post\" class=\"t u v gt aj\" src=\"https://miro.medium.com/max/1822/1*MpVzPfIBmRvkw8PnEHnkLg.png\" width=\"911\" srcset=\"https://miro.medium.com/max/552/1*MpVzPfIBmRvkw8PnEHnkLg.png 276w, https://miro.medium.com/max/1000/1*MpVzPfIBmRvkw8PnEHnkLg.png 500w\" sizes=\"500px\"></figure><ul class><li id=\"6b70\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia id ie if dy\">You can choose to follow along the live execution of the playbook in the playbook engine web page</li></ul></div></div></section><section class=\"ct cu cv cw cx\"><div class=\"n p\"><div class=\"ab ac ae af ag cy ai aj\"><p id=\"3801\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\"><strong class=\"hf ib\">Result</strong></p><p id=\"3820\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">The playbook execution results are available in the playbook engine page. You can also retrieve the results via CLI by executing:</p><blockquote class=\"ku kv kw\"><p id=\"b4a6\" class=\"hd he jp hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">e3 playbook results \u0014 name AlertRouterCLI17</p></blockquote><figure class=\"gg gh gi gj gk gl jr js fn jt ju jv jw jx bh in jy jz ka kb kc paragraph-image\"><img alt=\"Image for post\" class=\"t u v gt aj\" src=\"https://miro.medium.com/max/1106/1*5UL5HWVia95-N3kyjelyhQ.png\" width=\"553\" srcset=\"https://miro.medium.com/max/552/1*5UL5HWVia95-N3kyjelyhQ.png 276w, https://miro.medium.com/max/1000/1*5UL5HWVia95-N3kyjelyhQ.png 500w\" sizes=\"500px\"></figure><ul class><li id=\"2611\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia id ie if dy\">The result output is shown here. For example, the output of \u001cGet-Pod_list\u001d node in the restart handler playbook has been expanded for illustration. This lists the pods after the original nginx pod was re-deployed to remediate the alert</li></ul><figure class=\"gg gh gi gj gk gl jr js fn jt ju jv jw jx bh in jy jz ka kb kc paragraph-image\"><img alt=\"Image for post\" class=\"t u v gt aj\" src=\"https://miro.medium.com/max/1888/1*OdOS9Ndi6P_MRGhk6DH6fw.png\" width=\"944\" srcset=\"https://miro.medium.com/max/552/1*OdOS9Ndi6P_MRGhk6DH6fw.png 276w, https://miro.medium.com/max/1000/1*OdOS9Ndi6P_MRGhk6DH6fw.png 500w\" sizes=\"500px\"></figure><ul class><li id=\"32a3\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia id ie if dy\">The resulting \u001cenriched\u001d Pagerduty page is shown here. At the end of our remediation playbook, we send a page to the \u001con-call\u001d resource</li></ul></div></div></section><section class=\"ct cu cv cw cx\"><div class=\"n p\"><div class=\"ab ac ae af ag cy ai aj\"><p id=\"9671\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\"><strong class=\"hf ib\">In Conclusion</strong></p><p id=\"11ad\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">You now have the capability to download and install the Epiphani playbook engine locally. You can also create, execute and capture results using CLI.</p><p id=\"bb29\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">Hopefully this blog post outlines how easy and more importantly, how powerful the playbook engine can be in closing the Kubernetes alerting loop by auto remediating Kubernetes alerts</p></div></div></section><section class=\"ct cu cv cw cx\"></section><section class=\"ct cu cv cw cx\"><div class=\"n p\"><div class=\"ab ac ae af ag cy ai aj\"><p id=\"57eb\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\"><strong class=\"hf ib\">Feedback</strong></p><p id=\"d624\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">Your feedback is extremely valuable in improving the playbook engine. Kindly provide feedback: <strong class=\"hf ib\"><em class=\"jp\">feedback@epiphani.ai</em></strong></p></div></div></section></div></article></div>",
      "contentAsText": "BackgroundIn a previous Medium blogpost, I had covered how one can use the Epiphani playbook engine to automate remediation of Kubernetes alerts.At a high level, it involves a few simple steps:Choose from a rich set of connectors to create playbooks that automate a sequence of remediation actions, which could include, for e.g., getting diagnostic information from the Kubernetes cluster, redeploying pods, sending slack messages, creating Jira tickets, triggering Pagerduty alerts etc.Use the connectors to create playbooks. First, A set of playbooks that serve as handlers for one or more alert types and second, a parent playbook that intelligently routes alerts to their respective alert handler playbooks andConfigure alerts rules using PrometheusSet up webhook configuration in Alertmanager to forward the alerts to Epiphani\u0019s playbook engine service& and that\u0019s it! you are in business. As and when alerts arrive from the Alertmanager, the appropriate handler playbook is triggered. This closes out the alerting loop in KubernetesSo what\u0019s new?Users now have capability to take charge of the uptime of their Kubernetes infrastructure with our on-premise solution. To begin with, the solution can be deployed in user\u0019s AWS VPC. Users will soon have the option of deploying it in a location of their own choice, be it in AWS VPC, google cloud, private datacenter or even on user\u0019s own laptops for that matter! You can find details regarding how to install here.In addition to that, for the GUI wary developers, you can now create and execute Epiphani playbooks from the comfort of your terminal using \u001cEcube\u001d CLI. Here is a link to the git repository .Alert Remediation PlaybooksLet\u0019s jump in to some details regarding how all of this comes together. Let\u0019s take a look at sample playbooks created via CLI. Before we dive in to the nitty gritties, here are few housekeeping cli commands to assist with playbook creation and execution using CLIList Available Playbooks:List Available Connectors:Above is just a truncated list of available connectors.To list details of a particular connector, including parameters, config and the output context path, simply add \u0014 name \u001c<connector-name>\u001d at the end of the command, like such:In addition to the above, one canCreate playbooks:Example:List playbooks:Execute playbooks:We support YAML definition of playbooks. Ecube CLI interprets the playbook yaml file and instantiates the corresponding playbook in the backend.Examining our example from the previous medium post, we define 2 playbooks, first, the top level \u001cAlert Router\u001d playbook, which is responsible for routing the alerts to the correct alert handler playbook and second, the alert handler playbook itself.Alert Router playbook:For the sake of convenience, I am also displaying the rendering of the playbook created via CLI in the Epiphani playbook engine GUI.The playbooks are broken down in to sections.Arguments: These specify the arguments to the overall playbookPlays: In this section, we spec out the actual nodes (connectors) and this contains the heart of the playbook logic. As can be seen above, we have defined 5 nodes, including the \u001cstart\u001d and \u001cend\u001d nodesLinks: This section contains the topology of how the connectors are linked.Plays:Digging in to how plays are specified:connector: We specify the connector to use here. In our case it is the \u0018Alerts \u0014 Parser\u0019action: Each connector can support multiple actions. We specify \u0018epiphani-parse-prometheus-alerts\u0019 actionarguments: We specify connector level arguments here. We have specified a variable argument using Jinja styling. In our case, this variable is retrieved from the actual alert message that triggered this playbook runrules: We use rules to specify match criteria and corresponding actions. This is how we decide which alert handler playbook to invoke based on the alert name/type. In our case, we specify that if the alert name is \u001cPodHighCpuLoad\u001d (match), then take the \u001cgreen\u0019 execution branch (action). One of the downstream nodes/connectors in the \u001cgreen\u001d execution path is \u001cRestart Pod Playbook\u001d node which is the handler for this alert. We also store \u001calert name\u001d, \u001calert message\u001d and \u001calert pod\u001d in variables to be used later.Links:Here we specify how the nodes are linked to each other.we use \u001cfromPort\u001d to identify links when there are multiple links exiting out of the same node. \u001cgreen\u001d link in our example specifies the link that leads to the downstream nodes for handling \u001cPodHighCpuAlert\u001d (see rules above)Alert Handler Playbook:This playbook handles the actual remediation logicAs an example, We use \u001cssh\u001d connector to login to the host from where we can execute kubectl commands.The login credentials are stored in \u001csecure stash\u001d and are retrieved in the backend at execution timeThe corresponding GUI rendering of the playbook is also shownRemediation in actionWe had defined a simple alert rule in Prometheus to trigger an alert when the cpu for nginx container breached a certain thresholdThe playbook engine page shows the playbooks being triggered in response to an alertYou can choose to follow along the live execution of the playbook in the playbook engine web pageResultThe playbook execution results are available in the playbook engine page. You can also retrieve the results via CLI by executing:e3 playbook results \u0014 name AlertRouterCLI17The result output is shown here. For example, the output of \u001cGet-Pod_list\u001d node in the restart handler playbook has been expanded for illustration. This lists the pods after the original nginx pod was re-deployed to remediate the alertThe resulting \u001cenriched\u001d Pagerduty page is shown here. At the end of our remediation playbook, we send a page to the \u001con-call\u001d resourceIn ConclusionYou now have the capability to download and install the Epiphani playbook engine locally. You can also create, execute and capture results using CLI.Hopefully this blog post outlines how easy and more importantly, how powerful the playbook engine can be in closing the Kubernetes alerting loop by auto remediating Kubernetes alertsFeedbackYour feedback is extremely valuable in improving the playbook engine. Kindly provide feedback: feedback@epiphani.ai",
      "publishedDate": "2020-09-25T17:59:03.396Z",
      "description": "In a previous Medium blogpost, I had covered how one can use the Epiphani playbook engine to automate remediation of Kubernetes alerts. … and that’s it! you are in business. As and when alerts arrive…",
      "ogDescription": "Background"
    },
    {
      "url": "https://nirmata.com/2020/07/30/deny-rules-fine-grained-kubernetes-access-controls-with-kyverno/",
      "title": "Deny Rules! Fine-Grained Kubernetes Access Controls with Kyverno",
      "content": "<div class=\"post_content\"> <p><em>Kubernetes policies for dynamic permissions and fine-grained access controls.</em></p>\n<p id=\"dd96\" class=\"ff fg bo fh b fi fj fk fl fm fn fo fp fq fr fs ft fu fv fw fx cg df\">Kubernetes supports role-based access control (RBAC) that enables you to configure a set of permissions for a certain user or group of users, that can access Kubernetes objects in your cluster. This gives you the ability to control API-level resource access for users. But sometimes RBAC is not enough and you may need finer-grained or dynamic access controls. For example, preventing accidental deletion of critical resources. Or, preventing users from modifying or deleting a default network policy, while allowing them to self-manage other network policies. Here is one example of a community&#xFFFD;<a class=\"bh et ha hb hc hd\" href=\"https://github.com/kubernetes/kubernetes/issues/10179\">discussion</a>&#xFFFD;on this topic. Writing a custom admission controller is complicated, in this post, I will show you how Kyverno makes it easy to manage fine-grained access controls as custom policies.</p>\n<p id=\"6df8\" class=\"ff fg bo fh b fi fj fk fl fm fn fo fp fq fr fs ft fu fv fw fx cg df\">Briefly, Kyverno is a policy engine built for Kubernetes. It runs as an admission controller that can mutate and validate incoming resources. Also, Kyverno can generate any type of resource-based on various triggers of the admission request. For a validate policy, you can define the desired behavior of an admission request \u0014 the request will be blocked if you set the failure action to \u001cenforce\u001d, otherwise the policy will just \u001caudit\u001d the result and create policy violations to report non-compliant configurations. With the support of \u001cenforce\u001d action, it is possible to configure a validate policy to deny specific operations on the selected resources, which is what we will focus on for our use case.</p>\n<p id=\"e990\" class=\"ff fg bo fh b fi fj fk fl fm fn fo fp fq fr fs ft fu fv fw fx cg df\">A Kyverno deny policy rule blocks the admission request based on a given set of conditions. Similar to Kubernetes&#xFFFD;<a class=\"bh et ha hb hc hd\" href=\"https://kubernetes.io/docs/concepts/overview/working-with-objects/field-selectors/\">Field Selector</a>, Kyerno lets you select the requests by key-value pairs.</p>\n<p id=\"89e9\" class=\"ff fg bo fh b fi fj fk fl fm fn fo fp fq fr fs ft fu fv fw fx cg df\">The key can be defined with the fixed value, or it can be dynamically configured using JMESPath, i.e., \u001c{{request.operation}}\u001d will be replaced by the actual value of the admission request when applying the policy. The supported operators \u001cIn\u001d \u001cNotIn\u001d \u001cEquals\u001d and \u001cNotEquals\u001d, provide flexibility to write a generic and simple policy.</p>\n<p id=\"374b\" class=\"ff fg bo fh b fi fj fk fl fm fn fo fp fq fr fs ft fu fv fw fx cg df\">Here is an example of the deny policy that denies the UPDATE operation of resources with the label \u001capp=critical\u001d.</p>\n<figure class=\"fz ga gb gc gd ge\"> </figure>\n<p class=\"ff fg bo fh b fi fj fk fl fm fn fo fp fq fr fs ft fu fv fw fx cg df\"><a href=\"https://gist.githubusercontent.com/realshuting/23c506fc8d05ae2e60bffab7d2b48049/raw/1bd74233e10b9e21196462ae6006eabecfd43bce/block-deletion.yaml\"><img class=\"aligncenter wp-image-6837\" src=\"https://3p8owy1gdkoh452nrc36wbnp-wpengine.netdna-ssl.com/wp-content/uploads/2020/07/Kyverno1-1024x763.png\" alt width=\"600\" srcset=\"https://3p8owy1gdkoh452nrc36wbnp-wpengine.netdna-ssl.com/wp-content/uploads/2020/07/Kyverno1-1024x763.png 1024w, https://3p8owy1gdkoh452nrc36wbnp-wpengine.netdna-ssl.com/wp-content/uploads/2020/07/Kyverno1-300x223.png 300w, https://3p8owy1gdkoh452nrc36wbnp-wpengine.netdna-ssl.com/wp-content/uploads/2020/07/Kyverno1-768x572.png 768w, https://3p8owy1gdkoh452nrc36wbnp-wpengine.netdna-ssl.com/wp-content/uploads/2020/07/Kyverno1-700x521.png 700w, https://3p8owy1gdkoh452nrc36wbnp-wpengine.netdna-ssl.com/wp-content/uploads/2020/07/Kyverno1-1100x819.png 1100w, https://3p8owy1gdkoh452nrc36wbnp-wpengine.netdna-ssl.com/wp-content/uploads/2020/07/Kyverno1.png 1356w\" sizes=\"(max-width: 600px) 100vw, 600px\"></a></p>\n<p class=\"ff fg bo fh b fi fj fk fl fm fn fo fp fq fr fs ft fu fv fw fx cg df\">In a Kyverno policy, you can select the target resources based on various types. A policy rule can be filtered by the resource information, i.e., kinds, name, namespaces, and label selectors, just like the example above. Also, a rule can be selected by the user information of the admission request, i.e., clusterRoles, serviceAccounts, users, and groups. This is how you can customize fine-grained access for certain resources.</p>\n<p id=\"c2a4\" class=\"ff fg bo fh b fi fj fk fl fm fn fo fp fq fr fs ft fu fv fw fx cg df\">Assume we have a user \u001ctenant-admin\u001d who can already delete the deployment, with the following deny policy, you can still block the deletion of the deployments with label \u001capp=critical\u001d. The deletion of deployments without this label is allowed.</p>\n<p><a href=\"https://gist.githubusercontent.com/realshuting/23c506fc8d05ae2e60bffab7d2b48049/raw/1bd74233e10b9e21196462ae6006eabecfd43bce/block-deletion.yaml\"><img class=\"aligncenter wp-image-6838\" src=\"https://3p8owy1gdkoh452nrc36wbnp-wpengine.netdna-ssl.com/wp-content/uploads/2020/07/Kyverno2-1024x994.png\" alt width=\"600\" srcset=\"https://3p8owy1gdkoh452nrc36wbnp-wpengine.netdna-ssl.com/wp-content/uploads/2020/07/Kyverno2-1024x994.png 1024w, https://3p8owy1gdkoh452nrc36wbnp-wpengine.netdna-ssl.com/wp-content/uploads/2020/07/Kyverno2-300x291.png 300w, https://3p8owy1gdkoh452nrc36wbnp-wpengine.netdna-ssl.com/wp-content/uploads/2020/07/Kyverno2-768x745.png 768w, https://3p8owy1gdkoh452nrc36wbnp-wpengine.netdna-ssl.com/wp-content/uploads/2020/07/Kyverno2-700x679.png 700w, https://3p8owy1gdkoh452nrc36wbnp-wpengine.netdna-ssl.com/wp-content/uploads/2020/07/Kyverno2-1100x1068.png 1100w, https://3p8owy1gdkoh452nrc36wbnp-wpengine.netdna-ssl.com/wp-content/uploads/2020/07/Kyverno2.png 1358w\" sizes=\"(max-width: 600px) 100vw, 600px\"></a><br>\nTo summarize, the Kvyerno Deny policy helps achieve fine-grained access controls that cannot be performed using Kubernetes RBAC. These rules can be used to prevent critical resources from being deleted or mutated by unauthorized users.</p>\n<p id=\"9e67\" class=\"ff fg bo fh b fi fj fk fl fm fn fo fp fq fr fs ft fu fv fw fx cg df\">Check out the Kyverno&#xFFFD;<a class=\"bh et ha hb hc hd\" href=\"https://github.com/nirmata/kyverno#documentation\">GitHub</a>&#xFFFD;page to learn more about the validation policy and several other features, like generating configurations. Find more in the previous post&#xFFFD;<a class=\"bh et ha hb hc hd\" href=\"https://nirmata.com/2019/07/11/managing-kubernetes-configuration-with-policies/\">Kubernetes Policy Management with Kyverno</a>.</p> </div>",
      "contentAsText": " Kubernetes policies for dynamic permissions and fine-grained access controls.\nKubernetes supports role-based access control (RBAC) that enables you to configure a set of permissions for a certain user or group of users, that can access Kubernetes objects in your cluster. This gives you the ability to control API-level resource access for users. But sometimes RBAC is not enough and you may need finer-grained or dynamic access controls. For example, preventing accidental deletion of critical resources. Or, preventing users from modifying or deleting a default network policy, while allowing them to self-manage other network policies. Here is one example of a community�discussion�on this topic. Writing a custom admission controller is complicated, in this post, I will show you how Kyverno makes it easy to manage fine-grained access controls as custom policies.\nBriefly, Kyverno is a policy engine built for Kubernetes. It runs as an admission controller that can mutate and validate incoming resources. Also, Kyverno can generate any type of resource-based on various triggers of the admission request. For a validate policy, you can define the desired behavior of an admission request \u0014 the request will be blocked if you set the failure action to \u001cenforce\u001d, otherwise the policy will just \u001caudit\u001d the result and create policy violations to report non-compliant configurations. With the support of \u001cenforce\u001d action, it is possible to configure a validate policy to deny specific operations on the selected resources, which is what we will focus on for our use case.\nA Kyverno deny policy rule blocks the admission request based on a given set of conditions. Similar to Kubernetes�Field Selector, Kyerno lets you select the requests by key-value pairs.\nThe key can be defined with the fixed value, or it can be dynamically configured using JMESPath, i.e., \u001c{{request.operation}}\u001d will be replaced by the actual value of the admission request when applying the policy. The supported operators \u001cIn\u001d \u001cNotIn\u001d \u001cEquals\u001d and \u001cNotEquals\u001d, provide flexibility to write a generic and simple policy.\nHere is an example of the deny policy that denies the UPDATE operation of resources with the label \u001capp=critical\u001d.\n \n\nIn a Kyverno policy, you can select the target resources based on various types. A policy rule can be filtered by the resource information, i.e., kinds, name, namespaces, and label selectors, just like the example above. Also, a rule can be selected by the user information of the admission request, i.e., clusterRoles, serviceAccounts, users, and groups. This is how you can customize fine-grained access for certain resources.\nAssume we have a user \u001ctenant-admin\u001d who can already delete the deployment, with the following deny policy, you can still block the deletion of the deployments with label \u001capp=critical\u001d. The deletion of deployments without this label is allowed.\n\nTo summarize, the Kvyerno Deny policy helps achieve fine-grained access controls that cannot be performed using Kubernetes RBAC. These rules can be used to prevent critical resources from being deleted or mutated by unauthorized users.\nCheck out the Kyverno�GitHub�page to learn more about the validation policy and several other features, like generating configurations. Find more in the previous post�Kubernetes Policy Management with Kyverno. ",
      "publishedDate": "2020-07-29T16:00:00.000Z",
      "description": "Kubernetes policies for dynamic permissions and fine-grained access controls."
    },
    {
      "url": "https://humanitec.com/blog/configuration-version-changes-kubernetes-apps",
      "title": "Version changes to configurations for Kubernetes-ready applications",
      "content": "<div href class=\"article__content w-richtext\"><p><strong>\r</strong>Your team likely has a well defined and practised strategy in place for managing changes to a codebase using version control. However, when you use Kubernetes (K8s) to create and manage your application infrastructure, K8s configuration files fast become as important to manage as your code. These configuration files are key to creating, maintaining, scaling, and rolling back the infrastructure that powers your application, and managing them effectively is an essential strategy to have in place.<br>Changes to configuration files might include everything from small changes such as the name of a variable or resource limits, up to major rearchitecting of resources and their relations to each other. These changes can be as significant to your application as changes to the application code and have as much impact on customer experience. Why do we often treat changes to configuration as less important? In this post, I look at tools and practices to help you manage changes to configuration.</p><h2>\r<strong>Use a tool to help</strong></h2><p>While writing and versioning YAML is not the most complex process, it is tedious and prone to introducing errors. One solution is to use a tool that lets you create templates of your configuration which it populates with current values at a time you specify.</p><h3><strong>Helm</strong></h3><p>Considered the \u001cpackage manager for Kubernetes,\u001d <a href=\"https://helm.sh/\">Helm</a> is the most popular option many turn to for templating and versioning a complex application. Helm creates \u001ccharts,\u001d that allow you to define complex K8s applications into one easier to update and version package.</p><p>A chart consist of the following files:</p><ul><li><em>Chart.yaml</em>: Metadata about the chart.</li><li><em>values.yaml</em>: Default configuration values for the chart that you set and update during a build process.</li><li><em>charts/</em>: A directory of other charts (\u001csubcharts\u001d) the chart uses.</li><li><em>templates/</em>: A directory of template YAML configuration files that helm combines with values during a build process.</li></ul><p>This post won\u0019t go into detail on creating a chart (and a lot is possible), <a href=\"https://helm.sh/docs/topics/charts/\">the Helm documentation is the best source</a> for that, but look at the template part of a chart.</p><p><a href=\"https://humanitec.com/blog/deploy-with-kubectl-hands-on-with-kubernetes\">Our post on orchestrating an application with Kubernetes</a> had a <a href=\"https://gist.github.com/ChrisChinchilla/f34f8971a16d839b2b9c5b75f3a0b05b#file-postgres-statefulset-yml\">StatefulSet</a>. Here\u0019s a part of it:</p><p>&lt;p&gt; CODE: https://gist.github.com/ChrisChinchilla/675e6db0a5906141264157abc73abb1e.js &lt;p/&gt;<br></p><p>Say for example that you want to experiment with different versions of the Postgres Docker image between environments.</p><p>The Helm equivalent of the YAML file is the following, with the image as a variable:</p><p>&lt;p&gt; CODE: https://gist.github.com/ChrisChinchilla/7fa8970b162b5a594f6356411f0812c1.js &lt;p/&gt;</p><p>And the <em>Values.yaml</em> file has the associated variable:</p><p><em>postgresVersion</em><strong><em>:</em></strong><em> latest</em></p><p>Whereas <em>Values.yaml</em> for a testing environment (contained in a git branch or tag) has:</p><p><em>postgresVersion</em><strong><em>:</em></strong><em> 9.6.19</em></p><h3><strong>Kustomize</strong></h3><p>With a limited, but more relevant feature set, <a href=\"https://kustomize.io/\">Kustomize</a> is comparable to a specialized diff view for K8s configuration. Kustomize traverses a K8s manifest to add, remove or update configuration options without forking. Now part of kubectl since version 1.14, you can use kustomize to apply patches to configuration files, and create variants of files to match development environments by specifying the differences between common file bases.</p><p>Configuration managed by kustomize needs one <em>base</em> directory that contains a <a href=\"https://kubernetes-sigs.github.io/kustomize/api-reference/glossary/#kustomization\"><em>kustomization.yaml</em> file</a> that has metadata, resources included, and other fields that match kustomize functionality. The <em>base</em> directory also contains the \u001cdefault\u001d K8s resources.</p><p>You also need sub directories to match environments in an <em>overlays</em> folder. All these sub directories need their own <em>kustomize.yaml</em> file (with similar content to the above) and patches that contain instructions on how to change the <em>base</em> resources.</p><p>Confusingly, while kustomize uses the term \u001cpatch,\u001d it does not mean the same as a git patch. Kustomize has two patch formats, and the most commonly used is a \u001c<a href=\"https://kubernetes-sigs.github.io/kustomize/api-reference/glossary/#patchstrategicmerge\">patchStrategicMerge</a>\u001d (or SMP). An SMP contains just enough information to change relevant values, which has the potential to be confusing. But as K8s integrates kustomize better, <a href=\"https://kubernetes.io/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/\">there are solutions</a> to generate the patch files with kubectl.</p><p>Taking the Postgres example above, the overlay patch is:</p><p>&lt;p&gt; CODE: https://gist.github.com/ChrisChinchilla/9b06db8282b26e6dff4558be83ef450a.js &lt;p/&gt;</p><p><em>setElementOrder</em> is <a href=\"https://stupefied-goodall-e282f7.netlify.app/contributors/devel/strategic-merge-patch/#setelementorder-directive\">a directive</a> that specifies the order of a YAML list, which makes it easier for kustomize to apply the patch.</p><p>You then use kubectl apply <em>-k {OVERLAY_FOLDER}</em> to apply the patches.</p><h2><strong>Use a tool suited to the job</strong></h2><p>Whether you use a tool to assist you to generate configuration differences or not, you\u0019re only solving half the problem. You can keep different versions of your environment configuration in different repositories, branches or tags, but this is not how git was designed. Git, and other version control systems (VCS), are designed for handling short-lived changes that will be merged into a longer-living branch at some point. This process does not describe how teams typically handle environments, some of which can exist, in some form, for months, or years.</p><p>Using a traditional VCS to manage environment configuration requires you to duplicate configuration content (and keep those changes in sync). For example, if you used Helm to create a base chart, and change the <em>Values.yaml</em> file to suit each branch or tag, you still need to keep any changes to the base configuration in sync across these branches or tags.</p><p><a href=\"https://humanitec.com/\">Humanitec</a> takes a different approach to managing changes to <a href=\"https://humanitec.com/environment-management\">environment configuration</a>. Configuration is managed alongside the environment that it is intended for. This avoids trying to reconcile merging towards one single source of truth and the fact that there will be a (maybe growing) number of environments on hand.</p><h2><strong>The benefits of separated code and configuration</strong></h2><h3><strong>A different pace of development</strong></h3><p>Your configuration is a fundamental part of your application but moves at a different pace, and changes to each of them are not always related or should trigger pushes to a cluster.</p><h3><strong>Rollback changes</strong></h3><p>Maintaining configuration in a different way to your application means you can roll back to a previously good state of either code or config should you need to, and makes it easier for teams to recreate and restore clusters whenever they need to.</p><h3><strong>Clear connection</strong></h3><p>Using a different tool for managing changes to your environments makes it easier to match changes to a running environment, when the changes are visible in the environment itself.<br></p></div>",
      "contentAsText": "\nYour team likely has a well defined and practised strategy in place for managing changes to a codebase using version control. However, when you use Kubernetes (K8s) to create and manage your application infrastructure, K8s configuration files fast become as important to manage as your code. These configuration files are key to creating, maintaining, scaling, and rolling back the infrastructure that powers your application, and managing them effectively is an essential strategy to have in place.Changes to configuration files might include everything from small changes such as the name of a variable or resource limits, up to major rearchitecting of resources and their relations to each other. These changes can be as significant to your application as changes to the application code and have as much impact on customer experience. Why do we often treat changes to configuration as less important? In this post, I look at tools and practices to help you manage changes to configuration.\nUse a tool to helpWhile writing and versioning YAML is not the most complex process, it is tedious and prone to introducing errors. One solution is to use a tool that lets you create templates of your configuration which it populates with current values at a time you specify.HelmConsidered the \u001cpackage manager for Kubernetes,\u001d Helm is the most popular option many turn to for templating and versioning a complex application. Helm creates \u001ccharts,\u001d that allow you to define complex K8s applications into one easier to update and version package.A chart consist of the following files:Chart.yaml: Metadata about the chart.values.yaml: Default configuration values for the chart that you set and update during a build process.charts/: A directory of other charts (\u001csubcharts\u001d) the chart uses.templates/: A directory of template YAML configuration files that helm combines with values during a build process.This post won\u0019t go into detail on creating a chart (and a lot is possible), the Helm documentation is the best source for that, but look at the template part of a chart.Our post on orchestrating an application with Kubernetes had a StatefulSet. Here\u0019s a part of it:<p> CODE: https://gist.github.com/ChrisChinchilla/675e6db0a5906141264157abc73abb1e.js <p/>Say for example that you want to experiment with different versions of the Postgres Docker image between environments.The Helm equivalent of the YAML file is the following, with the image as a variable:<p> CODE: https://gist.github.com/ChrisChinchilla/7fa8970b162b5a594f6356411f0812c1.js <p/>And the Values.yaml file has the associated variable:postgresVersion: latestWhereas Values.yaml for a testing environment (contained in a git branch or tag) has:postgresVersion: 9.6.19KustomizeWith a limited, but more relevant feature set, Kustomize is comparable to a specialized diff view for K8s configuration. Kustomize traverses a K8s manifest to add, remove or update configuration options without forking. Now part of kubectl since version 1.14, you can use kustomize to apply patches to configuration files, and create variants of files to match development environments by specifying the differences between common file bases.Configuration managed by kustomize needs one base directory that contains a kustomization.yaml file that has metadata, resources included, and other fields that match kustomize functionality. The base directory also contains the \u001cdefault\u001d K8s resources.You also need sub directories to match environments in an overlays folder. All these sub directories need their own kustomize.yaml file (with similar content to the above) and patches that contain instructions on how to change the base resources.Confusingly, while kustomize uses the term \u001cpatch,\u001d it does not mean the same as a git patch. Kustomize has two patch formats, and the most commonly used is a \u001cpatchStrategicMerge\u001d (or SMP). An SMP contains just enough information to change relevant values, which has the potential to be confusing. But as K8s integrates kustomize better, there are solutions to generate the patch files with kubectl.Taking the Postgres example above, the overlay patch is:<p> CODE: https://gist.github.com/ChrisChinchilla/9b06db8282b26e6dff4558be83ef450a.js <p/>setElementOrder is a directive that specifies the order of a YAML list, which makes it easier for kustomize to apply the patch.You then use kubectl apply -k {OVERLAY_FOLDER} to apply the patches.Use a tool suited to the jobWhether you use a tool to assist you to generate configuration differences or not, you\u0019re only solving half the problem. You can keep different versions of your environment configuration in different repositories, branches or tags, but this is not how git was designed. Git, and other version control systems (VCS), are designed for handling short-lived changes that will be merged into a longer-living branch at some point. This process does not describe how teams typically handle environments, some of which can exist, in some form, for months, or years.Using a traditional VCS to manage environment configuration requires you to duplicate configuration content (and keep those changes in sync). For example, if you used Helm to create a base chart, and change the Values.yaml file to suit each branch or tag, you still need to keep any changes to the base configuration in sync across these branches or tags.Humanitec takes a different approach to managing changes to environment configuration. Configuration is managed alongside the environment that it is intended for. This avoids trying to reconcile merging towards one single source of truth and the fact that there will be a (maybe growing) number of environments on hand.The benefits of separated code and configurationA different pace of developmentYour configuration is a fundamental part of your application but moves at a different pace, and changes to each of them are not always related or should trigger pushes to a cluster.Rollback changesMaintaining configuration in a different way to your application means you can roll back to a previously good state of either code or config should you need to, and makes it easier for teams to recreate and restore clusters whenever they need to.Clear connectionUsing a different tool for managing changes to your environments makes it easier to match changes to a running environment, when the changes are visible in the environment itself.",
      "description": "Your Kubernetes configuration represents environments that are a fundamental part of your application, unyet we generally treat them as less important from our application code. In this post we look at best practices for managing changes to configuration, and how to treat it the way it deserves.",
      "ogDescription": "Your Kubernetes configuration represents environments that are a fundamental part of your application, unyet we generally treat them as less important from our application code. In this post we look at best practices for managing changes to configuration, and how to treat it the way it deserves."
    },
    {
      "url": "https://k21academy.com/docker-kubernetes/create-aks-cluster-step-by-step-procedure/?utm_source=reddit&utm_medium=referral&utm_campaign=kubernetes27_sep20_kubernetes",
      "title": "AKS Cluster | How to Create Cluster | Azure Kubernetes Cluster",
      "content": "<div class=\"entry-content\"><p><span id=\"dpsp-post-content-markup\"></span><strong>AKS Cluster</strong> is a Kubernetes cluster, which is created on the <strong>Azure Kubernetes Service (AKS)</strong> by Microsoft is one of the leading managed K8s services. <strong>Kubernetes</strong> is dominating all the containerization techniques that are available in today&#x2019;s world, and there is no match for it. Even though Kubernetes includes a set of various impressive features it requires significant manual configurations, that is where we need thrid-party services.</p>\n<p>Are you new to Kubernetes? Check out our blog post on <a href=\"/docker-kubernetes/kubernetes-for-beginners/\"><strong>Kubernetes for Beginners</strong></a> to know in detail.</p>\n<h2><strong>Azure Kubernetes Service (AKS)</strong></h2>\n<p><strong>Azure Kubernetes Service (AKS)</strong> is a managed Kubernetes service in which the master node is managed by Azure and end-users manages worker nodes. Users can use AKS to deploy, scale, and manage Docker containers and container-based applications across a cluster of container hosts. One of the best parts about <strong>AKS&#xFFFD;</strong>is that you only pay for the worker nodes within your clusters, not for the masters. You can create a cluster in the Azure portal, with the Azure CLI, or template-driven deployment options such as Resource Manager templates and Terraform.</p>\n<p>The AKS cluster can be accessed from a local machine\u0019s terminal to manage Kubernetes components like&#xFFFD;<strong>deployments</strong> and <strong><a href=\"/docker-kubernetes/kubernetes-pods-for-beginners/\">pods</a></strong>. It can even be used to create a Kubernetes deployment.</p>\n<p><em><strong>Note: </strong></em>Know more about <a href=\"/microsoft-azure/az-104/video-containers-docker-kubernetes-in-azure-for-beginners/\"><strong>Azure Kubernetes Service (AKS)</strong></a> and <a href=\"/docker-kubernetes/kubernetes-architecture-components-overview-for-beginners/\"><strong>Kubernetes Architecture</strong></a>.</p>\n<h2><strong>How To Create Azure Kubernetes Cluster</strong></h2>\n<p><strong>Azure Kubernetes Service (AKS)</strong> is a Kubernetes service that lets you quickly deploy and manage master and worker nodes in clusters. Creating an AKS cluster is easy and there are more than enough manuals who will guide you through the process. It is an effortless process to create a cluster by following the steps given below.</p>\n<p>There are 2 ways to deploy an Azure Kubernetes Cluster, which are using:<br>\n<strong>I) </strong>Azure Portal<br>\n<strong>II) </strong>Azure CLI</p>\n<p>Now let us look at how do we create a cluster using Azure Portal. But before all that you should an Azure Cloud account to create a cluster. Create a <em><a href=\"/microsoft-azure/create-free-microsoft-azure-trial-account/\"><strong>FREE TRIAL AZURE ACCOUNT</strong></a></em> to get <strong>$200 free credit</strong> on your Azure portal.</p>\n<p><span><strong>Also read: </strong>Overview of <a href=\"/docker-kubernetes/docker-image-and-layer-overview-for-beginners/\">Docker image</a></span></p>\n<h2 id=\"1\"><strong>Steps To Create AKS Cluster</strong></h2>\n<p>Once you have created an Azure account, or if you already have one, please follow the steps given below in order to create an Azure Kubernetes Cluster easily.</p> <p><strong>Step 1 [Kubernetes Services]: </strong>&#xFFFD;Go to the Azure Portal and search for <strong>Kubernetes Service&#xFFFD;</strong>in the search bar and click on it.</p>\n<p><img id=\"2\" class=\"lazyload alignnone wp-image-44850 size-full\" src=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1920,h_986/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-262.png\" alt=\"Kubernetes service\" width=\"1920\" srcset=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1920/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-262.png 1920w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_300/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-262-300x154.png 300w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1024/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-262-1024x526.png 1024w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1536/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-262-1536x789.png 1536w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_400/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-262-400x205.png 400w\"><img id=\"3\" class=\"lazyload alignnone wp-image-44854 size-full\" src=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1920,h_889/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-265.png\" alt=\"kubernetes cluster\" width=\"1920\" srcset=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1920/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-265.png 1920w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_300/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-265-300x139.png 300w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1024/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-265-1024x474.png 1024w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1536/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-265-1536x711.png 1536w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_400/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-265-400x185.png 400w\"></p>\n<p><strong>Step 2 [Add Kubernetes Cluster]: </strong>Once you reach the Kubernetes Services page, click on <strong>Add&#xFFFD;</strong>and then <strong>Add Kubernetes Cluster</strong>.</p>\n<p><strong>Step 3 [Basics]: </strong>Once you click on add Kubernetes cluster, the next step is to update the specifications of the cluster. So, click on <strong>Basics</strong>.</p>\n<ul>\n<li>Give the&#xFFFD;<strong>Resource Group&#xFFFD;</strong>name as per your requirement.</li>\n<li>Specify a name to your cluster in<strong> the Kubernetes cluster name&#xFFFD;</strong>field.</li>\n<li>Choose a&#xFFFD;<strong>Region</strong> in which you want to create your AKS cluster. In the specified region, our master node will be created.</li>\n<li>Select the&#xFFFD;<strong>Kubernetes Version.&#xFFFD; </strong>Here I am choosing the default, i.e., <strong>1.17.9</strong></li>\n</ul>\n<p><img class=\"lazyload alignnone wp-image-44858 size-full\" src=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1482,h_898/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-267.png\" alt=\"Kubernetes custer\" width=\"1482\" srcset=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1482/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-267.png 1482w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_300/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-267-300x182.png 300w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1024/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-267-1024x620.png 1024w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_400/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-267-400x242.png 400w\"></p>\n<p>Next, comes the size and count of the nodes of the AKS cluster that we are gonna create. These can be updated as per the requirements.</p>\n<ul>\n<li>Select the&#xFFFD;<strong>Node Size.&#xFFFD;</strong>We are choosing&#xFFFD;<strong>Standard Ds2 v2&#xFFFD;</strong>which has the following configuration: 2 vCPUs, 7 GiB RAM, 8 Data Disks, 14 GiB Temp Storage.</li>\n<li>Give the&#xFFFD;<strong>Node Count</strong> value which specifies how many Worker Nodes we want.</li>\n</ul>\n<p><img id=\"4\" class=\"lazyload alignnone wp-image-44862 size-full\" src=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1421,h_891/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-274.png\" alt=\"Kubernetes cluster\" width=\"1421\" srcset=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1421/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-274.png 1421w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_300/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-274-300x188.png 300w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1024/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-274-1024x642.png 1024w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_400/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-274-400x251.png 400w\"></p>\n<p><strong>Step 4 [Node Pools]: </strong>Next comes the <strong>Node Pools</strong>, follow the steps given below:</p>\n<ul>\n<li>The&#xFFFD;<strong>Virtual nodes&#xFFFD;</strong>are a type of Serverless container instance. As we want to create the Worker nodes as Virtual Machines, so we will&#xFFFD;<strong>Disable</strong> this option.</li>\n<li>The&#xFFFD;<strong>VM scale sets&#xFFFD;</strong>provide an auto-scaling option. Hence, we will keep it <strong>Enabled</strong><strong>.</strong></li>\n</ul>\n<p><img id=\"5\" class=\"lazyload alignnone wp-image-44879 size-full\" src=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1152,h_646/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-283.png\" alt=\"Kubernetes cluster\" width=\"1152\" srcset=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1152/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-283.png 1152w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_300/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-283-300x168.png 300w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1024/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-283-1024x574.png 1024w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_400/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-283-400x224.png 400w\"></p>\n<p><strong>Step 5 [Authentication]:</strong> Next is to click on <strong>Authentication</strong>.</p>\n<ul>\n<li>Choose the<strong> System-assigned managed identity.</strong></li>\n<li>If you want to go for <strong>Role-based Access Control </strong>(<strong>RBAC)</strong> then select <strong>Enabled.</strong></li>\n<li>Choose the&#xFFFD;<strong>Encryption Type</strong> of your choice, I will use the Default one.</li>\n</ul>\n<p><img id=\"6\" class=\"lazyload alignnone wp-image-44882 size-full\" src=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1149,h_623/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-284.png\" alt=\"Kubernetes cluster\" width=\"1149\" srcset=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1149/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-284.png 1149w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_300/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-284-300x163.png 300w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1024/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-284-1024x555.png 1024w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_400/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-284-400x217.png 400w\"></p>\n<p><strong>Step 6 [Networking]:</strong> Next is the<strong> Networking&#xFFFD;</strong>part.</p>\n<ul>\n<li>Select the <strong>Network Configuration.&#xFFFD;</strong>Basic will create a new Vnet using basic values. The advanced networking option allows us to create a new Vnet with customizable addresses. So I will be choosing <strong>Advanced</strong>.</li>\n<li>The <strong>Cluster Subnet&#xFFFD;</strong>option is to choose which Subnet you want the Nodes and Containers to be placed in.</li>\n<li><strong>Kubernetes service address range </strong>is the CIDR notation IP range from which to assign server cluster IPs.</li>\n<li><strong>Docker Bridge address&#xFFFD;</strong>is the IP address assigned to Docker Bridge. The Bridge Network is for the container to container communication.</li>\n</ul>\n<p><img class=\"lazyload alignnone wp-image-44886 size-full\" src=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1161,h_543/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-285.png\" alt=\"Kubernetes cluster\" width=\"1161\" srcset=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1161/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-285.png 1161w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_300/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-285-300x140.png 300w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1024/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-285-1024x479.png 1024w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_400/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-285-400x187.png 400w\"></p>\n<ul>\n<li>In <strong>Private Cluster</strong>, the communication between the nodes and the API server happens internally.</li>\n<li>So, I am <strong>Disabling&#xFFFD;</strong>the Private Cluster.</li>\n<li>Keep the Network Policy to&#xFFFD;<strong>None</strong>.</li>\n<li><strong>HTTP&#xFFFD;</strong>application routing to&#xFFFD;<strong>No</strong>.</li>\n</ul>\n<p><img id=\"7\" class=\"lazyload alignnone wp-image-44888 size-full\" src=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1141,h_601/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-286.png\" alt=\"Kubernetes cluster\" width=\"1141\" srcset=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1141/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-286.png 1141w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_300/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-286-300x158.png 300w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1024/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-286-1024x539.png 1024w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_400/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-286-400x211.png 400w\"></p>\n<p><strong>Step 7 [Integration]: </strong>Next is the <strong>Integration.</strong></p>\n<p>Here we keep all settings to default and move to the next step.</p>\n<p><img id=\"8\" class=\"lazyload alignnone wp-image-44890 size-full\" src=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1421,h_888/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-273.png\" alt=\"Kubernetes cluster\" width=\"1421\" srcset=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1421/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-273.png 1421w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_300/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-273-300x187.png 300w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1024/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-273-1024x640.png 1024w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_400/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-273-400x250.png 400w\"></p>\n<p><strong>Step 8 [Review &amp; Create]: </strong>The final step is to click on <strong>Review &amp; Create.</strong> If you click on <strong>Create</strong>, it will first Validate your&#xFFFD; AKS Cluster and if everything is fine then the cluster will be created.</p>\n<p><img class=\"lazyload alignnone wp-image-44896 size-full\" src=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_987,h_643/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-288.png\" alt=\"Kubernetes cluster\" width=\"987\" srcset=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_987/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-288.png 987w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_300/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-288-300x195.png 300w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_370/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-288-370x240.png 370w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_400/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-288-400x261.png 400w\"></p>\n<p>You can see that our new Azure Kubernetes cluster has been successfully created. Yayyy! But hold on, we will have to connect this cluster, which I am going to cover next.</p>\n<p><img class=\"lazyload wp-image-44904 aligncenter\" src=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1269,h_587/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-289.png\" alt=\"AKS\" width=\"1269\" srcset=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1906/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-289.png 1906w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_300/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-289-300x139.png 300w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1024/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-289-1024x474.png 1024w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1536/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-289-1536x711.png 1536w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_400/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-289-400x185.png 400w\"></p>\n<p>To view the cluster, go to <strong>Kubernetes services</strong> and there you can access the AKS cluster.</p>\n<p><img class=\"lazyload alignnone wp-image-44906 size-full\" src=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1920,h_884/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-279.png\" alt=\"AKS cluster\" width=\"1920\" srcset=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1920/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-279.png 1920w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_300/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-279-300x138.png 300w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1024/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-279-1024x471.png 1024w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1536/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-279-1536x707.png 1536w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_400/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-279-400x184.png 400w\"></p>\n<p><span><strong>Also check:</strong> All you need to know on <a href=\"/docker-kubernetes/rbac-role-based-access-control/\">Role Based Access Control</a></span></p>\n<h2><strong>Connect to the Azure Kubernetes Cluster</strong></h2>\n<p>There are two ways to connect the AKS cluster:<br>\n<strong>I) </strong>Using Cloud Shell<br>\n<strong>II) </strong>Using Azure CLI</p>\n<p>I am going to cover how do we connect using <strong>Cloud Shell</strong>. We can see the option on top of our screen (marked red in the below image).</p>\n<p><img class=\"lazyload alignnone wp-image-44912 size-full\" src=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1920,h_980/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-280.png\" alt=\"Cloud shell\" width=\"1920\" srcset=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1920/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-280.png 1920w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_300/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-280-300x153.png 300w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1024/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-280-1024x523.png 1024w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1536/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-280-1536x784.png 1536w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_400/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-280-400x204.png 400w\"></p>\n<p><strong>Step 1: </strong>Run the following command, on the Azure bash shell:</p>\n<div>\n<pre>   \r\n  $ az aks get-credentials --resource-groups &lt;name of resource group&gt; --name &lt;name of cluster\r\n\r\n</pre>\n</div>\n<p><img class=\"lazyload alignnone wp-image-44913 size-full\" src=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1913,h_890/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-289-1.png\" alt=\"Cloud shell\" width=\"1913\" srcset=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1913/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-289-1.png 1913w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_300/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-289-1-300x140.png 300w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1024/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-289-1-1024x476.png 1024w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1536/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-289-1-1536x715.png 1536w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_400/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-289-1-400x186.png 400w\"></p>\n<p><strong>Step 2: </strong>To get the Nodes running in our cluster, run the following command, and you will see all the nodes in your AKS cluster.</p> <p><img class=\"lazyload alignnone wp-image-44914 size-full\" src=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1920,h_883/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-290.png\" alt=\"cloud shell\" width=\"1920\" srcset=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1920/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-290.png 1920w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_300/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-290-300x138.png 300w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1024/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-290-1024x471.png 1024w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1536/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-290-1536x706.png 1536w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_400/https://k21academy.com/wp-content/uploads/2020/09/Screenshot-290-400x184.png 400w\"></p>\n<h2><strong>Summary</strong></h2>\n<p>If you want to be successful in orchestrating containers, Kubernetes is the way to go. If you have followed the above steps you will have an AKS cluster up and running. But, keep a watch on what Microsoft is doing with AKS and Azure. AKS is continuously in development and new features are added almost every day. Super cool new features are coming soon too. Microsoft is fully dedicated to Kubernetes!</p>\n<p><strong>Find out </strong>about what is <a href=\"/docker-kubernetes/labels-and-annotations-in-kubernetes/\">Kubernetes Label</a> here.</p>\n<h2><strong>Related Post</strong></h2> <h2><strong>Join FREE Class</strong></h2>\n<p>We cover Azure Kubernetes Service (AKS) as a bonus in our <strong><a href=\"/docker-kubernetes-certified-administrator-cka-training/\">Certified Kubernetes Administrator (CKA) training</a></strong> program. To know about what is the <strong>Roles and Responsibilities of Kubernetes administrator</strong>, why you should&#xFFFD;learn Docker and Kubernetes,&#xFFFD;Job opportunities for Kubernetes administrator&#xFFFD;in the market, and what to study Including&#xFFFD;<strong>Hands-On labs&#xFFFD;</strong>you must perform to clear<strong> the Certified Kubernetes Administrator (CKA)&#xFFFD;</strong>certification exam by registering for our&#xFFFD;<strong>FREE Masterclass. </strong></p>\n<p><strong>Click on the below image</strong> to Register for our <a href=\"/free-masterclass-dockers-kubernetes-administrations-with-certifications/?utm_source=blog_content_upgrade&amp;utm_medium=referral&amp;utm_campaign=kubernetes02_sep20\"><strong>FREE Masterclass</strong></a> Now!</p>\n<p><a href=\"/free-masterclass-dockers-kubernetes-administrations-with-certifications/?utm_source=blog_content_upgrade&amp;utm_medium=referral&amp;utm_campaign=kubernetes02_sep20\"><img class=\"lazyload aligncenter wp-image-35666\" src=\"https://1valoz20c7b3s41ocwkpz13z-wpengine.netdna-ssl.com/wp-content/uploads/2020/05/DK_ContentUpgradeGIF-2.gif\" alt=\"Free Masterclass\" width=\"1544\"></a></p> </div>",
      "contentAsText": "AKS Cluster is a Kubernetes cluster, which is created on the Azure Kubernetes Service (AKS) by Microsoft is one of the leading managed K8s services. Kubernetes is dominating all the containerization techniques that are available in today’s world, and there is no match for it. Even though Kubernetes includes a set of various impressive features it requires significant manual configurations, that is where we need thrid-party services.\nAre you new to Kubernetes? Check out our blog post on Kubernetes for Beginners to know in detail.\nAzure Kubernetes Service (AKS)\nAzure Kubernetes Service (AKS) is a managed Kubernetes service in which the master node is managed by Azure and end-users manages worker nodes. Users can use AKS to deploy, scale, and manage Docker containers and container-based applications across a cluster of container hosts. One of the best parts about AKS�is that you only pay for the worker nodes within your clusters, not for the masters. You can create a cluster in the Azure portal, with the Azure CLI, or template-driven deployment options such as Resource Manager templates and Terraform.\nThe AKS cluster can be accessed from a local machine\u0019s terminal to manage Kubernetes components like�deployments and pods. It can even be used to create a Kubernetes deployment.\nNote: Know more about Azure Kubernetes Service (AKS) and Kubernetes Architecture.\nHow To Create Azure Kubernetes Cluster\nAzure Kubernetes Service (AKS) is a Kubernetes service that lets you quickly deploy and manage master and worker nodes in clusters. Creating an AKS cluster is easy and there are more than enough manuals who will guide you through the process. It is an effortless process to create a cluster by following the steps given below.\nThere are 2 ways to deploy an Azure Kubernetes Cluster, which are using:\nI) Azure Portal\nII) Azure CLI\nNow let us look at how do we create a cluster using Azure Portal. But before all that you should an Azure Cloud account to create a cluster. Create a FREE TRIAL AZURE ACCOUNT to get $200 free credit on your Azure portal.\nAlso read: Overview of Docker image\nSteps To Create AKS Cluster\nOnce you have created an Azure account, or if you already have one, please follow the steps given below in order to create an Azure Kubernetes Cluster easily. Step 1 [Kubernetes Services]: �Go to the Azure Portal and search for Kubernetes Service�in the search bar and click on it.\n\nStep 2 [Add Kubernetes Cluster]: Once you reach the Kubernetes Services page, click on Add�and then Add Kubernetes Cluster.\nStep 3 [Basics]: Once you click on add Kubernetes cluster, the next step is to update the specifications of the cluster. So, click on Basics.\n\nGive the�Resource Group�name as per your requirement.\nSpecify a name to your cluster in the Kubernetes cluster name�field.\nChoose a�Region in which you want to create your AKS cluster. In the specified region, our master node will be created.\nSelect the�Kubernetes Version.� Here I am choosing the default, i.e., 1.17.9\n\n\nNext, comes the size and count of the nodes of the AKS cluster that we are gonna create. These can be updated as per the requirements.\n\nSelect the�Node Size.�We are choosing�Standard Ds2 v2�which has the following configuration: 2 vCPUs, 7 GiB RAM, 8 Data Disks, 14 GiB Temp Storage.\nGive the�Node Count value which specifies how many Worker Nodes we want.\n\n\nStep 4 [Node Pools]: Next comes the Node Pools, follow the steps given below:\n\nThe�Virtual nodes�are a type of Serverless container instance. As we want to create the Worker nodes as Virtual Machines, so we will�Disable this option.\nThe�VM scale sets�provide an auto-scaling option. Hence, we will keep it Enabled.\n\n\nStep 5 [Authentication]: Next is to click on Authentication.\n\nChoose the System-assigned managed identity.\nIf you want to go for Role-based Access Control (RBAC) then select Enabled.\nChoose the�Encryption Type of your choice, I will use the Default one.\n\n\nStep 6 [Networking]: Next is the Networking�part.\n\nSelect the Network Configuration.�Basic will create a new Vnet using basic values. The advanced networking option allows us to create a new Vnet with customizable addresses. So I will be choosing Advanced.\nThe Cluster Subnet�option is to choose which Subnet you want the Nodes and Containers to be placed in.\nKubernetes service address range is the CIDR notation IP range from which to assign server cluster IPs.\nDocker Bridge address�is the IP address assigned to Docker Bridge. The Bridge Network is for the container to container communication.\n\n\n\nIn Private Cluster, the communication between the nodes and the API server happens internally.\nSo, I am Disabling�the Private Cluster.\nKeep the Network Policy to�None.\nHTTP�application routing to�No.\n\n\nStep 7 [Integration]: Next is the Integration.\nHere we keep all settings to default and move to the next step.\n\nStep 8 [Review & Create]: The final step is to click on Review & Create. If you click on Create, it will first Validate your� AKS Cluster and if everything is fine then the cluster will be created.\n\nYou can see that our new Azure Kubernetes cluster has been successfully created. Yayyy! But hold on, we will have to connect this cluster, which I am going to cover next.\n\nTo view the cluster, go to Kubernetes services and there you can access the AKS cluster.\n\nAlso check: All you need to know on Role Based Access Control\nConnect to the Azure Kubernetes Cluster\nThere are two ways to connect the AKS cluster:\nI) Using Cloud Shell\nII) Using Azure CLI\nI am going to cover how do we connect using Cloud Shell. We can see the option on top of our screen (marked red in the below image).\n\nStep 1: Run the following command, on the Azure bash shell:\n\n   \n  $ az aks get-credentials --resource-groups <name of resource group> --name <name of cluster\n\n\n\n\nStep 2: To get the Nodes running in our cluster, run the following command, and you will see all the nodes in your AKS cluster. \nSummary\nIf you want to be successful in orchestrating containers, Kubernetes is the way to go. If you have followed the above steps you will have an AKS cluster up and running. But, keep a watch on what Microsoft is doing with AKS and Azure. AKS is continuously in development and new features are added almost every day. Super cool new features are coming soon too. Microsoft is fully dedicated to Kubernetes!\nFind out about what is Kubernetes Label here.\nRelated Post Join FREE Class\nWe cover Azure Kubernetes Service (AKS) as a bonus in our Certified Kubernetes Administrator (CKA) training program. To know about what is the Roles and Responsibilities of Kubernetes administrator, why you should�learn Docker and Kubernetes,�Job opportunities for Kubernetes administrator�in the market, and what to study Including�Hands-On labs�you must perform to clear the Certified Kubernetes Administrator (CKA)�certification exam by registering for our�FREE Masterclass. \nClick on the below image to Register for our FREE Masterclass Now!\n ",
      "publishedDate": "2020-09-23T14:25:58.000Z",
      "description": "AKS Cluster can be easily created, AKS is one of the most pioneers of managed K8s. This blogs covers how to create a Azure Kubernetes Cluster in detail.",
      "ogDescription": "AKS Cluster can be easily created, AKS is one of the most pioneers of managed K8s. This blogs covers how to create a Azure Kubernetes Cluster in detail."
    },
    {
      "url": "https://www.reddit.com/r/kubernetes/comments/izfkjx/k8s_hosting_comparison_looking_for_a_cheap_sane/",
      "title": ""
    },
    {
      "url": "https://determined.ai/blog/determined-on-kubernetes/",
      "title": "Determined Now Supports Kubernetes!",
      "content": "<div class=\"post-container\">\n<p><a href=\"https://github.com/determined-ai/determined\">Determined</a> is an open-source deep\nlearning training platform that makes building models fast and easy. We believe\ndeep learning practitioners should have access to a great model development\nenvironment that lets them focus on deep learning, regardless of whether they\nare using on-demand cloud VMs, spot or preemptible instances, or an on-premise\nGPU cluster.</p>\n<p>Today, we\u0019re excited to announce that we\u0019re expanding this vision by adding\n<strong>native support for Kubernetes</strong> to the Determined training platform!</p>\n<p><img src=\"/assets/images/posts/determined-and-k8s.png\" alt=\"Determined now includes native support for Kubernetes.\"></p>\n<h2 id=\"why-use-kubernetes-for-deep-learning\">Why Use Kubernetes for Deep Learning?</h2>\n<p>Deep learning requires massive computational resources. As your deep learning\nefforts go beyond a single GPU, a cluster scheduler is essential because it\nallows you to utilize all of your GPUs on the deep learning tasks that are the\nmost important at any given time. Realizing this, Determined has always included\na container scheduler; moreover, Determined\u0019s scheduler is designed to support\nthe unique requirements of common deep learning workloads like distributed\ntraining and hyperparameter tuning.</p>\n<p>When we set out to build Determined more than three years ago, we found that\n<em>most teams applying deep learning were not using Kubernetes to manage their\nGPUs!</em> While in some cases they had a Kubernetes cluster somewhere in their\norganization, their GPU resources were often managed separately \u0014 in part\nbecause support for GPUs in Kubernetes was still in alpha at the time. Adopting\nKubernetes also requires dealing with a fair bit of complexity, which can be\npremature for small deep learning clusters with a handful of multi-GPU\nnodes. Based on those considerations, we built our own cluster scheduler. That\nscheduler is in active production use today, managing thousands of GPUs on AWS,\nGCP, and on-premise clusters.</p>\n<p>Infrastructure for deep learning is increasingly moving from an R&amp;D project to a\ncore part of the production environment at many companies. Over time, GPU\nsupport in Kubernetes has matured and we\u0019ve seen more interest from customers\nand community members about using Determined with Kubernetes-managed\nresources. For platform teams that are comfortable using Kubernetes to manage\ntheir infrastructure, there are significant wins from being able to manage and\nmonitor deep learning workloads using the same tools they use for other compute\ntasks.</p>\n<h2 id=\"determined-on-kubernetes\">Determined on Kubernetes</h2>\n<p>Determined on Kubernetes works by scheduling Determined workloads, such as\nmodel training and hyperparameter tuning jobs, as a collection of Kubernetes\npods. That means that native Kubernetes tools for logging, metrics, and tracing\nwill work as expected. <a href=\"https://docs.determined.ai/latest/how-to/installation/kubernetes.html\">Determined can be easily installed onto a Kubernetes\ncluster using Helm</a>,\nand is compatible with managed Kubernetes offerings like Google Kubernetes\nEngine (GKE) and AWS Elastic Kubernetes Service (EKS). Using GKE or EKS with\nDetermined means you can use Kubernetes\u0019 <a href=\"https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/\">cluster\nautoscaler</a>\nto dynamically scale your compute resources as new deep learning jobs are\nsubmitted.</p>\n<p>All of Determined\u0019s features work out-of-the-box on Kubernetes (with a few\ncaveats noted below). Just as important, <em>using Determined with Kubernetes\ndoesn\u0019t mean imposing complexity on deep learning engineers</em>: users who just\nwant to develop DL models don\u0019t need to worry about the details of the\nunderlying infrastructure to use Determined on Kubernetes. With Determined,\nlaunching a new workload doesn\u0019t require writing Kubernetes YAML files or\nrunning <code class=\"language-plaintext highlighter-rouge\">kubectl</code>, and DL engineers can understand the progress and performance\nof their training tasks using Determined\u0019s intuitive WebUI, rather than needing\nto track the status of a collection of pods or containers.</p>\n<p>For those users who would like to customize the behavior of the pods used to run\ntheir deep learning workloads, a <a href=\"https://docs.determined.ai/latest/topic-guides/custom-pod-specs.html\">custom pod\nspec</a> can\nbe configured on either a per-cluster or per-task level. This feature makes it\neasy to support requirements like assigning tasks to specific nodes or attaching\nadditional volume mounts. Custom pod specs can also be used to seamlessly run\nworkloads on spot instances on AWS! <a href=\"https://docs.determined.ai/latest/reference/experiment-config.html#max-restarts\">Determined\u0019s built-in fault tolerance\ncapabilities</a>\nensure that any pods that are terminated abruptly will be restarted without\ndisrupting the progress of any deep learning training jobs.</p>\n<h2 id=\"ml-ecosystem-integration\">ML Ecosystem Integration</h2>\n<p>Determined is a deep learning training platform: we\u0019re focused on making it\nsimple to develop and train high-quality models in less time. Determined is also\ndesigned to integrate smoothly with best-of-breed tools for standard ML tasks\nsuch as <a href=\"https://www.pachyderm.com/\">Pachyderm</a> for data preparation, <a href=\"https://github.com/determined-ai/works-with-determined/blob/master/kubeflow_pipelines/README.md\">Kubeflow\nPipelines</a>\nfor ML workflows, and <a href=\"http://seldon.io/\">Seldon Core</a> for model serving. If\nyou\u0019re already using Kubernetes to manage the other ML tools in your stack,\nDetermined on Kubernetes enables your entire deep learning infrastructure to be\nmanaged as a unified whole, simplifying operations and increasing agility.</p>\n<p>In a recent <a href=\"/blog/production-training-pipelines-with-determined-and-kubeflow/\">blog post</a>,\nwe showed how Determined can be used with Kubeflow Pipelines and Seldon, and the\n<a href=\"https://github.com/determined-ai/works-with-determined\">works-with-determined</a>\nrepository showcases many more examples of how Determined can be integrated with\nother popular open-source ML tools. If you\u0019d like to see how Determined can work\nwith your favorite ML tools, <a href=\"/contact/\">get in touch with us</a>!</p>\n<h2 id=\"status-and-roadmap\">Status and Roadmap</h2>\n<p>Support for Kubernetes is included in <a href=\"https://docs.determined.ai/latest/release-notes.html#version-0-13-2\">Determined\n0.13.2</a> and\nis compatible with Kubernetes &gt;= 1.15.</p>\n<p>Although most of Determined\u0019s functionality works seamlessly on Kubernetes, a\nfew features are currently not supported. In particular, Determined on\nKubernetes does not currently support the scheduling policies that are available\nwhen deploying Determined on VMs, including priority scheduling, fair sharing of\nresources across experiments, and gang-scheduling for distributed deep learning\njobs. Determined relies on Kubernetes to handle pod scheduling and the default\nKubernetes scheduler does not support these policies. We have plans to address\nthis in the future!</p>\n<h2 id=\"try-it-out\">Try It Out</h2>\n<p>Determined on Kubernetes can be installed via Helm; for more details, check out\nthe <a href=\"https://docs.determined.ai/latest/how-to/installation/kubernetes.html\">installation\ninstructions</a>. For\ngeneral information about using Determined with Kubernetes, check out the\n<a href=\"https://docs.determined.ai/latest/topic-guides/determined-on-kubernetes.html\">documentation</a>.</p>\n<p>We would love your feedback on Determined in general, and feedback on our\nsupport for Kubernetes would be particularly welcome! If you run into any issues\nor have suggestions, we\u0019d love to hear from you! Please <a href=\"https://github.com/determined-ai/determined\">file an issue on\nGitHub</a> or <a href=\"https://join.slack.com/t/determined-community/shared_invite/zt-cnj7802v-KcVbaUrIzQOwmkmY7gP0Ew\">join our Slack\ncommunity</a>.</p>\n</div>",
      "contentAsText": "\nDetermined is an open-source deep\nlearning training platform that makes building models fast and easy. We believe\ndeep learning practitioners should have access to a great model development\nenvironment that lets them focus on deep learning, regardless of whether they\nare using on-demand cloud VMs, spot or preemptible instances, or an on-premise\nGPU cluster.\nToday, we\u0019re excited to announce that we\u0019re expanding this vision by adding\nnative support for Kubernetes to the Determined training platform!\n\nWhy Use Kubernetes for Deep Learning?\nDeep learning requires massive computational resources. As your deep learning\nefforts go beyond a single GPU, a cluster scheduler is essential because it\nallows you to utilize all of your GPUs on the deep learning tasks that are the\nmost important at any given time. Realizing this, Determined has always included\na container scheduler; moreover, Determined\u0019s scheduler is designed to support\nthe unique requirements of common deep learning workloads like distributed\ntraining and hyperparameter tuning.\nWhen we set out to build Determined more than three years ago, we found that\nmost teams applying deep learning were not using Kubernetes to manage their\nGPUs! While in some cases they had a Kubernetes cluster somewhere in their\norganization, their GPU resources were often managed separately \u0014 in part\nbecause support for GPUs in Kubernetes was still in alpha at the time. Adopting\nKubernetes also requires dealing with a fair bit of complexity, which can be\npremature for small deep learning clusters with a handful of multi-GPU\nnodes. Based on those considerations, we built our own cluster scheduler. That\nscheduler is in active production use today, managing thousands of GPUs on AWS,\nGCP, and on-premise clusters.\nInfrastructure for deep learning is increasingly moving from an R&D project to a\ncore part of the production environment at many companies. Over time, GPU\nsupport in Kubernetes has matured and we\u0019ve seen more interest from customers\nand community members about using Determined with Kubernetes-managed\nresources. For platform teams that are comfortable using Kubernetes to manage\ntheir infrastructure, there are significant wins from being able to manage and\nmonitor deep learning workloads using the same tools they use for other compute\ntasks.\nDetermined on Kubernetes\nDetermined on Kubernetes works by scheduling Determined workloads, such as\nmodel training and hyperparameter tuning jobs, as a collection of Kubernetes\npods. That means that native Kubernetes tools for logging, metrics, and tracing\nwill work as expected. Determined can be easily installed onto a Kubernetes\ncluster using Helm,\nand is compatible with managed Kubernetes offerings like Google Kubernetes\nEngine (GKE) and AWS Elastic Kubernetes Service (EKS). Using GKE or EKS with\nDetermined means you can use Kubernetes\u0019 cluster\nautoscaler\nto dynamically scale your compute resources as new deep learning jobs are\nsubmitted.\nAll of Determined\u0019s features work out-of-the-box on Kubernetes (with a few\ncaveats noted below). Just as important, using Determined with Kubernetes\ndoesn\u0019t mean imposing complexity on deep learning engineers: users who just\nwant to develop DL models don\u0019t need to worry about the details of the\nunderlying infrastructure to use Determined on Kubernetes. With Determined,\nlaunching a new workload doesn\u0019t require writing Kubernetes YAML files or\nrunning kubectl, and DL engineers can understand the progress and performance\nof their training tasks using Determined\u0019s intuitive WebUI, rather than needing\nto track the status of a collection of pods or containers.\nFor those users who would like to customize the behavior of the pods used to run\ntheir deep learning workloads, a custom pod\nspec can\nbe configured on either a per-cluster or per-task level. This feature makes it\neasy to support requirements like assigning tasks to specific nodes or attaching\nadditional volume mounts. Custom pod specs can also be used to seamlessly run\nworkloads on spot instances on AWS! Determined\u0019s built-in fault tolerance\ncapabilities\nensure that any pods that are terminated abruptly will be restarted without\ndisrupting the progress of any deep learning training jobs.\nML Ecosystem Integration\nDetermined is a deep learning training platform: we\u0019re focused on making it\nsimple to develop and train high-quality models in less time. Determined is also\ndesigned to integrate smoothly with best-of-breed tools for standard ML tasks\nsuch as Pachyderm for data preparation, Kubeflow\nPipelines\nfor ML workflows, and Seldon Core for model serving. If\nyou\u0019re already using Kubernetes to manage the other ML tools in your stack,\nDetermined on Kubernetes enables your entire deep learning infrastructure to be\nmanaged as a unified whole, simplifying operations and increasing agility.\nIn a recent blog post,\nwe showed how Determined can be used with Kubeflow Pipelines and Seldon, and the\nworks-with-determined\nrepository showcases many more examples of how Determined can be integrated with\nother popular open-source ML tools. If you\u0019d like to see how Determined can work\nwith your favorite ML tools, get in touch with us!\nStatus and Roadmap\nSupport for Kubernetes is included in Determined\n0.13.2 and\nis compatible with Kubernetes >= 1.15.\nAlthough most of Determined\u0019s functionality works seamlessly on Kubernetes, a\nfew features are currently not supported. In particular, Determined on\nKubernetes does not currently support the scheduling policies that are available\nwhen deploying Determined on VMs, including priority scheduling, fair sharing of\nresources across experiments, and gang-scheduling for distributed deep learning\njobs. Determined relies on Kubernetes to handle pod scheduling and the default\nKubernetes scheduler does not support these policies. We have plans to address\nthis in the future!\nTry It Out\nDetermined on Kubernetes can be installed via Helm; for more details, check out\nthe installation\ninstructions. For\ngeneral information about using Determined with Kubernetes, check out the\ndocumentation.\nWe would love your feedback on Determined in general, and feedback on our\nsupport for Kubernetes would be particularly welcome! If you run into any issues\nor have suggestions, we\u0019d love to hear from you! Please file an issue on\nGitHub or join our Slack\ncommunity.\n",
      "publishedDate": "2020-09-24T00:00:00.000Z",
      "description": "The Determined deep learning training platform now runs natively on Kubernetes, providing a simpler way to manage on-prem and cloud GPU resources.",
      "ogDescription": "The Determined deep learning training platform now runs natively on Kubernetes, providing a simpler way to manage on-prem and cloud GPU resources."
    },
    {
      "url": "https://www.nrmitchi.com/2020/09/non-terminating-namespace-removal/?utm_source=reddit&utm_medium=r%2Fkubernetes&utm_campaign=direct-post",
      "title": "Non-Terminating Namespace Removal",
      "content": "<section id=\"post-body\"> <p>Namespaces are a core component of the Kubernetes landscape which are often used as as a base level of resource isolation. As a resource which contains multiple others, the shutdown behaviour associated with terminating namespaces is complex. In effect, namespaces can often get &#x201C;stuck&#x201D; in a <code>Terminating</code> state. I ran into this recently with a variety of namespaces in a Digital Ocean Kubernetes cluster.</p> <p>Much advice on the internet will recommend editing the namespace to remove any <code>finalizers</code> on the object. This may work in many distributions, however was not an option for me. It appears that Digitalocean has protections against this (likely in the form of a MutatingAdmissionController) which dynamically added the finalizer back. Due to this, I had to track down the finalizer that was failing, and determine how to correct it.</p> <blockquote>\n<p>Note: A <code>finalizer</code> is a function which acts to enforce cleanup and/or deletion of related resources, or take other action when a resource is deleted.</p>\n</blockquote> <p>As a part of debugging this, your first step should be looking at the current state of the namespace. Although I&#x2019;m sure there is a better way, I used <code>kubectl edit ns &lt;name&gt;</code>. This quickly confirmed that the namespace termination was stuck on the <code>finalizer</code>, and from the error it appears that the finalizer was failing to list custom metrics. At ths time this felt unrelated, but becomes clear when you think about what must be cleaned up when a namespace is deleted.</p> <p>When a namespace is removed, the finalizer must ensure that all resources that it contains are removed. While a namespace still has resources within it, it is in the <code>Terminating</code> state. You can check what resources are in a namespace with this snippet:</p> <pre><code>kubectl api-resources --verbs=list --namespaced -o name \\\n  | xargs -n 1 kubectl get --show-kind --ignore-not-found -n &lt;namespace&gt;\n</code></pre> <blockquote>\n<p>Note that this is much more encompassing than <code>kubectl get all -n &lt;namespace&gt;</code></p>\n</blockquote> <p>This is, effectively, the same thing that the finalizer does, so it must fetch all resources. If you can&#x2019;t list all <code>api-resources</code>, the finalizer cannot continue. This is what was happening in my case.</p> <p>Making this call was failing because custom metrics could not be queried. My custom metrics were being provided by the <code>prometheus-adapter</code>, which luckily had a fairly clear error in the logs.</p> <pre><code>unable to update list of all metrics: unable to fetch metrics for query &quot;{namespace!=\\&quot;\\&quot;,__name__!~\\&quot;^container_.*\\&quot;}&quot;: Get http://prometheus-server.monitoring:9090/api/v1/series?match%5B%5D=%7Bnamespace%21%3D%22%22%2C__name__%21~%22%5Econtainer_.%2A%22%7D&amp;start=1599086760.41: dial tcp 10.245.96.140:9090: i/o timeout\nI0\n</code></pre> <p>The call to <code>prometheus</code> was timing out, leading me to check on Prometheus, where it was immediately clear from the logs that Prometheus was in a rough state. I quickly restarted Prometheus, and the namespace quickly terminated.</p> <p>Our chain of failures here was:</p> <ul>\n<li>Terminating Namespace, to</li>\n<li>Kubernetes Finalizer, to</li>\n<li>Listing <code>api-resources</code>, to</li>\n<li>Listing Custom Metrics, to</li>\n<li>Prometheus Custom Metric Adapter, to</li>\n<li>Prometheus</li>\n</ul> <p>In the end a slow-responding Prometheus was preventing an unrelated Namespace from terminating. As Kubernetes installations get more complex, I expect that we will start seeing more cross-cutting issues like this, which is why careful tracing and debugging skills are important. As the system gets more complex, a quick google search is unlikely to have the exact answer needed. Before this incident, I would not have suspected that a failing custom metric provider would prevent resource deletion.</p> </section>",
      "contentAsText": " Namespaces are a core component of the Kubernetes landscape which are often used as as a base level of resource isolation. As a resource which contains multiple others, the shutdown behaviour associated with terminating namespaces is complex. In effect, namespaces can often get “stuck” in a Terminating state. I ran into this recently with a variety of namespaces in a Digital Ocean Kubernetes cluster. Much advice on the internet will recommend editing the namespace to remove any finalizers on the object. This may work in many distributions, however was not an option for me. It appears that Digitalocean has protections against this (likely in the form of a MutatingAdmissionController) which dynamically added the finalizer back. Due to this, I had to track down the finalizer that was failing, and determine how to correct it. \nNote: A finalizer is a function which acts to enforce cleanup and/or deletion of related resources, or take other action when a resource is deleted.\n As a part of debugging this, your first step should be looking at the current state of the namespace. Although I’m sure there is a better way, I used kubectl edit ns <name>. This quickly confirmed that the namespace termination was stuck on the finalizer, and from the error it appears that the finalizer was failing to list custom metrics. At ths time this felt unrelated, but becomes clear when you think about what must be cleaned up when a namespace is deleted. When a namespace is removed, the finalizer must ensure that all resources that it contains are removed. While a namespace still has resources within it, it is in the Terminating state. You can check what resources are in a namespace with this snippet: kubectl api-resources --verbs=list --namespaced -o name \\\n  | xargs -n 1 kubectl get --show-kind --ignore-not-found -n <namespace>\n \nNote that this is much more encompassing than kubectl get all -n <namespace>\n This is, effectively, the same thing that the finalizer does, so it must fetch all resources. If you can’t list all api-resources, the finalizer cannot continue. This is what was happening in my case. Making this call was failing because custom metrics could not be queried. My custom metrics were being provided by the prometheus-adapter, which luckily had a fairly clear error in the logs. unable to update list of all metrics: unable to fetch metrics for query \"{namespace!=\\\"\\\",__name__!~\\\"^container_.*\\\"}\": Get http://prometheus-server.monitoring:9090/api/v1/series?match%5B%5D=%7Bnamespace%21%3D%22%22%2C__name__%21~%22%5Econtainer_.%2A%22%7D&start=1599086760.41: dial tcp 10.245.96.140:9090: i/o timeout\nI0\n The call to prometheus was timing out, leading me to check on Prometheus, where it was immediately clear from the logs that Prometheus was in a rough state. I quickly restarted Prometheus, and the namespace quickly terminated. Our chain of failures here was: \nTerminating Namespace, to\nKubernetes Finalizer, to\nListing api-resources, to\nListing Custom Metrics, to\nPrometheus Custom Metric Adapter, to\nPrometheus\n In the end a slow-responding Prometheus was preventing an unrelated Namespace from terminating. As Kubernetes installations get more complex, I expect that we will start seeing more cross-cutting issues like this, which is why careful tracing and debugging skills are important. As the system gets more complex, a quick google search is unlikely to have the exact answer needed. Before this incident, I would not have suspected that a failing custom metric provider would prevent resource deletion. "
    },
    {
      "url": "https://www.reddit.com/r/kubernetes/comments/iyjx2f/clusterverse_my_mac_app_for_managing_k8s_clusters/",
      "title": ""
    },
    {
      "url": "https://www.infracloud.io/blogs/tinkerbell-automated-bare-metal-provisioning-engine/?utm_source=reddit.com&utm_medium=social&utm_campaign=promoting_blog",
      "title": "Automated bare metal provisioning with Tinkerbell",
      "content": "<div class=\"row\"><p class=\"post-area\"><article id=\"post-4007\" class=\"regular post-4007 post type-post status-publish format-standard has-post-thumbnail category-bare-metal tag-automated-provisioning-engine tag-bare-metal tag-bare-metal-challenges tag-bare-metal-use-cases tag-tinkerbell-bare-metal\"><div class=\"inner-wrap animated\"><div class=\"post-content\"><div class=\"content-inner\"><div class><div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span><img class=\"lazyload aligncenter wp-image-4449 size-large\" src=\"data:image/svg+xml,%3Csvg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%201024%20224%22%3E%3C/svg%3E\" alt=\"Tinkerbell _ An Automated Bare Metal Provisioning Engine-header-image\" width=\"1024\"></span></div><div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span><br> Do you want to use bare metal servers? and are you not using it because you </span><span class=\"passivevoice\"><span>are scared</span></span><span> of setting and maintaining it up? </span><span class=\"hardreadability\"><span>Here we have the Tinkerbell which provides a quite simple way of setting up and provisioning a bare metal machine</span></span><span>. </span></div></div><div class><div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span class=\"hardreadability\"><span>This fulfills the need and now you should not worry about the complexity of setting up your bare metal machine</span></span><span>. You can even use Tinkerbell to setup your VMs and can do many more things besides provisioning. </span><span class=\"hardreadability\"><span>For example, installing some required software after provision or setting up a k8s cluster and much more</span></span><span>.</span></div></div><div class><div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span class=\"hardreadability\"><span>In this post, we will talk about the use cases of bare metal servers in today&#x2019;s cloud-native world and the challenges which people generally face</span></span><span>. </span><span class=\"hardreadability\"><span>And then we will talk about how Tinkerbell can help us as a complete solution to overcome those challenges</span></span><span>.</span></div></div><div class><div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span class=\"hardreadability\"><span>Bare metal servers is present everywhere from rack of servers that are present in a Data Center of a company to an RPi board in some small IoT devices</span></span><span>.</span></div></div><ul class=\"public-DraftStyleDefault-ul\"><li class=\"public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-reset public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\"><div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span class=\"hardreadability\"><span>There are organizations that would like to use their on-premise infrastructure and build a private cloud</span></span><span>.</span></div></li></ul><ul class=\"public-DraftStyleDefault-ul\"><li class=\"public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-reset public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\"><div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>Bare metal is also applicable where data security comes into the picture. </span><span class=\"hardreadability\"><span>Security is very important aspect&#xA0; and a private infrastructure lets you configure the way business would like</span></span><span>. For example the data of the people of account holders of any bank.</span></div></li></ul><ul class=\"public-DraftStyleDefault-ul\"><li class=\"public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-reset public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\"><div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span class=\"veryhardreadability\"><span>With the Bare Metal servers, organizations can tune their performance on the basis of cost, consistency, and predictability</span></span><span>. these properties are hard to bargain from the public on public clouds.</span></div></li></ul><ul class=\"public-DraftStyleDefault-ul\"><li class=\"public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-reset public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\"><div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>The increase of control is also a big advantage of bare metal servers as compared to cloud servers.</span></div></li></ul><ul class=\"public-DraftStyleDefault-ul\"><li class=\"public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-reset public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\"><div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span class=\"hardreadability\"><span>Administration cost</span><span>: If a user would like to setup a large Data Centers which consist of hundreds of servers</span></span><span>. In this case it would be difficult to manage this large infrastructure. For example, provisioning each server and maintaining its life cycle.</span></div></li><li class=\"public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-reset public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\"><div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span class=\"hardreadability\"><span>Hardware Cost:</span><span> There are cases where a user needs to setup the infrastructure with different CPU structures</span></span><span>. </span><span class=\"veryhardreadability\"><span>For example Intel x86-64, ARM, etc and different distros like CentOS, Ubuntu, etc. In this case, it is again a bit difficult to change the configuration of an already configured/setup server again and again as per the </span></span><span class=\"complexword\"><span>requirement</span></span><span class=\"veryhardreadability\"><span> changes</span></span><span>.</span></div></li><li class=\"public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-reset public-DraftStyleDefault-depth0 public-DraftStyleDefault-listLTR\"><div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>Maintenance:</span><span> &#x201C;Increase of Control comes with Increase of Complexity&#x201D;. The user will have more control over the bare metal server. But again to configure/reconfigure a bare metal server is more complex than a cloud server.</span></div></li></ul><div id=\"attachment_4444\" class=\"wp-caption aligncenter\"><img class=\"lazyload wp-image-4444\" src=\"data:image/svg+xml,%3Csvg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20701%20460%22%3E%3C/svg%3E\" alt=\"tinkerbell-services-blog-infracloud-aman-parauliya-001\" width=\"701\"><p id=\"caption-attachment-4444\" class=\"wp-caption-text\">Source: https://tinkerbell.org/</p></div><div class><div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span class=\"hardreadability\"><span><br> There are few micro services in Tinkerbell and each one of them handles a particular task while doing a Network boot for a bare metal server</span></span><span>. Following are the brief description about each of them:</span></div></div><ol><li class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>Boots: </span><span class=\"hardreadability\"><span>When you reboot a bare metal in iPXE/Network mode it first tries to fetch the IP by broadcasting a DHCPDISCOVER packet in the private network</span></span><span>. Boots handles this request/packet and offer the IP asked by to that machine.</span><span><br></span></li><li class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>Hegel: Hegel is the metadata service used by Tink and OSIE. It collects data from both and transforms it into a JSON format to which we call metadata.</span><span><br></span></li><li class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>OSIE: OSIE provides an in-memory installation environment. It installs the operating system and handles de-provisioning.</span><span><br></span></li><li class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>PbnJ: PBnJ is a microservice that can communicate with baseboard management controllers (BMCs). It controls power and boot settings.</span><span><br></span></li><li class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>Tink: This is the main workflow engine and responsible for processing the workflows. This component has the following three different binaries</span></li></ol><ul><li><ul class=\"public-DraftStyleDefault-ul\"><li class=\"public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-reset public-DraftStyleDefault-depth1 public-DraftStyleDefault-listLTR\"><div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span>tink-server: This is a server which handles the request for CRUD operation on gRPC . </span><span class=\"hardreadability\"><span>This handles the creation or workflows and it&#x2019;s building blocks, template, and targeted hardware</span></span><span>.</span></div></li></ul></li></ul><ul><li><ul class=\"public-DraftStyleDefault-ul\"><li class=\"public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-reset public-DraftStyleDefault-depth1 public-DraftStyleDefault-listLTR\"><div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span class=\"hardreadability\"><span>tink-cli: The CLI </span></span><span class=\"passivevoice\"><span>is used</span></span><span class=\"hardreadability\"><span> to create workflows and their building blocks, templates, and hardware data</span></span><span>.</span></div></li></ul></li></ul><ul><li><ul class=\"public-DraftStyleDefault-ul\"><li class=\"public-DraftStyleDefault-unorderedListItem public-DraftStyleDefault-reset public-DraftStyleDefault-depth1 public-DraftStyleDefault-listLTR\"><div class=\"public-DraftStyleDefault-block public-DraftStyleDefault-ltr\"><span class=\"hardreadability\"><span>tink-worker: This is a service that runs on the targeted hardware, a machine in which the user wants to provision and execute the workflow</span></span><span>.</span></div></li></ul></li></ul><div id=\"attachment_4445\" class=\"wp-caption aligncenter\"><img class=\"lazyload wp-image-4445\" src=\"data:image/svg+xml,%3Csvg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20904%20605%22%3E%3C/svg%3E\" alt=\"tinkerbell-services-blog-infracloud-aman-parauliya\" width=\"904\"><p id=\"caption-attachment-4445\" class=\"wp-caption-text\">Source: https://github.com/tinkerbell/tinkerbell.org/blob/master/static/images/docs/workflow-architecture.png</p></div><p><span>As can </span><span class=\"passivevoice\"><span>be seen</span></span><span> in the above diagram Tinkerbell has a control server which </span><span class=\"passivevoice\"><span>is called</span></span><span> Provisioner. </span><span class=\"hardreadability\"><span>On this machine all the services of Tinkerbell mentioned above will be running inside its own docker container</span></span><span>. </span><span class=\"hardreadability\"><span>The following are few other processes which will also be running on the Provisioner machine :</span></span></p><ol><li><a class=\"highlight\" href=\"https://www.postgresql.org/\"><strong>PostgreSQL</strong></a> : Tinkerbell uses PostgreSQL as its data store. Psql is required to store and maintain workflows and all the other information which is required to create/maintain a workflow like templates, information of targeted Hardwares etc.</li><li><a class=\"highlight\" href=\"https://hub.docker.com/_/registry\"><strong>Image Repository</strong></a> : A private docker registry will be hosted on the Provisioner. In this private registry docker images of tink-worker and of all the actions which are part of workflow (defined in template) will be stored and worker machine will fetch those images from this private docker registry. This is particularly useful for secure environments that don&#x2019;t have access to the internet.</li><li><a class=\"highlight\" href=\"https://www.nginx.com/\"><strong>NGINX</strong></a>: NGINX is a web server. Tinkerbell uses NGINX to serve the required boot files during workflow execution.</li></ol><p>On the Provisioner, the complete stack of containers services/components looks like as shown below:</p><p><img class=\"lazyload alignnone wp-image-4304 \" src=\"data:image/svg+xml,%3Csvg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20943%20157%22%3E%3C/svg%3E\" alt=\"Tinkerbell: Provisioner Stack\" width=\"943\"></p><h2>Provisioning Steps:</h2><p>The following are the steps through which a bare metal server can be provisioned to an Ubuntu 18.04 OS.<br> The following operation performs through the tink-cli container</p><h3>1. Setup the Provisioner:</h3><p>User can setup the Provisioner on a hardware machine or on a VM as well. The instruction for the setup are given <a href=\"https://tinkerbell.org/docs/setup/\">here</a>.</p><h3>2. Create a Hardware Inventory:</h3><p>In this step data of all the hardware (bare metal servers) which needs to be provisioned will be pushed to the database. The data will be in JSON format. Following is the minimal hardware data which you need to push for particular hardware:</p><pre class=\"EnlighterJSRAW\">{\n  &quot;metadata&quot;: {\n    &quot;instance&quot;: {},\n    &quot;facility&quot;: {\n      &quot;facility_code&quot;: &quot;onprem&quot;\n    }\n  },\n  &quot;network&quot;: {\n    &quot;interfaces&quot;: [\n      {\n        &quot;dhcp&quot;: {\n          &quot;mac&quot;: &quot;98:03:9b:89:d7:aa&quot;,\n          &quot;hostname&quot;: &quot;localhost&quot;,\n          &quot;arch&quot;: &quot;x86_64&quot;,\n          &quot;ip&quot;: {\n            &quot;address&quot;: &quot;192.168.1.6&quot;,\n            &quot;netmask&quot;: &quot;255.255.255.248&quot;,\n            &quot;gateway&quot;: &quot;192.168.1.1&quot;\n          }\n        },\n        &quot;netboot&quot;: {\n          &quot;allow_pxe&quot;: true,\n          &quot;allow_workflow&quot;: true\n        }\n      }\n    ]\n  },\n  &quot;id&quot;: &quot;f9f56dff-098a-4c5f-a51c-19ad35de85d4&quot;\n}</pre><p>For a detailed description of the hardware data you can visit <a href=\"https://tinkerbell.org/docs/hardware-data/\">Hardware_Data</a> page. User should keep the above information inside a file.</p><p> Following is the command of &#x201C;tink&#x201D; CLI through which you can push the data of the hardware:</p></div></div></div></article></p><pre class=\"EnlighterJSRAW\">docker exec -i deploy_tink-cli_1 tink hardware push &lt; &lt;path to hardware data file&gt;\n&gt; 2020/08/24 16:23:09 Hardware data pushed successfully</pre><p>Just like the above user can push the data of all the Hardwares which are required to be provisioned.</p><h3>3. Create Templates :</h3><p>In this step user needs to create templates. Templates are Yaml based definition of a workflow which contains all the tasks/actions which should execute to provision a machine. User can create multiple templates but each template with unique names. For example, if a template contains tasks/actions which are for Ubuntu provisioning, can have name as &#x201C;Ubuntu-template&#x201D; or a template for CentOS can have name as &#x201C;Centos-template&#x201D; etc.<br> Following is a sample of a template:</p><pre class=\"EnlighterJSRAW\">version: &quot;0.1&quot;\nname: ubuntu_provisioning\nglobal_timeout: 6000\ntasks:\n  - name: &quot;os-installation&quot;\n    worker: &quot;{{.device_1}}&quot;\n    volumes:\n      - /dev:/dev\n      - /dev/console:/dev/console\n      - /lib/firmware:/lib/firmware:ro\n    environment:\n      MIRROR_HOST: &lt;MIRROR_HOST_IP&gt;\n    actions:\n      - name: &quot;disk-wipe&quot;\n        image: disk-wipe\n        timeout: 90\n      - name: &quot;disk-partition&quot;\n        image: disk-partition\n        timeout: 600\n        environment:\n          MIRROR_HOST: &lt;MIRROR_HOST_IP&gt;\n        volumes:\n          - /statedir:/statedir\n      - name: &quot;install-root-fs&quot;\n        image: install-root-fs\n        timeout: 600\n      - name: &quot;install-grub&quot;\n        image: install-grub\n        timeout: 600\n        volumes:\n          - /statedir:/statedir</pre><p>As per the template definition above, it supports tasks and actions under each task. Each action will be running as a container on the worker machine. Templates have volume mount support at task and action levels as well.</p><p>If we provide a volume at a task level, each action under that particular task can mount the volume. And if we provide a volume at the action level, only that particular action can use that volume. The template also supports environment variables in the same way as volume mount support.</p><p> If there is any change in the template in the future you can always refer to <a href=\"https://tinkerbell.org/docs/templates/\">sample-ubuntu.tmpl</a>.</p> Each task contains a field &#x201C;worker&#x201D; which has value as &#x201C;{.device_1}&#x201D;. The value of &#x201C;device_1&#x201D; will be replaced with the mac address of the targeted hardware while creating the workflow in the next step.<p> Following the command through which you can create/insert a template in the database:</p><pre class=\"EnlighterJSRAW\">#docker exec -i deploy_tink-cli_1 tink template create -n &lt;template-name&gt; &lt; &lt;path to template file&gt;\nCreated Template:  489da4a1-6672-461b-9b09-9aa67b8938f9</pre><p>Creation of a template generates a unique id (UUID) which we will be use during workflow creation.<br> For example in the above case the UUID for the created template is&#xA0; &#x201C;489da4a1-6672-461b-9b09-9aa67b8938f9&#x201D;.</p><h3>4. Create Workflows :</h3><p>After completion of above two steps we need to create a workflow. To create a workflow we need one template (UUID) and MAC address or IP address of the targeted hardware.<br> Following is the command which creats the workflow:</p><pre class=\"EnlighterJSRAW\">#docker exec -i deploy_tink-cli_1 tink workflow create -t &lt;template uuid&gt; -r &apos;{&quot;device_1&quot;:&quot;&lt;MAC/IP&gt; address of targeted hardware&quot;}&apos;\nCreated Workflow:  2a4d1101-8ad2-4c5c-9822-c541d1775151\n</pre><h3>5. Reboot the targeted hardware in iPXE mode :</h3><p>After creating a workflow for a particular mac address, We should&#xA0; reboot the machine which has that mac address in the iPXE mode. Once the reboot starts, first it will ask for an IP through DHCP service provided by boots. Then it will ask for few files required for OS installation to OSIE through Nginx. After that it boots into base OS and then it ask tink-server to provide the workflow which we created for this hardware. Then that workflow start executing by the worker machine. Once the workflow completed successfully. we should reboot the machine so that it can boot into the newly installed OS.</p><h2>What&#x2019;s Next for Tinkerbell:</h2><p>Packet open-sourced the Tinkerbell few months back. We received a lot of responses from the community including few interesting things. People are already doing great things like automating Raspberry Pis, installing Kubernetes on their home lab setups, and, of course, finding a lot of areas for improvement.<br> We&#x2019;re loving it, and we welcome your comments, questions, and PRs or issues.</p><p>We hope you find it interesting and like to engage with us to contribute back to the community and help us with our mission of making infrastructure a competitive advantage.<br> Join us on <a href=\"https://slack.packet.com/\">Slack</a> and look for the #tinkerbell channel.</p><p>Also, have a look at the <a href=\"https://tinkerbell.org\">website</a> for more details about Tinkerbell.</p><p>Thanks</p><p>Aman Parauliya</p><p>Senior Software Engineer at InfraCloud Technologies.</p><!--/content-inner--><!--/post-content--><!--/inner-wrap--><!--/article--><!--/span_9--></div>",
      "contentAsText": " Do you want to use bare metal servers? and are you not using it because you are scared of setting and maintaining it up? Here we have the Tinkerbell which provides a quite simple way of setting up and provisioning a bare metal machine. This fulfills the need and now you should not worry about the complexity of setting up your bare metal machine. You can even use Tinkerbell to setup your VMs and can do many more things besides provisioning. For example, installing some required software after provision or setting up a k8s cluster and much more.In this post, we will talk about the use cases of bare metal servers in today’s cloud-native world and the challenges which people generally face. And then we will talk about how Tinkerbell can help us as a complete solution to overcome those challenges.Bare metal servers is present everywhere from rack of servers that are present in a Data Center of a company to an RPi board in some small IoT devices.There are organizations that would like to use their on-premise infrastructure and build a private cloud.Bare metal is also applicable where data security comes into the picture. Security is very important aspect  and a private infrastructure lets you configure the way business would like. For example the data of the people of account holders of any bank.With the Bare Metal servers, organizations can tune their performance on the basis of cost, consistency, and predictability. these properties are hard to bargain from the public on public clouds.The increase of control is also a big advantage of bare metal servers as compared to cloud servers.Administration cost: If a user would like to setup a large Data Centers which consist of hundreds of servers. In this case it would be difficult to manage this large infrastructure. For example, provisioning each server and maintaining its life cycle.Hardware Cost: There are cases where a user needs to setup the infrastructure with different CPU structures. For example Intel x86-64, ARM, etc and different distros like CentOS, Ubuntu, etc. In this case, it is again a bit difficult to change the configuration of an already configured/setup server again and again as per the requirement changes.Maintenance: “Increase of Control comes with Increase of Complexity”. The user will have more control over the bare metal server. But again to configure/reconfigure a bare metal server is more complex than a cloud server.Source: https://tinkerbell.org/ There are few micro services in Tinkerbell and each one of them handles a particular task while doing a Network boot for a bare metal server. Following are the brief description about each of them:Boots: When you reboot a bare metal in iPXE/Network mode it first tries to fetch the IP by broadcasting a DHCPDISCOVER packet in the private network. Boots handles this request/packet and offer the IP asked by to that machine.Hegel: Hegel is the metadata service used by Tink and OSIE. It collects data from both and transforms it into a JSON format to which we call metadata.OSIE: OSIE provides an in-memory installation environment. It installs the operating system and handles de-provisioning.PbnJ: PBnJ is a microservice that can communicate with baseboard management controllers (BMCs). It controls power and boot settings.Tink: This is the main workflow engine and responsible for processing the workflows. This component has the following three different binariestink-server: This is a server which handles the request for CRUD operation on gRPC . This handles the creation or workflows and it’s building blocks, template, and targeted hardware.tink-cli: The CLI is used to create workflows and their building blocks, templates, and hardware data.tink-worker: This is a service that runs on the targeted hardware, a machine in which the user wants to provision and execute the workflow.Source: https://github.com/tinkerbell/tinkerbell.org/blob/master/static/images/docs/workflow-architecture.pngAs can be seen in the above diagram Tinkerbell has a control server which is called Provisioner. On this machine all the services of Tinkerbell mentioned above will be running inside its own docker container. The following are few other processes which will also be running on the Provisioner machine :PostgreSQL : Tinkerbell uses PostgreSQL as its data store. Psql is required to store and maintain workflows and all the other information which is required to create/maintain a workflow like templates, information of targeted Hardwares etc.Image Repository : A private docker registry will be hosted on the Provisioner. In this private registry docker images of tink-worker and of all the actions which are part of workflow (defined in template) will be stored and worker machine will fetch those images from this private docker registry. This is particularly useful for secure environments that don’t have access to the internet.NGINX: NGINX is a web server. Tinkerbell uses NGINX to serve the required boot files during workflow execution.On the Provisioner, the complete stack of containers services/components looks like as shown below:Provisioning Steps:The following are the steps through which a bare metal server can be provisioned to an Ubuntu 18.04 OS. The following operation performs through the tink-cli container1. Setup the Provisioner:User can setup the Provisioner on a hardware machine or on a VM as well. The instruction for the setup are given here.2. Create a Hardware Inventory:In this step data of all the hardware (bare metal servers) which needs to be provisioned will be pushed to the database. The data will be in JSON format. Following is the minimal hardware data which you need to push for particular hardware:{\n  \"metadata\": {\n    \"instance\": {},\n    \"facility\": {\n      \"facility_code\": \"onprem\"\n    }\n  },\n  \"network\": {\n    \"interfaces\": [\n      {\n        \"dhcp\": {\n          \"mac\": \"98:03:9b:89:d7:aa\",\n          \"hostname\": \"localhost\",\n          \"arch\": \"x86_64\",\n          \"ip\": {\n            \"address\": \"192.168.1.6\",\n            \"netmask\": \"255.255.255.248\",\n            \"gateway\": \"192.168.1.1\"\n          }\n        },\n        \"netboot\": {\n          \"allow_pxe\": true,\n          \"allow_workflow\": true\n        }\n      }\n    ]\n  },\n  \"id\": \"f9f56dff-098a-4c5f-a51c-19ad35de85d4\"\n}For a detailed description of the hardware data you can visit Hardware_Data page. User should keep the above information inside a file. Following is the command of “tink” CLI through which you can push the data of the hardware:docker exec -i deploy_tink-cli_1 tink hardware push < <path to hardware data file>\n> 2020/08/24 16:23:09 Hardware data pushed successfullyJust like the above user can push the data of all the Hardwares which are required to be provisioned.3. Create Templates :In this step user needs to create templates. Templates are Yaml based definition of a workflow which contains all the tasks/actions which should execute to provision a machine. User can create multiple templates but each template with unique names. For example, if a template contains tasks/actions which are for Ubuntu provisioning, can have name as “Ubuntu-template” or a template for CentOS can have name as “Centos-template” etc. Following is a sample of a template:version: \"0.1\"\nname: ubuntu_provisioning\nglobal_timeout: 6000\ntasks:\n  - name: \"os-installation\"\n    worker: \"{{.device_1}}\"\n    volumes:\n      - /dev:/dev\n      - /dev/console:/dev/console\n      - /lib/firmware:/lib/firmware:ro\n    environment:\n      MIRROR_HOST: <MIRROR_HOST_IP>\n    actions:\n      - name: \"disk-wipe\"\n        image: disk-wipe\n        timeout: 90\n      - name: \"disk-partition\"\n        image: disk-partition\n        timeout: 600\n        environment:\n          MIRROR_HOST: <MIRROR_HOST_IP>\n        volumes:\n          - /statedir:/statedir\n      - name: \"install-root-fs\"\n        image: install-root-fs\n        timeout: 600\n      - name: \"install-grub\"\n        image: install-grub\n        timeout: 600\n        volumes:\n          - /statedir:/statedirAs per the template definition above, it supports tasks and actions under each task. Each action will be running as a container on the worker machine. Templates have volume mount support at task and action levels as well.If we provide a volume at a task level, each action under that particular task can mount the volume. And if we provide a volume at the action level, only that particular action can use that volume. The template also supports environment variables in the same way as volume mount support. If there is any change in the template in the future you can always refer to sample-ubuntu.tmpl. Each task contains a field “worker” which has value as “{.device_1}”. The value of “device_1” will be replaced with the mac address of the targeted hardware while creating the workflow in the next step. Following the command through which you can create/insert a template in the database:#docker exec -i deploy_tink-cli_1 tink template create -n <template-name> < <path to template file>\nCreated Template:  489da4a1-6672-461b-9b09-9aa67b8938f9Creation of a template generates a unique id (UUID) which we will be use during workflow creation. For example in the above case the UUID for the created template is  “489da4a1-6672-461b-9b09-9aa67b8938f9”.4. Create Workflows :After completion of above two steps we need to create a workflow. To create a workflow we need one template (UUID) and MAC address or IP address of the targeted hardware. Following is the command which creats the workflow:#docker exec -i deploy_tink-cli_1 tink workflow create -t <template uuid> -r '{\"device_1\":\"<MAC/IP> address of targeted hardware\"}'\nCreated Workflow:  2a4d1101-8ad2-4c5c-9822-c541d1775151\n5. Reboot the targeted hardware in iPXE mode :After creating a workflow for a particular mac address, We should  reboot the machine which has that mac address in the iPXE mode. Once the reboot starts, first it will ask for an IP through DHCP service provided by boots. Then it will ask for few files required for OS installation to OSIE through Nginx. After that it boots into base OS and then it ask tink-server to provide the workflow which we created for this hardware. Then that workflow start executing by the worker machine. Once the workflow completed successfully. we should reboot the machine so that it can boot into the newly installed OS.What’s Next for Tinkerbell:Packet open-sourced the Tinkerbell few months back. We received a lot of responses from the community including few interesting things. People are already doing great things like automating Raspberry Pis, installing Kubernetes on their home lab setups, and, of course, finding a lot of areas for improvement. We’re loving it, and we welcome your comments, questions, and PRs or issues.We hope you find it interesting and like to engage with us to contribute back to the community and help us with our mission of making infrastructure a competitive advantage. Join us on Slack and look for the #tinkerbell channel.Also, have a look at the website for more details about Tinkerbell.ThanksAman ParauliyaSenior Software Engineer at InfraCloud Technologies.",
      "publishedDate": "2020-09-16T12:37:16.000Z",
      "description": "We discuss use cases of bare metal servers, its challenges & how Tinkerbell can help us as a complete solution to overcome these challenges.",
      "ogDescription": "We discuss use cases of bare metal servers, its challenges & how Tinkerbell can help us as a complete solution to overcome these challenges."
    },
    {
      "url": "https://k21academy.com/docker-kubernetes/certified-kubernetes-security-specialist-cks-everything-you-must-know/?utm_source=reddit&utm_medium=referral&utm_campaign=kubernetessec11_sep20_kubernetes",
      "title": "Certified Kubernetes Security Specialist | CKS | Certification",
      "content": "<div class=\"entry-content\"><p><span id=\"dpsp-post-content-markup\"></span>Everyone is excited about <strong>Kubernetes</strong>, the Cloud Native Computing Foundation (CNCF) have planned to add a new <strong>Certified Kubernetes Security Specialist (CKS)</strong> to the growing list of Kubernetes certification programs. This brings the total number of CNCF Kubernetes certifications to 3 &#x2013; with the prior ones being the<strong>&#xFFFD;<a href=\"/docker-kubernetes/certified-kubernetes-administrator-cka-certification-exam-everything-you-must-know/\">Certified Kubernetes Administrator</a></strong>&#xFFFD;and the<strong>&#xFFFD;<a href=\"/docker-kubernetes/kubernetes-developer/certified-kubernetes-application-developer-ckad/\">Certified Kubernetes Application Developer</a></strong>. The first tests are expected to roll out and be generally available before <strong>November 2020</strong>.</p>\n<p>In this blog, we discuss in detail the following topics:</p> <h2><strong>What Is The Certified Kubernetes Security Specialist Exam?</strong></h2>\n<p><img class=\"lazyload aligncenter wp-image-43978\" src=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_250,h_238/https://k21academy.com/wp-content/uploads/2020/09/Blog-300x285.png\" alt=\"CKS Logo\" width=\"250\" srcset=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_300/https://k21academy.com/wp-content/uploads/2020/09/Blog-300x285.png 300w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_316/https://k21academy.com/wp-content/uploads/2020/09/Blog-316x300.png 316w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1012/https://k21academy.com/wp-content/uploads/2020/09/Blog.png 1012w\">The Certified Kubernetes Security Specialist (CKS) program will consist of a performance-based certification exam and assures that a CKS has the skills, knowledge, and competence on a broad range of best practices for <strong id=\"2\">securing container-based applications</strong> and Kubernetes platforms during build, deployment, and runtime. This new certification is designed to enable cloud-native professionals to demonstrate <strong>security skills </strong>to current and potential employers<strong>.</strong></p>\n<p><span><strong>Also check:</strong> All you need to know on <a href=\"/docker-kubernetes/rbac-role-based-access-control/\">Role Based Access Control</a></span></p>\n<h2><strong>Pre-requisites For CKS Exam &#xFFFD;<a href=\"#\">^</a></strong></h2>\n<p>To take the CKS exam, you must hold a current <strong id=\"3\">CKA certification</strong> to demonstrate you possess sufficient Kubernetes expertise. If you want to make sure you are ready for the <strong>CKS</strong> and have not already achieved the CKA, we encourage you to start today.</p>\n<p><span><strong>Check out:</strong> Difference between <a href=\"/docker-kubernetes/docker-vs-virtual-machine/\">Docker vs VM</a></span></p>\n<h2><strong>Who Is This Certification For? <a href=\"#\">^</a></strong></h2>\n<ul>\n<li>All those who are <strong>CKA certified</strong> and want to upgrade their skills.</li>\n<li>Candidates having an idea about<strong> Kubernetes </strong>and<strong> containers</strong>.</li>\n<li>Those who are in <strong>Cloud architecture</strong> and want to<strong> learn Security</strong>.</li>\n<li>For candidates who are interested in<strong> Security</strong>.</li>\n<li id=\"4\">Engineers who have some experience of <strong>Network security</strong>.</li>\n<li>Those who are looking for new&#xFFFD;<strong>career-changing</strong>&#xFFFD;opportunities.</li>\n<li>For those who are looking for&#xFFFD;<strong>adapting</strong>&#xFFFD;to new technologies.</li>\n</ul>\n<h2><strong>CKS Certification Benefits <a href=\"#\">^</a></strong></h2>\n<ul>\n<li>A Kubernetes certification makes your resume look good and <strong>stand out</strong> from the competition. As companies will be relying more and more on Kubernetes, your expertise will be an <strong>immediate asset</strong>.</li>\n<li>Passing<strong> CKA </strong>and<strong> CKS</strong>&#xFFFD; is not an easy task, so companies seeking Kubernetes engineers are willing to pay more which gives you the mighty potential for a <strong>hike in salary</strong>.</li>\n<li id=\"5\">The companies are looking for<strong> certified Kubernetes</strong> professionals, as the majority of them are moving their application towards containers.</li>\n</ul>\n<p>Since the Kubernetes is quite new in the industry, there is a&#xFFFD;<strong>huge market gap</strong>&#xFFFD;for certified professionals.</p>\n<h2><strong>CKS Exam Basics <a href=\"#\">^</a></strong></h2>\n<ul>\n<li id=\"6\"><strong>Certification Name: </strong>Certified&#xFFFD;Kubernetes&#xFFFD;Security Specialist</li>\n<li><strong>Prerequisites:&#xFFFD; </strong>One must hold a current CKA certification</li>\n<li><strong>Exam Duration:</strong> 2 hours</li>\n</ul>\n<p><strong>Also check:</strong>&#xFFFD;Difference between&#xFFFD;<a href=\"/docker-kubernetes/kubernetes-vs-docker/\">Kubernetes vs docker</a>.</p>\n<h2><strong>CKS </strong><strong>Exam Topics <a href=\"#\">^</a></strong></h2>\n<p>The CKS exam curriculum includes the following general domains and their weightage :</p>\n<h3><strong>1) Cluster Setup \u0013 10%</strong></h3>\n<ul>\n<li>Use Network security policies to restrict cluster level access</li>\n<li>Use CIS benchmark to review the security configuration of Kubernetes components (etcd, kubelet, kubedns, kubeapi)</li>\n<li>Properly set up Ingress objects with security control</li>\n<li>Protect node metadata and endpoints</li>\n<li>use of, and access to, GUI elements</li>\n<li>Verify platform binaries before deploying</li>\n</ul>\n<h3><strong>2)</strong> <strong>Cluster Hardening \u0013 15%</strong></h3>\n<ul>\n<li>Restrict access to Kubernetes API</li>\n<li>Use Role-Based Access Controls to minimize exposure</li>\n<li>Exercise caution in using service accounts e.g. disable defaults, minimize permissions on newly created ones</li>\n<li>Update Kubernetes frequently</li>\n</ul>\n<h3><strong>3)</strong> <strong>System Hardening \u0013 15%</strong></h3>\n<ul>\n<li>Minimize host OS footprint (reduce attack surface)</li>\n<li>Minimize IAM roles</li>\n<li>Minimize external access to the network</li>\n<li>Appropriately use kernel hardening tools such as AppArmor, seccomp</li>\n</ul>\n<h3><strong>4)</strong> <strong>Minimize Microservice Vulnerabilities \u0013 20%</strong></h3>\n<ul>\n<li>Setup appropriate OS-level security domains e.g. using PSP, OPA, security contexts</li>\n<li>Manage Kubernetes secrets</li>\n<li>Use container runtime sandboxes in multi-tenant environments (e.g. gvisor, kata containers)</li>\n<li>Implement pod to pod encryption by use of mTLS</li>\n</ul>\n<h3><strong>5) Supply Chain Security \u0013 20%</strong></h3>\n<ul>\n<li>Minimize base image footprint</li>\n<li>Secure your supply chain: whitelist allowed registries, sign and validate images</li>\n<li>Use static analysis of user workloads (e.g.Kubernetes resources, Docker files)</li>\n<li>Scan images for known vulnerabilities</li>\n</ul>\n<h3><strong>6) Monitoring, Logging, and Runtime Security \u0013 20%</strong></h3>\n<ul>\n<li>Perform behavioral analytics of syscall process and file activities at the host and container level to detect malicious activities</li>\n<li>Detect threats within a physical infrastructure, apps, networks, data, users, and workloads</li>\n<li>Detect all phases of attack regardless of where it occurs and how it spreads</li>\n<li id=\"7\">Perform deep analytical investigation and identification of bad actors within the environment</li>\n<li>Ensure the immutability of containers at runtime.</li>\n<li>Use Audit Logs to monitor access.</li>\n</ul>\n<h2><strong>Exam Retake Policy <a href=\"#\">^</a></strong></h2>\n<p>The Cloud Native Computing Foundation offers&#xFFFD;<strong>one (1)</strong>&#xFFFD;free retake per exam purchase in the event that a passing score is not achieved and the candidate has not otherwise been deemed ineligible for certification or retake.</p>\n<h2><strong><img class=\"lazyload aligncenter wp-image-44037 size-full\" src=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_827,h_225/https://k21academy.com/wp-content/uploads/2020/09/retake-policy.png\" alt=\"CKS retake-policy\" width=\"827\" srcset=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_827/https://k21academy.com/wp-content/uploads/2020/09/retake-policy.png 827w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_300/https://k21academy.com/wp-content/uploads/2020/09/retake-policy-300x82.png 300w, https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_400/https://k21academy.com/wp-content/uploads/2020/09/retake-policy-400x109.png 400w\"></strong></h2>\n<h2><strong>Related / References:</strong></h2> <h2><strong>Next Task For You</strong></h2>\n<p>Begin your journey towards becoming a&#xFFFD;<strong>Certified Kubernetes Security Specialist (CKS) </strong>and earning a lot more in 2020 by joining our&#xFFFD;<strong><a href=\"/kubernetes-security-specialist-cks-masterclass-waitlist/?utm_source=blog_content_upgrade&amp;utm_medium=referral&amp;utm_campaign=kubernetessec02_sept20\">Free Class Waitlist.</a><a href=\"/kubernetes-security-specialist-cks-masterclass-waitlist/?utm_source=blog_content_upgrade&amp;utm_medium=referral&amp;utm_campaign=kubernetessec02_sept20\"><img class=\"lazyload alignnone wp-image-44947 size-full\" src=\"https://cdn.shortpixel.ai/client/q_glossy,ret_img,w_1352,h_202/https://k21academy.com/wp-content/uploads/2020/09/CKS_CU_ed2.gif\" alt width=\"1352\"></a></strong></p> </div>",
      "contentAsText": "Everyone is excited about Kubernetes, the Cloud Native Computing Foundation (CNCF) have planned to add a new Certified Kubernetes Security Specialist (CKS) to the growing list of Kubernetes certification programs. This brings the total number of CNCF Kubernetes certifications to 3 – with the prior ones being the�Certified Kubernetes Administrator�and the�Certified Kubernetes Application Developer. The first tests are expected to roll out and be generally available before November 2020.\nIn this blog, we discuss in detail the following topics: What Is The Certified Kubernetes Security Specialist Exam?\nThe Certified Kubernetes Security Specialist (CKS) program will consist of a performance-based certification exam and assures that a CKS has the skills, knowledge, and competence on a broad range of best practices for securing container-based applications and Kubernetes platforms during build, deployment, and runtime. This new certification is designed to enable cloud-native professionals to demonstrate security skills to current and potential employers.\nAlso check: All you need to know on Role Based Access Control\nPre-requisites For CKS Exam �^\nTo take the CKS exam, you must hold a current CKA certification to demonstrate you possess sufficient Kubernetes expertise. If you want to make sure you are ready for the CKS and have not already achieved the CKA, we encourage you to start today.\nCheck out: Difference between Docker vs VM\nWho Is This Certification For? ^\n\nAll those who are CKA certified and want to upgrade their skills.\nCandidates having an idea about Kubernetes and containers.\nThose who are in Cloud architecture and want to learn Security.\nFor candidates who are interested in Security.\nEngineers who have some experience of Network security.\nThose who are looking for new�career-changing�opportunities.\nFor those who are looking for�adapting�to new technologies.\n\nCKS Certification Benefits ^\n\nA Kubernetes certification makes your resume look good and stand out from the competition. As companies will be relying more and more on Kubernetes, your expertise will be an immediate asset.\nPassing CKA and CKS� is not an easy task, so companies seeking Kubernetes engineers are willing to pay more which gives you the mighty potential for a hike in salary.\nThe companies are looking for certified Kubernetes professionals, as the majority of them are moving their application towards containers.\n\nSince the Kubernetes is quite new in the industry, there is a�huge market gap�for certified professionals.\nCKS Exam Basics ^\n\nCertification Name: Certified�Kubernetes�Security Specialist\nPrerequisites:� One must hold a current CKA certification\nExam Duration: 2 hours\n\nAlso check:�Difference between�Kubernetes vs docker.\nCKS Exam Topics ^\nThe CKS exam curriculum includes the following general domains and their weightage :\n1) Cluster Setup \u0013 10%\n\nUse Network security policies to restrict cluster level access\nUse CIS benchmark to review the security configuration of Kubernetes components (etcd, kubelet, kubedns, kubeapi)\nProperly set up Ingress objects with security control\nProtect node metadata and endpoints\nuse of, and access to, GUI elements\nVerify platform binaries before deploying\n\n2) Cluster Hardening \u0013 15%\n\nRestrict access to Kubernetes API\nUse Role-Based Access Controls to minimize exposure\nExercise caution in using service accounts e.g. disable defaults, minimize permissions on newly created ones\nUpdate Kubernetes frequently\n\n3) System Hardening \u0013 15%\n\nMinimize host OS footprint (reduce attack surface)\nMinimize IAM roles\nMinimize external access to the network\nAppropriately use kernel hardening tools such as AppArmor, seccomp\n\n4) Minimize Microservice Vulnerabilities \u0013 20%\n\nSetup appropriate OS-level security domains e.g. using PSP, OPA, security contexts\nManage Kubernetes secrets\nUse container runtime sandboxes in multi-tenant environments (e.g. gvisor, kata containers)\nImplement pod to pod encryption by use of mTLS\n\n5) Supply Chain Security \u0013 20%\n\nMinimize base image footprint\nSecure your supply chain: whitelist allowed registries, sign and validate images\nUse static analysis of user workloads (e.g.Kubernetes resources, Docker files)\nScan images for known vulnerabilities\n\n6) Monitoring, Logging, and Runtime Security \u0013 20%\n\nPerform behavioral analytics of syscall process and file activities at the host and container level to detect malicious activities\nDetect threats within a physical infrastructure, apps, networks, data, users, and workloads\nDetect all phases of attack regardless of where it occurs and how it spreads\nPerform deep analytical investigation and identification of bad actors within the environment\nEnsure the immutability of containers at runtime.\nUse Audit Logs to monitor access.\n\nExam Retake Policy ^\nThe Cloud Native Computing Foundation offers�one (1)�free retake per exam purchase in the event that a passing score is not achieved and the candidate has not otherwise been deemed ineligible for certification or retake.\n\nRelated / References: Next Task For You\nBegin your journey towards becoming a�Certified Kubernetes Security Specialist (CKS) and earning a lot more in 2020 by joining our�Free Class Waitlist. ",
      "publishedDate": "2020-09-18T05:32:32.000Z",
      "description": "This Blog cover everything about certified kubernetes security specialist (CKS) certification and about basic exam details",
      "ogDescription": "This Blog cover everything about certified kubernetes security specialist (CKS) certification and about basic exam details"
    },
    {
      "url": "https://medium.com/cloud-foundry-foundation/installing-cf-for-k8s-on-google-cloud-gke-5d6902ee99fa",
      "title": "Installing cf-for-k8s on Google Cloud GKE",
      "content": "<div><article class=\"meteredContent\"><section class=\"cj ck cl cm ak cn co s\"></section><div><section class=\"ct cu cv cw cx\"><div class=\"n p\"><div class=\"ac ae af ag ah cy aj ak\"><p id=\"a991\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">In this tutorial, we aim to walk you through the steps that you need to take to install cf-for-k8s on your own Kubernetes clusters. But, for every step along the way we ask ourselves why. And by answering the why, we aim to educate you about the reasoning behind our steps while also communicating our many design decisions. Let\u0019s begin with the first why -</p><h2 id=\"c7ff\" class=\"hs\">Why Install CF-for-K8s?</h2><p id=\"9cd0\" class=\"gw gx db gy b eb io ha hb ee ip hd he hf iq hh hi hj ir hl hm hn is hp hq hr ct dy\">In our experience, we\u0019ve observed that small teams and startups often don\u0019t pick up Kubernetes because of two reasons \u0014 one is the steep learning curves. Second, for fear of introducing complexity in the workflow of their developers.</p><p id=\"8892\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">By providing a \u001ccf\u001d wrapper around Kubernetes-based infrastructure, both these problems can be solved! The collective efforts of the Cloud Foundry community have resulted in the cf-for-k8s project which, when installed, allows application developers to deploy to Kubernetes using the tried and tested cf push workflow, which is known for its simplicity and ease of use. This way, teams no longer have to make a choice between simplicity, power, and the developer experience.</p><p id=\"9ea6\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">Kubernet-easy!</p><p id=\"532a\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\"><strong class=\"gy it\"><em class=\"iu\">(psst \u0014 if you already have the cf, kubectl, bosh, gcloud CLIs, and ytt, kapp \u0014 </em></strong><a href=\"https://medium.com/cloud-foundry-foundation/installing-cf-for-k8s-on-google-cloud-gke-5d6902ee99fa#a87e\" class=\"fl iv\"><strong class=\"gy it\"><em class=\"iu\">skip ahead</em></strong></a><strong class=\"gy it\"><em class=\"iu\"> to the install process)</em></strong></p><h2 id=\"3e66\" class=\"hs\">Prerequisites</h2><p id=\"9c63\" class=\"gw gx db gy b eb io ha hb ee ip hd he hf iq hh hi hj ir hl hm hn is hp hq hr ct dy\">There are some prerequisites to installing cf-for-k8s. Let\u0019s quickly get into each of them. This section is meant for the truly uninitiated. You will need a handful of CLI tools to help with various parts of the installation.</p><h2 id=\"6f25\" class=\"hs\">You will need the cf CLI. Why?</h2><p id=\"5c92\" class=\"gw gx db gy b eb io ha hb ee ip hd he hf iq hh hi hj ir hl hm hn is hp hq hr ct dy\">The Cloud Foundry CLI is the tool that manages your local machine and helps connect to the remote Cloud Foundry endpoint. It is the official command-line client for Cloud Foundry.</p><p id=\"c3b3\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">Here are the instructions on how to install the client on your machine:</p><pre class=\"iw ix iy iz ja jb jc ca\"></pre><p id=\"fb18\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">On a Mac, use Homebrew \u0013</p><pre class=\"iw ix iy iz ja jb jc ca\"></pre><h2 id=\"3609\" class=\"hs\">Next, you will need kubectl. Why?</h2><p id=\"b9c4\" class=\"gw gx db gy b eb io ha hb ee ip hd he hf iq hh hi hj ir hl hm hn is hp hq hr ct dy\">Technically speaking, kubectl is the client for the remote Kubernetes API. It acts as the \u001ccontrol center\u001d for your Kubernetes installation. You can conduct all control operations on your remote cluster by using kubectl.</p><pre class=\"iw ix iy iz ja jb jc ca\"></pre><h2 id=\"f238\" class=\"hs\">Third, install gcloud. Why?</h2><p id=\"057b\" class=\"gw gx db gy b eb io ha hb ee ip hd he hf iq hh hi hj ir hl hm hn is hp hq hr ct dy\">This tutorial assumes that you\u0019re going to be using Google Cloud as your IaaS provider. When using Google Cloud, the gcloud command-line interface helps you create and manage all your resources on Google Cloud. Install this using the following command:</p><pre class=\"iw ix iy iz ja jb jc ca\"></pre><p id=\"60e1\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">If you\u0019re on a Mac/Windows, use <a href=\"https://cloud.google.com/sdk/docs/downloads-interactive\" class=\"fl iv\">these instructions</a> to install the Google Cloud SDK.</p><p id=\"6f9d\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">In this tutorial, we will specifically use it to create a static IP that will front the Kubernetes cluster. More details later.</p><h2 id=\"69c5\" class=\"hs\">Install the BOSH CLI. Why?</h2><p id=\"a57a\" class=\"gw gx db gy b eb io ha hb ee ip hd he hf iq hh hi hj ir hl hm hn is hp hq hr ct dy\">One step in the install process requires you to generate a YAML file. In case you would like to use a script to create this file with some defaults, you will need the bosh CLI.</p><p id=\"1c15\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">The BOSH CLI is the command-line client for the core Cloud Foundry operations and releases management tool. In this installation process, it has only a small role to play.</p><p id=\"55c7\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">Here\u0019s how to install the BOSH CLI \u0013</p><p id=\"9a89\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">Download the right binary for your OS from <a href=\"https://github.com/cloudfoundry/bosh-cli/releases\" class=\"fl iv\">https://github.com/cloudfoundry/bosh-cli/releases</a></p><p id=\"d3d5\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">Then, run the following commands:</p><pre class=\"iw ix iy iz ja jb jc ca\"></pre><h2 id=\"3cd7\" class=\"hs\">YTT and KAPP</h2><p id=\"2a84\" class=\"gw gx db gy b eb io ha hb ee ip hd he hf iq hh hi hj ir hl hm hn is hp hq hr ct dy\">Finally, you will need 2 important tools from the k14s toolchain. These are ytt and kapp.</p><p id=\"bfc3\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">ytt is a YAML templating tool that is used to create the full YAML file needed for the cf-for-k8s installation. It is used to create a configuration file that parameterizes all the values and variables needed for installing cf-for-k8s.</p><p id=\"cae9\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">Download the binary from <a href=\"https://github.com/k14s/kapp/releases\" class=\"fl iv\">https://github.com/k14s/kapp/releases</a></p><pre class=\"iw ix iy iz ja jb jc ca\"></pre><p id=\"bd19\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\"><strong class=\"gy it\">kapp </strong>is a deployment tool that performs the actual installation of cf-for-k8s on the Kubernetes cluster. There are many advantages that the usage of kapp brings such as exempting custom resources, automatically ascertain create/update/delete operations, and providing deployment progress.</p><p id=\"d73c\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">Download the binary from <a href=\"https://github.com/k14s/kapp/releases\" class=\"fl iv\">https://github.com/k14s/kapp/releases</a></p><pre class=\"iw ix iy iz ja jb jc ca\"></pre><h2 id=\"fb8e\" class=\"hs\">Creating a Kubernetes Cluster</h2><p id=\"7701\" class=\"gw gx db gy b eb io ha hb ee ip hd he hf iq hh hi hj ir hl hm hn is hp hq hr ct dy\">Along with this handful of tools, we need a Kubernetes cluster on which to deploy cf-for-k8s. Here are the steps I took with Google Cloud to create a GKE cluster. I am using three nodes with 4vCPU each and 16 GB of RAM.</p><figure class=\"iw ix iy iz ja jp cl cm paragraph-image\"><img alt=\"Image for post\" class=\"t u v jx ak\" src=\"https://miro.medium.com/max/2706/0*SatFMIPz8e8MgGX4\" width=\"1353\" srcset=\"https://miro.medium.com/max/552/0*SatFMIPz8e8MgGX4 276w, https://miro.medium.com/max/1104/0*SatFMIPz8e8MgGX4 552w, https://miro.medium.com/max/1280/0*SatFMIPz8e8MgGX4 640w, https://miro.medium.com/max/1400/0*SatFMIPz8e8MgGX4 700w\" sizes=\"700px\"></figure><figure class=\"iw ix iy iz ja jp cl cm paragraph-image\"><img alt=\"Image for post\" class=\"t u v jx ak\" src=\"https://miro.medium.com/max/2710/0*fGSUbWg8nOIymqxB\" width=\"1355\" srcset=\"https://miro.medium.com/max/552/0*fGSUbWg8nOIymqxB 276w, https://miro.medium.com/max/1104/0*fGSUbWg8nOIymqxB 552w, https://miro.medium.com/max/1280/0*fGSUbWg8nOIymqxB 640w, https://miro.medium.com/max/1400/0*fGSUbWg8nOIymqxB 700w\" sizes=\"700px\"></figure><p id=\"1814\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\"><strong class=\"gy it\">Please note:</strong></p><ul class><li id=\"3c02\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr kj kk kl dy\">The cluster version should be any Kubernetes cluster in the version range 1.16.n to 1.18.n.</li><li id=\"28a8\" class=\"gw gx db gy b eb km ha hb ee kn hd he hf ko hh hi hj kp hl hm hn kq hp hq hr kj kk kl dy\">The installation has worked with both 2 and 3 nodes in a cluster. The default docs prescribe a minimum of 5 nodes.</li></ul><p id=\"1b4c\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">Now that we have all the tools and the cluster on which to deploy, we can begin the actual installation.</p><h2 id=\"a87e\" class=\"hs\">Preparing for the Installation</h2><h2 id=\"ea35\" class=\"hs\">Set the kubectl context</h2><p id=\"f876\" class=\"gw gx db gy b eb io ha hb ee ip hd he hf iq hh hi hj ir hl hm hn is hp hq hr ct dy\">In preparation for the installation, please ensure that the context of your kubectl command is set correctly.</p><figure class=\"iw ix iy iz ja jp cl cm paragraph-image\"><img alt=\"Image for post\" class=\"t u v jx ak\" src=\"https://miro.medium.com/max/2740/0*rkLqJKtqsjSZCF2m\" width=\"1370\" srcset=\"https://miro.medium.com/max/552/0*rkLqJKtqsjSZCF2m 276w, https://miro.medium.com/max/1104/0*rkLqJKtqsjSZCF2m 552w, https://miro.medium.com/max/1280/0*rkLqJKtqsjSZCF2m 640w, https://miro.medium.com/max/1400/0*rkLqJKtqsjSZCF2m 700w\" sizes=\"700px\"></figure><p id=\"9170\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">You can verify this by using</p><pre class=\"iw ix iy iz ja jb jc ca\"></pre><p id=\"b25b\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">If you don\u0019t find the cluster name on the console matching your target cluster, please change it. When using Kubernetes with Google Cloud, this command will help you configure the connection:</p><pre class=\"iw ix iy iz ja jb jc ca\"></pre><p id=\"ae58\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">Why is this needed? To be able to direct all the installation steps (and subsequently any governance/instructions that will be needed later).</p><h2 id=\"0c7a\" class=\"hs\">Static IPs</h2><p id=\"93d7\" class=\"gw gx db gy b eb io ha hb ee ip hd he hf iq hh hi hj ir hl hm hn is hp hq hr ct dy\">Next, create a static IP.</p><p id=\"9bf9\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">Let\u0019s answer the why first \u0014 This static IP will be externally visible and will serve as the ingress to the Kubernetes cluster. We will use this IP address to refer to the Cloud Foundry endpoint. We will include this IP address as part of the installation process so that the mapping is available as soon as the installation finishes.</p><p id=\"447a\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">How to create a static IP? On Google Cloud, indicate that you want to create an address along with the region in which you want it to be present. You can also specify a label with which you can identify the address. The exact command is</p><pre class=\"iw ix iy iz ja jb jc ca\"></pre><p id=\"95e7\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">Once the static IP address is generated, create a local variable called IP. Assign the value to the local variable. This is for use later when generating deployment YAML files.</p><pre class=\"iw ix iy iz ja jb jc ca\"></pre><p id=\"96ad\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">There are a couple of alternative approaches as well. You can always modify the property of the ingress service post-installation. You can set the static IP as the parameter for the load balancer. The command for doing that is</p><pre class=\"iw ix iy iz ja jb jc ca\"></pre><p id=\"4d65\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">The other approach is to use hostnames. Obviously, using static IP addresses is not desirable. For the same reason as websites having domain names and not IPs. :shrug: So, you can use a hostname and configure an \u0018A\u0019 record that redirects incoming requests to the ingress/load balancer that manages the Kubernetes cluster. (There will be a second tutorial that will provide instructions on using hostnames. Stay tuned!)</p><h2 id=\"a308\" class=\"hs\">Installation</h2><p id=\"c4e1\" class=\"gw gx db gy b eb io ha hb ee ip hd he hf iq hh hi hj ir hl hm hn is hp hq hr ct dy\">Now we get into the actual installation process.</p><h2 id=\"872e\" class=\"jd ht db ea hu lt lu ed hx lv lw eg ia eh lx ej ie ek ly em ii en lz ep im ma dy\">Step 1:</h2><p id=\"6b40\" class=\"gw gx db gy b eb io ha hb ee ip hd he hf iq hh hi hj ir hl hm hn is hp hq hr ct dy\">Clone the cf-for-k8s repo. Why? Duh!</p><pre class=\"iw ix iy iz ja jb jc ca\"></pre><h2 id=\"0fd8\" class=\"jd ht db ea hu lt lu ed hx lv lw eg ia eh lx ej ie ek ly em ii en lz ep im ma dy\">Step 2:</h2><p id=\"7345\" class=\"gw gx db gy b eb io ha hb ee ip hd he hf iq hh hi hj ir hl hm hn is hp hq hr ct dy\">Switch into the cloned directory.</p><pre class=\"iw ix iy iz ja jb jc ca\"></pre><h2 id=\"12c7\" class=\"jd ht db ea hu lt lu ed hx lv lw eg ia eh lx ej ie ek ly em ii en lz ep im ma dy\">Step 3:</h2><p id=\"4338\" class=\"gw gx db gy b eb io ha hb ee ip hd he hf iq hh hi hj ir hl hm hn is hp hq hr ct dy\">Create a directory to use as a temporary/swap space for the YAML files. Assign a local variable named TMP_DIR the path to this directory. We will be using this directory to store the YAML files that we generate and use for the final installation.</p><pre class=\"iw ix iy iz ja jb jc ca\"></pre><h2 id=\"cb9f\" class=\"jd ht db ea hu lt lu ed hx lv lw eg ia eh lx ej ie ek ly em ii en lz ep im ma dy\">Step 4:</h2><p id=\"97a5\" class=\"gw gx db gy b eb io ha hb ee ip hd he hf iq hh hi hj ir hl hm hn is hp hq hr ct dy\">A YAML file containing the deployment manifest is needed. This will contain the details of the endpoint, our container registry credentials, and static ip among other things. You can generate this by yourself or make use of the auto-generate script that is included in the cf-for-k8s repo. This is a temporary measure meant for illustrative purposes only.</p><p id=\"d30c\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">The command used to create the YAML file is:</p><pre class=\"iw ix iy iz ja jb jc ca\"></pre><p id=\"be41\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">Don\u0019t be alarmed when you see this message:</p><p id=\"bd75\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\"><em class=\"iu\">WARNING: The hack scripts are intended for the development of cf-for-k8s.</em></p><p id=\"e9eb\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\"><strong class=\"gy it\"><em class=\"iu\">They are not officially supported product bits. Their interface and behavior may change at any time without notice.</em></strong></p><h2 id=\"9eb7\" class=\"jd ht db ea hu lt lu ed hx lv lw eg ia eh lx ej ie ek ly em ii en lz ep im ma dy\">Step 5:</h2><p id=\"6297\" class=\"gw gx db gy b eb io ha hb ee ip hd he hf iq hh hi hj ir hl hm hn is hp hq hr ct dy\">A couple of additions have to be made to the cf-yalues.yml file that you created.</p><p id=\"2958\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">1. Add credentials that can help connect to your Container Registry. This could be Docker Hub or any private one.</p><pre class=\"iw ix iy iz ja jb jc ca\"></pre><p id=\"2834\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">2. Append the static IP address we generated before to the cf-values.yml file. This is to help assign the ingress gateway of the Kubernetes installation.</p><pre class=\"iw ix iy iz ja jb jc ca\"></pre><h2 id=\"c999\" class=\"jd ht db ea hu lt lu ed hx lv lw eg ia eh lx ej ie ek ly em ii en lz ep im ma dy\">Step 6:</h2><p id=\"4e78\" class=\"gw gx db gy b eb io ha hb ee ip hd he hf iq hh hi hj ir hl hm hn is hp hq hr ct dy\">Next, use the ytt tool to generate the final template to be used for the installation.</p><pre class=\"iw ix iy iz ja jb jc ca\"></pre><h2 id=\"1e74\" class=\"jd ht db ea hu lt lu ed hx lv lw eg ia eh lx ej ie ek ly em ii en lz ep im ma dy\">Step 7:</h2><p id=\"c693\" class=\"gw gx db gy b eb io ha hb ee ip hd he hf iq hh hi hj ir hl hm hn is hp hq hr ct dy\">Finally, use the kapp command to install cf-for-k8s on your Kubernetes cluster.</p><pre class=\"iw ix iy iz ja jb jc ca\"></pre><p id=\"3bfe\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">Wait for the installation to complete. It takes an estimated 8\u001310 minutes to complete.</p><h2 id=\"647d\" class=\"hs\">After installation</h2><p id=\"e228\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">The ability to connect with the Cloud Foundry API endpoint is the best validation of the installation. You can do this by setting the endpoint of your local cf CLI as follows</p><pre class=\"iw ix iy iz ja jb jc ca\"></pre><p id=\"72a5\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">This step will allow you to verify if the cf installation is functional. Next, log in to the instance to put the instance to some real use.</p><pre class=\"iw ix iy iz ja jb jc ca\"></pre><p id=\"0102\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">Create an org for your use and create spaces within them. Set the new space as your \u001ctarget\u001d to which you will deploy your applications.</p><pre class=\"iw ix iy iz ja jb jc ca\"></pre><p id=\"e097\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">The simplest app you can push is the Node.js app that is included within the cf-for-k8s directory. Use this command:</p><pre class=\"iw ix iy iz ja jb jc ca\"></pre><p id=\"126c\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">This will push a test application to the Kubernetes clusters. Use the URL endpoint generated and open the application in your browser.</p><figure class=\"iw ix iy iz ja jp cl cm paragraph-image\"><img alt=\"Image for post\" class=\"t u v jx ak\" src=\"https://miro.medium.com/max/1932/0*nCubMp3DJZ8W1rgd\" width=\"966\" srcset=\"https://miro.medium.com/max/552/0*nCubMp3DJZ8W1rgd 276w, https://miro.medium.com/max/1104/0*nCubMp3DJZ8W1rgd 552w, https://miro.medium.com/max/1280/0*nCubMp3DJZ8W1rgd 640w, https://miro.medium.com/max/1400/0*nCubMp3DJZ8W1rgd 700w\" sizes=\"700px\"></figure><h2 id=\"b0c4\" class=\"jd ht db ea hu lt lu ed hx lv lw eg ia eh lx ej ie ek ly em ii en lz ep im ma dy\">Conclusion</h2><p id=\"35a9\" class=\"gw gx db gy b eb io ha hb ee ip hd he hf iq hh hi hj ir hl hm hn is hp hq hr ct dy\">With this installation, you can help developers in your org realize the benefits of the simplified \u0018cf push\u0019 process while deploying to modern infrastructure. The cf-for-k8s community has got positive feedback for this abstraction layer over Kubernetes. There are several ongoing projects that aim to improve this offering.</p></div></div></section><section class=\"ct cu cv cw cx\"><div class=\"n p\"><div class=\"ac ae af ag ah cy aj ak\"><p id=\"be5d\" class=\"gw gx db gy b eb gz ha hb ee hc hd he hf hg hh hi hj hk hl hm hn ho hp hq hr ct dy\">The best way to connect with the Cloud Foundry community is to join our Slack Workspace at <a href=\"https://slack.cloudfoundry.org/\" class=\"fl iv\">https://slack.cloudfoundry.org/</a>. Those in the Slack community help you get quickly connected with other members or someone from the Cloud Foundry Foundation.</p></div></div></section></div></article></div>",
      "contentAsText": "In this tutorial, we aim to walk you through the steps that you need to take to install cf-for-k8s on your own Kubernetes clusters. But, for every step along the way we ask ourselves why. And by answering the why, we aim to educate you about the reasoning behind our steps while also communicating our many design decisions. Let\u0019s begin with the first why -Why Install CF-for-K8s?In our experience, we\u0019ve observed that small teams and startups often don\u0019t pick up Kubernetes because of two reasons \u0014 one is the steep learning curves. Second, for fear of introducing complexity in the workflow of their developers.By providing a \u001ccf\u001d wrapper around Kubernetes-based infrastructure, both these problems can be solved! The collective efforts of the Cloud Foundry community have resulted in the cf-for-k8s project which, when installed, allows application developers to deploy to Kubernetes using the tried and tested cf push workflow, which is known for its simplicity and ease of use. This way, teams no longer have to make a choice between simplicity, power, and the developer experience.Kubernet-easy!(psst \u0014 if you already have the cf, kubectl, bosh, gcloud CLIs, and ytt, kapp \u0014 skip ahead to the install process)PrerequisitesThere are some prerequisites to installing cf-for-k8s. Let\u0019s quickly get into each of them. This section is meant for the truly uninitiated. You will need a handful of CLI tools to help with various parts of the installation.You will need the cf CLI. Why?The Cloud Foundry CLI is the tool that manages your local machine and helps connect to the remote Cloud Foundry endpoint. It is the official command-line client for Cloud Foundry.Here are the instructions on how to install the client on your machine:On a Mac, use Homebrew \u0013Next, you will need kubectl. Why?Technically speaking, kubectl is the client for the remote Kubernetes API. It acts as the \u001ccontrol center\u001d for your Kubernetes installation. You can conduct all control operations on your remote cluster by using kubectl.Third, install gcloud. Why?This tutorial assumes that you\u0019re going to be using Google Cloud as your IaaS provider. When using Google Cloud, the gcloud command-line interface helps you create and manage all your resources on Google Cloud. Install this using the following command:If you\u0019re on a Mac/Windows, use these instructions to install the Google Cloud SDK.In this tutorial, we will specifically use it to create a static IP that will front the Kubernetes cluster. More details later.Install the BOSH CLI. Why?One step in the install process requires you to generate a YAML file. In case you would like to use a script to create this file with some defaults, you will need the bosh CLI.The BOSH CLI is the command-line client for the core Cloud Foundry operations and releases management tool. In this installation process, it has only a small role to play.Here\u0019s how to install the BOSH CLI \u0013Download the right binary for your OS from https://github.com/cloudfoundry/bosh-cli/releasesThen, run the following commands:YTT and KAPPFinally, you will need 2 important tools from the k14s toolchain. These are ytt and kapp.ytt is a YAML templating tool that is used to create the full YAML file needed for the cf-for-k8s installation. It is used to create a configuration file that parameterizes all the values and variables needed for installing cf-for-k8s.Download the binary from https://github.com/k14s/kapp/releaseskapp is a deployment tool that performs the actual installation of cf-for-k8s on the Kubernetes cluster. There are many advantages that the usage of kapp brings such as exempting custom resources, automatically ascertain create/update/delete operations, and providing deployment progress.Download the binary from https://github.com/k14s/kapp/releasesCreating a Kubernetes ClusterAlong with this handful of tools, we need a Kubernetes cluster on which to deploy cf-for-k8s. Here are the steps I took with Google Cloud to create a GKE cluster. I am using three nodes with 4vCPU each and 16 GB of RAM.Please note:The cluster version should be any Kubernetes cluster in the version range 1.16.n to 1.18.n.The installation has worked with both 2 and 3 nodes in a cluster. The default docs prescribe a minimum of 5 nodes.Now that we have all the tools and the cluster on which to deploy, we can begin the actual installation.Preparing for the InstallationSet the kubectl contextIn preparation for the installation, please ensure that the context of your kubectl command is set correctly.You can verify this by usingIf you don\u0019t find the cluster name on the console matching your target cluster, please change it. When using Kubernetes with Google Cloud, this command will help you configure the connection:Why is this needed? To be able to direct all the installation steps (and subsequently any governance/instructions that will be needed later).Static IPsNext, create a static IP.Let\u0019s answer the why first \u0014 This static IP will be externally visible and will serve as the ingress to the Kubernetes cluster. We will use this IP address to refer to the Cloud Foundry endpoint. We will include this IP address as part of the installation process so that the mapping is available as soon as the installation finishes.How to create a static IP? On Google Cloud, indicate that you want to create an address along with the region in which you want it to be present. You can also specify a label with which you can identify the address. The exact command isOnce the static IP address is generated, create a local variable called IP. Assign the value to the local variable. This is for use later when generating deployment YAML files.There are a couple of alternative approaches as well. You can always modify the property of the ingress service post-installation. You can set the static IP as the parameter for the load balancer. The command for doing that isThe other approach is to use hostnames. Obviously, using static IP addresses is not desirable. For the same reason as websites having domain names and not IPs. :shrug: So, you can use a hostname and configure an \u0018A\u0019 record that redirects incoming requests to the ingress/load balancer that manages the Kubernetes cluster. (There will be a second tutorial that will provide instructions on using hostnames. Stay tuned!)InstallationNow we get into the actual installation process.Step 1:Clone the cf-for-k8s repo. Why? Duh!Step 2:Switch into the cloned directory.Step 3:Create a directory to use as a temporary/swap space for the YAML files. Assign a local variable named TMP_DIR the path to this directory. We will be using this directory to store the YAML files that we generate and use for the final installation.Step 4:A YAML file containing the deployment manifest is needed. This will contain the details of the endpoint, our container registry credentials, and static ip among other things. You can generate this by yourself or make use of the auto-generate script that is included in the cf-for-k8s repo. This is a temporary measure meant for illustrative purposes only.The command used to create the YAML file is:Don\u0019t be alarmed when you see this message:WARNING: The hack scripts are intended for the development of cf-for-k8s.They are not officially supported product bits. Their interface and behavior may change at any time without notice.Step 5:A couple of additions have to be made to the cf-yalues.yml file that you created.1. Add credentials that can help connect to your Container Registry. This could be Docker Hub or any private one.2. Append the static IP address we generated before to the cf-values.yml file. This is to help assign the ingress gateway of the Kubernetes installation.Step 6:Next, use the ytt tool to generate the final template to be used for the installation.Step 7:Finally, use the kapp command to install cf-for-k8s on your Kubernetes cluster.Wait for the installation to complete. It takes an estimated 8\u001310 minutes to complete.After installationThe ability to connect with the Cloud Foundry API endpoint is the best validation of the installation. You can do this by setting the endpoint of your local cf CLI as followsThis step will allow you to verify if the cf installation is functional. Next, log in to the instance to put the instance to some real use.Create an org for your use and create spaces within them. Set the new space as your \u001ctarget\u001d to which you will deploy your applications.The simplest app you can push is the Node.js app that is included within the cf-for-k8s directory. Use this command:This will push a test application to the Kubernetes clusters. Use the URL endpoint generated and open the application in your browser.ConclusionWith this installation, you can help developers in your org realize the benefits of the simplified \u0018cf push\u0019 process while deploying to modern infrastructure. The cf-for-k8s community has got positive feedback for this abstraction layer over Kubernetes. There are several ongoing projects that aim to improve this offering.The best way to connect with the Cloud Foundry community is to join our Slack Workspace at https://slack.cloudfoundry.org/. Those in the Slack community help you get quickly connected with other members or someone from the Cloud Foundry Foundation.",
      "publishedDate": "2020-11-17T14:50:42.084Z",
      "description": "In this tutorial, we aim to walk you through the steps that you need to take to install cf-for-k8s on your own Kubernetes clusters. But, for every step along the way we ask ourselves why. And by…",
      "ogDescription": "In this tutorial, we aim to walk you through the steps that you need to take to install cf-for-k8s on your own Kubernetes clusters. But…"
    },
    {
      "url": "https://tansanrao.com/kubernetes-ha-cluster-with-kubeadm/",
      "title": "Guide: Kubernetes Multi-Master HA Cluster with kubeadm",
      "content": "<div class=\"pos-relative js-post-content\"> <p>Hello everybody, Tansanrao here! This post is going to guide you into setting up a Multi-Master HA (High-Availability) Kubernetes Cluster on bare-metal or virtual machines.</p><p>All our VM images will be based on Ubuntu 20.04.1 Server and for the purpose of this guide, will be Virtual Machines on a VMware ESXi host.</p><p>We will require 7 Virtual Machines with a minimum spec of 2 Cores and 4GB RAM per Node for decent performance. Also make sure that you have Static IPs assigned on your DHCP Server.</p><p>We are using the following Hostnames &amp; IP Assignments:</p><ul><li>1 HAProxy Load Balancer Node <br> \u0014 k8s-haproxy : <code>192.168.1.112</code></li><li>3 Etcd/Kubernetes Master Nodes<br> \u0014 k8s-master-a : <code>192.168.1.113</code><br> \u0014 k8s-master-b : <code>192.168.1.114</code><br> \u0014 k8s-master-c : <code>192.168.1.115</code></li><li>3 Kubernetes Worker Nodes<br> \u0014 k8s-node-a : <code>192.168.1.116</code><br> \u0014 k8s-node-b : <code>192.168.1.117</code><br> \u0014 k8s-node-c : <code>192.168.1.118</code></li></ul><p>We will also require 1 linux client machine, if unavailable, the client tools may be installed on the HAProxy node.</p><p>The minimum for production use is 2 physical hosts with at least 1 Master on each with the recommended being 3 hosts with 1 Master and 1 Worker Node Each with an external load balancer. For the sake of this guide, I am running all 7 nodes on the same ESXi host. A single host should be safe enough to use for lab and test environments but do not run anything mission critical on it.</p><p>Let\u0019s get started!</p><h2 id=\"prepare-virtual-machines-servers\">Prepare Virtual Machines / Servers</h2><p>Start by preparing 7 machines with Ubuntu 20.04.1 Server using the correct hostnames and IP addresses. Once done, power on all of them and apply the latest updates using:</p><pre><code class=\"language-bash\">sudo apt update &amp;&amp; sudo apt upgrade\n</code></pre><h3 id=\"installing-cfssl-\">Installing <code>cfssl</code>.</h3><p>CFSSL is an SSL tool by Cloudflare which lets us create our Certs and CAs.</p><h4 id=\"step-1-download-the-binaries\">Step 1 - Download the binaries</h4><pre><code class=\"language-bash\">wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64\nwget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64\n</code></pre><h4 id=\"step-2-add-the-execution-permission-to-the-binaries\">Step 2 - Add the execution permission to the binaries</h4><pre><code class=\"language-bash\">chmod +x cfssl*\n</code></pre><h4 id=\"step-3-move-the-binaries-to-usr-local-bin\">Step 3 - Move the binaries to <code>/usr/local/bin</code></h4><pre><code class=\"language-bash\">sudo mv cfssl_linux-amd64 /usr/local/bin/cfssl\nsudo mv cfssljson_linux-amd64 /usr/local/bin/cfssljson\n</code></pre><h4 id=\"step-4-verify-the-installation\">Step 4 - Verify the installation</h4><pre><code class=\"language-bash\">cfssl version\n</code></pre><h3 id=\"installing-kubectl\">Installing kubectl</h3><h4 id=\"step-1-get-the-binary\">Step 1 - Get the binary</h4><p>make sure it\u0019s the same version as the cluster, in our case we are using v1.19</p><pre><code class=\"language-bash\">curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.19.0/bin/linux/amd64/kubectl\n</code></pre><h4 id=\"step-2-add-the-execution-permission-to-the-binary\">Step 2 - Add the execution permission to the binary</h4><pre><code class=\"language-bash\">chmod +x kubectl\n</code></pre><h4 id=\"step-3-move-the-binary-to-usr-local-bin\">Step 3 - Move the binary to <code>/usr/local/bin</code></h4><pre><code class=\"language-bash\">sudo mv kubectl /usr/local/bin\n</code></pre><h4 id=\"step-4-verify-the-installation-1\">Step 4 - Verify the installation</h4><pre><code class=\"language-bash\">kubectl version\n</code></pre><h2 id=\"installing-haproxy-load-balancer\">Installing HAProxy Load Balancer</h2><p>As we will be deploying three Kubernetes master nodes, we need to deploy an HAProxy Load Balancer in front of them to distribute the traffic.</p><h3 id=\"step-1-ssh-to-the-haproxy-vm\">Step 1 - SSH to the HAProxy VM</h3><pre><code class=\"language-bash\">ssh <a href=\"/cdn-cgi/l/email-protection\" class=\"__cf_email__\">[email&#xA0;protected]</a>\n</code></pre><h3 id=\"step-2-install-haproxy\">Step 2 - Install HAProxy</h3><pre><code class=\"language-bash\">sudo apt-get install haproxy\n</code></pre><h3 id=\"step-3-configure-haproxy\">Step 3 - Configure HAProxy</h3><pre><code class=\"language-bash\">sudo nano /etc/haproxy/haproxy.cfg\n</code></pre><p>Enter the following config:</p><pre><code class=\"language-cfg\">global\n...\ndefault\n...\nfrontend kubernetes\nbind 192.168.1.112:6443\noption tcplog\nmode tcp\ndefault_backend kubernetes-master-nodes\n\n\nbackend kubernetes-master-nodes\nmode tcp\nbalance roundrobin\noption tcp-check\nserver k8s-master-a 192.168.1.113:6443 check fall 3 rise 2\nserver k8s-master-b 192.168.1.114:6443 check fall 3 rise 2\nserver k8s-master-c 192.168.1.115:6443 check fall 3 rise 2\n</code></pre><h3 id=\"step-4-restart-haproxy\">Step 4 - Restart HAProxy</h3><pre><code class=\"language-bash\">sudo systemctl restart haproxy\n</code></pre><h2 id=\"generating-the-tls-certificates\">Generating the TLS certificates</h2><p>These steps can be done on your Linux client if you have one or on the HAProxy machine depending on where you installed the cfssl tool.</p><pre><code class=\"language-bash\">nano ca-config.json\n</code></pre><p>Enter the following config:</p><pre><code class=\"language-json\">{\n  &quot;signing&quot;: {\n    &quot;default&quot;: {\n      &quot;expiry&quot;: &quot;8760h&quot;\n    },\n    &quot;profiles&quot;: {\n      &quot;kubernetes&quot;: {\n        &quot;usages&quot;: [&quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot;],\n        &quot;expiry&quot;: &quot;8760h&quot;\n      }\n    }\n  }\n}\n</code></pre><pre><code class=\"language-bash\">nano ca-csr.json\n</code></pre><p>Enter the following config, Change the names as necessary:</p><pre><code class=\"language-json\">{\n  &quot;CN&quot;: &quot;Kubernetes&quot;,\n  &quot;key&quot;: {\n    &quot;algo&quot;: &quot;rsa&quot;,\n    &quot;size&quot;: 2048\n  },\n  &quot;names&quot;: [\n  {\n    &quot;C&quot;: &quot;IN&quot;,\n    &quot;L&quot;: &quot;Belgaum&quot;,\n    &quot;O&quot;: &quot;Tansanrao&quot;,\n    &quot;OU&quot;: &quot;CA&quot;,\n    &quot;ST&quot;: &quot;Karnataka&quot;\n  }\n ]\n}\n</code></pre><pre><code class=\"language-bash\">cfssl gencert -initca ca-csr.json | cfssljson -bare ca\n</code></pre><h4 id=\"step-4-verify-that-the-ca-key-pem-and-the-ca-pem-were-generated\">Step 4 - Verify that the ca-key.pem and the ca.pem were generated</h4><pre><code class=\"language-bash\">ls -la\n</code></pre><h3 id=\"creating-the-certificate-for-the-etcd-cluster\">Creating the certificate for the Etcd cluster</h3><h4 id=\"step-1-create-the-certificate-signing-request-configuration-file\">Step 1 - Create the certificate signing request configuration file</h4><pre><code class=\"language-bash\">nano kubernetes-csr.json\n</code></pre><p>Add the following config:</p><pre><code class=\"language-json\">{\n  &quot;CN&quot;: &quot;Kubernetes&quot;,\n  &quot;key&quot;: {\n    &quot;algo&quot;: &quot;rsa&quot;,\n    &quot;size&quot;: 2048\n  },\n  &quot;names&quot;: [\n  {\n    &quot;C&quot;: &quot;IN&quot;,\n    &quot;L&quot;: &quot;Belgaum&quot;,\n    &quot;O&quot;: &quot;Tansanrao&quot;,\n    &quot;OU&quot;: &quot;CA&quot;,\n    &quot;ST&quot;: &quot;Karnataka&quot;\n  }\n ]\n}\n</code></pre><h4 id=\"step-2-generate-the-certificate-and-private-key\">Step 2 - Generate the certificate and private key</h4><pre><code class=\"language-bash\">cfssl gencert \\\n-ca=ca.pem \\\n-ca-key=ca-key.pem \\\n-config=ca-config.json \\\n-hostname=192.168.1.112,192.168.1.113,192.168.1.114,192.168.1.115,127.0.0.1,kubernetes.default \\\n-profile=kubernetes kubernetes-csr.json | \\\ncfssljson -bare kubernetes\n</code></pre><h4 id=\"step-3-verify-that-the-kubernetes-key-pem-and-the-kubernetes-pem-file-were-generated-\">Step 3 - Verify that the <code>kubernetes-key.pem</code> and the <code>kubernetes.pem</code> file were generated.</h4><pre><code class=\"language-bash\">ls -la\n</code></pre><h4 id=\"step-4-copy-the-certificate-to-each-nodes\">Step 4 - Copy the certificate to each nodes</h4><pre><code class=\"language-bash\">scp ca.pem kubernetes.pem kubernetes-key.pem <a href=\"/cdn-cgi/l/email-protection\" class=\"__cf_email__\">[email&#xA0;protected]</a>:~\nscp ca.pem kubernetes.pem kubernetes-key.pem <a href=\"/cdn-cgi/l/email-protection\" class=\"__cf_email__\">[email&#xA0;protected]</a>:~\nscp ca.pem kubernetes.pem kubernetes-key.pem <a href=\"/cdn-cgi/l/email-protection\" class=\"__cf_email__\">[email&#xA0;protected]</a>:~\nscp ca.pem kubernetes.pem kubernetes-key.pem <a href=\"/cdn-cgi/l/email-protection\" class=\"__cf_email__\">[email&#xA0;protected]</a>:~\nscp ca.pem kubernetes.pem kubernetes-key.pem <a href=\"/cdn-cgi/l/email-protection\" class=\"__cf_email__\">[email&#xA0;protected]</a>:~\nscp ca.pem kubernetes.pem kubernetes-key.pem <a href=\"/cdn-cgi/l/email-protection\" class=\"__cf_email__\">[email&#xA0;protected]</a>:~\n</code></pre><h2 id=\"preparing-the-nodes-for-kubeadm\">Preparing the nodes for kubeadm</h2><h3 id=\"initial-setup-for-all-master-and-node-machines\">Initial Setup for all master and node machines</h3><p>Copy the commands below and paste them into a <code>setup.sh</code> file and then execute it with <code>. setup.sh</code>.</p><p>This script will check for and uninstall older versions of docker and will replace it with the latest version of docker-ce for ubuntu 20.04. It will also add the kubernetes repository and install <code>kubelet</code>,<code> kubeadm</code>, <code>kubectl</code> and will also mark the packages to prevent auto updates.</p><pre><code class=\"language-bash\">sudo apt-get remove docker docker-engine docker.io containerd runc\n\nsudo apt-get install -y \\\n    apt-transport-https \\\n    ca-certificates \\\n    curl \\\n    gnupg-agent \\\n    software-properties-common\n\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\n\nsudo add-apt-repository \\\n   &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \\\n   $(lsb_release -cs) \\\n   stable&quot;\n\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io\n\nsudo usermod -aG docker tansanrao\n\ncurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\ncat &lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list\ndeb https://apt.kubernetes.io/ kubernetes-xenial main\nEOF\nsudo apt-get update\nsudo apt-get install -y kubelet kubeadm kubectl\nsudo apt-mark hold kubelet kubeadm kubectl\n\nsudo swapoff -a\n</code></pre><p>Now we need to turn swap off for the nodes by editing <code>/etc/fstab</code> on each machine.</p><pre><code class=\"language-bash\">sudo nano /etc/fstab\n</code></pre><p>Comment the line that starts with <code>/swap</code> or <code>/swap.img</code>. My <code>/etc/fstab</code> looks like this after making the change.</p><pre><code class=\"language-bash\"># /etc/fstab: static file system information.\n#\n# Use &apos;blkid&apos; to print the universally unique identifier for a\n# device; this may be used with UUID= as a more robust way to name devices\n# that works even if disks are added and removed. See fstab(5).\n#\n# &lt;file system&gt; &lt;mount point&gt;   &lt;type&gt;  &lt;options&gt;       &lt;dump&gt;  &lt;pass&gt;\n# / was on /dev/ubuntu-vg/ubuntu-lv during curtin installation\n/dev/disk/by-id/dm-uuid-LVM-s96R5iaP77QRtKuZZ0mYLuJcarDuQldMUj3yYFLQDRKWOqz9PHtLTnMMl2cbxpkC / ext4 defaults 0 0\n# /boot was on /dev/sda2 during curtin installation\n/dev/disk/by-uuid/bcc851c2-bbc4-44c0-bb36-c142eedd63a6 /boot ext4 defaults 0 0\n#/swap.img      none    swap    sw      0       0\n\n</code></pre><h3 id=\"installing-and-configuring-etcd-on-all-3-master-nodes\">Installing and configuring Etcd on all 3 Master Nodes</h3><h4 id=\"step-1-download-and-move-etcd-files-and-certs-to-their-respective-places\">Step 1 - Download and move etcd files and certs to their respective places</h4><pre><code class=\"language-bash\">sudo mkdir /etc/etcd /var/lib/etcd\n\nsudo mv ~/ca.pem ~/kubernetes.pem ~/kubernetes-key.pem /etc/etcd\n\nwget https://github.com/etcd-io/etcd/releases/download/v3.4.13/etcd-v3.4.13-linux-amd64.tar.gz\n\ntar xvzf etcd-v3.4.13-linux-amd64.tar.gz\n\nsudo mv etcd-v3.4.13-linux-amd64/etcd* /usr/local/bin/\n</code></pre><h4 id=\"step-2-create-an-etcd-systemd-unit-file\">Step 2 - Create an etcd systemd unit file</h4><pre><code class=\"language-bash\">sudo nano /etc/systemd/system/etcd.service\n</code></pre><p>Enter the following config:</p><pre><code class=\"language-bash\">[Unit]\nDescription=etcd\nDocumentation=https://github.com/coreos\n\n\n[Service]\nExecStart=/usr/local/bin/etcd \\\n  --name 192.168.1.113 \\\n  --cert-file=/etc/etcd/kubernetes.pem \\\n  --key-file=/etc/etcd/kubernetes-key.pem \\\n  --peer-cert-file=/etc/etcd/kubernetes.pem \\\n  --peer-key-file=/etc/etcd/kubernetes-key.pem \\\n  --trusted-ca-file=/etc/etcd/ca.pem \\\n  --peer-trusted-ca-file=/etc/etcd/ca.pem \\\n  --peer-client-cert-auth \\\n  --client-cert-auth \\\n  --initial-advertise-peer-urls https://192.168.1.113:2380 \\\n  --listen-peer-urls https://192.168.1.113:2380 \\\n  --listen-client-urls https://192.168.1.113:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls https://192.168.1.113:2379 \\\n  --initial-cluster-token etcd-cluster-0 \\\n  --initial-cluster 192.168.1.113=https://192.168.1.113:2380,192.168.1.114=https://192.168.1.114:2380,192.168.1.115=https://192.168.1.115:2380 \\\n  --initial-cluster-state new \\\n  --data-dir=/var/lib/etcd\nRestart=on-failure\nRestartSec=5\n\n\n\n[Install]\nWantedBy=multi-user.target\n</code></pre><p>Replace the IP address on all fields except the <code>\u0014initial-cluster</code> field to match the machine IP.</p><h4 id=\"step-3-reload-the-daemon-configuration-\">Step 3 - Reload the daemon configuration.</h4><pre><code class=\"language-bash\">sudo systemctl daemon-reload\n</code></pre><h4 id=\"step-4-enable-etcd-to-start-at-boot-time-\">Step 4 - Enable etcd to start at boot time.</h4><pre><code class=\"language-bash\">sudo systemctl enable etcd\n</code></pre><h4 id=\"step-5-start-etcd-\">Step 5 - Start etcd.</h4><pre><code class=\"language-bash\">sudo systemctl start etcd\n</code></pre><p><strong><em>Repeat the process for all 3 master nodes and then move to step 6.</em></strong></p><h3 id=\"step-6-verify-that-the-cluster-is-up-and-running-\">Step 6 - Verify that the cluster is up and running.</h3><pre><code class=\"language-bash\">ETCDCTL_API=3 etcdctl member list\n</code></pre><p>It should give you an output similar to this:</p><pre><code class=\"language-bash\">73ea126859b3ba4, started, 192.168.1.114, https://192.168.1.114:2380, https://192.168.1.114:2379, false\na28911111213cc6c, started, 192.168.1.115, https://192.168.1.115:2380, https://192.168.1.115:2379, false\nfeadb5a763a32caa, started, 192.168.1.113, https://192.168.1.113:2380, https://192.168.1.113:2379, false\n</code></pre><h2 id=\"initialising-the-master-nodes\">Initialising the Master Nodes</h2><h3 id=\"initialising-the-first-master-node\">Initialising the first Master Node</h3><h4 id=\"step-1-ssh-to-the-first-master-node\">Step 1 - SSH to the first Master Node</h4><pre><code class=\"language-bash\">ssh <a href=\"/cdn-cgi/l/email-protection\" class=\"__cf_email__\">[email&#xA0;protected]</a>\n</code></pre><h4 id=\"step-2-create-the-configuration-file-for-kubeadm\">Step 2 - Create the configuration file for <code>kubeadm</code></h4><pre><code class=\"language-yaml\">nano config.yaml\n</code></pre><p>Enter the following config:</p><pre><code class=\"language-yaml\">apiVersion: kubeadm.k8s.io/v1beta2\nkind: ClusterConfiguration\nkubernetesVersion: v1.19.0\ncontrolPlaneEndpoint: &quot;192.168.1.112:6443&quot;\netcd:\n  external:\n    endpoints:\n      - https://192.168.1.113:2379\n      - https://192.168.1.114:2379\n      - https://192.168.1.115:2379\n    caFile: /etc/etcd/ca.pem\n    certFile: /etc/etcd/kubernetes.pem\n    keyFile: /etc/etcd/kubernetes-key.pem\nnetworking:\n  podSubnet: 10.30.0.0/24\napiServer:\n  certSANs:\n    - &quot;192.168.1.112&quot;\n  extraArgs:\n    apiserver-count: &quot;3&quot;\n</code></pre><p>Add any additional domains or IP Addresses that you would want to connect to the cluster under <code>certSANs</code>.</p><h4 id=\"step-3-initialise-the-machine-as-a-master-node\">Step 3 - Initialise the machine as a master node</h4><pre><code class=\"language-bash\">sudo kubeadm init --config=config.yaml\n</code></pre><h4 id=\"step-4-copy-the-certificates-to-the-two-other-masters\">Step 4 - Copy the certificates to the two other masters</h4><pre><code class=\"language-bash\">sudo scp -r /etc/kubernetes/pki <a href=\"/cdn-cgi/l/email-protection\" class=\"__cf_email__\">[email&#xA0;protected]</a>:~\nsudo scp -r /etc/kubernetes/pki <a href=\"/cdn-cgi/l/email-protection\" class=\"__cf_email__\">[email&#xA0;protected]</a>:~\n</code></pre><h3 id=\"initialising-the-second-master-node\">Initialising the second Master Node</h3><h4 id=\"step-1-ssh-to-the-second-master-node\">Step 1 - SSH to the second Master Node</h4><pre><code class=\"language-bash\">ssh <a href=\"/cdn-cgi/l/email-protection\" class=\"__cf_email__\">[email&#xA0;protected]</a>\n</code></pre><h4 id=\"step-2-remove-the-apiserver-crt-and-apiserver-key\">Step 2 - Remove the <code>apiserver.crt</code> and <code>apiserver.key</code></h4><pre><code class=\"language-bash\">rm ~/pki/apiserver.*\n</code></pre><h4 id=\"step-3-move-the-certificates-to-the-etc-kubernetes-directory-\">Step 3 - Move the certificates to the <code>/etc/kubernetes</code> directory.</h4><pre><code class=\"language-bash\">sudo mv ~/pki /etc/kubernetes/\n</code></pre><h4 id=\"step-4-create-the-configuration-file-for-kubeadm\">Step 4 - Create the configuration file for <code>kubeadm</code></h4><pre><code class=\"language-bash\">nano config.yaml\n</code></pre><p>Enter the following config:</p><pre><code class=\"language-yaml\">apiVersion: kubeadm.k8s.io/v1beta2\nkind: ClusterConfiguration\nkubernetesVersion: v1.19.0\ncontrolPlaneEndpoint: &quot;192.168.1.112:6443&quot;\netcd:\n  external:\n    endpoints:\n      - https://192.168.1.113:2379\n      - https://192.168.1.114:2379\n      - https://192.168.1.115:2379\n    caFile: /etc/etcd/ca.pem\n    certFile: /etc/etcd/kubernetes.pem\n    keyFile: /etc/etcd/kubernetes-key.pem\nnetworking:\n  podSubnet: 10.30.0.0/24\napiServer:\n  certSANs:\n    - &quot;192.168.1.112&quot;\n  extraArgs:\n    apiserver-count: &quot;3&quot;\n</code></pre><h4 id=\"step-5-initialise-the-machine-as-a-master-node-\">Step 5 - Initialise the machine as a master node.</h4><pre><code class=\"language-bash\">sudo kubeadm init --config=config.yaml\n</code></pre><h3 id=\"initialising-the-third-master-node\">Initialising the third master node</h3><h4 id=\"step-1-ssh-to-the-third-master-node\">Step 1 - SSH to the third Master Node</h4><pre><code class=\"language-bash\">ssh <a href=\"/cdn-cgi/l/email-protection\" class=\"__cf_email__\">[email&#xA0;protected]</a>\n</code></pre><h4 id=\"step-2-remove-the-apiserver-crt-and-apiserver-key-1\">Step 2 - Remove the <code>apiserver.crt</code> and <code>apiserver.key</code></h4><pre><code class=\"language-bash\">rm ~/pki/apiserver.*\n</code></pre><h4 id=\"step-3-move-the-certificates-to-the-etc-kubernetes-directory--1\">Step 3 - Move the certificates to the <code>/etc/kubernetes</code> directory.</h4><pre><code class=\"language-bash\">sudo mv ~/pki /etc/kubernetes/\n</code></pre><h4 id=\"step-4-create-the-configuration-file-for-kubeadm-1\">Step 4 - Create the configuration file for kubeadm</h4><pre><code class=\"language-bash\">nano config.yaml\n</code></pre><p>Enter the following config:</p><pre><code class=\"language-yaml\">apiVersion: kubeadm.k8s.io/v1beta2\nkind: ClusterConfiguration\nkubernetesVersion: v1.19.0\ncontrolPlaneEndpoint: &quot;192.168.1.112:6443&quot;\netcd:\n  external:\n    endpoints:\n      - https://192.168.1.113:2379\n      - https://192.168.1.114:2379\n      - https://192.168.1.115:2379\n    caFile: /etc/etcd/ca.pem\n    certFile: /etc/etcd/kubernetes.pem\n    keyFile: /etc/etcd/kubernetes-key.pem\nnetworking:\n  podSubnet: 10.30.0.0/24\napiServer:\n  certSANs:\n    - &quot;192.168.1.112&quot;\n  extraArgs:\n    apiserver-count: &quot;3&quot;\n</code></pre><h4 id=\"step-5-initialise-the-machine-as-a-master-node--1\">Step 5 - Initialise the machine as a master node.</h4><pre><code class=\"language-bash\">sudo kubeadm init --config=config.yaml\n</code></pre><h4 id=\"step-6-save-the-join-command-printed-in-the-output-after-the-above-command\">Step 6 - Save the join command printed in the output after the above command</h4><p>Example Output:</p><pre><code class=\"language-bash\">kubeadm join 192.168.1.112:6443 --token c5tkdt.47tjw72synw7qbn9 \\\n    --discovery-token-ca-cert-hash sha256:069081b1116e821958da62e8d1c185b1df94849bdeb414761e992585f4034ce8 \n</code></pre><p>NOTE: use the output from your terminal and not this post.</p><h2 id=\"configure-kubectl-on-the-client-machine\">Configure <code>kubectl</code> on the client machine</h2><h4 id=\"step-1-ssh-to-one-of-the-master-nodes\">Step 1 - SSH to one of the master nodes</h4><pre><code class=\"language-bash\">ssh <a href=\"/cdn-cgi/l/email-protection\" class=\"__cf_email__\">[email&#xA0;protected]</a>\n</code></pre><h4 id=\"step-2-add-permissions-to-the-admin-conf-file\">Step 2 - Add permissions to the <code>admin.conf</code> file</h4><pre><code class=\"language-bash\">sudo chmod +r /etc/kubernetes/admin.conf\n</code></pre><h4 id=\"step-3-from-the-client-machine-copy-the-configuration-file-\">Step 3 - From the client machine, copy the configuration file.</h4><pre><code class=\"language-bash\">scp <a href=\"/cdn-cgi/l/email-protection\" class=\"__cf_email__\">[email&#xA0;protected]</a>:/etc/kubernetes/admin.conf .\n</code></pre><h4 id=\"step-4-create-and-configure-the-kubectl-configuration-directory-\">Step 4 - Create and configure the <code>kubectl</code> configuration directory.</h4><pre><code class=\"language-bash\">mkdir ~/.kube\nmv admin.conf ~/.kube/config\nchmod 600 ~/.kube/config\n</code></pre><h4 id=\"step-5-go-back-to-the-ssh-session-and-revert-the-permissions-of-the-config-file\">Step 5 - Go back to the SSH session and revert the permissions of the config file</h4><pre><code class=\"language-bash\">sudo chmod 600 /etc/kubernetes/admin.conf\n</code></pre><h4 id=\"step-6-test-to-see-if-you-can-access-the-kubernetes-api-from-the-client-machine\">Step 6 - Test to see if you can access the Kubernetes API from the client machine</h4><pre><code class=\"language-bash\">kubectl get nodes\n</code></pre><p>Expected Output:</p><pre><code class=\"language-bash\">NAME           STATUS     ROLES    AGE     VERSION\nk8s-master-a   NotReady   master   44m     v1.19.2\nk8s-master-b   NotReady   master   11m     v1.19.2\nk8s-master-c   NotReady   master   5m50s   v1.19.2\n</code></pre><h2 id=\"initialise-the-worker-nodes\">Initialise the worker nodes</h2><p>SSH into each worker node and execute the <code>kubeadm join</code> command that you copied previously.</p><pre><code class=\"language-bash\">sudo kubeadm join 192.168.1.112:6443 --token c5tkdt.47tjw72synw7qbn9 \\\n    --discovery-token-ca-cert-hash sha256:069081b1116e821958da62e8d1c185b1df94849bdeb414761e992585f4034ce8 \n</code></pre><p>Once all three worker nodes have joined the cluster, test the API to check the available nodes from the client machine.</p><pre><code class=\"language-bash\">kubectl get nodes\n</code></pre><p>Expected Output:</p><pre><code class=\"language-bash\">NAME           STATUS     ROLES    AGE   VERSION\nk8s-master-a   NotReady   master   53m   v1.19.2\nk8s-master-b   NotReady   master   20m   v1.19.2\nk8s-master-c   NotReady   master   14m   v1.19.2\nk8s-node-a     NotReady   &lt;none&gt;   26s   v1.19.2\nk8s-node-b     NotReady   &lt;none&gt;   19s   v1.19.2\nk8s-node-c     NotReady   &lt;none&gt;   18s   v1.19.2\n</code></pre><h2 id=\"deploying-the-overlay-network\">Deploying the overlay network</h2><p>We will be using Project Calico as the overlay network but you are free to use any other alternatives such as Flannel or WeaveNet</p><h3 id=\"apply-the-manifest-to-deploy-calico-overlay\">Apply the manifest to deploy calico overlay</h3><pre><code class=\"language-bash\">curl https://docs.projectcalico.org/manifests/calico.yaml -O\nkubectl apply -f calico.yaml\n</code></pre><h3 id=\"check-that-all-the-pods-deployed-correctly\">Check that all the pods deployed correctly</h3><pre><code class=\"language-bash\">kubectl get pods -n kube-system\n</code></pre><p>Congratulations! Your Bare-metal HA Cluster is ready for use. I recommend setting up Rancher Server for managing it and to setup Traefik as Ingress Controller, Longhorn as a Persistent Volume Provider, Prometheus &amp; Grafana for Metrics and EFK Stack for Logging and Distributed Tracing. Guides for the same are in the works and will be posted in the coming weeks.</p><p>For any doubts, suggestions or issues, leave a comment below and I will get back to you asap! Follow me on <a href=\"https://twitter.com/tansanrao\">Twitter</a> &amp; <a href=\"https://instagram.com/tansanrao\">Instagram</a> for behind the scenes and updates.</p>\n<section class=\"m-tags in-post\"> </section>\n</div>",
      "contentAsText": " Hello everybody, Tansanrao here! This post is going to guide you into setting up a Multi-Master HA (High-Availability) Kubernetes Cluster on bare-metal or virtual machines.All our VM images will be based on Ubuntu 20.04.1 Server and for the purpose of this guide, will be Virtual Machines on a VMware ESXi host.We will require 7 Virtual Machines with a minimum spec of 2 Cores and 4GB RAM per Node for decent performance. Also make sure that you have Static IPs assigned on your DHCP Server.We are using the following Hostnames & IP Assignments:1 HAProxy Load Balancer Node  \u0014 k8s-haproxy : 192.168.1.1123 Etcd/Kubernetes Master Nodes \u0014 k8s-master-a : 192.168.1.113 \u0014 k8s-master-b : 192.168.1.114 \u0014 k8s-master-c : 192.168.1.1153 Kubernetes Worker Nodes \u0014 k8s-node-a : 192.168.1.116 \u0014 k8s-node-b : 192.168.1.117 \u0014 k8s-node-c : 192.168.1.118We will also require 1 linux client machine, if unavailable, the client tools may be installed on the HAProxy node.The minimum for production use is 2 physical hosts with at least 1 Master on each with the recommended being 3 hosts with 1 Master and 1 Worker Node Each with an external load balancer. For the sake of this guide, I am running all 7 nodes on the same ESXi host. A single host should be safe enough to use for lab and test environments but do not run anything mission critical on it.Let\u0019s get started!Prepare Virtual Machines / ServersStart by preparing 7 machines with Ubuntu 20.04.1 Server using the correct hostnames and IP addresses. Once done, power on all of them and apply the latest updates using:sudo apt update && sudo apt upgrade\nInstalling cfssl.CFSSL is an SSL tool by Cloudflare which lets us create our Certs and CAs.Step 1 - Download the binarieswget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64\nwget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64\nStep 2 - Add the execution permission to the binarieschmod +x cfssl*\nStep 3 - Move the binaries to /usr/local/binsudo mv cfssl_linux-amd64 /usr/local/bin/cfssl\nsudo mv cfssljson_linux-amd64 /usr/local/bin/cfssljson\nStep 4 - Verify the installationcfssl version\nInstalling kubectlStep 1 - Get the binarymake sure it\u0019s the same version as the cluster, in our case we are using v1.19curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.19.0/bin/linux/amd64/kubectl\nStep 2 - Add the execution permission to the binarychmod +x kubectl\nStep 3 - Move the binary to /usr/local/binsudo mv kubectl /usr/local/bin\nStep 4 - Verify the installationkubectl version\nInstalling HAProxy Load BalancerAs we will be deploying three Kubernetes master nodes, we need to deploy an HAProxy Load Balancer in front of them to distribute the traffic.Step 1 - SSH to the HAProxy VMssh [email protected]\nStep 2 - Install HAProxysudo apt-get install haproxy\nStep 3 - Configure HAProxysudo nano /etc/haproxy/haproxy.cfg\nEnter the following config:global\n...\ndefault\n...\nfrontend kubernetes\nbind 192.168.1.112:6443\noption tcplog\nmode tcp\ndefault_backend kubernetes-master-nodes\n\n\nbackend kubernetes-master-nodes\nmode tcp\nbalance roundrobin\noption tcp-check\nserver k8s-master-a 192.168.1.113:6443 check fall 3 rise 2\nserver k8s-master-b 192.168.1.114:6443 check fall 3 rise 2\nserver k8s-master-c 192.168.1.115:6443 check fall 3 rise 2\nStep 4 - Restart HAProxysudo systemctl restart haproxy\nGenerating the TLS certificatesThese steps can be done on your Linux client if you have one or on the HAProxy machine depending on where you installed the cfssl tool.nano ca-config.json\nEnter the following config:{\n  \"signing\": {\n    \"default\": {\n      \"expiry\": \"8760h\"\n    },\n    \"profiles\": {\n      \"kubernetes\": {\n        \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"],\n        \"expiry\": \"8760h\"\n      }\n    }\n  }\n}\nnano ca-csr.json\nEnter the following config, Change the names as necessary:{\n  \"CN\": \"Kubernetes\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n  {\n    \"C\": \"IN\",\n    \"L\": \"Belgaum\",\n    \"O\": \"Tansanrao\",\n    \"OU\": \"CA\",\n    \"ST\": \"Karnataka\"\n  }\n ]\n}\ncfssl gencert -initca ca-csr.json | cfssljson -bare ca\nStep 4 - Verify that the ca-key.pem and the ca.pem were generatedls -la\nCreating the certificate for the Etcd clusterStep 1 - Create the certificate signing request configuration filenano kubernetes-csr.json\nAdd the following config:{\n  \"CN\": \"Kubernetes\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n  {\n    \"C\": \"IN\",\n    \"L\": \"Belgaum\",\n    \"O\": \"Tansanrao\",\n    \"OU\": \"CA\",\n    \"ST\": \"Karnataka\"\n  }\n ]\n}\nStep 2 - Generate the certificate and private keycfssl gencert \\\n-ca=ca.pem \\\n-ca-key=ca-key.pem \\\n-config=ca-config.json \\\n-hostname=192.168.1.112,192.168.1.113,192.168.1.114,192.168.1.115,127.0.0.1,kubernetes.default \\\n-profile=kubernetes kubernetes-csr.json | \\\ncfssljson -bare kubernetes\nStep 3 - Verify that the kubernetes-key.pem and the kubernetes.pem file were generated.ls -la\nStep 4 - Copy the certificate to each nodesscp ca.pem kubernetes.pem kubernetes-key.pem [email protected]:~\nscp ca.pem kubernetes.pem kubernetes-key.pem [email protected]:~\nscp ca.pem kubernetes.pem kubernetes-key.pem [email protected]:~\nscp ca.pem kubernetes.pem kubernetes-key.pem [email protected]:~\nscp ca.pem kubernetes.pem kubernetes-key.pem [email protected]:~\nscp ca.pem kubernetes.pem kubernetes-key.pem [email protected]:~\nPreparing the nodes for kubeadmInitial Setup for all master and node machinesCopy the commands below and paste them into a setup.sh file and then execute it with . setup.sh.This script will check for and uninstall older versions of docker and will replace it with the latest version of docker-ce for ubuntu 20.04. It will also add the kubernetes repository and install kubelet, kubeadm, kubectl and will also mark the packages to prevent auto updates.sudo apt-get remove docker docker-engine docker.io containerd runc\n\nsudo apt-get install -y \\\n    apt-transport-https \\\n    ca-certificates \\\n    curl \\\n    gnupg-agent \\\n    software-properties-common\n\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\n\nsudo add-apt-repository \\\n   \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\\n   $(lsb_release -cs) \\\n   stable\"\n\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io\n\nsudo usermod -aG docker tansanrao\n\ncurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\ncat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list\ndeb https://apt.kubernetes.io/ kubernetes-xenial main\nEOF\nsudo apt-get update\nsudo apt-get install -y kubelet kubeadm kubectl\nsudo apt-mark hold kubelet kubeadm kubectl\n\nsudo swapoff -a\nNow we need to turn swap off for the nodes by editing /etc/fstab on each machine.sudo nano /etc/fstab\nComment the line that starts with /swap or /swap.img. My /etc/fstab looks like this after making the change.# /etc/fstab: static file system information.\n#\n# Use 'blkid' to print the universally unique identifier for a\n# device; this may be used with UUID= as a more robust way to name devices\n# that works even if disks are added and removed. See fstab(5).\n#\n# <file system> <mount point>   <type>  <options>       <dump>  <pass>\n# / was on /dev/ubuntu-vg/ubuntu-lv during curtin installation\n/dev/disk/by-id/dm-uuid-LVM-s96R5iaP77QRtKuZZ0mYLuJcarDuQldMUj3yYFLQDRKWOqz9PHtLTnMMl2cbxpkC / ext4 defaults 0 0\n# /boot was on /dev/sda2 during curtin installation\n/dev/disk/by-uuid/bcc851c2-bbc4-44c0-bb36-c142eedd63a6 /boot ext4 defaults 0 0\n#/swap.img      none    swap    sw      0       0\n\nInstalling and configuring Etcd on all 3 Master NodesStep 1 - Download and move etcd files and certs to their respective placessudo mkdir /etc/etcd /var/lib/etcd\n\nsudo mv ~/ca.pem ~/kubernetes.pem ~/kubernetes-key.pem /etc/etcd\n\nwget https://github.com/etcd-io/etcd/releases/download/v3.4.13/etcd-v3.4.13-linux-amd64.tar.gz\n\ntar xvzf etcd-v3.4.13-linux-amd64.tar.gz\n\nsudo mv etcd-v3.4.13-linux-amd64/etcd* /usr/local/bin/\nStep 2 - Create an etcd systemd unit filesudo nano /etc/systemd/system/etcd.service\nEnter the following config:[Unit]\nDescription=etcd\nDocumentation=https://github.com/coreos\n\n\n[Service]\nExecStart=/usr/local/bin/etcd \\\n  --name 192.168.1.113 \\\n  --cert-file=/etc/etcd/kubernetes.pem \\\n  --key-file=/etc/etcd/kubernetes-key.pem \\\n  --peer-cert-file=/etc/etcd/kubernetes.pem \\\n  --peer-key-file=/etc/etcd/kubernetes-key.pem \\\n  --trusted-ca-file=/etc/etcd/ca.pem \\\n  --peer-trusted-ca-file=/etc/etcd/ca.pem \\\n  --peer-client-cert-auth \\\n  --client-cert-auth \\\n  --initial-advertise-peer-urls https://192.168.1.113:2380 \\\n  --listen-peer-urls https://192.168.1.113:2380 \\\n  --listen-client-urls https://192.168.1.113:2379,http://127.0.0.1:2379 \\\n  --advertise-client-urls https://192.168.1.113:2379 \\\n  --initial-cluster-token etcd-cluster-0 \\\n  --initial-cluster 192.168.1.113=https://192.168.1.113:2380,192.168.1.114=https://192.168.1.114:2380,192.168.1.115=https://192.168.1.115:2380 \\\n  --initial-cluster-state new \\\n  --data-dir=/var/lib/etcd\nRestart=on-failure\nRestartSec=5\n\n\n\n[Install]\nWantedBy=multi-user.target\nReplace the IP address on all fields except the \u0014initial-cluster field to match the machine IP.Step 3 - Reload the daemon configuration.sudo systemctl daemon-reload\nStep 4 - Enable etcd to start at boot time.sudo systemctl enable etcd\nStep 5 - Start etcd.sudo systemctl start etcd\nRepeat the process for all 3 master nodes and then move to step 6.Step 6 - Verify that the cluster is up and running.ETCDCTL_API=3 etcdctl member list\nIt should give you an output similar to this:73ea126859b3ba4, started, 192.168.1.114, https://192.168.1.114:2380, https://192.168.1.114:2379, false\na28911111213cc6c, started, 192.168.1.115, https://192.168.1.115:2380, https://192.168.1.115:2379, false\nfeadb5a763a32caa, started, 192.168.1.113, https://192.168.1.113:2380, https://192.168.1.113:2379, false\nInitialising the Master NodesInitialising the first Master NodeStep 1 - SSH to the first Master Nodessh [email protected]\nStep 2 - Create the configuration file for kubeadmnano config.yaml\nEnter the following config:apiVersion: kubeadm.k8s.io/v1beta2\nkind: ClusterConfiguration\nkubernetesVersion: v1.19.0\ncontrolPlaneEndpoint: \"192.168.1.112:6443\"\netcd:\n  external:\n    endpoints:\n      - https://192.168.1.113:2379\n      - https://192.168.1.114:2379\n      - https://192.168.1.115:2379\n    caFile: /etc/etcd/ca.pem\n    certFile: /etc/etcd/kubernetes.pem\n    keyFile: /etc/etcd/kubernetes-key.pem\nnetworking:\n  podSubnet: 10.30.0.0/24\napiServer:\n  certSANs:\n    - \"192.168.1.112\"\n  extraArgs:\n    apiserver-count: \"3\"\nAdd any additional domains or IP Addresses that you would want to connect to the cluster under certSANs.Step 3 - Initialise the machine as a master nodesudo kubeadm init --config=config.yaml\nStep 4 - Copy the certificates to the two other masterssudo scp -r /etc/kubernetes/pki [email protected]:~\nsudo scp -r /etc/kubernetes/pki [email protected]:~\nInitialising the second Master NodeStep 1 - SSH to the second Master Nodessh [email protected]\nStep 2 - Remove the apiserver.crt and apiserver.keyrm ~/pki/apiserver.*\nStep 3 - Move the certificates to the /etc/kubernetes directory.sudo mv ~/pki /etc/kubernetes/\nStep 4 - Create the configuration file for kubeadmnano config.yaml\nEnter the following config:apiVersion: kubeadm.k8s.io/v1beta2\nkind: ClusterConfiguration\nkubernetesVersion: v1.19.0\ncontrolPlaneEndpoint: \"192.168.1.112:6443\"\netcd:\n  external:\n    endpoints:\n      - https://192.168.1.113:2379\n      - https://192.168.1.114:2379\n      - https://192.168.1.115:2379\n    caFile: /etc/etcd/ca.pem\n    certFile: /etc/etcd/kubernetes.pem\n    keyFile: /etc/etcd/kubernetes-key.pem\nnetworking:\n  podSubnet: 10.30.0.0/24\napiServer:\n  certSANs:\n    - \"192.168.1.112\"\n  extraArgs:\n    apiserver-count: \"3\"\nStep 5 - Initialise the machine as a master node.sudo kubeadm init --config=config.yaml\nInitialising the third master nodeStep 1 - SSH to the third Master Nodessh [email protected]\nStep 2 - Remove the apiserver.crt and apiserver.keyrm ~/pki/apiserver.*\nStep 3 - Move the certificates to the /etc/kubernetes directory.sudo mv ~/pki /etc/kubernetes/\nStep 4 - Create the configuration file for kubeadmnano config.yaml\nEnter the following config:apiVersion: kubeadm.k8s.io/v1beta2\nkind: ClusterConfiguration\nkubernetesVersion: v1.19.0\ncontrolPlaneEndpoint: \"192.168.1.112:6443\"\netcd:\n  external:\n    endpoints:\n      - https://192.168.1.113:2379\n      - https://192.168.1.114:2379\n      - https://192.168.1.115:2379\n    caFile: /etc/etcd/ca.pem\n    certFile: /etc/etcd/kubernetes.pem\n    keyFile: /etc/etcd/kubernetes-key.pem\nnetworking:\n  podSubnet: 10.30.0.0/24\napiServer:\n  certSANs:\n    - \"192.168.1.112\"\n  extraArgs:\n    apiserver-count: \"3\"\nStep 5 - Initialise the machine as a master node.sudo kubeadm init --config=config.yaml\nStep 6 - Save the join command printed in the output after the above commandExample Output:kubeadm join 192.168.1.112:6443 --token c5tkdt.47tjw72synw7qbn9 \\\n    --discovery-token-ca-cert-hash sha256:069081b1116e821958da62e8d1c185b1df94849bdeb414761e992585f4034ce8 \nNOTE: use the output from your terminal and not this post.Configure kubectl on the client machineStep 1 - SSH to one of the master nodesssh [email protected]\nStep 2 - Add permissions to the admin.conf filesudo chmod +r /etc/kubernetes/admin.conf\nStep 3 - From the client machine, copy the configuration file.scp [email protected]:/etc/kubernetes/admin.conf .\nStep 4 - Create and configure the kubectl configuration directory.mkdir ~/.kube\nmv admin.conf ~/.kube/config\nchmod 600 ~/.kube/config\nStep 5 - Go back to the SSH session and revert the permissions of the config filesudo chmod 600 /etc/kubernetes/admin.conf\nStep 6 - Test to see if you can access the Kubernetes API from the client machinekubectl get nodes\nExpected Output:NAME           STATUS     ROLES    AGE     VERSION\nk8s-master-a   NotReady   master   44m     v1.19.2\nk8s-master-b   NotReady   master   11m     v1.19.2\nk8s-master-c   NotReady   master   5m50s   v1.19.2\nInitialise the worker nodesSSH into each worker node and execute the kubeadm join command that you copied previously.sudo kubeadm join 192.168.1.112:6443 --token c5tkdt.47tjw72synw7qbn9 \\\n    --discovery-token-ca-cert-hash sha256:069081b1116e821958da62e8d1c185b1df94849bdeb414761e992585f4034ce8 \nOnce all three worker nodes have joined the cluster, test the API to check the available nodes from the client machine.kubectl get nodes\nExpected Output:NAME           STATUS     ROLES    AGE   VERSION\nk8s-master-a   NotReady   master   53m   v1.19.2\nk8s-master-b   NotReady   master   20m   v1.19.2\nk8s-master-c   NotReady   master   14m   v1.19.2\nk8s-node-a     NotReady   <none>   26s   v1.19.2\nk8s-node-b     NotReady   <none>   19s   v1.19.2\nk8s-node-c     NotReady   <none>   18s   v1.19.2\nDeploying the overlay networkWe will be using Project Calico as the overlay network but you are free to use any other alternatives such as Flannel or WeaveNetApply the manifest to deploy calico overlaycurl https://docs.projectcalico.org/manifests/calico.yaml -O\nkubectl apply -f calico.yaml\nCheck that all the pods deployed correctlykubectl get pods -n kube-system\nCongratulations! Your Bare-metal HA Cluster is ready for use. I recommend setting up Rancher Server for managing it and to setup Traefik as Ingress Controller, Longhorn as a Persistent Volume Provider, Prometheus & Grafana for Metrics and EFK Stack for Logging and Distributed Tracing. Guides for the same are in the works and will be posted in the coming weeks.For any doubts, suggestions or issues, leave a comment below and I will get back to you asap! Follow me on Twitter & Instagram for behind the scenes and updates.\n \n",
      "publishedDate": "2020-09-22T04:30:00.000Z",
      "description": "This post is going to guide you into setting up a Multi-Master HA (High-Availability) Kubernetes Cluster on bare-metal or virtual machines.",
      "ogDescription": "This post is going to guide you into setting up a Multi-Master HA (High-Availability) Kubernetes Cluster on bare-metal or virtual machines."
    },
    {
      "url": "https://github.com/micnncim/kubectl-reap",
      "title": "micnncim/kubectl-reap",
      "content": "<div><div><article class=\"markdown-body entry-content container-lg\">\n<p><a href=\"https://github.com/micnncim/kubectl-reap/actions?query=workflow%3ATest\"><img src=\"https://camo.githubusercontent.com/28db367972bc15502e6a78155c6b5ed32aa04d135a8a51ce9277fbd3b0eed82d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f776f726b666c6f772f7374617475732f6d69636e6e63696d2f6b75626563746c2d726561702f546573743f6c6162656c3d54657374267374796c653d666f722d7468652d6261646765266c6f676f3d676974687562\" alt=\"actions-workflow-test\"></a>\n<a href=\"https://github.com/micnncim/kubectl-reap/releases\"><img src=\"https://camo.githubusercontent.com/2fa6a7091b42930b4b6be3d6f17dc193ee0890f81682d2f1674f25d24ff58c2d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f6d69636e6e63696d2f6b75626563746c2d726561703f7374796c653d666f722d7468652d6261646765266c6f676f3d676974687562\" alt=\"release\"></a>\n<a href=\"https://pkg.go.dev/github.com/micnncim/kubectl-reap?tab=overview\"><img src=\"https://camo.githubusercontent.com/ab543534e49e7a413ea45b2e511004d964075a26928c8937039bbf24a725647c/687474703a2f2f6269742e6c792f706b672d676f2d6465762d6261646765\" alt=\"pkg.go.dev\"></a>\n<a href=\"https://github.com/micnncim/kubectl-reap/blob/master/LICENSE\"><img src=\"https://camo.githubusercontent.com/3f5a0ac00e327676e6ef7092dbcce0d3e63d692c06aea8fd8f733208913a3baf/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d69636e6e63696d2f6b75626563746c2d726561703f7374796c653d666f722d7468652d6261646765\" alt=\"license\"></a></p>\n<p><code>kubectl-reap</code> is a kubectl plugin that deletes unused Kubernetes resources.</p>\n<p><a href=\"https://github.com/micnncim/kubectl-reap/blob/master/docs/assets/screencast.gif\"><img src=\"https://github.com/micnncim/kubectl-reap/raw/master/docs/assets/screencast.gif\" alt=\"screencast\"></a></p>\n<p>Supported resources:</p>\n<table>\n<thead>\n<tr>\n<th>Kind</th>\n<th>Condition</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Pod</td>\n<td>Not running</td>\n</tr>\n<tr>\n<td>ConfigMap</td>\n<td>Not used by any Pods</td>\n</tr>\n<tr>\n<td>Secret</td>\n<td>Not used by any Pods or ServiceAccounts</td>\n</tr>\n<tr>\n<td>PersistentVolume</td>\n<td>Not satisfying any PersistentVolumeClaims</td>\n</tr>\n<tr>\n<td>PersistentVolumeClaim</td>\n<td>Not used by any Pods</td>\n</tr>\n<tr>\n<td>Job</td>\n<td>Completed</td>\n</tr>\n<tr>\n<td>PodDisruptionBudget</td>\n<td>Not targeting any Pods</td>\n</tr>\n<tr>\n<td>HorizontalPodAutoscaler</td>\n<td>Not targeting any resources</td>\n</tr>\n</tbody>\n</table>\n<p>Since this plugin supports dry-run as described below, it also helps you to find resources you misconfigured or forgot to delete.</p>\n<p>Before getting started, read <a href=\"https://github.com/micnncim/kubectl-reap#caveats\">the caveats of using this plugin</a>.</p>\n<h2><a id=\"user-content-installation\" class=\"anchor\" href=\"https://github.com/micnncim/kubectl-reap#installation\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Installation</h2>\n<p>Download precompiled binaries from <a href=\"https://github.com/micnncim/kubectl-reap/releases\">GitHub Releases</a>.</p>\n<h3><a id=\"user-content-via-krew\" class=\"anchor\" href=\"https://github.com/micnncim/kubectl-reap#via-krew\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Via <a href=\"https://github.com/kubernetes-sigs/krew\">Krew</a></h3>\n<pre><code>$ kubectl krew install reap\n</code></pre>\n<h3><a id=\"user-content-via-go\" class=\"anchor\" href=\"https://github.com/micnncim/kubectl-reap#via-go\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Via Go</h3>\n<pre><code>$ go get github.com/micnncim/kubectl-reap/cmd/kubectl-reap\n</code></pre>\n<h2><a id=\"user-content-examples\" class=\"anchor\" href=\"https://github.com/micnncim/kubectl-reap#examples\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Examples</h2>\n<h3><a id=\"user-content-pods\" class=\"anchor\" href=\"https://github.com/micnncim/kubectl-reap#pods\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Pods</h3>\n<p>In this example, this plugin deletes all Pods whose status is not <code>Running</code>.</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>$ <span class=\"pl-s1\">kubectl get po</span>\n<span class=\"pl-c1\">NAME          READY   STATUS      RESTARTS   AGE</span>\n<span class=\"pl-c1\">pod-running   1/1     Running     0          10s</span>\n<span class=\"pl-c1\">pod-pending   0/1     Pending     0          20s</span>\n<span class=\"pl-c1\">pod-failed    0/1     Failed      0          30s</span>\n<span class=\"pl-c1\">pod-unknown   0/1     Unknown     0          40s</span>\n<span class=\"pl-c1\">job-kqpxc     0/1     Completed   0          50s</span>\n\n$ <span class=\"pl-s1\">kubectl reap po</span>\n<span class=\"pl-c1\">pod/pod-pending deleted</span>\n<span class=\"pl-c1\">pod/pod-failed deleted</span>\n<span class=\"pl-c1\">pod/pod-unknown deleted</span>\n<span class=\"pl-c1\">pod/job-kqpxc deleted</span></pre></div>\n<h3><a id=\"user-content-configmaps\" class=\"anchor\" href=\"https://github.com/micnncim/kubectl-reap#configmaps\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>ConfigMaps</h3>\n<p>In this example, this plugin deletes the unused ConfigMap <code>config-2</code>.</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>$ <span class=\"pl-s1\">kubectl get cm</span>\n<span class=\"pl-c1\">NAME       DATA   AGE</span>\n<span class=\"pl-c1\">config-1   1      0m15s</span>\n<span class=\"pl-c1\">config-2   1      0m10s</span>\n\n$ <span class=\"pl-s1\">cat <span class=\"pl-s\"><span class=\"pl-k\">&lt;&lt;</span><span class=\"pl-k\">EOF</span> | kubectl apply -f -</span></span>\n<span class=\"pl-c1\">apiVersion: v1</span>\n<span class=\"pl-c1\">kind: Pod</span>\n<span class=\"pl-c1\">metadata:</span>\n<span class=\"pl-c1\">  name: nginx</span>\n<span class=\"pl-c1\">spec:</span>\n<span class=\"pl-c1\">  containers:</span>\n<span class=\"pl-c1\">  - name: nginx</span>\n<span class=\"pl-c1\">    image: nginx</span>\n<span class=\"pl-c1\">    volumeMounts:</span>\n<span class=\"pl-c1\">    - name: config-1-volume</span>\n<span class=\"pl-c1\">      mountPath: /var/config</span>\n<span class=\"pl-c1\">  volumes:</span>\n<span class=\"pl-c1\">  - name: config-1-volume</span>\n<span class=\"pl-c1\">    configMap:</span>\n<span class=\"pl-c1\">      name: config-1</span>\n<span class=\"pl-c1\">EOF</span>\n\n$ <span class=\"pl-s1\">kubectl reap cm</span>\n<span class=\"pl-c1\">configmap/config-2 deleted</span></pre></div>\n<h3><a id=\"user-content-interactive-mode\" class=\"anchor\" href=\"https://github.com/micnncim/kubectl-reap#interactive-mode\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Interactive Mode</h3>\n<p>You can choose which resource you will delete one by one by interactive mode.</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>$ <span class=\"pl-s1\">kubectl reap cm --interactive <span class=\"pl-c\"><span class=\"pl-c\">#</span> or &apos;-i&apos;</span></span>\n<span class=\"pl-c1\">? Are you sure to delete configmap/config-1? Yes</span>\n<span class=\"pl-c1\">configmap/config-1 deleted</span>\n<span class=\"pl-c1\">? Are you sure to delete configmap/config-2? No</span>\n<span class=\"pl-c1\">? Are you sure to delete configmap/config-3? Yes</span>\n<span class=\"pl-c1\">configmap/config-3 deleted</span></pre></div>\n<h2><a id=\"user-content-usage\" class=\"anchor\" href=\"https://github.com/micnncim/kubectl-reap#usage\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Usage</h2>\n<div class=\"highlight highlight-text-shell-session\"><pre>$ <span class=\"pl-s1\">kubectl reap --help</span>\n\n<span class=\"pl-c1\">Delete unused resources. Supported resources:</span>\n\n<span class=\"pl-c1\">- Pods (whose status is not Running)</span>\n<span class=\"pl-c1\">- ConfigMaps (not used by any Pods)</span>\n<span class=\"pl-c1\">- Secrets (not used by any Pods or ServiceAccounts)</span>\n<span class=\"pl-c1\">- PersistentVolumes (not satisfying any PersistentVolumeClaims)</span>\n<span class=\"pl-c1\">- PersistentVolumeClaims (not used by any Pods)</span>\n<span class=\"pl-c1\">- Jobs (completed)</span>\n<span class=\"pl-c1\">- PodDisruptionBudgets (not targeting any Pods)</span>\n<span class=\"pl-c1\">- HorizontalPodAutoscalers (not targeting any resources)</span>\n\n<span class=\"pl-c1\">Usage:</span>\n<span class=\"pl-c1\">  kubectl reap RESOURCE_TYPE [flags]</span>\n\n<span class=\"pl-c1\">Examples:</span>\n\n<span class=\"pl-c1\">  # Delete ConfigMaps not mounted on any Pods and in the current namespace and context</span>\n<span class=\"pl-c1\">  $ kubectl reap configmaps</span>\n\n<span class=\"pl-c1\">  # Delete unused ConfigMaps and Secrets in the namespace/my-namespace and context/my-context</span>\n<span class=\"pl-c1\">  $ kubectl reap cm,secret -n my-namespace --context my-context</span>\n\n<span class=\"pl-c1\">  # Delete ConfigMaps not mounted on any Pods and across all namespace</span>\n<span class=\"pl-c1\">  $ kubectl reap cm --all-namespaces</span>\n\n<span class=\"pl-c1\">  # Delete Pods whose status is not Running as client-side dry-run</span>\n<span class=\"pl-c1\">  $ kubectl reap po --dry-run=client</span>\n\n<span class=\"pl-c1\">Flags:</span>\n<span class=\"pl-c1\">  -A, --all-namespaces                 If true, delete the targeted resources across all namespace except kube-system</span>\n<span class=\"pl-c1\">      --allow-missing-template-keys    If true, ignore any errors in templates when a field or map key is missing in the template. Only applies to golang and jsonpath output formats. (default true)</span>\n<span class=\"pl-c1\">      --as string                      Username to impersonate for the operation</span>\n<span class=\"pl-c1\">      --as-group stringArray           Group to impersonate for the operation, this flag can be repeated to specify multiple groups.</span>\n<span class=\"pl-c1\">      --cache-dir string               Default cache directory (default &quot;/Users/micnncim/.kube/cache&quot;)</span>\n<span class=\"pl-c1\">      --certificate-authority string   Path to a cert file for the certificate authority</span>\n<span class=\"pl-c1\">      --client-certificate string      Path to a client certificate file for TLS</span>\n<span class=\"pl-c1\">      --client-key string              Path to a client key file for TLS</span>\n<span class=\"pl-c1\">      --cluster string                 The name of the kubeconfig cluster to use</span>\n<span class=\"pl-c1\">      --context string                 The name of the kubeconfig context to use</span>\n<span class=\"pl-c1\">      --dry-run string[=&quot;unchanged&quot;]   Must be &quot;none&quot;, &quot;server&quot;, or &quot;client&quot;. If client strategy, only print the object that would be sent, without sending it. If server strategy, submit server-side request without persisting the resource. (default &quot;none&quot;)</span>\n<span class=\"pl-c1\">      --field-selector string          Selector (field query) to filter on, supports &apos;=&apos;, &apos;==&apos;, and &apos;!=&apos;.(e.g. --field-selector key1=value1,key2=value2). The server only supports a limited number of field queries per type.</span>\n<span class=\"pl-c1\">      --force                          If true, immediately remove resources from API and bypass graceful deletion. Note that immediate deletion of some resources may result in inconsistency or data loss and requires confirmation.</span>\n<span class=\"pl-c1\">      --grace-period int               Period of time in seconds given to the resource to terminate gracefully. Ignored if negative. Set to 1 for immediate shutdown. Can only be set to 0 when --force is true (force deletion). (default -1)</span>\n<span class=\"pl-c1\">  -h, --help                           help for kubectl</span>\n<span class=\"pl-c1\">      --insecure-skip-tls-verify       If true, the server&apos;s certificate will not be checked for validity. This will make your HTTPS connections insecure</span>\n<span class=\"pl-c1\">  -i, --interactive                    If true, a prompt asks whether resources can be deleted</span>\n<span class=\"pl-c1\">      --kubeconfig string              Path to the kubeconfig file to use for CLI requests.</span>\n<span class=\"pl-c1\">  -n, --namespace string               If present, the namespace scope for this CLI request</span>\n<span class=\"pl-c1\">  -o, --output string                  Output format. One of: json|yaml|name|go-template|go-template-file|template|templatefile|jsonpath|jsonpath-as-json|jsonpath-file.</span>\n<span class=\"pl-c1\">  -q, --quiet                          If true, no output is produced</span>\n<span class=\"pl-c1\">      --request-timeout string         The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don&apos;t timeout requests. (default &quot;0&quot;)</span>\n<span class=\"pl-c1\">  -l, --selector string                Selector (label query) to filter on, supports &apos;=&apos;, &apos;==&apos;, and &apos;!=&apos;.(e.g. -l key1=value1,key2=value2)</span>\n<span class=\"pl-c1\">  -s, --server string                  The address and port of the Kubernetes API server</span>\n<span class=\"pl-c1\">      --template string                Template string or path to template file to use when -o=go-template, -o=go-template-file. The template format is golang templates [http://golang.org/pkg/text/template/#pkg-overview].</span>\n<span class=\"pl-c1\">      --timeout duration               The length of time to wait before giving up on a delete, zero means determine a timeout from the size of the object</span>\n<span class=\"pl-c1\">      --tls-server-name string         Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used</span>\n<span class=\"pl-c1\">      --token string                   Bearer token for authentication to the API server</span>\n<span class=\"pl-c1\">      --user string                    The name of the kubeconfig user to use</span>\n<span class=\"pl-c1\">  -v, --version                        If true, show the version of this plugin</span>\n<span class=\"pl-c1\">      --wait                           If true, wait for resources to be gone before returning. This waits for finalizers.</span>\n</pre></div>\n<h3><a id=\"user-content-caveats\" class=\"anchor\" href=\"https://github.com/micnncim/kubectl-reap#caveats\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Caveats</h3>\n<ul>\n<li>It&apos;s recommended to run this plugin as dry-run (<code>--dry-run=client</code> or <code>--dry-run=server</code>) first or interactive mode (<code>--interactive</code>) in order to examine what resources will be deleted when running it, especially when you&apos;re trying to run it in a production environment.</li>\n<li>Even if you use <code>--namespace kube-system</code> or <code>--all-namespaces</code>, this plugin never deletes any resources in <code>kube-system</code> so that it prevents unexpected resource deletion.</li>\n<li>This plugin doesn&apos;t determine whether custom controllers or CRDs consume or depend on the supported resources. Make sure the resources you want to reap aren&apos;t used by them.\n<ul>\n<li>e.g.) A Secret, which isn&apos;t used by any Pods or ServiceAccounts but used by <a href=\"https://cert-manager.io/\">cert-manager</a>, can be deleted</li>\n</ul>\n</li>\n</ul>\n<h2><a id=\"user-content-background\" class=\"anchor\" href=\"https://github.com/micnncim/kubectl-reap#background\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Background</h2>\n<p><code>kubectl apply --prune</code> allows us to delete unused resources.\nHowever, it&apos;s not very flexible when we want to choose what kind resource to be deleted.\nthis plugin provides more flexible, easy way to delete resources.</p>\n<h2><a id=\"user-content-similar-projects\" class=\"anchor\" href=\"https://github.com/micnncim/kubectl-reap#similar-projects\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Similar Projects</h2>\n\n\n</article></div></div><hr><h4>Page 2</h4><div><div><article class=\"markdown-body entry-content container-lg\">\n<p><a href=\"https://github.com/micnncim/kubectl-reap/actions?query=workflow%3ATest\"><img src=\"https://camo.githubusercontent.com/28db367972bc15502e6a78155c6b5ed32aa04d135a8a51ce9277fbd3b0eed82d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f776f726b666c6f772f7374617475732f6d69636e6e63696d2f6b75626563746c2d726561702f546573743f6c6162656c3d54657374267374796c653d666f722d7468652d6261646765266c6f676f3d676974687562\" alt=\"actions-workflow-test\"></a>\n<a href=\"https://github.com/micnncim/kubectl-reap/releases\"><img src=\"https://camo.githubusercontent.com/2fa6a7091b42930b4b6be3d6f17dc193ee0890f81682d2f1674f25d24ff58c2d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f6d69636e6e63696d2f6b75626563746c2d726561703f7374796c653d666f722d7468652d6261646765266c6f676f3d676974687562\" alt=\"release\"></a>\n<a href=\"https://pkg.go.dev/github.com/micnncim/kubectl-reap?tab=overview\"><img src=\"https://camo.githubusercontent.com/ab543534e49e7a413ea45b2e511004d964075a26928c8937039bbf24a725647c/687474703a2f2f6269742e6c792f706b672d676f2d6465762d6261646765\" alt=\"pkg.go.dev\"></a>\n<a href=\"/micnncim/kubectl-reap/blob/57c2c76c3a3afe541debd0b68dfe92f03964c9e3/LICENSE\"><img src=\"https://camo.githubusercontent.com/3f5a0ac00e327676e6ef7092dbcce0d3e63d692c06aea8fd8f733208913a3baf/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d69636e6e63696d2f6b75626563746c2d726561703f7374796c653d666f722d7468652d6261646765\" alt=\"license\"></a></p>\n<p><code>kubectl-reap</code> is a kubectl plugin that deletes unused Kubernetes resources.</p>\n<p><a href=\"/micnncim/kubectl-reap/blob/57c2c76c3a3afe541debd0b68dfe92f03964c9e3/docs/assets/screencast.gif\"><img src=\"/micnncim/kubectl-reap/raw/57c2c76c3a3afe541debd0b68dfe92f03964c9e3/docs/assets/screencast.gif\" alt=\"screencast\"></a></p>\n<p>Supported resources:</p>\n<table>\n<thead>\n<tr>\n<th>Kind</th>\n<th>Condition</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Pod</td>\n<td>Not running</td>\n</tr>\n<tr>\n<td>ConfigMap</td>\n<td>Not used by any Pods</td>\n</tr>\n<tr>\n<td>Secret</td>\n<td>Not used by any Pods or ServiceAccounts</td>\n</tr>\n<tr>\n<td>PersistentVolume</td>\n<td>Not satisfying any PersistentVolumeClaims</td>\n</tr>\n<tr>\n<td>PersistentVolumeClaim</td>\n<td>Not used by any Pods</td>\n</tr>\n<tr>\n<td>Job</td>\n<td>Completed</td>\n</tr>\n<tr>\n<td>PodDisruptionBudget</td>\n<td>Not targeting any Pods</td>\n</tr>\n<tr>\n<td>HorizontalPodAutoscaler</td>\n<td>Not targeting any resources</td>\n</tr>\n</tbody>\n</table>\n<p>Since this plugin supports dry-run as described below, it also helps you to find resources you misconfigured or forgot to delete.</p>\n<p>Before getting started, read <a href=\"#caveats\">the caveats of using this plugin</a>.</p>\n<h2><a id=\"user-content-installation\" class=\"anchor\" href=\"#installation\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Installation</h2>\n<p>Download precompiled binaries from <a href=\"https://github.com/micnncim/kubectl-reap/releases\">GitHub Releases</a>.</p>\n<h3><a id=\"user-content-via-krew\" class=\"anchor\" href=\"#via-krew\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Via <a href=\"https://github.com/kubernetes-sigs/krew\">Krew</a></h3>\n<pre><code>$ kubectl krew install reap\n</code></pre>\n<h3><a id=\"user-content-via-go\" class=\"anchor\" href=\"#via-go\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Via Go</h3>\n<pre><code>$ go get github.com/micnncim/kubectl-reap/cmd/kubectl-reap\n</code></pre>\n<h2><a id=\"user-content-examples\" class=\"anchor\" href=\"#examples\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Examples</h2>\n<h3><a id=\"user-content-pods\" class=\"anchor\" href=\"#pods\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Pods</h3>\n<p>In this example, this plugin deletes all Pods whose status is not <code>Running</code>.</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>$ <span class=\"pl-s1\">kubectl get po</span>\n<span class=\"pl-c1\">NAME          READY   STATUS      RESTARTS   AGE</span>\n<span class=\"pl-c1\">pod-running   1/1     Running     0          10s</span>\n<span class=\"pl-c1\">pod-pending   0/1     Pending     0          20s</span>\n<span class=\"pl-c1\">pod-failed    0/1     Failed      0          30s</span>\n<span class=\"pl-c1\">pod-unknown   0/1     Unknown     0          40s</span>\n<span class=\"pl-c1\">job-kqpxc     0/1     Completed   0          50s</span>\n\n$ <span class=\"pl-s1\">kubectl reap po</span>\n<span class=\"pl-c1\">pod/pod-pending deleted</span>\n<span class=\"pl-c1\">pod/pod-failed deleted</span>\n<span class=\"pl-c1\">pod/pod-unknown deleted</span>\n<span class=\"pl-c1\">pod/job-kqpxc deleted</span></pre></div>\n<h3><a id=\"user-content-configmaps\" class=\"anchor\" href=\"#configmaps\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>ConfigMaps</h3>\n<p>In this example, this plugin deletes the unused ConfigMap <code>config-2</code>.</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>$ <span class=\"pl-s1\">kubectl get cm</span>\n<span class=\"pl-c1\">NAME       DATA   AGE</span>\n<span class=\"pl-c1\">config-1   1      0m15s</span>\n<span class=\"pl-c1\">config-2   1      0m10s</span>\n\n$ <span class=\"pl-s1\">cat <span class=\"pl-s\"><span class=\"pl-k\">&lt;&lt;</span><span class=\"pl-k\">EOF</span> | kubectl apply -f -</span></span>\n<span class=\"pl-c1\">apiVersion: v1</span>\n<span class=\"pl-c1\">kind: Pod</span>\n<span class=\"pl-c1\">metadata:</span>\n<span class=\"pl-c1\">  name: nginx</span>\n<span class=\"pl-c1\">spec:</span>\n<span class=\"pl-c1\">  containers:</span>\n<span class=\"pl-c1\">  - name: nginx</span>\n<span class=\"pl-c1\">    image: nginx</span>\n<span class=\"pl-c1\">    volumeMounts:</span>\n<span class=\"pl-c1\">    - name: config-1-volume</span>\n<span class=\"pl-c1\">      mountPath: /var/config</span>\n<span class=\"pl-c1\">  volumes:</span>\n<span class=\"pl-c1\">  - name: config-1-volume</span>\n<span class=\"pl-c1\">    configMap:</span>\n<span class=\"pl-c1\">      name: config-1</span>\n<span class=\"pl-c1\">EOF</span>\n\n$ <span class=\"pl-s1\">kubectl reap cm</span>\n<span class=\"pl-c1\">configmap/config-2 deleted</span></pre></div>\n<h3><a id=\"user-content-interactive-mode\" class=\"anchor\" href=\"#interactive-mode\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Interactive Mode</h3>\n<p>You can choose which resource you will delete one by one by interactive mode.</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>$ <span class=\"pl-s1\">kubectl reap cm --interactive <span class=\"pl-c\"><span class=\"pl-c\">#</span> or &apos;-i&apos;</span></span>\n<span class=\"pl-c1\">? Are you sure to delete configmap/config-1? Yes</span>\n<span class=\"pl-c1\">configmap/config-1 deleted</span>\n<span class=\"pl-c1\">? Are you sure to delete configmap/config-2? No</span>\n<span class=\"pl-c1\">? Are you sure to delete configmap/config-3? Yes</span>\n<span class=\"pl-c1\">configmap/config-3 deleted</span></pre></div>\n<h2><a id=\"user-content-usage\" class=\"anchor\" href=\"#usage\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Usage</h2>\n<div class=\"highlight highlight-text-shell-session\"><pre>$ <span class=\"pl-s1\">kubectl reap --help</span>\n\n<span class=\"pl-c1\">Delete unused resources. Supported resources:</span>\n\n<span class=\"pl-c1\">- Pods (whose status is not Running)</span>\n<span class=\"pl-c1\">- ConfigMaps (not used by any Pods)</span>\n<span class=\"pl-c1\">- Secrets (not used by any Pods or ServiceAccounts)</span>\n<span class=\"pl-c1\">- PersistentVolumes (not satisfying any PersistentVolumeClaims)</span>\n<span class=\"pl-c1\">- PersistentVolumeClaims (not used by any Pods)</span>\n<span class=\"pl-c1\">- Jobs (completed)</span>\n<span class=\"pl-c1\">- PodDisruptionBudgets (not targeting any Pods)</span>\n<span class=\"pl-c1\">- HorizontalPodAutoscalers (not targeting any resources)</span>\n\n<span class=\"pl-c1\">Usage:</span>\n<span class=\"pl-c1\">  kubectl reap RESOURCE_TYPE [flags]</span>\n\n<span class=\"pl-c1\">Examples:</span>\n\n<span class=\"pl-c1\">  # Delete ConfigMaps not mounted on any Pods and in the current namespace and context</span>\n<span class=\"pl-c1\">  $ kubectl reap configmaps</span>\n\n<span class=\"pl-c1\">  # Delete unused ConfigMaps and Secrets in the namespace/my-namespace and context/my-context</span>\n<span class=\"pl-c1\">  $ kubectl reap cm,secret -n my-namespace --context my-context</span>\n\n<span class=\"pl-c1\">  # Delete ConfigMaps not mounted on any Pods and across all namespace</span>\n<span class=\"pl-c1\">  $ kubectl reap cm --all-namespaces</span>\n\n<span class=\"pl-c1\">  # Delete Pods whose status is not Running as client-side dry-run</span>\n<span class=\"pl-c1\">  $ kubectl reap po --dry-run=client</span>\n\n<span class=\"pl-c1\">Flags:</span>\n<span class=\"pl-c1\">  -A, --all-namespaces                 If true, delete the targeted resources across all namespace except kube-system</span>\n<span class=\"pl-c1\">      --allow-missing-template-keys    If true, ignore any errors in templates when a field or map key is missing in the template. Only applies to golang and jsonpath output formats. (default true)</span>\n<span class=\"pl-c1\">      --as string                      Username to impersonate for the operation</span>\n<span class=\"pl-c1\">      --as-group stringArray           Group to impersonate for the operation, this flag can be repeated to specify multiple groups.</span>\n<span class=\"pl-c1\">      --cache-dir string               Default cache directory (default &quot;/Users/micnncim/.kube/cache&quot;)</span>\n<span class=\"pl-c1\">      --certificate-authority string   Path to a cert file for the certificate authority</span>\n<span class=\"pl-c1\">      --client-certificate string      Path to a client certificate file for TLS</span>\n<span class=\"pl-c1\">      --client-key string              Path to a client key file for TLS</span>\n<span class=\"pl-c1\">      --cluster string                 The name of the kubeconfig cluster to use</span>\n<span class=\"pl-c1\">      --context string                 The name of the kubeconfig context to use</span>\n<span class=\"pl-c1\">      --dry-run string[=&quot;unchanged&quot;]   Must be &quot;none&quot;, &quot;server&quot;, or &quot;client&quot;. If client strategy, only print the object that would be sent, without sending it. If server strategy, submit server-side request without persisting the resource. (default &quot;none&quot;)</span>\n<span class=\"pl-c1\">      --field-selector string          Selector (field query) to filter on, supports &apos;=&apos;, &apos;==&apos;, and &apos;!=&apos;.(e.g. --field-selector key1=value1,key2=value2). The server only supports a limited number of field queries per type.</span>\n<span class=\"pl-c1\">      --force                          If true, immediately remove resources from API and bypass graceful deletion. Note that immediate deletion of some resources may result in inconsistency or data loss and requires confirmation.</span>\n<span class=\"pl-c1\">      --grace-period int               Period of time in seconds given to the resource to terminate gracefully. Ignored if negative. Set to 1 for immediate shutdown. Can only be set to 0 when --force is true (force deletion). (default -1)</span>\n<span class=\"pl-c1\">  -h, --help                           help for kubectl</span>\n<span class=\"pl-c1\">      --insecure-skip-tls-verify       If true, the server&apos;s certificate will not be checked for validity. This will make your HTTPS connections insecure</span>\n<span class=\"pl-c1\">  -i, --interactive                    If true, a prompt asks whether resources can be deleted</span>\n<span class=\"pl-c1\">      --kubeconfig string              Path to the kubeconfig file to use for CLI requests.</span>\n<span class=\"pl-c1\">  -n, --namespace string               If present, the namespace scope for this CLI request</span>\n<span class=\"pl-c1\">  -o, --output string                  Output format. One of: json|yaml|name|go-template|go-template-file|template|templatefile|jsonpath|jsonpath-as-json|jsonpath-file.</span>\n<span class=\"pl-c1\">  -q, --quiet                          If true, no output is produced</span>\n<span class=\"pl-c1\">      --request-timeout string         The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don&apos;t timeout requests. (default &quot;0&quot;)</span>\n<span class=\"pl-c1\">  -l, --selector string                Selector (label query) to filter on, supports &apos;=&apos;, &apos;==&apos;, and &apos;!=&apos;.(e.g. -l key1=value1,key2=value2)</span>\n<span class=\"pl-c1\">  -s, --server string                  The address and port of the Kubernetes API server</span>\n<span class=\"pl-c1\">      --template string                Template string or path to template file to use when -o=go-template, -o=go-template-file. The template format is golang templates [http://golang.org/pkg/text/template/#pkg-overview].</span>\n<span class=\"pl-c1\">      --timeout duration               The length of time to wait before giving up on a delete, zero means determine a timeout from the size of the object</span>\n<span class=\"pl-c1\">      --tls-server-name string         Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used</span>\n<span class=\"pl-c1\">      --token string                   Bearer token for authentication to the API server</span>\n<span class=\"pl-c1\">      --user string                    The name of the kubeconfig user to use</span>\n<span class=\"pl-c1\">  -v, --version                        If true, show the version of this plugin</span>\n<span class=\"pl-c1\">      --wait                           If true, wait for resources to be gone before returning. This waits for finalizers.</span>\n</pre></div>\n<h3><a id=\"user-content-caveats\" class=\"anchor\" href=\"#caveats\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Caveats</h3>\n<ul>\n<li>It&apos;s recommended to run this plugin as dry-run (<code>--dry-run=client</code> or <code>--dry-run=server</code>) first or interactive mode (<code>--interactive</code>) in order to examine what resources will be deleted when running it, especially when you&apos;re trying to run it in a production environment.</li>\n<li>Even if you use <code>--namespace kube-system</code> or <code>--all-namespaces</code>, this plugin never deletes any resources in <code>kube-system</code> so that it prevents unexpected resource deletion.</li>\n<li>This plugin doesn&apos;t determine whether custom controllers or CRDs consume or depend on the supported resources. Make sure the resources you want to reap aren&apos;t used by them.\n<ul>\n<li>e.g.) A Secret, which isn&apos;t used by any Pods or ServiceAccounts but used by <a href=\"https://cert-manager.io\">cert-manager</a>, can be deleted</li>\n</ul>\n</li>\n</ul>\n<h2><a id=\"user-content-background\" class=\"anchor\" href=\"#background\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Background</h2>\n<p><code>kubectl apply --prune</code> allows us to delete unused resources.\nHowever, it&apos;s not very flexible when we want to choose what kind resource to be deleted.\nthis plugin provides more flexible, easy way to delete resources.</p>\n<h2><a id=\"user-content-similar-projects\" class=\"anchor\" href=\"#similar-projects\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Similar Projects</h2>\n\n\n</article></div></div>",
      "contentAsText": "\n\n\n\n\nkubectl-reap is a kubectl plugin that deletes unused Kubernetes resources.\n\nSupported resources:\n\n\n\nKind\nCondition\n\n\n\n\nPod\nNot running\n\n\nConfigMap\nNot used by any Pods\n\n\nSecret\nNot used by any Pods or ServiceAccounts\n\n\nPersistentVolume\nNot satisfying any PersistentVolumeClaims\n\n\nPersistentVolumeClaim\nNot used by any Pods\n\n\nJob\nCompleted\n\n\nPodDisruptionBudget\nNot targeting any Pods\n\n\nHorizontalPodAutoscaler\nNot targeting any resources\n\n\n\nSince this plugin supports dry-run as described below, it also helps you to find resources you misconfigured or forgot to delete.\nBefore getting started, read the caveats of using this plugin.\nInstallation\nDownload precompiled binaries from GitHub Releases.\nVia Krew\n$ kubectl krew install reap\n\nVia Go\n$ go get github.com/micnncim/kubectl-reap/cmd/kubectl-reap\n\nExamples\nPods\nIn this example, this plugin deletes all Pods whose status is not Running.\n$ kubectl get po\nNAME          READY   STATUS      RESTARTS   AGE\npod-running   1/1     Running     0          10s\npod-pending   0/1     Pending     0          20s\npod-failed    0/1     Failed      0          30s\npod-unknown   0/1     Unknown     0          40s\njob-kqpxc     0/1     Completed   0          50s\n\n$ kubectl reap po\npod/pod-pending deleted\npod/pod-failed deleted\npod/pod-unknown deleted\npod/job-kqpxc deleted\nConfigMaps\nIn this example, this plugin deletes the unused ConfigMap config-2.\n$ kubectl get cm\nNAME       DATA   AGE\nconfig-1   1      0m15s\nconfig-2   1      0m10s\n\n$ cat <<EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    volumeMounts:\n    - name: config-1-volume\n      mountPath: /var/config\n  volumes:\n  - name: config-1-volume\n    configMap:\n      name: config-1\nEOF\n\n$ kubectl reap cm\nconfigmap/config-2 deleted\nInteractive Mode\nYou can choose which resource you will delete one by one by interactive mode.\n$ kubectl reap cm --interactive # or '-i'\n? Are you sure to delete configmap/config-1? Yes\nconfigmap/config-1 deleted\n? Are you sure to delete configmap/config-2? No\n? Are you sure to delete configmap/config-3? Yes\nconfigmap/config-3 deleted\nUsage\n$ kubectl reap --help\n\nDelete unused resources. Supported resources:\n\n- Pods (whose status is not Running)\n- ConfigMaps (not used by any Pods)\n- Secrets (not used by any Pods or ServiceAccounts)\n- PersistentVolumes (not satisfying any PersistentVolumeClaims)\n- PersistentVolumeClaims (not used by any Pods)\n- Jobs (completed)\n- PodDisruptionBudgets (not targeting any Pods)\n- HorizontalPodAutoscalers (not targeting any resources)\n\nUsage:\n  kubectl reap RESOURCE_TYPE [flags]\n\nExamples:\n\n  # Delete ConfigMaps not mounted on any Pods and in the current namespace and context\n  $ kubectl reap configmaps\n\n  # Delete unused ConfigMaps and Secrets in the namespace/my-namespace and context/my-context\n  $ kubectl reap cm,secret -n my-namespace --context my-context\n\n  # Delete ConfigMaps not mounted on any Pods and across all namespace\n  $ kubectl reap cm --all-namespaces\n\n  # Delete Pods whose status is not Running as client-side dry-run\n  $ kubectl reap po --dry-run=client\n\nFlags:\n  -A, --all-namespaces                 If true, delete the targeted resources across all namespace except kube-system\n      --allow-missing-template-keys    If true, ignore any errors in templates when a field or map key is missing in the template. Only applies to golang and jsonpath output formats. (default true)\n      --as string                      Username to impersonate for the operation\n      --as-group stringArray           Group to impersonate for the operation, this flag can be repeated to specify multiple groups.\n      --cache-dir string               Default cache directory (default \"/Users/micnncim/.kube/cache\")\n      --certificate-authority string   Path to a cert file for the certificate authority\n      --client-certificate string      Path to a client certificate file for TLS\n      --client-key string              Path to a client key file for TLS\n      --cluster string                 The name of the kubeconfig cluster to use\n      --context string                 The name of the kubeconfig context to use\n      --dry-run string[=\"unchanged\"]   Must be \"none\", \"server\", or \"client\". If client strategy, only print the object that would be sent, without sending it. If server strategy, submit server-side request without persisting the resource. (default \"none\")\n      --field-selector string          Selector (field query) to filter on, supports '=', '==', and '!='.(e.g. --field-selector key1=value1,key2=value2). The server only supports a limited number of field queries per type.\n      --force                          If true, immediately remove resources from API and bypass graceful deletion. Note that immediate deletion of some resources may result in inconsistency or data loss and requires confirmation.\n      --grace-period int               Period of time in seconds given to the resource to terminate gracefully. Ignored if negative. Set to 1 for immediate shutdown. Can only be set to 0 when --force is true (force deletion). (default -1)\n  -h, --help                           help for kubectl\n      --insecure-skip-tls-verify       If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure\n  -i, --interactive                    If true, a prompt asks whether resources can be deleted\n      --kubeconfig string              Path to the kubeconfig file to use for CLI requests.\n  -n, --namespace string               If present, the namespace scope for this CLI request\n  -o, --output string                  Output format. One of: json|yaml|name|go-template|go-template-file|template|templatefile|jsonpath|jsonpath-as-json|jsonpath-file.\n  -q, --quiet                          If true, no output is produced\n      --request-timeout string         The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\")\n  -l, --selector string                Selector (label query) to filter on, supports '=', '==', and '!='.(e.g. -l key1=value1,key2=value2)\n  -s, --server string                  The address and port of the Kubernetes API server\n      --template string                Template string or path to template file to use when -o=go-template, -o=go-template-file. The template format is golang templates [http://golang.org/pkg/text/template/#pkg-overview].\n      --timeout duration               The length of time to wait before giving up on a delete, zero means determine a timeout from the size of the object\n      --tls-server-name string         Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used\n      --token string                   Bearer token for authentication to the API server\n      --user string                    The name of the kubeconfig user to use\n  -v, --version                        If true, show the version of this plugin\n      --wait                           If true, wait for resources to be gone before returning. This waits for finalizers.\n\nCaveats\n\nIt's recommended to run this plugin as dry-run (--dry-run=client or --dry-run=server) first or interactive mode (--interactive) in order to examine what resources will be deleted when running it, especially when you're trying to run it in a production environment.\nEven if you use --namespace kube-system or --all-namespaces, this plugin never deletes any resources in kube-system so that it prevents unexpected resource deletion.\nThis plugin doesn't determine whether custom controllers or CRDs consume or depend on the supported resources. Make sure the resources you want to reap aren't used by them.\n\ne.g.) A Secret, which isn't used by any Pods or ServiceAccounts but used by cert-manager, can be deleted\n\n\n\nBackground\nkubectl apply --prune allows us to delete unused resources.\nHowever, it's not very flexible when we want to choose what kind resource to be deleted.\nthis plugin provides more flexible, easy way to delete resources.\nSimilar Projects\n\n\nPage 2\n\n\n\n\nkubectl-reap is a kubectl plugin that deletes unused Kubernetes resources.\n\nSupported resources:\n\n\n\nKind\nCondition\n\n\n\n\nPod\nNot running\n\n\nConfigMap\nNot used by any Pods\n\n\nSecret\nNot used by any Pods or ServiceAccounts\n\n\nPersistentVolume\nNot satisfying any PersistentVolumeClaims\n\n\nPersistentVolumeClaim\nNot used by any Pods\n\n\nJob\nCompleted\n\n\nPodDisruptionBudget\nNot targeting any Pods\n\n\nHorizontalPodAutoscaler\nNot targeting any resources\n\n\n\nSince this plugin supports dry-run as described below, it also helps you to find resources you misconfigured or forgot to delete.\nBefore getting started, read the caveats of using this plugin.\nInstallation\nDownload precompiled binaries from GitHub Releases.\nVia Krew\n$ kubectl krew install reap\n\nVia Go\n$ go get github.com/micnncim/kubectl-reap/cmd/kubectl-reap\n\nExamples\nPods\nIn this example, this plugin deletes all Pods whose status is not Running.\n$ kubectl get po\nNAME          READY   STATUS      RESTARTS   AGE\npod-running   1/1     Running     0          10s\npod-pending   0/1     Pending     0          20s\npod-failed    0/1     Failed      0          30s\npod-unknown   0/1     Unknown     0          40s\njob-kqpxc     0/1     Completed   0          50s\n\n$ kubectl reap po\npod/pod-pending deleted\npod/pod-failed deleted\npod/pod-unknown deleted\npod/job-kqpxc deleted\nConfigMaps\nIn this example, this plugin deletes the unused ConfigMap config-2.\n$ kubectl get cm\nNAME       DATA   AGE\nconfig-1   1      0m15s\nconfig-2   1      0m10s\n\n$ cat <<EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    volumeMounts:\n    - name: config-1-volume\n      mountPath: /var/config\n  volumes:\n  - name: config-1-volume\n    configMap:\n      name: config-1\nEOF\n\n$ kubectl reap cm\nconfigmap/config-2 deleted\nInteractive Mode\nYou can choose which resource you will delete one by one by interactive mode.\n$ kubectl reap cm --interactive # or '-i'\n? Are you sure to delete configmap/config-1? Yes\nconfigmap/config-1 deleted\n? Are you sure to delete configmap/config-2? No\n? Are you sure to delete configmap/config-3? Yes\nconfigmap/config-3 deleted\nUsage\n$ kubectl reap --help\n\nDelete unused resources. Supported resources:\n\n- Pods (whose status is not Running)\n- ConfigMaps (not used by any Pods)\n- Secrets (not used by any Pods or ServiceAccounts)\n- PersistentVolumes (not satisfying any PersistentVolumeClaims)\n- PersistentVolumeClaims (not used by any Pods)\n- Jobs (completed)\n- PodDisruptionBudgets (not targeting any Pods)\n- HorizontalPodAutoscalers (not targeting any resources)\n\nUsage:\n  kubectl reap RESOURCE_TYPE [flags]\n\nExamples:\n\n  # Delete ConfigMaps not mounted on any Pods and in the current namespace and context\n  $ kubectl reap configmaps\n\n  # Delete unused ConfigMaps and Secrets in the namespace/my-namespace and context/my-context\n  $ kubectl reap cm,secret -n my-namespace --context my-context\n\n  # Delete ConfigMaps not mounted on any Pods and across all namespace\n  $ kubectl reap cm --all-namespaces\n\n  # Delete Pods whose status is not Running as client-side dry-run\n  $ kubectl reap po --dry-run=client\n\nFlags:\n  -A, --all-namespaces                 If true, delete the targeted resources across all namespace except kube-system\n      --allow-missing-template-keys    If true, ignore any errors in templates when a field or map key is missing in the template. Only applies to golang and jsonpath output formats. (default true)\n      --as string                      Username to impersonate for the operation\n      --as-group stringArray           Group to impersonate for the operation, this flag can be repeated to specify multiple groups.\n      --cache-dir string               Default cache directory (default \"/Users/micnncim/.kube/cache\")\n      --certificate-authority string   Path to a cert file for the certificate authority\n      --client-certificate string      Path to a client certificate file for TLS\n      --client-key string              Path to a client key file for TLS\n      --cluster string                 The name of the kubeconfig cluster to use\n      --context string                 The name of the kubeconfig context to use\n      --dry-run string[=\"unchanged\"]   Must be \"none\", \"server\", or \"client\". If client strategy, only print the object that would be sent, without sending it. If server strategy, submit server-side request without persisting the resource. (default \"none\")\n      --field-selector string          Selector (field query) to filter on, supports '=', '==', and '!='.(e.g. --field-selector key1=value1,key2=value2). The server only supports a limited number of field queries per type.\n      --force                          If true, immediately remove resources from API and bypass graceful deletion. Note that immediate deletion of some resources may result in inconsistency or data loss and requires confirmation.\n      --grace-period int               Period of time in seconds given to the resource to terminate gracefully. Ignored if negative. Set to 1 for immediate shutdown. Can only be set to 0 when --force is true (force deletion). (default -1)\n  -h, --help                           help for kubectl\n      --insecure-skip-tls-verify       If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure\n  -i, --interactive                    If true, a prompt asks whether resources can be deleted\n      --kubeconfig string              Path to the kubeconfig file to use for CLI requests.\n  -n, --namespace string               If present, the namespace scope for this CLI request\n  -o, --output string                  Output format. One of: json|yaml|name|go-template|go-template-file|template|templatefile|jsonpath|jsonpath-as-json|jsonpath-file.\n  -q, --quiet                          If true, no output is produced\n      --request-timeout string         The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\")\n  -l, --selector string                Selector (label query) to filter on, supports '=', '==', and '!='.(e.g. -l key1=value1,key2=value2)\n  -s, --server string                  The address and port of the Kubernetes API server\n      --template string                Template string or path to template file to use when -o=go-template, -o=go-template-file. The template format is golang templates [http://golang.org/pkg/text/template/#pkg-overview].\n      --timeout duration               The length of time to wait before giving up on a delete, zero means determine a timeout from the size of the object\n      --tls-server-name string         Server name to use for server certificate validation. If it is not provided, the hostname used to contact the server is used\n      --token string                   Bearer token for authentication to the API server\n      --user string                    The name of the kubeconfig user to use\n  -v, --version                        If true, show the version of this plugin\n      --wait                           If true, wait for resources to be gone before returning. This waits for finalizers.\n\nCaveats\n\nIt's recommended to run this plugin as dry-run (--dry-run=client or --dry-run=server) first or interactive mode (--interactive) in order to examine what resources will be deleted when running it, especially when you're trying to run it in a production environment.\nEven if you use --namespace kube-system or --all-namespaces, this plugin never deletes any resources in kube-system so that it prevents unexpected resource deletion.\nThis plugin doesn't determine whether custom controllers or CRDs consume or depend on the supported resources. Make sure the resources you want to reap aren't used by them.\n\ne.g.) A Secret, which isn't used by any Pods or ServiceAccounts but used by cert-manager, can be deleted\n\n\n\nBackground\nkubectl apply --prune allows us to delete unused resources.\nHowever, it's not very flexible when we want to choose what kind resource to be deleted.\nthis plugin provides more flexible, easy way to delete resources.\nSimilar Projects\n\n\n",
      "description": "kubectl plugin that deletes unused Kubernetes resources - micnncim/kubectl-reap",
      "ogDescription": "kubectl plugin that deletes unused Kubernetes resources - micnncim/kubectl-reap"
    },
    {
      "url": "https://github.com/gitrgoliveira/minikube_demo",
      "title": "gitrgoliveira/minikube_demo",
      "content": "<div><div><article class=\"markdown-body entry-content container-lg\">\n<p>This repo has some demos for HashiCorp Vault, using minikube clusters as clients.</p>\n<p><em>Note: This is an advanced topic using a time limited public image of Vault Enterprise.</em></p>\n\n<h3><a id=\"user-content-requirements\" class=\"anchor\" href=\"https://github.com/gitrgoliveira/minikube_demo#requirements\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Requirements</h3>\n<p><em>This demo environment was setup in MacOSX.</em>\n<em>You will need to know bash to debug if something goes wrong</em></p>\n<ul>\n<li>Minikube</li>\n<li>Helm</li>\n<li>docker and docker-compose</li>\n<li>kubectl</li>\n<li>Vault cli</li>\n<li>Vault Enterprise license</li>\n</ul>\n<h2><a id=\"user-content-kubernetes-auth\" class=\"anchor\" href=\"https://github.com/gitrgoliveira/minikube_demo#kubernetes-auth\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Kubernetes auth</h2>\n<h3><a id=\"user-content-overview\" class=\"anchor\" href=\"https://github.com/gitrgoliveira/minikube_demo#overview\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Overview</h3>\n<p>The objective with this demo is to demonstrate how to isolate the access to Vault secrets between applications running in Kubernetes.</p>\n<p><a href=\"https://github.com/gitrgoliveira/minikube_demo/blob/master/graphics/k8s-auth.svg\"><img src=\"https://github.com/gitrgoliveira/minikube_demo/raw/master/graphics/k8s-auth.svg\" alt=\"Kubernetes auth\"></a></p>\n<p>The above image illustrates three use cases:</p>\n<ol>\n<li>\n<p>The happy path: an application running on <code>ns1</code> k8s namespace is able to authenticate to <code>cluster-1</code> Vault Namespace and retrieve a <em>KV</em> secret.</p>\n<ul>\n<li><em>Vault Namespace isolation ensures the authentication backend is not able to povide access to the <code>cluster-2</code> secrets, because the token provided is scoped to <code>cluster-1</code> namespace.</em></li>\n</ul>\n</li>\n<li>\n<p>Fail path 1: An application running on <code>ns2</code> k8s namespace  tries to authenticate to <code>cluster-1</code> Vault Namespace, but fails, because k8s namespace <code>ns2</code> is not authorized.</p>\n</li>\n<li>\n<p>Fail path 2: An application running on <code>ns1</code> k8s namespace  tries to authenticate to <code>cluster-1</code> Vault Namespace, but fails, because the cluster is not the one configured in Vault Kubernetes auth backend.</p>\n</li>\n</ol>\n<h3><a id=\"user-content-setup-steps\" class=\"anchor\" href=\"https://github.com/gitrgoliveira/minikube_demo#setup-steps\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Setup steps</h3>\n<ol>\n<li>Run <code>00_start.sh</code></li>\n<li>Run <code>01_setup.sh</code></li>\n<li>Run <code>02_deploy.sh</code></li>\n</ol>\n<p>To work with this setup you can <code>source helper.sh</code>, which provides you with some helper commands and setup.</p>\n<p>To clear your machine, just run <code>99_teardown.sh</code></p>\n<h2><a id=\"user-content-jenkins-workflow\" class=\"anchor\" href=\"https://github.com/gitrgoliveira/minikube_demo#jenkins-workflow\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Jenkins workflow</h2>\n<h3><a id=\"user-content-overview-1\" class=\"anchor\" href=\"https://github.com/gitrgoliveira/minikube_demo#overview-1\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Overview</h3>\n<p>The main objective of this workflow is to provide a way for Jenkins to securely retrieve dynamic Vault and Cloud credentials, to submit these to Terraform Enterprise via an API call.</p>\n<p>To achieve this, we have set the following targets:</p>\n<ul>\n<li>Reduce the value of the auth credential used to access Vault, in case of a leak</li>\n<li>Reduce the secret sprawl, by removing secrets from the Jenkins credential store</li>\n<li>Isolate the pipeline code used as much as possible</li>\n<li>Use a credential to access Vault that can be rotated.</li>\n</ul>\n<p><a href=\"https://github.com/gitrgoliveira/minikube_demo/blob/master/graphics/jenkins-k8s-auth.svg\"><img src=\"https://github.com/gitrgoliveira/minikube_demo/raw/master/graphics/jenkins-k8s-auth.svg\" alt=\"Jenkins workflow\"></a></p>\n<h3><a id=\"user-content-setup-steps-1\" class=\"anchor\" href=\"https://github.com/gitrgoliveira/minikube_demo#setup-steps-1\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Setup steps</h3>\n<ol>\n<li>Run <code>00_start.sh</code></li>\n<li>Run <code>01_setup.sh</code></li>\n<li>Run <code>add_jenkins.sh</code></li>\n</ol>\n<p>To work with this setup you can <code>source helper.sh</code>, which provides you with some helper commands and setup.</p>\n<p>To clear your machine, just run <code>03_teardown.sh</code></p>\n<h2><a id=\"user-content-envconsul-workflow\" class=\"anchor\" href=\"https://github.com/gitrgoliveira/minikube_demo#envconsul-workflow\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Envconsul workflow</h2>\n<p>Sometimes it&apos;s difficult to adapt an application do read variables from a file, instead of using environment variables. For these use cases, there are a few options:</p>\n<p><strong>Option one</strong> is to have Vault agent inject a variable file in the right format and modify the docker entry point to wrap binary into <code>export $(cat envfile | xargs) &amp;&amp; webapp</code></p>\n<p>The advantage here is that the changes are minimum, but it&apos;s also a <em>one-shot</em> injection, with no refresh ability.</p>\n<p><strong>Option two</strong> same as above, but the script also watches for file changes and restarts the process.</p>\n<p><strong>Option three</strong> is to use <code>envconsul</code>, to keep track of changes in Vault and restart the <code>webapp</code> process accordingly.</p>\n<p>This last option requires many changes, for example:</p>\n<ul>\n<li>Vault namespace specified in Dockerfile</li>\n<li>Vault secrets path specified in Dockerfile</li>\n<li>envconsul becomes a dependency</li>\n<li>Management of envconsul.hcl configuration file</li>\n<li>Vault token TTL needs to be increased, since envconsul will not refresh the token from the file.</li>\n</ul>\n<h3><a id=\"user-content-setup-steps-2\" class=\"anchor\" href=\"https://github.com/gitrgoliveira/minikube_demo#setup-steps-2\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Setup steps</h3>\n<ol>\n<li>Run <code>00_start.sh</code></li>\n<li>Run <code>01_setup.sh</code></li>\n<li>Run <code>add_webapp-env.sh</code></li>\n</ol>\n</article></div></div><hr><h4>Page 2</h4><body class=\"logged-out env-production page-responsive page-blob\"> <include-fragment class=\"js-notification-shelf-include-fragment\"></include-fragment> <div class=\"application-main \"> </div> <p id=\"ajax-error-message\" class=\"ajax-error-message\"> <svg class=\"octicon octicon-alert\" width=\"16\" height=\"16\"><path/></svg> You can&#x2019;t perform that action at this time. </p> <p class=\"js-stale-session-flash\"> <svg class=\"octicon octicon-alert\" width=\"16\" height=\"16\"><path/></svg> <span class=\"js-stale-session-flash-signed-in\">You signed in with another tab or window. <a href>Reload</a> to refresh your session.</span> <span class=\"js-stale-session-flash-signed-out\">You signed out in another tab or window. <a href>Reload</a> to refresh your session.</span> </p> <template id=\"site-details-dialog\"> <details class=\"details-reset details-overlay details-overlay-dark lh-default text-gray-dark hx_rsm\"> <summary></summary> <details-dialog class=\"Box Box--overlay d-flex flex-column anim-fade-in fast hx_rsm-dialog hx_rsm-modal\"> </details-dialog> </details>\n</template> <div class=\"js-cookie-consent-banner\"> <div class=\"hx_cookie-banner p-2 p-sm-3 p-md-4\"> <div class=\"Box hx_cookie-banner-box box-shadow-medium mx-auto\"> <div class=\"Box-body border-0 py-0 px-3 px-md-4\"> <div class=\"js-main-cookie-banner hx_cookie-banner-main\"> <div class=\"d-md-flex flex-items-center py-3\"> <p class=\"f5 flex-1 mb-3 mb-md-0\"> We use <span class=\"text-bold\">optional</span> third-party analytics cookies to understand how you use GitHub.com so we can build better products. <span class=\"btn-link js-cookie-consent-learn-more\">Learn more</span>. </p> </div> </div> <div class=\"js-cookie-details hx_cookie-banner-details\"> <div class=\"d-md-flex flex-items-center py-3\"> <p class=\"f5 flex-1 mb-2 mb-md-0\"> We use <span class=\"text-bold\">optional</span> third-party analytics cookies to understand how you use GitHub.com so we can build better products. <br> You can always update your selection by clicking <span class=\"text-bold\">Cookie Preferences</span> at the bottom of the page. For more information, see our <a href=\"https://docs.github.com/en/free-pro-team@latest/github/site-policy/github-privacy-statement\">Privacy Statement</a>. </p> </div> <div class=\"d-md-flex flex-items-center py-3 border-top\"> <div class=\"f5 flex-1 mb-2 mb-md-0\"> <p class=\"f6 mb-md-0\">We use essential cookies to perform essential website functions, e.g. they&apos;re used to log you in. <a href=\"https://docs.github.com/en/github/site-policy/github-subprocessors-and-cookies\">Learn more</a> </p> </div> <h5 class=\"text-blue\">Always active</h5> </div> <div class=\"d-md-flex flex-items-center py-3 border-top\"> <div class=\"f5 flex-1 mb-2 mb-md-0\"> <p class=\"f6 mb-md-0\">We use analytics cookies to understand how you use our websites so we can make them better, e.g. they&apos;re used to gather information about the pages you visit and how many clicks you need to accomplish a task. <a href=\"https://docs.github.com/en/github/site-policy/github-subprocessors-and-cookies\">Learn more</a> </p> </div> </div> </div>\n</div></div> </div>\n</div> </body>",
      "contentAsText": "\nThis repo has some demos for HashiCorp Vault, using minikube clusters as clients.\nNote: This is an advanced topic using a time limited public image of Vault Enterprise.\n\nRequirements\nThis demo environment was setup in MacOSX.\nYou will need to know bash to debug if something goes wrong\n\nMinikube\nHelm\ndocker and docker-compose\nkubectl\nVault cli\nVault Enterprise license\n\nKubernetes auth\nOverview\nThe objective with this demo is to demonstrate how to isolate the access to Vault secrets between applications running in Kubernetes.\n\nThe above image illustrates three use cases:\n\n\nThe happy path: an application running on ns1 k8s namespace is able to authenticate to cluster-1 Vault Namespace and retrieve a KV secret.\n\nVault Namespace isolation ensures the authentication backend is not able to povide access to the cluster-2 secrets, because the token provided is scoped to cluster-1 namespace.\n\n\n\nFail path 1: An application running on ns2 k8s namespace  tries to authenticate to cluster-1 Vault Namespace, but fails, because k8s namespace ns2 is not authorized.\n\n\nFail path 2: An application running on ns1 k8s namespace  tries to authenticate to cluster-1 Vault Namespace, but fails, because the cluster is not the one configured in Vault Kubernetes auth backend.\n\n\nSetup steps\n\nRun 00_start.sh\nRun 01_setup.sh\nRun 02_deploy.sh\n\nTo work with this setup you can source helper.sh, which provides you with some helper commands and setup.\nTo clear your machine, just run 99_teardown.sh\nJenkins workflow\nOverview\nThe main objective of this workflow is to provide a way for Jenkins to securely retrieve dynamic Vault and Cloud credentials, to submit these to Terraform Enterprise via an API call.\nTo achieve this, we have set the following targets:\n\nReduce the value of the auth credential used to access Vault, in case of a leak\nReduce the secret sprawl, by removing secrets from the Jenkins credential store\nIsolate the pipeline code used as much as possible\nUse a credential to access Vault that can be rotated.\n\n\nSetup steps\n\nRun 00_start.sh\nRun 01_setup.sh\nRun add_jenkins.sh\n\nTo work with this setup you can source helper.sh, which provides you with some helper commands and setup.\nTo clear your machine, just run 03_teardown.sh\nEnvconsul workflow\nSometimes it's difficult to adapt an application do read variables from a file, instead of using environment variables. For these use cases, there are a few options:\nOption one is to have Vault agent inject a variable file in the right format and modify the docker entry point to wrap binary into export $(cat envfile | xargs) && webapp\nThe advantage here is that the changes are minimum, but it's also a one-shot injection, with no refresh ability.\nOption two same as above, but the script also watches for file changes and restarts the process.\nOption three is to use envconsul, to keep track of changes in Vault and restart the webapp process accordingly.\nThis last option requires many changes, for example:\n\nVault namespace specified in Dockerfile\nVault secrets path specified in Dockerfile\nenvconsul becomes a dependency\nManagement of envconsul.hcl configuration file\nVault token TTL needs to be increased, since envconsul will not refresh the token from the file.\n\nSetup steps\n\nRun 00_start.sh\nRun 01_setup.sh\nRun add_webapp-env.sh\n\nPage 2      You can’t perform that action at this time.    You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session.          We use optional third-party analytics cookies to understand how you use GitHub.com so we can build better products. Learn more.       We use optional third-party analytics cookies to understand how you use GitHub.com so we can build better products.  You can always update your selection by clicking Cookie Preferences at the bottom of the page. For more information, see our Privacy Statement.     We use essential cookies to perform essential website functions, e.g. they're used to log you in. Learn more   Always active    We use analytics cookies to understand how you use our websites so we can make them better, e.g. they're used to gather information about the pages you visit and how many clicks you need to accomplish a task. Learn more    \n \n ",
      "description": "Vault demos with minikube and kubernetes. Contribute to gitrgoliveira/minikube_demo development by creating an account on GitHub.",
      "ogDescription": "Vault demos with minikube and kubernetes. Contribute to gitrgoliveira/minikube_demo development by creating an account on GitHub."
    },
    {
      "url": "https://github.com/Juniper/contrail-operator",
      "title": "Juniper/contrail-operator",
      "content": "<div><div><article class=\"markdown-body entry-content container-lg\">\n<p>This is first check-in to R2005</p>\n<h2><a id=\"user-content-references\" class=\"anchor\" href=\"https://github.com/Juniper/contrail-operator#references\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>References</h2>\n<p><a href=\"https://github.com/Juniper/contrail-operator/blob/master/test/env/README.md\">E2E test guide</a><br>\n<a href=\"https://github.com/Juniper/contrail-operator/blob/master/DEVELOPMENT.md\">Detailed development guide</a><br>\n<a href=\"https://github.com/Juniper/contrail-operator/blob/master/deploy/openshift/README.md\">Deployment on Openshift 4 and AWS</a><br>\n<a href=\"https://github.com/Juniper/contrail-operator/blob/master/deploy/openshift/docs/Openshift-KVM.md\">Deployment on Openshift 4 and KVM</a></p>\n<h2><a id=\"user-content-requirements\" class=\"anchor\" href=\"https://github.com/Juniper/contrail-operator#requirements\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Requirements</h2>\n\n\n<h2><a id=\"user-content-install-go\" class=\"anchor\" href=\"https://github.com/Juniper/contrail-operator#install-go\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Install Go</h2>\n<ul>\n<li><a href=\"https://golang.org/doc/install#install\">https://golang.org/doc/install#install</a></li>\n</ul>\n<h2><a id=\"user-content-checkout-contrail-operator-source-code\" class=\"anchor\" href=\"https://github.com/Juniper/contrail-operator#checkout-contrail-operator-source-code\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Checkout contrail-operator source code</h2>\n<p>Contrail-Operator is a Go Module therefore can be downloaded to a folder outside the GOPATH.</p>\n<pre><code>git clone git@github.com:Juniper/contrail-operator.git\n</code></pre>\n<h2><a id=\"user-content-verify-if-contrail-operator-can-be-built\" class=\"anchor\" href=\"https://github.com/Juniper/contrail-operator#verify-if-contrail-operator-can-be-built\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Verify if contrail-operator can be built</h2>\n<pre><code>go build cmd/manager/main.go\n</code></pre>\n<h2><a id=\"user-content-install-kubernetes-client\" class=\"anchor\" href=\"https://github.com/Juniper/contrail-operator#install-kubernetes-client\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Install Kubernetes Client</h2>\n<p>On Mac OS: <a href=\"https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-on-macos\">https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-on-macos</a><br>\nOn Linux: <a href=\"https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-on-linux\">https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-on-linux</a></p>\n<h2><a id=\"user-content-install-ide\" class=\"anchor\" href=\"https://github.com/Juniper/contrail-operator#install-ide\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Install IDE</h2>\n<p>We use Goland and Visual Studio Code. Install your favourite one.</p>\n<h2><a id=\"user-content-install-kind\" class=\"anchor\" href=\"https://github.com/Juniper/contrail-operator#install-kind\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Install Kind</h2>\n<p>Kind is used as a lightweight Kubernetes cluster for development purposes</p>\n<pre><code>GO111MODULE=&quot;on&quot; go get sigs.k8s.io/kind@v0.9.0\n</code></pre>\n<p>Verify if it works (Mac OS):</p>\n<pre><code>$ kind version\nkind v0.9.0 go(...) darwin/amd64\n</code></pre>\n<p>Verify if it works (Linux):</p>\n<pre><code>$ kind version\nkind v0.9.0 go(...) linux/amd64\n</code></pre>\n<p>If command is not found, then reload <code>~/.zshrc</code> (on Mac OS) or <code>~/.bashrc</code> (on Linux) and verify if <code>~/go/bin</code> is in <code>$PATH</code>.</p>\n<h2><a id=\"user-content-install-docker-for-desktop-mac-os-only\" class=\"anchor\" href=\"https://github.com/Juniper/contrail-operator#install-docker-for-desktop-mac-os-only\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Install Docker for Desktop (Mac OS only)</h2>\n<ul>\n<li><a href=\"https://hub.docker.com/editions/community/docker-ce-desktop-mac\">https://hub.docker.com/editions/community/docker-ce-desktop-mac</a></li>\n</ul>\n<h3><a id=\"user-content-increase-memory-amount-in-settings-to-8gb\" class=\"anchor\" href=\"https://github.com/Juniper/contrail-operator#increase-memory-amount-in-settings-to-8gb\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Increase memory amount in settings to 8GB:</h3>\n<ul>\n<li>click Docker icon</li>\n<li>select Preferences</li>\n<li>go to Resources/Advanced</li>\n<li>increase memory to 8GB</li>\n<li>restart Docker for Desktop</li>\n</ul>\n<h2><a id=\"user-content-install-docker-engine-linux-only\" class=\"anchor\" href=\"https://github.com/Juniper/contrail-operator#install-docker-engine-linux-only\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Install Docker Engine (Linux only)</h2>\n<p>Instruction for Ubuntu (other distros are available as well): <a href=\"https://docs.docker.com/install/linux/docker-ce/ubuntu/\">https://docs.docker.com/install/linux/docker-ce/ubuntu/</a></p>\n<h2><a id=\"user-content-create-kind-test-environment\" class=\"anchor\" href=\"https://github.com/Juniper/contrail-operator#create-kind-test-environment\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Create Kind test environment</h2>\n<p>Following commands will create Kubernetes cluster.</p>\n<p>It also starts Docker registry on port 5000. All pods deployed in the cluster will pull images from this Docker Registry.</p>\n<pre><code>cd test/env\n./create_testenv.sh\n</code></pre>\n<p>Verify if it works:</p>\n<pre><code>$ kind get clusters\nkind\n</code></pre>\n<h2><a id=\"user-content-pull-images-to-locker-docker-registry\" class=\"anchor\" href=\"https://github.com/Juniper/contrail-operator#pull-images-to-locker-docker-registry\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Pull images to locker Docker registry</h2>\n<pre><code>cd test/env\n./update_local_registry.sh\n</code></pre>\n<p>In case of timeouts disable VPN and retry.</p>\n<h2><a id=\"user-content-install-operator-sdk\" class=\"anchor\" href=\"https://github.com/Juniper/contrail-operator#install-operator-sdk\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Install operator-sdk</h2>\n<p>Operator-SDK is a set of tools for developing Kubernates Operators. It is needed for:</p>\n<ul>\n<li>Go code generation</li>\n<li>K8s Custom Resource Definitions generation</li>\n<li>building contrail-operator image</li>\n<li>running e2e tests (aka system tests)</li>\n</ul>\n<h3><a id=\"user-content-operator-sdk-installation-on-mac-os\" class=\"anchor\" href=\"https://github.com/Juniper/contrail-operator#operator-sdk-installation-on-mac-os\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Operator-sdk installation on Mac OS</h3>\n<pre><code>$ curl -LO https://github.com/operator-framework/operator-sdk/releases/download/v0.17.1/operator-sdk-v0.17.1-x86_64-apple-darwin\n$ chmod u+x ./operator-sdk-v0.17.1-x86_64-apple-darwin\n$ sudo mv ./operator-sdk-v0.17.1-x86_64-apple-darwin /usr/local/bin/operator-sdk\n</code></pre>\n<p>Verify if it works:</p>\n<pre><code>$ operator-sdk version\noperator-sdk version: &quot;v0.17.1&quot;, (...)\n</code></pre>\n<h3><a id=\"user-content-operator-sdk-installation-on-linux\" class=\"anchor\" href=\"https://github.com/Juniper/contrail-operator#operator-sdk-installation-on-linux\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Operator-sdk installation on Linux</h3>\n<pre><code>$ curl -LO https://github.com/operator-framework/operator-sdk/releases/download/v0.17.1/operator-sdk-v0.17.1-x86_64-linux-gnu\n$ chmod u+x ./operator-sdk-v0.17.1-x86_64-linux-gnu  \n$ sudo mv ./operator-sdk-v0.17.1-x86_64-linux-gnu /usr/local/bin/operator-sdk\n</code></pre>\n<p>Verify if it works:</p>\n<pre><code>$ operator-sdk version\noperator-sdk version: &quot;v0.17.1&quot;, (...)\n</code></pre>\n<h2><a id=\"user-content-install-bazel\" class=\"anchor\" href=\"https://github.com/Juniper/contrail-operator#install-bazel\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Install bazel</h2>\n<h3><a id=\"user-content-bazel-on-linux\" class=\"anchor\" href=\"https://github.com/Juniper/contrail-operator#bazel-on-linux\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Bazel on linux</h3>\n<pre><code>https://docs.bazel.build/versions/3.4.0/install-ubuntu.html\n</code></pre>\n<h3><a id=\"user-content-bazel-on-mac\" class=\"anchor\" href=\"https://github.com/Juniper/contrail-operator#bazel-on-mac\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Bazel on Mac</h3>\n<pre><code>https://docs.bazel.build/versions/3.4.0/install-os-x.html#install-on-mac-os-x-homebrew\n</code></pre>\n<h2><a id=\"user-content-build-contrail-operator\" class=\"anchor\" href=\"https://github.com/Juniper/contrail-operator#build-contrail-operator\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Build Contrail-Operator</h2>\n<p>In order to run Contrail-Operator in the Kubernetes cluster we have to build Docker Image.</p>\n<pre><code># local registry address\nexport LOCAL_REGISTRY=localhost:5000\n\n# it builds and pushes image: {LOCAL_REGISTRY}/contrail-operator/engprod-269421/contrail-operator-crdsloader:master.latest\nbazel run //cmd/crdsloader:contrail-operator-crdsloader-push-local\n\n# it builds and pushes image: {LOCAL_REGISTRY}/contrail-operator/engprod-269421/contrail-operator:master.latest\nbazel run //cmd/manager:contrail-operator-push-local\n</code></pre>\n<p>Verify:</p>\n<pre><code>$ docker images | grep contrail-operator\ncontrail-operator   latest   5c0148fdb7e8   4 seconds ago   125MB\n</code></pre>\n<p>After image is created we have to push it into local Docker registry.</p>\n<pre><code>docker push localhost:5000/contrail-operator:latest\n</code></pre>\n<h2><a id=\"user-content-run-contrail-operator-with-sample-contrail-configuration\" class=\"anchor\" href=\"https://github.com/Juniper/contrail-operator#run-contrail-operator-with-sample-contrail-configuration\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Run Contrail-Operator with sample Contrail configuration</h2>\n<p>Following command will deploy Contrail-Operator on a working Kubernetes cluster. It will also create a sample Contrail configuration. Note: you can change this configuration by editing <code>test/env/deploy/cluster.yaml</code> file.</p>\n<pre><code>cd test/env\n./apply_contrail_cluster.sh\n</code></pre>\n<p>As soon as contrail-operator is started, it deploys Contrail services. It is a time-consuming process. You can watch the progress using following command:</p>\n<pre><code>watch kubectl get pods -n contrail\n</code></pre>\n<p>Eventually all pods should be Running:</p>\n<pre><code>NAME                                          READY   STATUS      RESTARTS   AGE\ncassandra1-cassandra-statefulset-0            1/1     Running     0          8m15s\ncommand-command-deployment-77644668cf-dpp6f   1/1     Running     0          7m21s\nconfig1-config-statefulset-0                  9/9     Running     0          4m47s\ncontrail-operator-585f5bd8b5-hfdrz            1/1     Running     0          9m24s\ncontrol1-control-statefulset-0                4/4     Running     0          2m56s\nkeystone-keystone-statefulset-0               3/3     Running     0          7m8s\nmemcached-deployment-5f5f974bd9-gthzx         1/1     Running     0          8m15s\npostgres-pod                                  1/1     Running     0          8m16s\nprovmanager1-provisionmanager-statefulset-0   1/1     Running     0          2m57s\nrabbitmq1-rabbitmq-statefulset-0              1/1     Running     0          8m15s\nswift-proxy-deployment-754f87448b-6l5nc       1/1     Running     0          4m32s\nswift-ring-account-job-rnsxs                  0/1     Completed   0          7s\nswift-ring-container-job-pkb2k                0/1     Completed   0          7s\nswift-ring-object-job-7nn44                   0/1     Completed   0          7s\nswift-storage-statefulset-0                   13/13   Running     0          8m11s\nwebui1-webui-statefulset-0                    3/3     Running     0          2m56s\nzookeeper1-zookeeper-statefulset-0            1/1     Running     0          8m16s\n</code></pre>\n<h3><a id=\"user-content-verify-if-contrail-command-is-working\" class=\"anchor\" href=\"https://github.com/Juniper/contrail-operator#verify-if-contrail-command-is-working\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Verify if Contrail Command is working</h3>\n<p>You can access Contrail Command application via web browser. Before that you have to forward the network traffic from localhost to Command&apos;s pod.</p>\n<pre><code>kubectl port-forward $(kubectl get pods -l command=command -n contrail -o name) -n contrail 9091:9091\n</code></pre>\n<p>Go to <a href=\"http://localhost:9091/\">http://localhost:9091</a></p>\n<p>Authenticate using <code>admin</code> username, <code>contrail123</code> password and <code>Default</code> domain.</p>\n<h2><a id=\"user-content-run-unit-tests\" class=\"anchor\" href=\"https://github.com/Juniper/contrail-operator#run-unit-tests\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Run unit tests</h2>\n<p>You can run unit test tests on your favourite IDE by executing all tests in <code>pkg</code> package.</p>\n<p>You can also use command line tool:</p>\n<pre><code>go test ./pkg/...\n</code></pre>\n<p>Eventually you should get results and command should return success:</p>\n<pre><code>?       github.com/Juniper/contrail-operator/pkg/apis    [no test files]\n?       github.com/Juniper/contrail-operator/pkg/apis/contrail    [no test files]\n?       github.com/Juniper/contrail-operator/pkg/apis/contrail/v1alpha1    [no test files]\n?       github.com/Juniper/contrail-operator/pkg/apis/contrail/v1alpha1/templates    [no test files]\nok      github.com/Juniper/contrail-operator/pkg/apis/contrail/v1alpha1/tests    0.943s\n?       github.com/Juniper/contrail-operator/pkg/client/keystone    [no test files]\n?       github.com/Juniper/contrail-operator/pkg/client/kubeproxy    [no test files]\n?       github.com/Juniper/contrail-operator/pkg/client/swift    [no test files]\n?       github.com/Juniper/contrail-operator/pkg/controller    [no test files]\nok      github.com/Juniper/contrail-operator/pkg/controller/cassandra    1.414s\nok      github.com/Juniper/contrail-operator/pkg/controller/command    2.316s\n?       github.com/Juniper/contrail-operator/pkg/controller/config    [no test files]\n?       github.com/Juniper/contrail-operator/pkg/controller/control    [no test files]\n?       github.com/Juniper/contrail-operator/pkg/controller/enqueue    [no test files]\nok      github.com/Juniper/contrail-operator/pkg/controller/keystone    4.196s\n?       github.com/Juniper/contrail-operator/pkg/controller/kubemanager    [no test files]\nok      github.com/Juniper/contrail-operator/pkg/controller/manager    1.789s\n?       github.com/Juniper/contrail-operator/pkg/controller/manager/crs    [no test files]\nok      github.com/Juniper/contrail-operator/pkg/controller/memcached    1.097s\nok      github.com/Juniper/contrail-operator/pkg/controller/postgres    1.779s\n?       github.com/Juniper/contrail-operator/pkg/controller/provisionmanager    [no test files]\n?       github.com/Juniper/contrail-operator/pkg/controller/rabbitmq    [no test files]\nok      github.com/Juniper/contrail-operator/pkg/controller/swift    1.002s\nok      github.com/Juniper/contrail-operator/pkg/controller/swiftproxy    0.870s\nok      github.com/Juniper/contrail-operator/pkg/controller/swiftstorage    1.147s\n?       github.com/Juniper/contrail-operator/pkg/controller/utils    [no test files]\n?       github.com/Juniper/contrail-operator/pkg/controller/vrouter    [no test files]\n?       github.com/Juniper/contrail-operator/pkg/controller/webui    [no test files]\n?       github.com/Juniper/contrail-operator/pkg/controller/zookeeper    [no test files]\nok      github.com/Juniper/contrail-operator/pkg/job    0.389s\nok      github.com/Juniper/contrail-operator/pkg/k8s    0.558s\n?       github.com/Juniper/contrail-operator/pkg/randomstring    [no test files]\nok      github.com/Juniper/contrail-operator/pkg/swift/ring    0.416s\n</code></pre>\n<h2><a id=\"user-content-run-e2e-tests-aka-system-tests\" class=\"anchor\" href=\"https://github.com/Juniper/contrail-operator#run-e2e-tests-aka-system-tests\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Run e2e tests (aka system tests)</h2>\n<p>In order to test if the whole system works as expected we have a few plumbing tests. They verify if after deployment all Contrail services can talk to each other and operate as expected.</p>\n<p>Before tests can be run you have to have clean the cluster. The fastest way is to delete the cluster:</p>\n<pre><code>kind delete cluster\n</code></pre>\n<p>Then you have to create a new one plus a <code>contrail</code> namespace:</p>\n<pre><code>cd test/env\n./create_testenv.sh\nkubectl create namespace contrail\n</code></pre>\n<p>System tests can be run using operator-sdk tool</p>\n<pre><code># From contrail-operator root directory\n# To run aio e2e test\noperator-sdk test local ./test/e2e/aio/ --namespace contrail --go-test-flags &quot;-v -timeout=30m&quot; --up-local\n# To run ha e2e test\noperator-sdk test local ./test/e2e/ha/ --namespace contrail --go-test-flags &quot;-v -timeout=30m&quot; --up-local\n</code></pre>\n<h2><a id=\"user-content-before-submitting-pull-request\" class=\"anchor\" href=\"https://github.com/Juniper/contrail-operator#before-submitting-pull-request\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Before submitting pull request</h2>\n<p>There is a set of tools that can check your code automatically. This includes static code checks, unit-tests and e2e integration tests. Most of those checks are run for every pull request and vote.</p>\n<h3><a id=\"user-content-run-gazelle-to-make-sure-that-all-buildbazel-files-get-updated\" class=\"anchor\" href=\"https://github.com/Juniper/contrail-operator#run-gazelle-to-make-sure-that-all-buildbazel-files-get-updated\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Run gazelle to make sure that all BUILD.bazel files get updated</h3>\n<pre><code>bazel run //:gazelle\n</code></pre>\n<h3><a id=\"user-content-build-and-test-code-this-commands-also-runs-nogo-linters\" class=\"anchor\" href=\"https://github.com/Juniper/contrail-operator#build-and-test-code-this-commands-also-runs-nogo-linters\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Build and test code. This commands also runs <code>nogo</code> linters</h3>\n<pre><code>bazel build //... &amp;&amp; bazel test //...\n</code></pre>\n<p>It will report errors in case:</p>\n<ul>\n<li>code doesn&apos;t build</li>\n<li>unit-tests fails</li>\n<li>static checks don&apos;t pass</li>\n<li>code is not correctly formatted</li>\n</ul>\n<h2><a id=\"user-content-notes\" class=\"anchor\" href=\"https://github.com/Juniper/contrail-operator#notes\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Notes</h2>\n<ul>\n<li>Contrail Operator creates Persistent Volumes that are used by some of the deployed pods. After deletion of Contrail resources (e.g. after deleting the Manager Custom Resource), those Persistent Volumes will not be deleted. Administrator has to delete them manually and make sure that directories created by these volumes on cluster nodes are in the expected state. Example Persistent Volumes deletion command:</li>\n</ul>\n<pre><code>kubectl delete pv $(kubectl get pv -o=jsonpath=&apos;{.items[?(@.spec.storageClassName==&quot;local-storage&quot;)].metadata.name}&apos;)\n</code></pre>\n</article></div></div>",
      "contentAsText": "\nThis is first check-in to R2005\nReferences\nE2E test guide\nDetailed development guide\nDeployment on Openshift 4 and AWS\nDeployment on Openshift 4 and KVM\nRequirements\n\n\nInstall Go\n\nhttps://golang.org/doc/install#install\n\nCheckout contrail-operator source code\nContrail-Operator is a Go Module therefore can be downloaded to a folder outside the GOPATH.\ngit clone git@github.com:Juniper/contrail-operator.git\n\nVerify if contrail-operator can be built\ngo build cmd/manager/main.go\n\nInstall Kubernetes Client\nOn Mac OS: https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-on-macos\nOn Linux: https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-on-linux\nInstall IDE\nWe use Goland and Visual Studio Code. Install your favourite one.\nInstall Kind\nKind is used as a lightweight Kubernetes cluster for development purposes\nGO111MODULE=\"on\" go get sigs.k8s.io/kind@v0.9.0\n\nVerify if it works (Mac OS):\n$ kind version\nkind v0.9.0 go(...) darwin/amd64\n\nVerify if it works (Linux):\n$ kind version\nkind v0.9.0 go(...) linux/amd64\n\nIf command is not found, then reload ~/.zshrc (on Mac OS) or ~/.bashrc (on Linux) and verify if ~/go/bin is in $PATH.\nInstall Docker for Desktop (Mac OS only)\n\nhttps://hub.docker.com/editions/community/docker-ce-desktop-mac\n\nIncrease memory amount in settings to 8GB:\n\nclick Docker icon\nselect Preferences\ngo to Resources/Advanced\nincrease memory to 8GB\nrestart Docker for Desktop\n\nInstall Docker Engine (Linux only)\nInstruction for Ubuntu (other distros are available as well): https://docs.docker.com/install/linux/docker-ce/ubuntu/\nCreate Kind test environment\nFollowing commands will create Kubernetes cluster.\nIt also starts Docker registry on port 5000. All pods deployed in the cluster will pull images from this Docker Registry.\ncd test/env\n./create_testenv.sh\n\nVerify if it works:\n$ kind get clusters\nkind\n\nPull images to locker Docker registry\ncd test/env\n./update_local_registry.sh\n\nIn case of timeouts disable VPN and retry.\nInstall operator-sdk\nOperator-SDK is a set of tools for developing Kubernates Operators. It is needed for:\n\nGo code generation\nK8s Custom Resource Definitions generation\nbuilding contrail-operator image\nrunning e2e tests (aka system tests)\n\nOperator-sdk installation on Mac OS\n$ curl -LO https://github.com/operator-framework/operator-sdk/releases/download/v0.17.1/operator-sdk-v0.17.1-x86_64-apple-darwin\n$ chmod u+x ./operator-sdk-v0.17.1-x86_64-apple-darwin\n$ sudo mv ./operator-sdk-v0.17.1-x86_64-apple-darwin /usr/local/bin/operator-sdk\n\nVerify if it works:\n$ operator-sdk version\noperator-sdk version: \"v0.17.1\", (...)\n\nOperator-sdk installation on Linux\n$ curl -LO https://github.com/operator-framework/operator-sdk/releases/download/v0.17.1/operator-sdk-v0.17.1-x86_64-linux-gnu\n$ chmod u+x ./operator-sdk-v0.17.1-x86_64-linux-gnu  \n$ sudo mv ./operator-sdk-v0.17.1-x86_64-linux-gnu /usr/local/bin/operator-sdk\n\nVerify if it works:\n$ operator-sdk version\noperator-sdk version: \"v0.17.1\", (...)\n\nInstall bazel\nBazel on linux\nhttps://docs.bazel.build/versions/3.4.0/install-ubuntu.html\n\nBazel on Mac\nhttps://docs.bazel.build/versions/3.4.0/install-os-x.html#install-on-mac-os-x-homebrew\n\nBuild Contrail-Operator\nIn order to run Contrail-Operator in the Kubernetes cluster we have to build Docker Image.\n# local registry address\nexport LOCAL_REGISTRY=localhost:5000\n\n# it builds and pushes image: {LOCAL_REGISTRY}/contrail-operator/engprod-269421/contrail-operator-crdsloader:master.latest\nbazel run //cmd/crdsloader:contrail-operator-crdsloader-push-local\n\n# it builds and pushes image: {LOCAL_REGISTRY}/contrail-operator/engprod-269421/contrail-operator:master.latest\nbazel run //cmd/manager:contrail-operator-push-local\n\nVerify:\n$ docker images | grep contrail-operator\ncontrail-operator   latest   5c0148fdb7e8   4 seconds ago   125MB\n\nAfter image is created we have to push it into local Docker registry.\ndocker push localhost:5000/contrail-operator:latest\n\nRun Contrail-Operator with sample Contrail configuration\nFollowing command will deploy Contrail-Operator on a working Kubernetes cluster. It will also create a sample Contrail configuration. Note: you can change this configuration by editing test/env/deploy/cluster.yaml file.\ncd test/env\n./apply_contrail_cluster.sh\n\nAs soon as contrail-operator is started, it deploys Contrail services. It is a time-consuming process. You can watch the progress using following command:\nwatch kubectl get pods -n contrail\n\nEventually all pods should be Running:\nNAME                                          READY   STATUS      RESTARTS   AGE\ncassandra1-cassandra-statefulset-0            1/1     Running     0          8m15s\ncommand-command-deployment-77644668cf-dpp6f   1/1     Running     0          7m21s\nconfig1-config-statefulset-0                  9/9     Running     0          4m47s\ncontrail-operator-585f5bd8b5-hfdrz            1/1     Running     0          9m24s\ncontrol1-control-statefulset-0                4/4     Running     0          2m56s\nkeystone-keystone-statefulset-0               3/3     Running     0          7m8s\nmemcached-deployment-5f5f974bd9-gthzx         1/1     Running     0          8m15s\npostgres-pod                                  1/1     Running     0          8m16s\nprovmanager1-provisionmanager-statefulset-0   1/1     Running     0          2m57s\nrabbitmq1-rabbitmq-statefulset-0              1/1     Running     0          8m15s\nswift-proxy-deployment-754f87448b-6l5nc       1/1     Running     0          4m32s\nswift-ring-account-job-rnsxs                  0/1     Completed   0          7s\nswift-ring-container-job-pkb2k                0/1     Completed   0          7s\nswift-ring-object-job-7nn44                   0/1     Completed   0          7s\nswift-storage-statefulset-0                   13/13   Running     0          8m11s\nwebui1-webui-statefulset-0                    3/3     Running     0          2m56s\nzookeeper1-zookeeper-statefulset-0            1/1     Running     0          8m16s\n\nVerify if Contrail Command is working\nYou can access Contrail Command application via web browser. Before that you have to forward the network traffic from localhost to Command's pod.\nkubectl port-forward $(kubectl get pods -l command=command -n contrail -o name) -n contrail 9091:9091\n\nGo to http://localhost:9091\nAuthenticate using admin username, contrail123 password and Default domain.\nRun unit tests\nYou can run unit test tests on your favourite IDE by executing all tests in pkg package.\nYou can also use command line tool:\ngo test ./pkg/...\n\nEventually you should get results and command should return success:\n?       github.com/Juniper/contrail-operator/pkg/apis    [no test files]\n?       github.com/Juniper/contrail-operator/pkg/apis/contrail    [no test files]\n?       github.com/Juniper/contrail-operator/pkg/apis/contrail/v1alpha1    [no test files]\n?       github.com/Juniper/contrail-operator/pkg/apis/contrail/v1alpha1/templates    [no test files]\nok      github.com/Juniper/contrail-operator/pkg/apis/contrail/v1alpha1/tests    0.943s\n?       github.com/Juniper/contrail-operator/pkg/client/keystone    [no test files]\n?       github.com/Juniper/contrail-operator/pkg/client/kubeproxy    [no test files]\n?       github.com/Juniper/contrail-operator/pkg/client/swift    [no test files]\n?       github.com/Juniper/contrail-operator/pkg/controller    [no test files]\nok      github.com/Juniper/contrail-operator/pkg/controller/cassandra    1.414s\nok      github.com/Juniper/contrail-operator/pkg/controller/command    2.316s\n?       github.com/Juniper/contrail-operator/pkg/controller/config    [no test files]\n?       github.com/Juniper/contrail-operator/pkg/controller/control    [no test files]\n?       github.com/Juniper/contrail-operator/pkg/controller/enqueue    [no test files]\nok      github.com/Juniper/contrail-operator/pkg/controller/keystone    4.196s\n?       github.com/Juniper/contrail-operator/pkg/controller/kubemanager    [no test files]\nok      github.com/Juniper/contrail-operator/pkg/controller/manager    1.789s\n?       github.com/Juniper/contrail-operator/pkg/controller/manager/crs    [no test files]\nok      github.com/Juniper/contrail-operator/pkg/controller/memcached    1.097s\nok      github.com/Juniper/contrail-operator/pkg/controller/postgres    1.779s\n?       github.com/Juniper/contrail-operator/pkg/controller/provisionmanager    [no test files]\n?       github.com/Juniper/contrail-operator/pkg/controller/rabbitmq    [no test files]\nok      github.com/Juniper/contrail-operator/pkg/controller/swift    1.002s\nok      github.com/Juniper/contrail-operator/pkg/controller/swiftproxy    0.870s\nok      github.com/Juniper/contrail-operator/pkg/controller/swiftstorage    1.147s\n?       github.com/Juniper/contrail-operator/pkg/controller/utils    [no test files]\n?       github.com/Juniper/contrail-operator/pkg/controller/vrouter    [no test files]\n?       github.com/Juniper/contrail-operator/pkg/controller/webui    [no test files]\n?       github.com/Juniper/contrail-operator/pkg/controller/zookeeper    [no test files]\nok      github.com/Juniper/contrail-operator/pkg/job    0.389s\nok      github.com/Juniper/contrail-operator/pkg/k8s    0.558s\n?       github.com/Juniper/contrail-operator/pkg/randomstring    [no test files]\nok      github.com/Juniper/contrail-operator/pkg/swift/ring    0.416s\n\nRun e2e tests (aka system tests)\nIn order to test if the whole system works as expected we have a few plumbing tests. They verify if after deployment all Contrail services can talk to each other and operate as expected.\nBefore tests can be run you have to have clean the cluster. The fastest way is to delete the cluster:\nkind delete cluster\n\nThen you have to create a new one plus a contrail namespace:\ncd test/env\n./create_testenv.sh\nkubectl create namespace contrail\n\nSystem tests can be run using operator-sdk tool\n# From contrail-operator root directory\n# To run aio e2e test\noperator-sdk test local ./test/e2e/aio/ --namespace contrail --go-test-flags \"-v -timeout=30m\" --up-local\n# To run ha e2e test\noperator-sdk test local ./test/e2e/ha/ --namespace contrail --go-test-flags \"-v -timeout=30m\" --up-local\n\nBefore submitting pull request\nThere is a set of tools that can check your code automatically. This includes static code checks, unit-tests and e2e integration tests. Most of those checks are run for every pull request and vote.\nRun gazelle to make sure that all BUILD.bazel files get updated\nbazel run //:gazelle\n\nBuild and test code. This commands also runs nogo linters\nbazel build //... && bazel test //...\n\nIt will report errors in case:\n\ncode doesn't build\nunit-tests fails\nstatic checks don't pass\ncode is not correctly formatted\n\nNotes\n\nContrail Operator creates Persistent Volumes that are used by some of the deployed pods. After deletion of Contrail resources (e.g. after deleting the Manager Custom Resource), those Persistent Volumes will not be deleted. Administrator has to delete them manually and make sure that directories created by these volumes on cluster nodes are in the expected state. Example Persistent Volumes deletion command:\n\nkubectl delete pv $(kubectl get pv -o=jsonpath='{.items[?(@.spec.storageClassName==\"local-storage\")].metadata.name}')\n\n",
      "description": "Juniper K8s contrail operator. Contribute to Juniper/contrail-operator development by creating an account on GitHub.",
      "ogDescription": "Juniper K8s contrail operator. Contribute to Juniper/contrail-operator development by creating an account on GitHub."
    },
    {
      "url": "https://github.com/kubealex/libvirt-k8s-provisioner",
      "title": "kubealex/libvirt-k8s-provisioner",
      "content": "<div><div><article class=\"markdown-body entry-content container-lg\"><p><a href=\"https://opensource.org/licenses/MIT\"><img src=\"https://camo.githubusercontent.com/78f47a09877ba9d28da1887a93e5c3bc2efb309c1e910eb21135becd2998238a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667\" alt=\"License: MIT\"></a></p>\n\n<p>Welcome to the home of the project!</p>\n<p>With this project, you can build up in minutes a fully working k8s cluster (single master/HA) with as many worker nodes as you want.</p>\n<p>Terraform will take care of the provisioning of:</p>\n<ul>\n<li>Loadbalancer machine with <strong>haproxy</strong> installed and configured for <strong>HA</strong> clusters</li>\n<li>k8s Master(s) VM(s)</li>\n<li>k8s Worker(s) VM(s)</li>\n</ul>\n<p>It also takes care of preparing the host machine with needed packages, configuring:</p>\n\n<p>You can customize the setup choosing:</p>\n<ul>\n<li><strong>container runtime</strong> that you want to use (docker, cri-o, containerd).</li>\n<li><strong>schedulable master</strong> if you want to schedule on your master nodes or leave the taint.</li>\n<li><strong>service CIDR</strong> to be used during installation.</li>\n<li><strong>pod CIDR</strong> to be used during installation.</li>\n<li><strong>network plugin</strong> to be used, based on the documentation. <strong><a href=\"https://www.projectcalico.org/calico-networking-for-kubernetes/\">Project Calico</a></strong> <strong><a href=\"https://github.com/coreos/flannel\">Flannel</a></strong></li>\n<li><strong><a href=\"https://kubernetes.github.io/ingress-nginx/\">nginx-ingress-controller</a></strong> or <strong><a href=\"https://github.com/haproxytech/kubernetes-ingress\">haproxy-ingress-controller</a></strong> if you want to enable ingress management.</li>\n<li><strong><a href=\"https://rancher.com/\">Rancher</a></strong> installation to manage your cluster.</li>\n<li><strong><a href=\"https://rook.io/docs/rook/v1.4/ceph-storage.html\">Rook-Ceph</a></strong> - <strong>WIP</strong></li>\n</ul>\n<h2><a id=\"user-content-all-vms-are-specularprepared-with\" class=\"anchor\" href=\"https://github.com/kubealex/libvirt-k8s-provisioner#all-vms-are-specularprepared-with\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>All VMs are specular,prepared with:</h2>\n\n<p>The user is capable of logging via SSH too.</p>\n<h2><a id=\"user-content-quickstart\" class=\"anchor\" href=\"https://github.com/kubealex/libvirt-k8s-provisioner#quickstart\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Quickstart</h2>\n<p>The playbook is meant to be ran against a/many local or remote host/s, defined under <strong>vm_host</strong> group, depending on how many clusters you want to configure at once.</p>\n<pre><code>ansible-playbook main.yml\n</code></pre>\n<p>You can quickly make it work by configuring the needed vars, but you can go straight with the defaults!</p>\n<p>Recommended sizings are:</p>\n<table>\n<thead>\n<tr>\n<th>Role</th>\n<th>vCPU</th>\n<th>RAM</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>master</td>\n<td>2</td>\n<td>2G</td>\n</tr>\n<tr>\n<td>worker</td>\n<td>1</td>\n<td>1G</td>\n</tr>\n</tbody>\n</table>\n<p><strong>vars/k8s_cluster.yml</strong></p>\n<pre><code># General configuration\nk8s:\n  container_runtime: crio\n  master_schedulable: false\n\n# Nodes configuration\n\n  control_plane:\n    vcpu: 1\n        mem: 2\n    vms: 3\n    disk: 30\n\n  worker_nodes:\n    vcpu: 1\n    mem: 2\n    vms: 3\n    disk: 30\n\n# Network configuration\n\n  network:\n    pod_cidr: 10.200.0.0/16\n    service_cidr: 10.50.0.0/16\n    cni_plugin: calico\n\n# Rook configuration\n\nrook_ceph:\n  install_rook: false\n  volume_size: 50\n\n# Ingress controller configuration [nginx/haproxy]\n\ningress_controller:\n  install_ingress_controller: true\n  type: nginx\n\nrancher:\n  install_rancher: false\n</code></pre>\n<p>Size for <strong>disk</strong> and <strong>mem</strong> is in GB.\n<strong>disk</strong> allows to provision space in the cloud image for pod&apos;s ephemeral storage.</p>\n<p>VMS are created with these names by default (customizing them is work in progress):</p>\n<pre><code>- k8s-loadbalancer.**domain**\n- k8s-master-N.**domain**\n- k8s-worker-N.**domain**\n</code></pre>\n<p>These are the default for libvirt resources:</p>\n<p><strong>vars/libvirt.yml</strong></p>\n<pre><code>libvirt:\n  network:\n    domain: k8s.lab\n    name: k8s\n    net: 192.168.200.0/24\n  storage:\n    pool_name: k8s\n    pool_path: /var/lib/libvirt/images/k8s\n</code></pre>\n<p>Feel free to suggest modifications/improvements.</p>\n<p>Alex</p>\n</article></div></div><hr><h4>Page 2</h4><body class=\"logged-out env-production page-responsive page-blob\"> <include-fragment class=\"js-notification-shelf-include-fragment\"></include-fragment> <div class=\"application-main \"> </div> <p id=\"ajax-error-message\" class=\"ajax-error-message\"> <svg class=\"octicon octicon-alert\" width=\"16\" height=\"16\"><path/></svg> You can&#x2019;t perform that action at this time. </p> <p class=\"js-stale-session-flash\"> <svg class=\"octicon octicon-alert\" width=\"16\" height=\"16\"><path/></svg> <span class=\"js-stale-session-flash-signed-in\">You signed in with another tab or window. <a href>Reload</a> to refresh your session.</span> <span class=\"js-stale-session-flash-signed-out\">You signed out in another tab or window. <a href>Reload</a> to refresh your session.</span> </p> <template id=\"site-details-dialog\"> <details class=\"details-reset details-overlay details-overlay-dark lh-default text-gray-dark hx_rsm\"> <summary></summary> <details-dialog class=\"Box Box--overlay d-flex flex-column anim-fade-in fast hx_rsm-dialog hx_rsm-modal\"> </details-dialog> </details>\n</template> <div class=\"js-cookie-consent-banner\"> <div class=\"hx_cookie-banner p-2 p-sm-3 p-md-4\"> <div class=\"Box hx_cookie-banner-box box-shadow-medium mx-auto\"> <div class=\"Box-body border-0 py-0 px-3 px-md-4\"> <div class=\"js-main-cookie-banner hx_cookie-banner-main\"> <div class=\"d-md-flex flex-items-center py-3\"> <p class=\"f5 flex-1 mb-3 mb-md-0\"> We use <span class=\"text-bold\">optional</span> third-party analytics cookies to understand how you use GitHub.com so we can build better products. <span class=\"btn-link js-cookie-consent-learn-more\">Learn more</span>. </p> </div> </div> <div class=\"js-cookie-details hx_cookie-banner-details\"> <div class=\"d-md-flex flex-items-center py-3\"> <p class=\"f5 flex-1 mb-2 mb-md-0\"> We use <span class=\"text-bold\">optional</span> third-party analytics cookies to understand how you use GitHub.com so we can build better products. <br> You can always update your selection by clicking <span class=\"text-bold\">Cookie Preferences</span> at the bottom of the page. For more information, see our <a href=\"https://docs.github.com/en/free-pro-team@latest/github/site-policy/github-privacy-statement\">Privacy Statement</a>. </p> </div> <div class=\"d-md-flex flex-items-center py-3 border-top\"> <div class=\"f5 flex-1 mb-2 mb-md-0\"> <p class=\"f6 mb-md-0\">We use essential cookies to perform essential website functions, e.g. they&apos;re used to log you in. <a href=\"https://docs.github.com/en/github/site-policy/github-subprocessors-and-cookies\">Learn more</a> </p> </div> <h5 class=\"text-blue\">Always active</h5> </div> <div class=\"d-md-flex flex-items-center py-3 border-top\"> <div class=\"f5 flex-1 mb-2 mb-md-0\"> <p class=\"f6 mb-md-0\">We use analytics cookies to understand how you use our websites so we can make them better, e.g. they&apos;re used to gather information about the pages you visit and how many clicks you need to accomplish a task. <a href=\"https://docs.github.com/en/github/site-policy/github-subprocessors-and-cookies\">Learn more</a> </p> </div> </div> </div>\n</div></div> </div>\n</div> </body>",
      "contentAsText": "\n\nWelcome to the home of the project!\nWith this project, you can build up in minutes a fully working k8s cluster (single master/HA) with as many worker nodes as you want.\nTerraform will take care of the provisioning of:\n\nLoadbalancer machine with haproxy installed and configured for HA clusters\nk8s Master(s) VM(s)\nk8s Worker(s) VM(s)\n\nIt also takes care of preparing the host machine with needed packages, configuring:\n\nYou can customize the setup choosing:\n\ncontainer runtime that you want to use (docker, cri-o, containerd).\nschedulable master if you want to schedule on your master nodes or leave the taint.\nservice CIDR to be used during installation.\npod CIDR to be used during installation.\nnetwork plugin to be used, based on the documentation. Project Calico Flannel\nnginx-ingress-controller or haproxy-ingress-controller if you want to enable ingress management.\nRancher installation to manage your cluster.\nRook-Ceph - WIP\n\nAll VMs are specular,prepared with:\n\nThe user is capable of logging via SSH too.\nQuickstart\nThe playbook is meant to be ran against a/many local or remote host/s, defined under vm_host group, depending on how many clusters you want to configure at once.\nansible-playbook main.yml\n\nYou can quickly make it work by configuring the needed vars, but you can go straight with the defaults!\nRecommended sizings are:\n\n\n\nRole\nvCPU\nRAM\n\n\n\n\nmaster\n2\n2G\n\n\nworker\n1\n1G\n\n\n\nvars/k8s_cluster.yml\n# General configuration\nk8s:\n  container_runtime: crio\n  master_schedulable: false\n\n# Nodes configuration\n\n  control_plane:\n    vcpu: 1\n        mem: 2\n    vms: 3\n    disk: 30\n\n  worker_nodes:\n    vcpu: 1\n    mem: 2\n    vms: 3\n    disk: 30\n\n# Network configuration\n\n  network:\n    pod_cidr: 10.200.0.0/16\n    service_cidr: 10.50.0.0/16\n    cni_plugin: calico\n\n# Rook configuration\n\nrook_ceph:\n  install_rook: false\n  volume_size: 50\n\n# Ingress controller configuration [nginx/haproxy]\n\ningress_controller:\n  install_ingress_controller: true\n  type: nginx\n\nrancher:\n  install_rancher: false\n\nSize for disk and mem is in GB.\ndisk allows to provision space in the cloud image for pod's ephemeral storage.\nVMS are created with these names by default (customizing them is work in progress):\n- k8s-loadbalancer.**domain**\n- k8s-master-N.**domain**\n- k8s-worker-N.**domain**\n\nThese are the default for libvirt resources:\nvars/libvirt.yml\nlibvirt:\n  network:\n    domain: k8s.lab\n    name: k8s\n    net: 192.168.200.0/24\n  storage:\n    pool_name: k8s\n    pool_path: /var/lib/libvirt/images/k8s\n\nFeel free to suggest modifications/improvements.\nAlex\nPage 2      You can’t perform that action at this time.    You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session.          We use optional third-party analytics cookies to understand how you use GitHub.com so we can build better products. Learn more.       We use optional third-party analytics cookies to understand how you use GitHub.com so we can build better products.  You can always update your selection by clicking Cookie Preferences at the bottom of the page. For more information, see our Privacy Statement.     We use essential cookies to perform essential website functions, e.g. they're used to log you in. Learn more   Always active    We use analytics cookies to understand how you use our websites so we can make them better, e.g. they're used to gather information about the pages you visit and how many clicks you need to accomplish a task. Learn more    \n \n ",
      "description": "Automate your k8s installation. Contribute to kubealex/libvirt-k8s-provisioner development by creating an account on GitHub.",
      "ogDescription": "Automate your k8s installation. Contribute to kubealex/libvirt-k8s-provisioner development by creating an account on GitHub."
    },
    {
      "url": "https://www.reddit.com/r/kubernetes/comments/ivcaj3/verifying_your_kubernetes_setup_in_your_end_to/",
      "title": "r/kubernetes - Verifying your Kubernetes setup in your end to end tests",
      "content": "<div><div><p class=\"_1qeIAgB0cPwnLhDF9XSiJM\">I&apos;m a test automation engineer who learned to love k8s.<br>I usually run my test automation right after a new deployment. Often I had to wait for everything to be ready, which made me rely in some cases on static waits.<br>Since we run our applications on Kubernetes, I was able to add verification tests for all Kubernetes objects being deployed/changed.<br>I&apos;m using RobotFramework and KubeLibrary to do this: <a href=\"https://humanitec.com/blog/kubelibrary-testing-kubernetes-with-robotframework\" class=\"_3t5uN8xUmg0TOwRCOGQEcU\">https://humanitec.com/blog/kubelibrary-testing-kubernetes-with-robotframework</a></p></div></div>",
      "contentAsText": "I'm a test automation engineer who learned to love k8s.I usually run my test automation right after a new deployment. Often I had to wait for everything to be ready, which made me rely in some cases on static waits.Since we run our applications on Kubernetes, I was able to add verification tests for all Kubernetes objects being deployed/changed.I'm using RobotFramework and KubeLibrary to do this: https://humanitec.com/blog/kubelibrary-testing-kubernetes-with-robotframework",
      "description": "52.4k members in the kubernetes community. Kubernetes discussion, news, support, and link sharing.",
      "ogDescription": "1 vote and 0 comments so far on Reddit"
    },
    {
      "url": "https://humanitec.com/blog/kubelibrary-testing-kubernetes-with-robotframework",
      "title": "KubeLibrary: Testing Kubernetes with RobotFramework | Humanitec",
      "content": "<div href class=\"article__content w-richtext\"><p>As <a href=\"https://github.com/devopsspiral/KubeLibrary\">KubeLibrary</a> is based on the <a href=\"https://github.com/kubernetes-client/python/blob/master/kubernetes\">official python kubernetes client</a> you can connect to your Kubernetes cluster while executing any Kubernetes API command.</p><p>Being part of the broader RobotFramework Library, all code is wrapped into keywords that can be used in test cases defined in ATDD (Acceptance Test Driven Development) or in BDD (Behavioral Driven Development) syntax.</p><h2>Why use the KubeLibrary?</h2><p>When running end to end tests, you need to ensure that your system-under-test is fully available and runs the latest deployed version. With the KubeLibrary you can easily build tests to verify that all objects in your Kubernetes cluster are up and running and in the state that you expect them. This ensures that a successful end to end test run can be performed.<br></p><h2>How to use the KubeLibrary</h2><p>There are <a href=\"https://github.com/devopsspiral/KubeLibrary/tree/master/testcases\">many different examples available within the GitHub repository of the KubeLibrary</a>. But let\u0019s show you a small example here: Let\u0019s say you want to make sure that all pods in a certain namespace are running and use a specific image version.<p>&lt;p&gt; CODE: https://gist.github.com/Nilsty/83ad625eb76a3fe5f9891919bb4f169a.js &lt;p/&gt;</p></p><p><br>If you run this test in your Kubernetes cluster, it would check for pods matching the name pattern in <em>${POD_NAME_PATTERN}</em> in a namespace <em>${NAMESPACE}</em>. It will search for running pods for 2 minutes. Once the pods are confirmed running it will confirm the image they are using.<br></p><p>After you have confirmed that your pods are running you can continue with any application-level testing.<br></p><p>The KubeLibrary can be also used for checking nodes, jobs, configmaps, persistent volume claims, services, and other Kubernetes objects. Current keyword documentation is available on <a href=\"https://github.com/devopsspiral/KubeLibrary/blob/master/docs/KubeLibrary.html\">GitHub</a>.<br></p><h2>Contributing to the KubeLibrary</h2><p>Almost every week new keywords are added to the library, but the Kubernetes API is big and not all of its functionality is covered in the KubeLibrary, yet. If you are missing certain functionality please file an issue in the GitHub project or create a pull request. Adding new functionality can be quite easy as the <a href=\"https://github.com/kubernetes-client/python/blob/master/kubernetes/README.md\">python client for kubernetes</a> is well documented and often only a wrapper for the existing definitions is required.<br></p><h2>How do we use the KubeLibrary at Humanitec?</h2><p>At Humanitec our end to end test automation is organized in Robot Framework test suites. We utilize several libraries to gain a holistic view through our testing. For API testing we use the <a href=\"https://github.com/MarketSquare/robotframework-requests\">Requests Library</a>, for UI testing we use the <a href=\"https://robotframework.org/SeleniumLibrary/SeleniumLibrary.html\">Selenium Library</a>.Humanitec enables the user to develop, maintain, and deploy applications to Kubernetes clusters. Thus, we also utilize the KubeLibrary to verify the expected state of Kubernetes objects created and updated by Humanitec. For example, if you choose to update environment variables for your application through Humanitec, this will trigger an update to a configmap object in your applications cluster. In our tests, we trigger this update with an API call to the Humanitec API, then verify with the KubeLibrary if the update was done as expected in the cluster. This way, we can build easily automated regression tests for all the features provided by Humanitec.</p><figure class=\"w-richtext-align-center w-richtext-figure-type-image\"><a href=\"https://humanitec.com/webinars/test-automation-in-continuous-deployment\" class=\"w-inline-block\"><div><img src=\"https://assets.website-files.com/5c73bbfe3312822f153dd310/5f8db155db915777946a8a25_Webinar%20KubeLibrary%20(3).png\" alt></div></a><figcaption><a href=\"https://humanitec.com/webinars/test-automation-in-continuous-deployment\">Join the webinar</a></figcaption></figure></div>",
      "contentAsText": "As KubeLibrary is based on the official python kubernetes client you can connect to your Kubernetes cluster while executing any Kubernetes API command.Being part of the broader RobotFramework Library, all code is wrapped into keywords that can be used in test cases defined in ATDD (Acceptance Test Driven Development) or in BDD (Behavioral Driven Development) syntax.Why use the KubeLibrary?When running end to end tests, you need to ensure that your system-under-test is fully available and runs the latest deployed version. With the KubeLibrary you can easily build tests to verify that all objects in your Kubernetes cluster are up and running and in the state that you expect them. This ensures that a successful end to end test run can be performed.How to use the KubeLibraryThere are many different examples available within the GitHub repository of the KubeLibrary. But let\u0019s show you a small example here: Let\u0019s say you want to make sure that all pods in a certain namespace are running and use a specific image version.<p> CODE: https://gist.github.com/Nilsty/83ad625eb76a3fe5f9891919bb4f169a.js <p/>If you run this test in your Kubernetes cluster, it would check for pods matching the name pattern in ${POD_NAME_PATTERN} in a namespace ${NAMESPACE}. It will search for running pods for 2 minutes. Once the pods are confirmed running it will confirm the image they are using.After you have confirmed that your pods are running you can continue with any application-level testing.The KubeLibrary can be also used for checking nodes, jobs, configmaps, persistent volume claims, services, and other Kubernetes objects. Current keyword documentation is available on GitHub.Contributing to the KubeLibraryAlmost every week new keywords are added to the library, but the Kubernetes API is big and not all of its functionality is covered in the KubeLibrary, yet. If you are missing certain functionality please file an issue in the GitHub project or create a pull request. Adding new functionality can be quite easy as the python client for kubernetes is well documented and often only a wrapper for the existing definitions is required.How do we use the KubeLibrary at Humanitec?At Humanitec our end to end test automation is organized in Robot Framework test suites. We utilize several libraries to gain a holistic view through our testing. For API testing we use the Requests Library, for UI testing we use the Selenium Library.Humanitec enables the user to develop, maintain, and deploy applications to Kubernetes clusters. Thus, we also utilize the KubeLibrary to verify the expected state of Kubernetes objects created and updated by Humanitec. For example, if you choose to update environment variables for your application through Humanitec, this will trigger an update to a configmap object in your applications cluster. In our tests, we trigger this update with an API call to the Humanitec API, then verify with the KubeLibrary if the update was done as expected in the cluster. This way, we can build easily automated regression tests for all the features provided by Humanitec.Join the webinar",
      "description": "This article explores the RobotFramework library KubeLibrary. KubeLibrary is a wrapper for the Python Kubernetes Client. It enables you to assert the status of various objects in your Kubernetes Clusters. As the library can be integrated with any RobotFramework test suite, it is ideal to verify the testability of your System-under-Test by asserting the status of your nodes, deployments, pods, configmaps, and others Kubernetes objects before running any end to end tests.",
      "ogDescription": "This article explores the RobotFramework library KubeLibrary. KubeLibrary is a wrapper for the Python Kubernetes Client. It enables you to assert the status of various objects in your Kubernetes Clusters. As the library can be integrated with any RobotFramework test suite, it is ideal to verify the testability of your System-under-Test by asserting the status of your nodes, deployments, pods, configmaps, and others Kubernetes objects before running any end to end tests."
    },
    {
      "url": "https://github.com/tonsV2/kube-notes",
      "title": "tonsV2/kube-notes",
      "content": "<div><div><article class=\"markdown-body entry-content container-lg\">\n\n<div class=\"highlight highlight-source-shell\"><pre>curl -sfL https://get.k3s.io <span class=\"pl-k\">|</span> sh -</pre></div>\n\n<div class=\"highlight highlight-source-shell\"><pre>sudo cp /etc/rancher/k3s/k3s.yaml <span class=\"pl-k\">~</span>/.kube/\nsudo chown tons.tons <span class=\"pl-k\">~</span>/.kube/k3s.yaml\n<span class=\"pl-k\">export</span> KUBECONFIG=<span class=\"pl-smi\">$HOME</span>/.kube/k3s.yaml</pre></div>\n\n\n\n<p>See <a href=\"https://helm.sh/docs/intro/quickstart/\">https://helm.sh/docs/intro/quickstart/</a></p>\n<h2><a id=\"user-content-ssl---lets-encrypt-optional\" class=\"anchor\" href=\"https://github.com/tonsV2/kube-notes#ssl---lets-encrypt-optional\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>SSL - Lets Encrypt (Optional)</h2>\n\n<div class=\"highlight highlight-source-shell\"><pre>kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.0.1/cert-manager.crds.yaml\nhelm repo add jetstack https://charts.jetstack.io\nkubectl create namespace cert-manager\nhelm install cert-manager jetstack/cert-manager --version 1.0.1 --namespace cert-manager\n\n<span class=\"pl-c1\">echo</span> <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>apiVersion: cert-manager.io/v1beta1</span>\n<span class=\"pl-s\">kind: ClusterIssuer</span>\n<span class=\"pl-s\">metadata:</span>\n<span class=\"pl-s\">  name: letsencrypt-prod</span>\n<span class=\"pl-s\">spec:</span>\n<span class=\"pl-s\">  acme:</span>\n<span class=\"pl-s\">    email: sebastianthegreatful@gmail.com</span>\n<span class=\"pl-s\">    privateKeySecretRef:</span>\n<span class=\"pl-s\">      name: prod-issuer-account-key</span>\n<span class=\"pl-s\">    server: https://acme-v02.api.letsencrypt.org/directory</span>\n<span class=\"pl-s\">    solvers:</span>\n<span class=\"pl-s\">      - http01:</span>\n<span class=\"pl-s\">          ingress:</span>\n<span class=\"pl-s\">            class: traefik</span>\n<span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span></span> <span class=\"pl-k\">|</span> kubectl apply --validate=false -f -\n\n<span class=\"pl-c1\">echo</span> <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>apiVersion: cert-manager.io/v1beta1</span>\n<span class=\"pl-s\">kind: ClusterIssuer</span>\n<span class=\"pl-s\">metadata:</span>\n<span class=\"pl-s\">  name: letsencrypt-staging</span>\n<span class=\"pl-s\">spec:</span>\n<span class=\"pl-s\">  acme:</span>\n<span class=\"pl-s\">    email: sebastianthegreatful@gmail.com</span>\n<span class=\"pl-s\">    privateKeySecretRef:</span>\n<span class=\"pl-s\">      name: staging-issuer-account-key</span>\n<span class=\"pl-s\">    server: https://acme-staging-v02.api.letsencrypt.org/directory</span>\n<span class=\"pl-s\">    solvers:</span>\n<span class=\"pl-s\">      - http01:</span>\n<span class=\"pl-s\">          ingress:</span>\n<span class=\"pl-s\">            class: traefik</span>\n<span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span></span> <span class=\"pl-k\">|</span> kubectl apply --validate=false -f -</pre></div>\n\n<p>There are quite a few options when it comes to Kubernetes dashboards. I recommend using either the official one or <a href=\"https://k8slens.dev/\">Lens</a>.</p>\n<ul>\n<li><a href=\"https://rancher.com/docs/k3s/latest/en/installation/kube-dashboard/\">https://rancher.com/docs/k3s/latest/en/installation/kube-dashboard/</a></li>\n</ul>\n<h2><a id=\"user-content-install-the-dashboard\" class=\"anchor\" href=\"https://github.com/tonsV2/kube-notes#install-the-dashboard\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Install the Dashboard</h2>\n<div class=\"highlight highlight-source-shell\"><pre>GITHUB_URL=https://github.com/kubernetes/dashboard/releases\nVERSION_KUBE_DASHBOARD=<span class=\"pl-s\"><span class=\"pl-pds\">$(</span>curl -w <span class=\"pl-s\"><span class=\"pl-pds\">&apos;</span>%{url_effective}<span class=\"pl-pds\">&apos;</span></span> -I -L -s -S <span class=\"pl-smi\">${GITHUB_URL}</span>/latest -o /dev/null <span class=\"pl-k\">|</span> sed -e <span class=\"pl-s\"><span class=\"pl-pds\">&apos;</span>s|.*/||<span class=\"pl-pds\">&apos;</span></span><span class=\"pl-pds\">)</span></span>\nkubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/<span class=\"pl-smi\">${VERSION_KUBE_DASHBOARD}</span>/aio/deploy/recommended.yaml</pre></div>\n<h2><a id=\"user-content-create-service-account\" class=\"anchor\" href=\"https://github.com/tonsV2/kube-notes#create-service-account\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Create Service Account</h2>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c1\">echo</span> <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>apiVersion: v1</span>\n<span class=\"pl-s\">kind: ServiceAccount</span>\n<span class=\"pl-s\">metadata:</span>\n<span class=\"pl-s\">  name: admin-user</span>\n<span class=\"pl-s\">  namespace: kubernetes-dashboard</span>\n<span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span></span> <span class=\"pl-k\">|</span> kubectl create -f -</pre></div>\n<h2><a id=\"user-content-create-cluster-role-binding\" class=\"anchor\" href=\"https://github.com/tonsV2/kube-notes#create-cluster-role-binding\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Create Cluster Role Binding</h2>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c1\">echo</span> <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>apiVersion: rbac.authorization.k8s.io/v1</span>\n<span class=\"pl-s\">kind: ClusterRoleBinding</span>\n<span class=\"pl-s\">metadata:</span>\n<span class=\"pl-s\">  name: admin-user</span>\n<span class=\"pl-s\">roleRef:</span>\n<span class=\"pl-s\">  apiGroup: rbac.authorization.k8s.io</span>\n<span class=\"pl-s\">  kind: ClusterRole</span>\n<span class=\"pl-s\">  name: cluster-admin</span>\n<span class=\"pl-s\">subjects:</span>\n<span class=\"pl-s\">- kind: ServiceAccount</span>\n<span class=\"pl-s\">  name: admin-user</span>\n<span class=\"pl-s\">  namespace: kubernetes-dashboard</span>\n<span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span></span> <span class=\"pl-k\">|</span> kubectl create -f -</pre></div>\n<h2><a id=\"user-content-get-access-token\" class=\"anchor\" href=\"https://github.com/tonsV2/kube-notes#get-access-token\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Get Access Token</h2>\n<div class=\"highlight highlight-source-shell\"><pre>kubectl -n kubernetes-dashboard describe secret admin-user-token <span class=\"pl-k\">|</span> grep ^token</pre></div>\n<h2><a id=\"user-content-forward-port-to-localhost\" class=\"anchor\" href=\"https://github.com/tonsV2/kube-notes#forward-port-to-localhost\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Forward port to localhost</h2>\n\n<h2><a id=\"user-content-browse-dashboard\" class=\"anchor\" href=\"https://github.com/tonsV2/kube-notes#browse-dashboard\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Browse Dashboard</h2>\n<p>Open the below url in your favorite browser</p>\n<p><code>http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/logi</code></p>\n<h2><a id=\"user-content-helmfile---docker-compose-for-kubernetes-optional\" class=\"anchor\" href=\"https://github.com/tonsV2/kube-notes#helmfile---docker-compose-for-kubernetes-optional\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Helmfile - docker-compose for Kubernetes (Optional)</h2>\n<p><a href=\"https://github.com/roboll/helmfile#installation\">https://github.com/roboll/helmfile#installation</a></p>\n<h2><a id=\"user-content-chartmuseum---helm-chart-repository-optional\" class=\"anchor\" href=\"https://github.com/tonsV2/kube-notes#chartmuseum---helm-chart-repository-optional\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Chartmuseum - Helm Chart Repository (Optional)</h2>\n<p>Export <code>HARTMUSEUM_AUTH_USER</code> and <code>HARTMUSEUM_AUTH_PASS</code> with your desired username and password</p>\n<p>Use your favorite editor to save the below snippet in a file called helmfile.yaml</p>\n<div class=\"highlight highlight-source-yaml\"><pre><span class=\"pl-ent\">releases</span>:\n  - <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">chartmuseum</span>\n    <span class=\"pl-ent\">namespace</span>: <span class=\"pl-s\">chartmuseum</span>\n    <span class=\"pl-ent\">chart</span>: <span class=\"pl-s\">stable/chartmuseum</span>\n    <span class=\"pl-ent\">version</span>: <span class=\"pl-s\">2.13.3</span>\n    <span class=\"pl-ent\">values</span>:\n      - <span class=\"pl-ent\">ingress</span>:\n          <span class=\"pl-ent\">enabled</span>: <span class=\"pl-c1\">true</span>\n          <span class=\"pl-ent\">annotations</span>:\n            <span class=\"pl-ent\">kubernetes.io/ingress.class</span>: <span class=\"pl-s\">traefik</span>\n            <span class=\"pl-ent\">cert-manager.io/cluster-issuer</span>: <span class=\"pl-s\">letsencrypt-prod</span>\n          <span class=\"pl-ent\">hosts</span>:\n            - <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">helm-charts.your-domain.com</span>\n              <span class=\"pl-ent\">path</span>: <span class=\"pl-s\">/</span>\n              <span class=\"pl-ent\">tls</span>: <span class=\"pl-c1\">true</span>\n              <span class=\"pl-ent\">tlsSecret</span>: <span class=\"pl-s\">chartmuseum-tls</span>\n      - <span class=\"pl-ent\">env</span>:\n          <span class=\"pl-ent\">open</span>:\n            <span class=\"pl-ent\">DISABLE_API</span>: <span class=\"pl-c1\">false</span>\n            <span class=\"pl-ent\">STORAGE</span>: <span class=\"pl-s\">local</span>\n          <span class=\"pl-ent\">secret</span>:\n            <span class=\"pl-ent\">BASIC_AUTH_USER</span>: <span class=\"pl-s\">{{ requiredEnv &quot;CHARTMUSEUM_AUTH_USER&quot; }}</span>\n            <span class=\"pl-ent\">BASIC_AUTH_PASS</span>: <span class=\"pl-s\">{{ requiredEnv &quot;CHARTMUSEUM_AUTH_PASS&quot; }}</span>\n      - <span class=\"pl-ent\">persistence</span>:\n          <span class=\"pl-ent\">enabled</span>: <span class=\"pl-c1\">true</span>\n          <span class=\"pl-ent\">accessMode</span>: <span class=\"pl-s\">ReadWriteOnce</span>\n          <span class=\"pl-ent\">size</span>: <span class=\"pl-c1\">100M</span></pre></div>\n<h2><a id=\"user-content-install-chartmuseum\" class=\"anchor\" href=\"https://github.com/tonsV2/kube-notes#install-chartmuseum\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Install Chartmuseum</h2>\n\n<h2><a id=\"user-content-install-chartmuseum-helm-plugin\" class=\"anchor\" href=\"https://github.com/tonsV2/kube-notes#install-chartmuseum-helm-plugin\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Install Chartmuseum helm plugin</h2>\n<div class=\"highlight highlight-source-shell\"><pre>helm plugin install https://github.com/chartmuseum/helm-push.git</pre></div>\n<h2><a id=\"user-content-install-whoami---a-simple-application-which-will-return-the-hostname-of-the-pods-its-running-on\" class=\"anchor\" href=\"https://github.com/tonsV2/kube-notes#install-whoami---a-simple-application-which-will-return-the-hostname-of-the-pods-its-running-on\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Install WhoAmI - A simple application which will return the hostname of the pods it&apos;s running on</h2>\n<p>This app can serve as a simple example of the relation between a <code>deployment</code>, <code>service</code> and <code>ingress</code> resource. For a more detailed explanation please see the below url.</p>\n<p><code>https://medium.com/@dwdraju/how-deployment-service-ingress-are-related-in-their-manifest-a2e553cf0ffb</code></p>\n<p>Use git to clone or fork the below reository</p>\n<p><code>https://github.com/tonsV2/whoami-mn</code></p>\n<p>Replace the <code>kubeContext</code>with the name of your context. Possible <code>default</code></p>\n<p><a href=\"https://github.com/tonsV2/whoami-mn/blob/master/helmfile.yaml#L4\">https://github.com/tonsV2/whoami-mn/blob/master/helmfile.yaml#L4</a></p>\n<p>Replace the repository with the url of your Chartmuseum</p>\n<p><code>https://github.com/tonsV2/whoami-mn/blob/master/helmfile.yaml#L36</code></p>\n<h2><a id=\"user-content-push-the-chart-to-the-repository\" class=\"anchor\" href=\"https://github.com/tonsV2/kube-notes#push-the-chart-to-the-repository\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Push the chart to the repository</h2>\n<div class=\"highlight highlight-source-shell\"><pre>helm push src/main/helm tons --username <span class=\"pl-smi\">$CHARTMUSEUM_AUTH_USER</span> --password <span class=\"pl-smi\">$CHARTMUSEUM_AUTH_PASS</span></pre></div>\n<h2><a id=\"user-content-deploy-using-helmfile\" class=\"anchor\" href=\"https://github.com/tonsV2/kube-notes#deploy-using-helmfile\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Deploy using helmfile</h2>\n<p>See <a href=\"https://github.com/tonsV2/whoami-mn#deploy\">https://github.com/tonsV2/whoami-mn#deploy</a></p>\n\n</article></div></div><hr><h4>Page 2</h4><div><div><article class=\"markdown-body entry-content container-lg\">\n\n<div class=\"highlight highlight-source-shell\"><pre>curl -sfL https://get.k3s.io <span class=\"pl-k\">|</span> sh -</pre></div>\n\n<div class=\"highlight highlight-source-shell\"><pre>sudo cp /etc/rancher/k3s/k3s.yaml <span class=\"pl-k\">~</span>/.kube/\nsudo chown tons.tons <span class=\"pl-k\">~</span>/.kube/k3s.yaml\n<span class=\"pl-k\">export</span> KUBECONFIG=<span class=\"pl-smi\">$HOME</span>/.kube/k3s.yaml</pre></div>\n\n\n\n<p>See <a href=\"https://helm.sh/docs/intro/quickstart/\">https://helm.sh/docs/intro/quickstart/</a></p>\n<h2><a id=\"user-content-ssl---lets-encrypt-optional\" class=\"anchor\" href=\"#ssl---lets-encrypt-optional\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>SSL - Lets Encrypt (Optional)</h2>\n\n<div class=\"highlight highlight-source-shell\"><pre>kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.0.1/cert-manager.crds.yaml\nhelm repo add jetstack https://charts.jetstack.io\nkubectl create namespace cert-manager\nhelm install cert-manager jetstack/cert-manager --version 1.0.1 --namespace cert-manager\n\n<span class=\"pl-c1\">echo</span> <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>apiVersion: cert-manager.io/v1beta1</span>\n<span class=\"pl-s\">kind: ClusterIssuer</span>\n<span class=\"pl-s\">metadata:</span>\n<span class=\"pl-s\">  name: letsencrypt-prod</span>\n<span class=\"pl-s\">spec:</span>\n<span class=\"pl-s\">  acme:</span>\n<span class=\"pl-s\">    email: sebastianthegreatful@gmail.com</span>\n<span class=\"pl-s\">    privateKeySecretRef:</span>\n<span class=\"pl-s\">      name: prod-issuer-account-key</span>\n<span class=\"pl-s\">    server: https://acme-v02.api.letsencrypt.org/directory</span>\n<span class=\"pl-s\">    solvers:</span>\n<span class=\"pl-s\">      - http01:</span>\n<span class=\"pl-s\">          ingress:</span>\n<span class=\"pl-s\">            class: traefik</span>\n<span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span></span> <span class=\"pl-k\">|</span> kubectl apply --validate=false -f -\n\n<span class=\"pl-c1\">echo</span> <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>apiVersion: cert-manager.io/v1beta1</span>\n<span class=\"pl-s\">kind: ClusterIssuer</span>\n<span class=\"pl-s\">metadata:</span>\n<span class=\"pl-s\">  name: letsencrypt-staging</span>\n<span class=\"pl-s\">spec:</span>\n<span class=\"pl-s\">  acme:</span>\n<span class=\"pl-s\">    email: sebastianthegreatful@gmail.com</span>\n<span class=\"pl-s\">    privateKeySecretRef:</span>\n<span class=\"pl-s\">      name: staging-issuer-account-key</span>\n<span class=\"pl-s\">    server: https://acme-staging-v02.api.letsencrypt.org/directory</span>\n<span class=\"pl-s\">    solvers:</span>\n<span class=\"pl-s\">      - http01:</span>\n<span class=\"pl-s\">          ingress:</span>\n<span class=\"pl-s\">            class: traefik</span>\n<span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span></span> <span class=\"pl-k\">|</span> kubectl apply --validate=false -f -</pre></div>\n\n<p>There are quite a few options when it comes to Kubernetes dashboards. I recommend using either the official one or <a href=\"https://k8slens.dev/\">Lens</a>.</p>\n<ul>\n<li><a href=\"https://rancher.com/docs/k3s/latest/en/installation/kube-dashboard/\">https://rancher.com/docs/k3s/latest/en/installation/kube-dashboard/</a></li>\n</ul>\n<h2><a id=\"user-content-install-the-dashboard\" class=\"anchor\" href=\"#install-the-dashboard\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Install the Dashboard</h2>\n<div class=\"highlight highlight-source-shell\"><pre>GITHUB_URL=https://github.com/kubernetes/dashboard/releases\nVERSION_KUBE_DASHBOARD=<span class=\"pl-s\"><span class=\"pl-pds\">$(</span>curl -w <span class=\"pl-s\"><span class=\"pl-pds\">&apos;</span>%{url_effective}<span class=\"pl-pds\">&apos;</span></span> -I -L -s -S <span class=\"pl-smi\">${GITHUB_URL}</span>/latest -o /dev/null <span class=\"pl-k\">|</span> sed -e <span class=\"pl-s\"><span class=\"pl-pds\">&apos;</span>s|.*/||<span class=\"pl-pds\">&apos;</span></span><span class=\"pl-pds\">)</span></span>\nkubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/<span class=\"pl-smi\">${VERSION_KUBE_DASHBOARD}</span>/aio/deploy/recommended.yaml</pre></div>\n<h2><a id=\"user-content-create-service-account\" class=\"anchor\" href=\"#create-service-account\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Create Service Account</h2>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c1\">echo</span> <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>apiVersion: v1</span>\n<span class=\"pl-s\">kind: ServiceAccount</span>\n<span class=\"pl-s\">metadata:</span>\n<span class=\"pl-s\">  name: admin-user</span>\n<span class=\"pl-s\">  namespace: kubernetes-dashboard</span>\n<span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span></span> <span class=\"pl-k\">|</span> kubectl create -f -</pre></div>\n<h2><a id=\"user-content-create-cluster-role-binding\" class=\"anchor\" href=\"#create-cluster-role-binding\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Create Cluster Role Binding</h2>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-c1\">echo</span> <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>apiVersion: rbac.authorization.k8s.io/v1</span>\n<span class=\"pl-s\">kind: ClusterRoleBinding</span>\n<span class=\"pl-s\">metadata:</span>\n<span class=\"pl-s\">  name: admin-user</span>\n<span class=\"pl-s\">roleRef:</span>\n<span class=\"pl-s\">  apiGroup: rbac.authorization.k8s.io</span>\n<span class=\"pl-s\">  kind: ClusterRole</span>\n<span class=\"pl-s\">  name: cluster-admin</span>\n<span class=\"pl-s\">subjects:</span>\n<span class=\"pl-s\">- kind: ServiceAccount</span>\n<span class=\"pl-s\">  name: admin-user</span>\n<span class=\"pl-s\">  namespace: kubernetes-dashboard</span>\n<span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span></span> <span class=\"pl-k\">|</span> kubectl create -f -</pre></div>\n<h2><a id=\"user-content-get-access-token\" class=\"anchor\" href=\"#get-access-token\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Get Access Token</h2>\n<div class=\"highlight highlight-source-shell\"><pre>kubectl -n kubernetes-dashboard describe secret admin-user-token <span class=\"pl-k\">|</span> grep ^token</pre></div>\n<h2><a id=\"user-content-forward-port-to-localhost\" class=\"anchor\" href=\"#forward-port-to-localhost\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Forward port to localhost</h2>\n\n<h2><a id=\"user-content-browse-dashboard\" class=\"anchor\" href=\"#browse-dashboard\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Browse Dashboard</h2>\n<p>Open the below url in your favorite browser</p>\n<p><code>http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/logi</code></p>\n<h2><a id=\"user-content-helmfile---docker-compose-for-kubernetes-optional\" class=\"anchor\" href=\"#helmfile---docker-compose-for-kubernetes-optional\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Helmfile - docker-compose for Kubernetes (Optional)</h2>\n<p><a href=\"https://github.com/roboll/helmfile#installation\">https://github.com/roboll/helmfile#installation</a></p>\n<h2><a id=\"user-content-chartmuseum---helm-chart-repository-optional\" class=\"anchor\" href=\"#chartmuseum---helm-chart-repository-optional\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Chartmuseum - Helm Chart Repository (Optional)</h2>\n<p>Export <code>HARTMUSEUM_AUTH_USER</code> and <code>HARTMUSEUM_AUTH_PASS</code> with your desired username and password</p>\n<p>Use your favorite editor to save the below snippet in a file called helmfile.yaml</p>\n<div class=\"highlight highlight-source-yaml\"><pre><span class=\"pl-ent\">releases</span>:\n  - <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">chartmuseum</span>\n    <span class=\"pl-ent\">namespace</span>: <span class=\"pl-s\">chartmuseum</span>\n    <span class=\"pl-ent\">chart</span>: <span class=\"pl-s\">stable/chartmuseum</span>\n    <span class=\"pl-ent\">version</span>: <span class=\"pl-s\">2.13.3</span>\n    <span class=\"pl-ent\">values</span>:\n      - <span class=\"pl-ent\">ingress</span>:\n          <span class=\"pl-ent\">enabled</span>: <span class=\"pl-c1\">true</span>\n          <span class=\"pl-ent\">annotations</span>:\n            <span class=\"pl-ent\">kubernetes.io/ingress.class</span>: <span class=\"pl-s\">traefik</span>\n            <span class=\"pl-ent\">cert-manager.io/cluster-issuer</span>: <span class=\"pl-s\">letsencrypt-prod</span>\n          <span class=\"pl-ent\">hosts</span>:\n            - <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">helm-charts.your-domain.com</span>\n              <span class=\"pl-ent\">path</span>: <span class=\"pl-s\">/</span>\n              <span class=\"pl-ent\">tls</span>: <span class=\"pl-c1\">true</span>\n              <span class=\"pl-ent\">tlsSecret</span>: <span class=\"pl-s\">chartmuseum-tls</span>\n      - <span class=\"pl-ent\">env</span>:\n          <span class=\"pl-ent\">open</span>:\n            <span class=\"pl-ent\">DISABLE_API</span>: <span class=\"pl-c1\">false</span>\n            <span class=\"pl-ent\">STORAGE</span>: <span class=\"pl-s\">local</span>\n          <span class=\"pl-ent\">secret</span>:\n            <span class=\"pl-ent\">BASIC_AUTH_USER</span>: <span class=\"pl-s\">{{ requiredEnv &quot;CHARTMUSEUM_AUTH_USER&quot; }}</span>\n            <span class=\"pl-ent\">BASIC_AUTH_PASS</span>: <span class=\"pl-s\">{{ requiredEnv &quot;CHARTMUSEUM_AUTH_PASS&quot; }}</span>\n      - <span class=\"pl-ent\">persistence</span>:\n          <span class=\"pl-ent\">enabled</span>: <span class=\"pl-c1\">true</span>\n          <span class=\"pl-ent\">accessMode</span>: <span class=\"pl-s\">ReadWriteOnce</span>\n          <span class=\"pl-ent\">size</span>: <span class=\"pl-c1\">100M</span></pre></div>\n<h2><a id=\"user-content-install-chartmuseum\" class=\"anchor\" href=\"#install-chartmuseum\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Install Chartmuseum</h2>\n\n<h2><a id=\"user-content-install-chartmuseum-helm-plugin\" class=\"anchor\" href=\"#install-chartmuseum-helm-plugin\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Install Chartmuseum helm plugin</h2>\n<div class=\"highlight highlight-source-shell\"><pre>helm plugin install https://github.com/chartmuseum/helm-push.git</pre></div>\n<h2><a id=\"user-content-install-whoami---a-simple-application-which-will-return-the-hostname-of-the-pods-its-running-on\" class=\"anchor\" href=\"#install-whoami---a-simple-application-which-will-return-the-hostname-of-the-pods-its-running-on\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Install WhoAmI - A simple application which will return the hostname of the pods it&apos;s running on</h2>\n<p>This app can serve as a simple example of the relation between a <code>deployment</code>, <code>service</code> and <code>ingress</code> resource. For a more detailed explanation please see the below url.</p>\n<p><code>https://medium.com/@dwdraju/how-deployment-service-ingress-are-related-in-their-manifest-a2e553cf0ffb</code></p>\n<p>Use git to clone or fork the below reository</p>\n<p><code>https://github.com/tonsV2/whoami-mn</code></p>\n<p>Replace the <code>kubeContext</code>with the name of your context. Possible <code>default</code></p>\n<p><a href=\"https://github.com/tonsV2/whoami-mn/blob/master/helmfile.yaml#L4\">https://github.com/tonsV2/whoami-mn/blob/master/helmfile.yaml#L4</a></p>\n<p>Replace the repository with the url of your Chartmuseum</p>\n<p><code>https://github.com/tonsV2/whoami-mn/blob/master/helmfile.yaml#L36</code></p>\n<h2><a id=\"user-content-push-the-chart-to-the-repository\" class=\"anchor\" href=\"#push-the-chart-to-the-repository\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Push the chart to the repository</h2>\n<div class=\"highlight highlight-source-shell\"><pre>helm push src/main/helm tons --username <span class=\"pl-smi\">$CHARTMUSEUM_AUTH_USER</span> --password <span class=\"pl-smi\">$CHARTMUSEUM_AUTH_PASS</span></pre></div>\n<h2><a id=\"user-content-deploy-using-helmfile\" class=\"anchor\" href=\"#deploy-using-helmfile\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Deploy using helmfile</h2>\n<p>See <a href=\"https://github.com/tonsV2/whoami-mn#deploy\">https://github.com/tonsV2/whoami-mn#deploy</a></p>\n\n</article></div></div>",
      "contentAsText": "\n\ncurl -sfL https://get.k3s.io | sh -\n\nsudo cp /etc/rancher/k3s/k3s.yaml ~/.kube/\nsudo chown tons.tons ~/.kube/k3s.yaml\nexport KUBECONFIG=$HOME/.kube/k3s.yaml\n\n\n\nSee https://helm.sh/docs/intro/quickstart/\nSSL - Lets Encrypt (Optional)\n\nkubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.0.1/cert-manager.crds.yaml\nhelm repo add jetstack https://charts.jetstack.io\nkubectl create namespace cert-manager\nhelm install cert-manager jetstack/cert-manager --version 1.0.1 --namespace cert-manager\n\necho \"apiVersion: cert-manager.io/v1beta1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    email: sebastianthegreatful@gmail.com\n    privateKeySecretRef:\n      name: prod-issuer-account-key\n    server: https://acme-v02.api.letsencrypt.org/directory\n    solvers:\n      - http01:\n          ingress:\n            class: traefik\n\" | kubectl apply --validate=false -f -\n\necho \"apiVersion: cert-manager.io/v1beta1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    email: sebastianthegreatful@gmail.com\n    privateKeySecretRef:\n      name: staging-issuer-account-key\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    solvers:\n      - http01:\n          ingress:\n            class: traefik\n\" | kubectl apply --validate=false -f -\n\nThere are quite a few options when it comes to Kubernetes dashboards. I recommend using either the official one or Lens.\n\nhttps://rancher.com/docs/k3s/latest/en/installation/kube-dashboard/\n\nInstall the Dashboard\nGITHUB_URL=https://github.com/kubernetes/dashboard/releases\nVERSION_KUBE_DASHBOARD=$(curl -w '%{url_effective}' -I -L -s -S ${GITHUB_URL}/latest -o /dev/null | sed -e 's|.*/||')\nkubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/${VERSION_KUBE_DASHBOARD}/aio/deploy/recommended.yaml\nCreate Service Account\necho \"apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: admin-user\n  namespace: kubernetes-dashboard\n\" | kubectl create -f -\nCreate Cluster Role Binding\necho \"apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: admin-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: admin-user\n  namespace: kubernetes-dashboard\n\" | kubectl create -f -\nGet Access Token\nkubectl -n kubernetes-dashboard describe secret admin-user-token | grep ^token\nForward port to localhost\n\nBrowse Dashboard\nOpen the below url in your favorite browser\nhttp://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/logi\nHelmfile - docker-compose for Kubernetes (Optional)\nhttps://github.com/roboll/helmfile#installation\nChartmuseum - Helm Chart Repository (Optional)\nExport HARTMUSEUM_AUTH_USER and HARTMUSEUM_AUTH_PASS with your desired username and password\nUse your favorite editor to save the below snippet in a file called helmfile.yaml\nreleases:\n  - name: chartmuseum\n    namespace: chartmuseum\n    chart: stable/chartmuseum\n    version: 2.13.3\n    values:\n      - ingress:\n          enabled: true\n          annotations:\n            kubernetes.io/ingress.class: traefik\n            cert-manager.io/cluster-issuer: letsencrypt-prod\n          hosts:\n            - name: helm-charts.your-domain.com\n              path: /\n              tls: true\n              tlsSecret: chartmuseum-tls\n      - env:\n          open:\n            DISABLE_API: false\n            STORAGE: local\n          secret:\n            BASIC_AUTH_USER: {{ requiredEnv \"CHARTMUSEUM_AUTH_USER\" }}\n            BASIC_AUTH_PASS: {{ requiredEnv \"CHARTMUSEUM_AUTH_PASS\" }}\n      - persistence:\n          enabled: true\n          accessMode: ReadWriteOnce\n          size: 100M\nInstall Chartmuseum\n\nInstall Chartmuseum helm plugin\nhelm plugin install https://github.com/chartmuseum/helm-push.git\nInstall WhoAmI - A simple application which will return the hostname of the pods it's running on\nThis app can serve as a simple example of the relation between a deployment, service and ingress resource. For a more detailed explanation please see the below url.\nhttps://medium.com/@dwdraju/how-deployment-service-ingress-are-related-in-their-manifest-a2e553cf0ffb\nUse git to clone or fork the below reository\nhttps://github.com/tonsV2/whoami-mn\nReplace the kubeContextwith the name of your context. Possible default\nhttps://github.com/tonsV2/whoami-mn/blob/master/helmfile.yaml#L4\nReplace the repository with the url of your Chartmuseum\nhttps://github.com/tonsV2/whoami-mn/blob/master/helmfile.yaml#L36\nPush the chart to the repository\nhelm push src/main/helm tons --username $CHARTMUSEUM_AUTH_USER --password $CHARTMUSEUM_AUTH_PASS\nDeploy using helmfile\nSee https://github.com/tonsV2/whoami-mn#deploy\n\nPage 2\n\ncurl -sfL https://get.k3s.io | sh -\n\nsudo cp /etc/rancher/k3s/k3s.yaml ~/.kube/\nsudo chown tons.tons ~/.kube/k3s.yaml\nexport KUBECONFIG=$HOME/.kube/k3s.yaml\n\n\n\nSee https://helm.sh/docs/intro/quickstart/\nSSL - Lets Encrypt (Optional)\n\nkubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.0.1/cert-manager.crds.yaml\nhelm repo add jetstack https://charts.jetstack.io\nkubectl create namespace cert-manager\nhelm install cert-manager jetstack/cert-manager --version 1.0.1 --namespace cert-manager\n\necho \"apiVersion: cert-manager.io/v1beta1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    email: sebastianthegreatful@gmail.com\n    privateKeySecretRef:\n      name: prod-issuer-account-key\n    server: https://acme-v02.api.letsencrypt.org/directory\n    solvers:\n      - http01:\n          ingress:\n            class: traefik\n\" | kubectl apply --validate=false -f -\n\necho \"apiVersion: cert-manager.io/v1beta1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    email: sebastianthegreatful@gmail.com\n    privateKeySecretRef:\n      name: staging-issuer-account-key\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    solvers:\n      - http01:\n          ingress:\n            class: traefik\n\" | kubectl apply --validate=false -f -\n\nThere are quite a few options when it comes to Kubernetes dashboards. I recommend using either the official one or Lens.\n\nhttps://rancher.com/docs/k3s/latest/en/installation/kube-dashboard/\n\nInstall the Dashboard\nGITHUB_URL=https://github.com/kubernetes/dashboard/releases\nVERSION_KUBE_DASHBOARD=$(curl -w '%{url_effective}' -I -L -s -S ${GITHUB_URL}/latest -o /dev/null | sed -e 's|.*/||')\nkubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/${VERSION_KUBE_DASHBOARD}/aio/deploy/recommended.yaml\nCreate Service Account\necho \"apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: admin-user\n  namespace: kubernetes-dashboard\n\" | kubectl create -f -\nCreate Cluster Role Binding\necho \"apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: admin-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: admin-user\n  namespace: kubernetes-dashboard\n\" | kubectl create -f -\nGet Access Token\nkubectl -n kubernetes-dashboard describe secret admin-user-token | grep ^token\nForward port to localhost\n\nBrowse Dashboard\nOpen the below url in your favorite browser\nhttp://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/logi\nHelmfile - docker-compose for Kubernetes (Optional)\nhttps://github.com/roboll/helmfile#installation\nChartmuseum - Helm Chart Repository (Optional)\nExport HARTMUSEUM_AUTH_USER and HARTMUSEUM_AUTH_PASS with your desired username and password\nUse your favorite editor to save the below snippet in a file called helmfile.yaml\nreleases:\n  - name: chartmuseum\n    namespace: chartmuseum\n    chart: stable/chartmuseum\n    version: 2.13.3\n    values:\n      - ingress:\n          enabled: true\n          annotations:\n            kubernetes.io/ingress.class: traefik\n            cert-manager.io/cluster-issuer: letsencrypt-prod\n          hosts:\n            - name: helm-charts.your-domain.com\n              path: /\n              tls: true\n              tlsSecret: chartmuseum-tls\n      - env:\n          open:\n            DISABLE_API: false\n            STORAGE: local\n          secret:\n            BASIC_AUTH_USER: {{ requiredEnv \"CHARTMUSEUM_AUTH_USER\" }}\n            BASIC_AUTH_PASS: {{ requiredEnv \"CHARTMUSEUM_AUTH_PASS\" }}\n      - persistence:\n          enabled: true\n          accessMode: ReadWriteOnce\n          size: 100M\nInstall Chartmuseum\n\nInstall Chartmuseum helm plugin\nhelm plugin install https://github.com/chartmuseum/helm-push.git\nInstall WhoAmI - A simple application which will return the hostname of the pods it's running on\nThis app can serve as a simple example of the relation between a deployment, service and ingress resource. For a more detailed explanation please see the below url.\nhttps://medium.com/@dwdraju/how-deployment-service-ingress-are-related-in-their-manifest-a2e553cf0ffb\nUse git to clone or fork the below reository\nhttps://github.com/tonsV2/whoami-mn\nReplace the kubeContextwith the name of your context. Possible default\nhttps://github.com/tonsV2/whoami-mn/blob/master/helmfile.yaml#L4\nReplace the repository with the url of your Chartmuseum\nhttps://github.com/tonsV2/whoami-mn/blob/master/helmfile.yaml#L36\nPush the chart to the repository\nhelm push src/main/helm tons --username $CHARTMUSEUM_AUTH_USER --password $CHARTMUSEUM_AUTH_PASS\nDeploy using helmfile\nSee https://github.com/tonsV2/whoami-mn#deploy\n\n",
      "description": "Contribute to tonsV2/kube-notes development by creating an account on GitHub.",
      "ogDescription": "Contribute to tonsV2/kube-notes development by creating an account on GitHub."
    },
    {
      "url": "https://github.com/tonsV2/whoami-mn",
      "title": "tonsV2/whoami-mn",
      "content": "<div><div><article class=\"markdown-body entry-content container-lg\">\n\n\n\n\n\n<p>Simple Micronaut &quot;Who Am I&quot; microservice</p>\n<p>Executing an HTTP request to / will return the hostname of the server</p>\n<h2><a id=\"user-content-run-using-gradle\" class=\"anchor\" href=\"https://github.com/tonsV2/whoami-mn#run-using-gradle\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Run using Gradle</h2>\n\n\n<p>Perform a http request using HTTPie</p>\n\n<h2><a id=\"user-content-development\" class=\"anchor\" href=\"https://github.com/tonsV2/whoami-mn#development\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Development</h2>\n\n\n<p>Perform a http request using HTTPie</p>\n<div class=\"highlight highlight-source-shell\"><pre>http whoami-mn.127.0.0.1.nip.io</pre></div>\n<h2><a id=\"user-content-release\" class=\"anchor\" href=\"https://github.com/tonsV2/whoami-mn#release\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Release</h2>\n\n<p>Update version in <code>build.gradle</code></p>\n\n<h2><a id=\"user-content-release-helm-chart\" class=\"anchor\" href=\"https://github.com/tonsV2/whoami-mn#release-helm-chart\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Release Helm chart</h2>\n<p>Update version (or versions) in <code>src/main/helm/Chart.yaml</code></p>\n<div class=\"highlight highlight-source-shell\"><pre>helm package --sign --key <span class=\"pl-s\"><span class=\"pl-pds\">&apos;</span>sj<span class=\"pl-pds\">&apos;</span></span> src/main/helm\n\ncurl --user <span class=\"pl-smi\">$CHARTMUSEUM_AUTH_USER</span>:<span class=\"pl-smi\">$CHARTMUSEUM_AUTH_PASS</span> \\\n  -F <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>chart=@surf-screenshotter-<span class=\"pl-smi\">$version</span>.tgz<span class=\"pl-pds\">&quot;</span></span> \\\n  -F <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>prov=@surf-screenshotter-<span class=\"pl-smi\">$version</span>.tgz.prov<span class=\"pl-pds\">&quot;</span></span> \\\n  https://helm-charts.fitfit.dk/api/charts</pre></div>\n<p>Commit and tag commit: RELEASE-$version</p>\n<h2><a id=\"user-content-deploy\" class=\"anchor\" href=\"https://github.com/tonsV2/whoami-mn#deploy\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Deploy</h2>\n<p>Update version, for the given environment, in <code>helmfile.yaml</code></p>\n\n</article></div></div><hr><h4>Page 2</h4><div><div><article class=\"markdown-body entry-content container-lg\">\n\n\n\n\n\n<p>Simple Micronaut &quot;Who Am I&quot; microservice</p>\n<p>Executing an HTTP request to / will return the hostname of the server</p>\n<h2><a id=\"user-content-run-using-gradle\" class=\"anchor\" href=\"#run-using-gradle\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Run using Gradle</h2>\n\n\n<p>Perform a http request using HTTPie</p>\n\n<h2><a id=\"user-content-development\" class=\"anchor\" href=\"#development\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Development</h2>\n\n\n<p>Perform a http request using HTTPie</p>\n<div class=\"highlight highlight-source-shell\"><pre>http whoami-mn.127.0.0.1.nip.io</pre></div>\n<h2><a id=\"user-content-release\" class=\"anchor\" href=\"#release\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Release</h2>\n\n<p>Update version in <code>build.gradle</code></p>\n\n<h2><a id=\"user-content-release-helm-chart\" class=\"anchor\" href=\"#release-helm-chart\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Release Helm chart</h2>\n<p>Update version (or versions) in <code>src/main/helm/Chart.yaml</code></p>\n<div class=\"highlight highlight-source-shell\"><pre>helm package --sign --key <span class=\"pl-s\"><span class=\"pl-pds\">&apos;</span>sj<span class=\"pl-pds\">&apos;</span></span> src/main/helm\n\ncurl --user <span class=\"pl-smi\">$CHARTMUSEUM_AUTH_USER</span>:<span class=\"pl-smi\">$CHARTMUSEUM_AUTH_PASS</span> \\\n  -F <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>chart=@surf-screenshotter-<span class=\"pl-smi\">$version</span>.tgz<span class=\"pl-pds\">&quot;</span></span> \\\n  -F <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>prov=@surf-screenshotter-<span class=\"pl-smi\">$version</span>.tgz.prov<span class=\"pl-pds\">&quot;</span></span> \\\n  https://helm-charts.fitfit.dk/api/charts</pre></div>\n<p>Commit and tag commit: RELEASE-$version</p>\n<h2><a id=\"user-content-deploy\" class=\"anchor\" href=\"#deploy\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Deploy</h2>\n<p>Update version, for the given environment, in <code>helmfile.yaml</code></p>\n\n</article></div></div>",
      "contentAsText": "\n\n\n\n\n\nSimple Micronaut \"Who Am I\" microservice\nExecuting an HTTP request to / will return the hostname of the server\nRun using Gradle\n\n\nPerform a http request using HTTPie\n\nDevelopment\n\n\nPerform a http request using HTTPie\nhttp whoami-mn.127.0.0.1.nip.io\nRelease\n\nUpdate version in build.gradle\n\nRelease Helm chart\nUpdate version (or versions) in src/main/helm/Chart.yaml\nhelm package --sign --key 'sj' src/main/helm\n\ncurl --user $CHARTMUSEUM_AUTH_USER:$CHARTMUSEUM_AUTH_PASS \\\n  -F \"chart=@surf-screenshotter-$version.tgz\" \\\n  -F \"prov=@surf-screenshotter-$version.tgz.prov\" \\\n  https://helm-charts.fitfit.dk/api/charts\nCommit and tag commit: RELEASE-$version\nDeploy\nUpdate version, for the given environment, in helmfile.yaml\n\nPage 2\n\n\n\n\n\nSimple Micronaut \"Who Am I\" microservice\nExecuting an HTTP request to / will return the hostname of the server\nRun using Gradle\n\n\nPerform a http request using HTTPie\n\nDevelopment\n\n\nPerform a http request using HTTPie\nhttp whoami-mn.127.0.0.1.nip.io\nRelease\n\nUpdate version in build.gradle\n\nRelease Helm chart\nUpdate version (or versions) in src/main/helm/Chart.yaml\nhelm package --sign --key 'sj' src/main/helm\n\ncurl --user $CHARTMUSEUM_AUTH_USER:$CHARTMUSEUM_AUTH_PASS \\\n  -F \"chart=@surf-screenshotter-$version.tgz\" \\\n  -F \"prov=@surf-screenshotter-$version.tgz.prov\" \\\n  https://helm-charts.fitfit.dk/api/charts\nCommit and tag commit: RELEASE-$version\nDeploy\nUpdate version, for the given environment, in helmfile.yaml\n\n",
      "description": "Simple Micronaut \"whoami\" microservice. Contribute to tonsV2/whoami-mn development by creating an account on GitHub.",
      "ogDescription": "Simple Micronaut \"whoami\" microservice. Contribute to tonsV2/whoami-mn development by creating an account on GitHub."
    },
    {
      "url": "https://www.infracloud.io/blogs/3-autoscaling-projects-optimising-kubernetes-costs/?utm_source=reddit.com&utm_medium=social&utm_campaign=promoting_blog",
      "title": "3 Autoscaling Projects to Optimise Kubernetes Costs",
      "content": "<div class=\"content-inner\"><p><img class=\"lazyload aligncenter wp-image-4387 size-full\" src=\"data:image/svg+xml,%3Csvg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%201600%20350%22%3E%3C/svg%3E\" alt=\"3-Autoscaling-Projects-to-Optimise-Kubernetes-Costs-header\" width=\"1600\"></p><p>First of all my apologies for choosing a catchy headline &#x2013; as much as I have derided such posts, I genuinely had only three things to talk about which I felt had a connection and added value to many users and their workloads. Autoscaling is often not as clearly understood as I think it should be, at least by the consumers who may not want to understand all the nuts and bolts of how it works. Autoscaling can be of the application work units (Pods in case of Kubernetes) and of the nodes &#x2013; which form the underlying infrastructure. The application work units &#x2013; or pods in case of Kubernetes also need slightly different treatment based on the kind of workloads. The workloads can be divided into two broad categories &#x2013; the synchronous workloads such as the HTTP server responding to requests in realtime. The second kind of workload is which is more async in nature such as a process being invoked when a message arrives in a message queue. Both workloads need different ways to scale and be efficient.</p><p>In this post, we will talk about each of the three autoscaling use cases, which projects solve these problems when using them, etc. For a quick summary, we will cover three aspects:</p><ul><li>Autoscaling Event-driven workloads</li><li>Autoscaling real-time workloads</li><li>Autoscaling Nodes/Infrastructure</li></ul><h2></h2><h2>Autoscaling Event-Driven Workloads</h2><p>For these workloads, the events are generated from a source and they land typically in some form of a message queue. Message queue enables handling scale as well as to manage back pressure etc. gracefully and divide the workload among many workers. The implementation of consumer and scaling is to an extent coupled with the message queue being used here. Also, many organizations may have more than one message queue in use.</p><p>The ideal scenario here would be that when there are no messages in the queue, the consumer pods are scaled down to zero. When messages start arriving the consumer pods are scaled up from zero to one and beyond one based on messages arriving.</p><p>One project that solves this problem nicely is the <a href=\"https://keda.sh/\">Keda project</a>. We recently <a href=\"https://docs.fission.io/docs/triggers/message-queue-trigger-kind-keda/\">integrated Keda in the Fission</a> project &#x2013; which is a FaaS platform that works on Kubernetes. We also <a href=\"https://github.com/fission/keda-connectors\">open-sourced the generic Keda connectors</a> which enable you to consume messages from message queues and do generic things such as sending that message to an HTTP endpoint.</p><p>I also recently gave a talk where Keda was used to autoscale ACI (Azure Container Instance) containers using Keda from within a Kubernetes cluster using Virtual Kubelet. You can find the <a href=\"https://www.youtube.com/watch?v=3ii722dr_K0\">talk here</a> and the <a href=\"https://docs.google.com/presentation/d/e/2PACX-1vQ8pmEpbuFPKcfRGcRQ17VfF9W7W3H0Lq5kXq9Yy1NvctbkDz6yFv2O10DUg07ocOxhKzAqEz-4nGRg/pub?start=false&amp;loop=false&amp;delayms=3000\">slides here</a>.</p><h2><img class=\"lazyload alignnone wp-image-4342 size-large\" src=\"data:image/svg+xml,%3Csvg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%201024%20600%22%3E%3C/svg%3E\" alt=\"Autoscaling workloads with Keda\" width=\"1024\"></h2><h2>Autoscaling Realtime Workloads</h2><p>Many times you want to scale down the HTTP servers to zero or based on demand so that you can conserve infrastructure or use for some other workloads which will be processed in off-peak hours etc. While it is relatively easy for event-driven workloads, it is not so straightforward for realtime workloads. On one hand you don&#x2019;t want the end-user to wait for the pods to spin up etc. &#x2013; so this is definitely not recommended for usage in production. This can be used though effectively in stage/dev environments especially when your users are not going to consume the resources.</p><p>Scaling down the standard HPA along with the monitoring system will give you enough to get it working. You can see this in action in a <a href=\"https://www.youtube.com/watch?v=YWLrvj3XOD0\">talk given by Hrishikesh here</a> and the <a href=\"https://docs.google.com/presentation/d/e/2PACX-1vRk7O80Npx0J4So50qYMpzl-JHTMOkyW3VQzWCC1YGvO5vI89h85Wfr-MGgNoIXbvx7wCEQHLWtf4kS/pub?start=false&amp;loop=false&amp;delayms=3000\">presentation can be found here</a>.</p><p>If you must scale to zero there are projects like <a href=\"https://github.com/deislabs/osiris\">Osiris</a> &#x2013; which are highly experimental but can be still useful for non-prod workloads and to optimize the infrastructure usage.</p><h2></h2><h2>Autoscaling Nodes/Infrastructure</h2><p>Once you have optimized the workloads, the next natural step is to optimize the underlying infrastructure. All the benefits of optimizing the workloads can be only derived only if you can actually shut down a few instances and save some $$$! Especially for non-prod workloads &#x2013; this can be a great cost-saving exercise.</p><p>There are definitely details here and the second part of a <a href=\"https://www.youtube.com/watch?v=YWLrvj3XOD0\">talk given by one of the InfraCloud members here</a> discusses these things in detail. The <a href=\"https://github.com/kubernetes/autoscaler\">Kubernetes Autoscaler project</a> gives you all tools and levers to manage the infrastructure and scale it up and down based on need.</p><blockquote><p>While we are on the topic of cost-saving in your Kubernetes clusters, you might also want to check out when to switch from CloudWatch to Prometheus &#x2013; <a href=\"https://www.infracloud.io/blogs/prometheus-vs-cloudwatch/\">details in this post</a>!</p></blockquote><h2>Bonus</h2><p>There are some interesting projects on <a href=\"https://github.com/kubecost\">KubeCost Github repo</a> that enable things such as turning down the clusters at a specific schedule etc. They can be a great compliment to the other three projects and also give you visibility of cluster usage &amp; costs.</p><h2>Summary</h2><ul><li>Autoscaling can be of workloads &amp; the underlying nodes/infrastructure. To save costs in pre-prod environments you will need to look at both aspects.</li><li>For autoscaling event-driven workloads<ul><li>Keda project enables and can be used in production too effectively.</li></ul></li><li>For autoscaling real-time workloads<ul><li>Projects such as <a href=\"https://github.com/deislabs/osiris\">Osiris</a> enable it but is not recommended for production workloads!</li></ul></li><li>The real value will be realized only after scaling underlying nodes/infrastructure and <a href=\"https://github.com/kubernetes/autoscaler\">Kubernetes Autoscaler</a> enables that. While you can use this in production too &#x2013; the right design and testing are crucial to avoid costly mistakes and get it right.</li></ul><p><strong>References &amp; Images from</strong>:</p></div>",
      "contentAsText": "First of all my apologies for choosing a catchy headline – as much as I have derided such posts, I genuinely had only three things to talk about which I felt had a connection and added value to many users and their workloads. Autoscaling is often not as clearly understood as I think it should be, at least by the consumers who may not want to understand all the nuts and bolts of how it works. Autoscaling can be of the application work units (Pods in case of Kubernetes) and of the nodes – which form the underlying infrastructure. The application work units – or pods in case of Kubernetes also need slightly different treatment based on the kind of workloads. The workloads can be divided into two broad categories – the synchronous workloads such as the HTTP server responding to requests in realtime. The second kind of workload is which is more async in nature such as a process being invoked when a message arrives in a message queue. Both workloads need different ways to scale and be efficient.In this post, we will talk about each of the three autoscaling use cases, which projects solve these problems when using them, etc. For a quick summary, we will cover three aspects:Autoscaling Event-driven workloadsAutoscaling real-time workloadsAutoscaling Nodes/InfrastructureAutoscaling Event-Driven WorkloadsFor these workloads, the events are generated from a source and they land typically in some form of a message queue. Message queue enables handling scale as well as to manage back pressure etc. gracefully and divide the workload among many workers. The implementation of consumer and scaling is to an extent coupled with the message queue being used here. Also, many organizations may have more than one message queue in use.The ideal scenario here would be that when there are no messages in the queue, the consumer pods are scaled down to zero. When messages start arriving the consumer pods are scaled up from zero to one and beyond one based on messages arriving.One project that solves this problem nicely is the Keda project. We recently integrated Keda in the Fission project – which is a FaaS platform that works on Kubernetes. We also open-sourced the generic Keda connectors which enable you to consume messages from message queues and do generic things such as sending that message to an HTTP endpoint.I also recently gave a talk where Keda was used to autoscale ACI (Azure Container Instance) containers using Keda from within a Kubernetes cluster using Virtual Kubelet. You can find the talk here and the slides here.Autoscaling Realtime WorkloadsMany times you want to scale down the HTTP servers to zero or based on demand so that you can conserve infrastructure or use for some other workloads which will be processed in off-peak hours etc. While it is relatively easy for event-driven workloads, it is not so straightforward for realtime workloads. On one hand you don’t want the end-user to wait for the pods to spin up etc. – so this is definitely not recommended for usage in production. This can be used though effectively in stage/dev environments especially when your users are not going to consume the resources.Scaling down the standard HPA along with the monitoring system will give you enough to get it working. You can see this in action in a talk given by Hrishikesh here and the presentation can be found here.If you must scale to zero there are projects like Osiris – which are highly experimental but can be still useful for non-prod workloads and to optimize the infrastructure usage.Autoscaling Nodes/InfrastructureOnce you have optimized the workloads, the next natural step is to optimize the underlying infrastructure. All the benefits of optimizing the workloads can be only derived only if you can actually shut down a few instances and save some $$$! Especially for non-prod workloads – this can be a great cost-saving exercise.There are definitely details here and the second part of a talk given by one of the InfraCloud members here discusses these things in detail. The Kubernetes Autoscaler project gives you all tools and levers to manage the infrastructure and scale it up and down based on need.While we are on the topic of cost-saving in your Kubernetes clusters, you might also want to check out when to switch from CloudWatch to Prometheus – details in this post!BonusThere are some interesting projects on KubeCost Github repo that enable things such as turning down the clusters at a specific schedule etc. They can be a great compliment to the other three projects and also give you visibility of cluster usage & costs.SummaryAutoscaling can be of workloads & the underlying nodes/infrastructure. To save costs in pre-prod environments you will need to look at both aspects.For autoscaling event-driven workloadsKeda project enables and can be used in production too effectively.For autoscaling real-time workloadsProjects such as Osiris enable it but is not recommended for production workloads!The real value will be realized only after scaling underlying nodes/infrastructure and Kubernetes Autoscaler enables that. While you can use this in production too – the right design and testing are crucial to avoid costly mistakes and get it right.References & Images from:",
      "publishedDate": "2020-09-03T14:31:48.000Z",
      "description": "Autoscaling for non prod workloads can save cost and for production workloads can enable a seamless user experience - we look at how in this post!",
      "ogDescription": "Autoscaling for non prod workloads can save cost and for production workloads can enable a seamless user experience - we look at how in this post!"
    },
    {
      "url": "https://www.padok.fr/en/blog/scaling-prometheus-rabbitmq",
      "title": "Custom HPA scaling for K8s with Prometheus and RabbitMQ",
      "content": "<div id=\"hs_cos_wrapper_post_body\" class=\"hs_cos_wrapper\"> <p>You are going to need</p>\n<ul>\n<li>A <strong>Kubernetes cluster</strong></li>\n<li><strong>Prometheus operator</strong></li>\n<li><strong>Prometheus adapter</strong></li>\n<li>An <strong>exporter for the metric</strong> you need</li>\n</ul>\n<h2 id=\"step\">Step by step</h2>\n<p>First of all you need to be <strong>connected to your Kubernetes cluster</strong> in 1.13+, and have the v2beta2 of the autoscaling API. (This is important because without this your HPA will not be able to read custom metrics)</p>\n<p>Make sure to have <strong>enough nodes to scale your cluster</strong>.</p>\n<h3 id=\"prometheus-operator\">Install the Prometheus operator</h3>\n<p>The <strong>Prometheus operator</strong> allows <strong>storing of Kubernetes metrics</strong> and your custom metrics!</p>\n<p>To install Prometheus in our cluster we used the <strong>Prometheus helm operator</strong>:</p>\n<p><a href=\"https://github.com/helm/charts/tree/master/stable/prometheus-operator\">helm/charts</a></p>\n<p>To install it we used</p>\n<p><code>helm install prometheus-operator -f prometheus-operator-value.yaml stable/prometheus-operator</code></p>\n<p>This deploys Prometheus, Grafana, alert manager, etc.</p>\n<p>You can now access <strong>Prometheus dashboard</strong> with the following command:</p>\n<p><code>kubectl port-forward svc/prometheus-operator-prometheus 8002:9090</code></p>\n<p>For <strong>Grafana</strong></p>\n<p><code>kubectl port-forward svc/prometheus-operator-grafana 8002:80</code></p> <h3 id=\"prometheus-adapter\">Install the Prometheus adapter</h3>\n<p>To allow Kubernetes to read metrics from Prometheus an <strong>adapter is needed</strong>.</p>\n<p>We used the <strong>helm chart to install</strong> it in our cluster</p>\n<p><a href=\"https://github.com/helm/charts/tree/master/stable/prometheus-adapter\">helm/charts</a></p>\n<p>To install</p>\n<p><code>helm install -f prometheus-adapter-value.yaml prometheus-adapter stable/prometheus-adapter</code></p>\n<p>When installed you can use the following command to see <strong>all the metrics</strong> that are now exposed to Kubernetes</p>\n<p><code>kubectl get --raw &quot;/apis/custom.metrics.k8s.io/v1beta1/&quot;</code></p>\n<p>or</p>\n<p><code>kubectl get --raw &quot;/apis/custom.metrics.k8s.io/v1beta1/&quot; | jq/</code></p> <h3 id=\"custom-metric\">Install an exporter for your custom metric</h3>\n<p>To scarp data from our RabbitMQ deployment and make them available for Prometheus we need to <strong>deploy an exporter pod</strong> that will do that for use.</p>\n<p>We used the Prometheus exporter</p>\n<p><a href=\"https://github.com/kbudde/rabbitmq_exporter\">kbudde/rabbitmq_exporter</a></p> <h3 id=\"service-monitor\">Service monitor</h3>\n<p>Now that you have configured Prometheus and your exporter you should be able to see date in <strong>K8s metric API</strong>.</p> <h3 id=\"prometheus-rules\">Prometheus rules</h3>\n<p>Now to query <strong>specific information within a metric</strong> we need to query Prometheus. To do so we need to create a <strong>PrometheusRule</strong>.</p>\n<p>This configuration will expose a new metric for the HPA to consume.</p>\n<p>Here it will be the number of messages within a specific a <strong>RabbitMQ queue</strong>.</p>\n<p>This is the syntax of it :</p> <h3 id=\"hpa\">HPA</h3>\n<p>Now you can <strong>configure your HPA</strong> (Horizontal pod autoscaling) with a custom metric.</p> <p>Done you should now be able to see your metrics when describing your HPA.</p> <p>Taking some time to <strong>figure out your KPI for scaling</strong> will make the difference between a successful or failure to manage a surge in traffic.</p>\n<p>Refining an <strong>autoscaling rule</strong> or <strong>HPA for Kubernetes</strong> is a forced path for any resilient architecture. Here the example is the size of a RabbitMQ queue, but it requires the consumers to process message at a fix rate. It has to be constant because if some messages take 1 hour to be processed vs 2 sec for others the scaling <strong>will not be reliable</strong>.</p>\n<p>You can make sure your<strong> infrastructure is robust</strong> by <a href=\"https://www.padok.fr/en/blog/kube-monkey-kubernetes\">using chaos engineering</a>.</p></div>",
      "contentAsText": " You are going to need\n\nA Kubernetes cluster\nPrometheus operator\nPrometheus adapter\nAn exporter for the metric you need\n\nStep by step\nFirst of all you need to be connected to your Kubernetes cluster in 1.13+, and have the v2beta2 of the autoscaling API. (This is important because without this your HPA will not be able to read custom metrics)\nMake sure to have enough nodes to scale your cluster.\nInstall the Prometheus operator\nThe Prometheus operator allows storing of Kubernetes metrics and your custom metrics!\nTo install Prometheus in our cluster we used the Prometheus helm operator:\nhelm/charts\nTo install it we used\nhelm install prometheus-operator -f prometheus-operator-value.yaml stable/prometheus-operator\nThis deploys Prometheus, Grafana, alert manager, etc.\nYou can now access Prometheus dashboard with the following command:\nkubectl port-forward svc/prometheus-operator-prometheus 8002:9090\nFor Grafana\nkubectl port-forward svc/prometheus-operator-grafana 8002:80 Install the Prometheus adapter\nTo allow Kubernetes to read metrics from Prometheus an adapter is needed.\nWe used the helm chart to install it in our cluster\nhelm/charts\nTo install\nhelm install -f prometheus-adapter-value.yaml prometheus-adapter stable/prometheus-adapter\nWhen installed you can use the following command to see all the metrics that are now exposed to Kubernetes\nkubectl get --raw \"/apis/custom.metrics.k8s.io/v1beta1/\"\nor\nkubectl get --raw \"/apis/custom.metrics.k8s.io/v1beta1/\" | jq/ Install an exporter for your custom metric\nTo scarp data from our RabbitMQ deployment and make them available for Prometheus we need to deploy an exporter pod that will do that for use.\nWe used the Prometheus exporter\nkbudde/rabbitmq_exporter Service monitor\nNow that you have configured Prometheus and your exporter you should be able to see date in K8s metric API. Prometheus rules\nNow to query specific information within a metric we need to query Prometheus. To do so we need to create a PrometheusRule.\nThis configuration will expose a new metric for the HPA to consume.\nHere it will be the number of messages within a specific a RabbitMQ queue.\nThis is the syntax of it : HPA\nNow you can configure your HPA (Horizontal pod autoscaling) with a custom metric. Done you should now be able to see your metrics when describing your HPA. Taking some time to figure out your KPI for scaling will make the difference between a successful or failure to manage a surge in traffic.\nRefining an autoscaling rule or HPA for Kubernetes is a forced path for any resilient architecture. Here the example is the size of a RabbitMQ queue, but it requires the consumers to process message at a fix rate. It has to be constant because if some messages take 1 hour to be processed vs 2 sec for others the scaling will not be reliable.\nYou can make sure your infrastructure is robust by using chaos engineering.",
      "description": "Scaling is critical for any resilient architecture. Learn with here how to set up autoscaling rule or HPA for Kubernetes with Prometheus and RabbitMQ metric.",
      "ogDescription": "Scaling is critical for any resilient architecture. Learn with here how to set up autoscaling rule or HPA for Kubernetes with Prometheus and RabbitMQ metric."
    },
    {
      "url": "https://www.reddit.com/r/kubernetes/comments/iqware/yaml_is_the_worst_thing_ever_created_k8s_should/",
      "title": "r/kubernetes - YAML is the worst thing ever created. K8S should move to a different file format.",
      "content": "<div><div><div class=\"_2FCtq-QzlfuN-SwVMUZMM3 _2v9pwVh0VUYrmhoMv1tHPm t3_iqware\"><div class=\"_1hLrLjnE1G_RBCNcN9MVQf\">\n              <img alt src=\"https://www.redditstatic.com/desktop2x/img/renderTimingPixel.png\">\n            </div></div><div class=\"_1hwEKkB_38tIoal6fcdrt9\"><div class=\"_3-miAEojrCvx_4FQ8x3P-s\"><a class=\"_1UoeAeSRhOKSNdY_h3iS1O _1Hw7tY9pMr-T1F4P1C-xNU _2qww3J5KKzsD7e5DO0BvvU\" href=\"https://www.reddit.com/r/kubernetes/comments/iqware/yaml_is_the_worst_thing_ever_created_k8s_should/\"><span class=\"FHCV02u6Cp2zYL0fhQPsO\">23 comments</span></a></div></div></div></div><hr><h4>Page 2</h4><div><div><div class=\"_3MC4c3Q_Y41YKtl1TcvyMt\"><div class=\"_3UMN4RCVY5288m_fOZlkcg\"><div class=\"_2FCtq-QzlfuN-SwVMUZMM3 _2v9pwVh0VUYrmhoMv1tHPm t3_iqware\"><div class=\"_1hLrLjnE1G_RBCNcN9MVQf\">\n              <img alt src=\"https://www.redditstatic.com/desktop2x/img/renderTimingPixel.png\">\n            </div></div><div><a class=\"_1sq8G2ap3_yMYvXINVLxFm RvLtAcdRtbOQbhFB7MD_T\" href=\"https://www.reddit.com/r/kubernetes/comments/iqware/yaml_is_the_worst_thing_ever_created_k8s_should/\"></a><div class=\"_3-miAEojrCvx_4FQ8x3P-s _3P3ghhoNky7Bzspbfw7--R\"><a class=\"_1UoeAeSRhOKSNdY_h3iS1O _1Hw7tY9pMr-T1F4P1C-xNU _2qww3J5KKzsD7e5DO0BvvU\" href=\"https://www.reddit.com/r/kubernetes/comments/iqware/yaml_is_the_worst_thing_ever_created_k8s_should/\"><span class=\"FHCV02u6Cp2zYL0fhQPsO\">23 comments</span></a></div></div></div></div></div></div>",
      "contentAsText": "\n              \n            23 commentsPage 2\n              \n            23 comments",
      "publishedDate": "2020-09-22T12:20:44.760Z",
      "description": "52.4k members in the kubernetes community. Kubernetes discussion, news, support, and link sharing.",
      "ogDescription": "0 votes and 23 comments so far on Reddit"
    },
    {
      "url": "https://kalm.dev/",
      "title": "Kalm",
      "content": "<div class=\"main-wrapper\"><header class=\"hero hero--primary heroBanner_2Ftp\"><div class=\"container\"><p class=\"hero__subtitle\">Get what you want out of Kubernetes without having to write and maintain a ton of custom tooling. Deploy apps, handle requests, and hook up CI/CD, all through an intuitive web interface.</p></div></header><div class=\"container\"><section class=\"features_P2SU\"><div class=\"div\"><div class=\"row\"><div class=\"col col--4\"><img class=\"primary-icon\" src=\"img/github.svg\"><p>Kalm is free, open source, and actively maintained. </p></div><div class=\"col col--4\"><img class=\"primary-icon\" src=\"img/build-24px.svg\"><p>Kalm simplifies the common workflows related to Kubernetes, including deploying applications, routing, and integrating with your existing pipeline.</p></div><div class=\"col col--4\"><img class=\"primary-icon\" src=\"img/install.svg\"><p>Kalm works on Google GKE, Amazon EKS, Azure AKS, and most Kubernetes configurations. Take it with you if you decide to migrate someday.</p></div></div></div></section><div class=\"row featureRow_2ZVN\"><div class=\"col col--6\"><p>Kalm provides an intuitive web interface for core Kubernetes functionalities:</p><ul><li>Configuring and deploying applications</li><li>Managing ports and container networking</li><li>Probes and Auto-Healing</li><li>Scaling</li><li>Mounting Volumes</li><li>Scheduling according to Resources</li></ul></div><p class=\"col\"><iframe width=\"538\" height=\"303\" src=\"https://www.youtube.com/embed/F5wuQaPQ50s\" class></iframe></p></div><div class=\"row featureRow_2ZVN\"><div class=\"col col--6\"><p>Kalm supports the Service Mesh <a href=\"https://istio.io/\">Istio</a> out of the box. This gives you full control over traffic entering the cluster. You can setup Request Routing, Error Injection, Mirroring, Traffic Shifting, and more.</p></div><div class=\"col col--6\"><img class=\"feature-img\" src=\"/img/feature-routes.png\" alt=\"Powerful Ingress\"></div></div><div class=\"row featureRow_2ZVN\"><div class=\"col col--6\"><p>Want the Heroku-like experience of &quot;git push, update app&quot;? Kalm provides webhooks which you can use to invoke deployment updates. In addition, you can generate snippets for popular build tools.</p></div><div class=\"col col--6\"><img class=\"feature-img\" src=\"/img/feature-cicd.png\" alt=\"CI/CD Integration\"></div></div><div class=\"row featureRow_2ZVN\"><div class=\"col col--6\"><p>Easily obtain and renew HTTPS Certificates via <a href=\"https://letsencrypt.org/\">Let&apos;s Encrypt</a>. Kalm currently supports specific domain name certificates using http-01 challenge. (Wildcard certificates coming soon)</p></div><div class=\"col col--6\"><img class=\"feature-img\" src=\"/img/feature-cert.png\" alt=\"Automatic HTTPS Certification\"></div></div><div class=\"row featureRow_2ZVN\"><div class=\"col col--6\"><p>New to Kubernetes and struggling with log collection? Kalm can help you setup a logging solution within minutes. Choose either <a href=\"https://grafana.com/oss/loki/\">Loki(PLG stack) </a>or <a href=\"https://www.elastic.co/what-is/elk-stack\">ELK</a>.</p></div><div class=\"col col--6\"><img class=\"feature-img\" src=\"/img/feature-logs.png\" alt=\"Log Collection\"></div></div><div class=\"row featureRow_2ZVN\"><div class=\"col col--6\"><p>Kalm abides by Kubernetes standards and tries to avoid platform specific dependencies. Kalm has been tested on:</p><ul><li>Amazon EKS</li><li>Google GKE</li><li>Azure AKS</li><li>Digital Ocean Kubernetes</li><li>Linode Kubernetes Engine</li><li>k3s raspberry pi</li><li>Minikube</li></ul> </div><div class=\"col col--6\"><img class=\"feature-img\" src=\"/img/feature-clusters.png\" alt=\"Works With Any Kubernetes Cluster\"></div></div><div class=\"row featureRow_2ZVN\"><div class=\"col col--6\"><p>Kalm utilizes <a href=\"https://github.com/dexidp/dex\">dex</a>, which lets you use your existing team authentication system(i.e Github, Gitlab, Auth0) to control access to applications running on your Kubernetes cluster. Kalm supports RBAC mode and application-level access control.</p></div><div class=\"col col--6\"><img class=\"feature-img\" src=\"/img/feature-sso.png\" alt=\"Built-in Single Sign-On\"></div></div></div></div>",
      "contentAsText": "Get what you want out of Kubernetes without having to write and maintain a ton of custom tooling. Deploy apps, handle requests, and hook up CI/CD, all through an intuitive web interface.Kalm is free, open source, and actively maintained. Kalm simplifies the common workflows related to Kubernetes, including deploying applications, routing, and integrating with your existing pipeline.Kalm works on Google GKE, Amazon EKS, Azure AKS, and most Kubernetes configurations. Take it with you if you decide to migrate someday.Kalm provides an intuitive web interface for core Kubernetes functionalities:Configuring and deploying applicationsManaging ports and container networkingProbes and Auto-HealingScalingMounting VolumesScheduling according to ResourcesKalm supports the Service Mesh Istio out of the box. This gives you full control over traffic entering the cluster. You can setup Request Routing, Error Injection, Mirroring, Traffic Shifting, and more.Want the Heroku-like experience of \"git push, update app\"? Kalm provides webhooks which you can use to invoke deployment updates. In addition, you can generate snippets for popular build tools.Easily obtain and renew HTTPS Certificates via Let's Encrypt. Kalm currently supports specific domain name certificates using http-01 challenge. (Wildcard certificates coming soon)New to Kubernetes and struggling with log collection? Kalm can help you setup a logging solution within minutes. Choose either Loki(PLG stack) or ELK.Kalm abides by Kubernetes standards and tries to avoid platform specific dependencies. Kalm has been tested on:Amazon EKSGoogle GKEAzure AKSDigital Ocean KubernetesLinode Kubernetes Enginek3s raspberry piMinikube Kalm utilizes dex, which lets you use your existing team authentication system(i.e Github, Gitlab, Auth0) to control access to applications running on your Kubernetes cluster. Kalm supports RBAC mode and application-level access control."
    },
    {
      "url": "https://determined.ai/blog/production-training-pipelines-with-determined-and-kubeflow/",
      "title": "Lightning-fast ML pipelines with Determined and Kubeflow",
      "content": "<div class=\"post-container\">\n<center><img src=\"/assets/images/blogs/kubeflow-integration/dai-kf.png\" alt=\"Determined and Kubeflow\"></center>\n<p>As machine learning teams grow, we commonly see the need for \u001cMLOps\u001d emerge: teams need sophisticated automation of their common ML workflows. By the time your team has developed 10+ models, you\u0019ll quickly run out of time to babysit them; manually monitoring, retraining, and redeploying models will use up all of your energy, leaving you no time to iterate on new models or improve existing models.</p>\n<p>In this blog post, we\u0019ll show you how to build an automated model training and deployment pipeline by combining three leading open source tools:</p> <h2 id=\"kubeflow-pipelines-to-the-rescue\">Kubeflow Pipelines to the Rescue</h2>\n<center><img src=\"/assets/images/blogs/kubeflow-integration/kf-logo.jpg\" alt=\"Kubeflow Pipelines help build production workflows\"></center>\n<p><a href=\"https://www.kubeflow.org/docs/pipelines/overview/pipelines-overview/\">Kubeflow Pipelines</a> are designed to make it easier to build production machine learning pipelines. Kubeflow is Kubernetes-native, meaning you can take advantage of the scaling that comes with using Kubernetes. Kubeflow Pipelines are defined using the <a href=\"https://www.kubeflow.org/docs/pipelines/sdk/sdk-overview/\">Kubeflow Pipeline DSL</a> \u0014 making it easy to declare pipelines using the same Python code you\u0019re using to build your ML models.</p>\n<p>You\u0019ll still need a tool to manage the actual training process, as well as to keep track of the artifacts of training. Kubeflow has tools for training (like Katib, TFJob, PyTorchJob, MPIJob), but all of them have significant shortcomings when you\u0019re doing ML at scale. None of them provide integrated experiment or artifact tracking, meaning you\u0019ll need to build a solution to keep track of the metrics and artifacts of training for every model you write. Further they\u0019ll require you to write extensive Kubernetes manifests \u0014 a dive into systems engineering that most data scientists would rather avoid.</p>\n<h2 id=\"determined-for-production-grade-training\">Determined for Production-Grade Training</h2>\n<p><a href=\"https://github.com/determined-ai/determined\">Determined</a> provides a scalable and production-ready model training environment. You can use Determined to configure distributed training by simply changing one line of a configuration file, and every time you train a model all of the artifacts, metrics, and hyperparameters associated with that training job are automatically tracked and programmatically accessible.</p>\n<p>Further, Determined includes a <strong><em><a href=\"https://docs.determined.ai/latest/tutorials/model-registry.html\">Model Registry</a></em></strong>, allowing you to version production-ready models and access those models via a clean API endpoint. The model registry is built to meet the need of production workflows, where having clean APIs to access the newest versions of your models will make the deployment process seamless.</p>\n<h2 id=\"using-determined-with-kubeflow-pipelines\">Using Determined with Kubeflow Pipelines</h2>\n<p>Let\u0019s check out what a production-grade pipeline looks like using Determined and Kubeflow Pipelines:</p>\n<p><img src=\"/assets/images/blogs/kubeflow-integration/pipeline.png\" alt=\"A complete ML pipeline in Kubeflow, using Determined for model training and Seldon for model serving\"></p>\n<p>In this example we will:</p>\n<ol>\n<li>Clone a GitHub repository that defines an ML model and Determined experiment</li>\n<li>Train the model using Determined</li>\n<li>Compare the performance of the newly trained model with the previous version of the model in the Determined model registry</li>\n<li>If the newly trained model is an improvement, update the model registry with the new version</li>\n<li>Deploy the best model to a REST endpoint using <a href=\"https://github.com/SeldonIO/seldon-core\">Seldon Core</a></li>\n</ol>\n<p>This workflow can be easily expanded and customized \u0014 for instance, you can add whatever checks or tests you need at the end of training to ensure a model is ready for production. Some examples are fairness testing, testing on true holdout data, or setting up an A/B deployment for real world testing.</p>\n<h3 id=\"clone-a-model-from-github\">Clone a Model from GitHub</h3>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">def</span> <span class=\"nf\">clone_mlrepo</span><span class=\"p\">(</span><span class=\"n\">repo_url</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">branch</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">volume</span><span class=\"p\">:</span> <span class=\"n\">dsl</span><span class=\"p\">.</span><span class=\"n\">PipelineVolume</span><span class=\"p\">):</span> <span class=\"n\">image</span> <span class=\"o\">=</span> <span class=\"s\">&quot;alpine/git:latest&quot;</span> <span class=\"n\">commands</span> <span class=\"o\">=</span> <span class=\"p\">[</span> <span class=\"s\">f&quot;git clone --single-branch --branch </span><span class=\"si\">{</span><span class=\"n\">branch</span><span class=\"si\">}</span><span class=\"s\"> </span><span class=\"si\">{</span><span class=\"n\">repo_url</span><span class=\"si\">}</span><span class=\"s\"> /src/mlrepo/&quot;</span><span class=\"p\">,</span> <span class=\"s\">f&quot;cd /src/mlrepo/&quot;</span><span class=\"p\">,</span> <span class=\"s\">f&quot;ls&quot;</span><span class=\"p\">,</span> <span class=\"p\">]</span> <span class=\"n\">op</span> <span class=\"o\">=</span> <span class=\"n\">dsl</span><span class=\"p\">.</span><span class=\"n\">ContainerOp</span><span class=\"p\">(</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">&quot;git clone&quot;</span><span class=\"p\">,</span> <span class=\"n\">image</span><span class=\"o\">=</span><span class=\"n\">image</span><span class=\"p\">,</span> <span class=\"n\">command</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">&quot;sh&quot;</span><span class=\"p\">],</span> <span class=\"n\">arguments</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">&quot;-c&quot;</span><span class=\"p\">,</span> <span class=\"s\">&quot; &amp;&amp; &quot;</span><span class=\"p\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">commands</span><span class=\"p\">)],</span> <span class=\"n\">pvolumes</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s\">&quot;/src/&quot;</span><span class=\"p\">:</span> <span class=\"n\">volume</span><span class=\"p\">},</span> <span class=\"p\">)</span> <span class=\"k\">return</span> <span class=\"n\">op</span>\n</code></pre></div></div>\n<p>The first operation clones a Git repository. This repository should define a Determined experiment (both a model and an experiment configuration). You can use any model and experiment configuration you like \u0014 for example, <a href=\"https://github.com/determined-ai/determined/tree/master/examples/computer_vision/fasterrcnn_coco_pytorch\">you could do distributed training of an object detection model with this example</a>.</p>\n<h3 id=\"train-a-model-with-determined\">Train a Model with Determined</h3>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">def</span> <span class=\"nf\">run_det_and_wait</span><span class=\"p\">(</span><span class=\"n\">detmaster</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">config</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">context</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">int</span><span class=\"p\">:</span> <span class=\"c1\"># Submit Determined experiment via CLI\n</span> <span class=\"kn\">import</span> <span class=\"nn\">logging</span> <span class=\"kn\">import</span> <span class=\"nn\">os</span> <span class=\"kn\">import</span> <span class=\"nn\">re</span> <span class=\"kn\">import</span> <span class=\"nn\">subprocess</span> <span class=\"n\">logging</span><span class=\"p\">.</span><span class=\"n\">basicConfig</span><span class=\"p\">(</span><span class=\"n\">level</span><span class=\"o\">=</span><span class=\"n\">logging</span><span class=\"p\">.</span><span class=\"n\">INFO</span><span class=\"p\">)</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s\">&quot;DET_MASTER&quot;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">detmaster</span> <span class=\"n\">repo_dir</span> <span class=\"o\">=</span> <span class=\"s\">&quot;/src/mlrepo/&quot;</span> <span class=\"n\">config</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">path</span><span class=\"p\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">repo_dir</span><span class=\"p\">,</span> <span class=\"n\">config</span><span class=\"p\">)</span> <span class=\"n\">context</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">path</span><span class=\"p\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">repo_dir</span><span class=\"p\">,</span> <span class=\"n\">context</span><span class=\"p\">)</span> <span class=\"n\">cmd</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">&quot;det&quot;</span><span class=\"p\">,</span> <span class=\"s\">&quot;e&quot;</span><span class=\"p\">,</span> <span class=\"s\">&quot;create&quot;</span><span class=\"p\">,</span> <span class=\"n\">config</span><span class=\"p\">,</span> <span class=\"n\">context</span><span class=\"p\">]</span> <span class=\"n\">submit</span> <span class=\"o\">=</span> <span class=\"n\">subprocess</span><span class=\"p\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">cmd</span><span class=\"p\">,</span> <span class=\"n\">capture_output</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span> <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">submit</span><span class=\"p\">.</span><span class=\"n\">stdout</span><span class=\"p\">)</span> <span class=\"n\">experiment_id</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">re</span><span class=\"p\">.</span><span class=\"n\">search</span><span class=\"p\">(</span><span class=\"s\">&quot;Created experiment (\\d+)&quot;</span><span class=\"p\">,</span> <span class=\"n\">output</span><span class=\"p\">)[</span><span class=\"mi\">1</span><span class=\"p\">])</span> <span class=\"n\">logging</span><span class=\"p\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"s\">f&quot;Created experiment </span><span class=\"si\">{</span><span class=\"n\">experiment_id</span><span class=\"si\">}</span><span class=\"s\">&quot;</span><span class=\"p\">)</span> <span class=\"c1\"># Wait for experiment to complete via CLI\n</span> <span class=\"n\">wait</span> <span class=\"o\">=</span> <span class=\"n\">subprocess</span><span class=\"p\">.</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"s\">&quot;det&quot;</span><span class=\"p\">,</span> <span class=\"s\">&quot;e&quot;</span><span class=\"p\">,</span> <span class=\"s\">&quot;wait&quot;</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">experiment_id</span><span class=\"p\">)])</span> <span class=\"n\">logging</span><span class=\"p\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"s\">f&quot;Experiment </span><span class=\"si\">{</span><span class=\"n\">experiment_id</span><span class=\"si\">}</span><span class=\"s\"> completed!&quot;</span><span class=\"p\">)</span> <span class=\"k\">return</span> <span class=\"n\">experiment_id</span> <span class=\"n\">run_det_and_wait_op</span> <span class=\"o\">=</span> <span class=\"n\">func_to_container_op</span><span class=\"p\">(</span> <span class=\"n\">run_det_and_wait</span><span class=\"p\">,</span> <span class=\"n\">base_image</span><span class=\"o\">=</span><span class=\"s\">&quot;davidhershey/detcli:1.9&quot;</span>\n<span class=\"p\">)</span>\n</code></pre></div></div>\n<p>Next we\u0019ll submit that experiment to Determined using the Determined CLI. Here we use the Kubeflow DSL to provide a Python function that submits the experiment, waits for that experiment to finish, and returns the unique ID of the experiment for use in our next step. The Kubeflow DSL then converts that function into a pipeline component.</p>\n<h3 id=\"compare-the-model-to-previous-versions\">Compare the Model to Previous Versions</h3>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">def</span> <span class=\"nf\">decide</span><span class=\"p\">(</span><span class=\"n\">detmaster</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">experiment_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">model_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">bool</span><span class=\"p\">:</span> <span class=\"s\">&quot;&quot;&quot; Compare new model to previous best; if better, save that version and deploy &quot;&quot;&quot;</span> <span class=\"kn\">from</span> <span class=\"nn\">determined.experimental</span> <span class=\"kn\">import</span> <span class=\"n\">Determined</span> <span class=\"kn\">import</span> <span class=\"nn\">os</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">environ</span><span class=\"p\">[</span><span class=\"s\">&apos;DET_MASTER&apos;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">detmaster</span> <span class=\"k\">def</span> <span class=\"nf\">get_validation_metric</span><span class=\"p\">(</span><span class=\"n\">checkpoint</span><span class=\"p\">):</span> <span class=\"n\">config</span> <span class=\"o\">=</span> <span class=\"n\">checkpoint</span><span class=\"p\">.</span><span class=\"n\">experiment_config</span> <span class=\"n\">searcher</span> <span class=\"o\">=</span> <span class=\"n\">config</span><span class=\"p\">[</span><span class=\"s\">&apos;searcher&apos;</span><span class=\"p\">]</span> <span class=\"n\">smaller_is_better</span> <span class=\"o\">=</span> <span class=\"nb\">bool</span><span class=\"p\">(</span><span class=\"n\">searcher</span><span class=\"p\">[</span><span class=\"s\">&apos;smaller_is_better&apos;</span><span class=\"p\">])</span> <span class=\"n\">metric_name</span> <span class=\"o\">=</span> <span class=\"n\">searcher</span><span class=\"p\">[</span><span class=\"s\">&apos;metric&apos;</span><span class=\"p\">]</span> <span class=\"n\">metrics</span> <span class=\"o\">=</span> <span class=\"n\">checkpoint</span><span class=\"p\">.</span><span class=\"n\">validation</span><span class=\"p\">[</span><span class=\"s\">&apos;metrics&apos;</span><span class=\"p\">]</span> <span class=\"n\">metric</span> <span class=\"o\">=</span> <span class=\"n\">metrics</span><span class=\"p\">[</span><span class=\"s\">&apos;validationMetrics&apos;</span><span class=\"p\">][</span><span class=\"n\">metric_name</span><span class=\"p\">]</span> <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"n\">metric</span><span class=\"p\">,</span> <span class=\"n\">smaller_is_better</span><span class=\"p\">)</span> <span class=\"k\">def</span> <span class=\"nf\">is_better</span><span class=\"p\">(</span><span class=\"n\">c1</span><span class=\"p\">,</span> <span class=\"n\">c2</span><span class=\"p\">):</span> <span class=\"n\">m1</span><span class=\"p\">,</span> <span class=\"n\">smaller_is_better</span> <span class=\"o\">=</span> <span class=\"n\">get_validation_metric</span><span class=\"p\">(</span><span class=\"n\">c1</span><span class=\"p\">)</span> <span class=\"n\">m2</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">get_validation_metric</span><span class=\"p\">(</span><span class=\"n\">c2</span><span class=\"p\">)</span> <span class=\"k\">if</span> <span class=\"n\">smaller_is_better</span> <span class=\"ow\">and</span> <span class=\"n\">m1</span> <span class=\"o\">&lt;</span> <span class=\"n\">m2</span><span class=\"p\">:</span> <span class=\"k\">return</span> <span class=\"bp\">True</span> <span class=\"k\">return</span> <span class=\"bp\">False</span> <span class=\"n\">d</span> <span class=\"o\">=</span> <span class=\"n\">Determined</span><span class=\"p\">()</span> <span class=\"n\">checkpoint</span> <span class=\"o\">=</span> <span class=\"n\">d</span><span class=\"p\">.</span><span class=\"n\">get_experiment</span><span class=\"p\">(</span><span class=\"n\">experiment_id</span><span class=\"p\">).</span><span class=\"n\">top_checkpoint</span><span class=\"p\">()</span> <span class=\"k\">try</span><span class=\"p\">:</span> <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">d</span><span class=\"p\">.</span><span class=\"n\">get_model</span><span class=\"p\">(</span><span class=\"n\">model_name</span><span class=\"p\">)</span> <span class=\"k\">except</span><span class=\"p\">:</span> <span class=\"c1\"># Model not yet in registry\n</span> <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">f&apos;Registering new Model: </span><span class=\"si\">{</span><span class=\"n\">model_name</span><span class=\"si\">}</span><span class=\"s\">&apos;</span><span class=\"p\">)</span> <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">d</span><span class=\"p\">.</span><span class=\"n\">create_model</span><span class=\"p\">(</span><span class=\"n\">model_name</span><span class=\"p\">)</span> <span class=\"n\">latest_version</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">get_version</span><span class=\"p\">()</span> <span class=\"k\">if</span> <span class=\"n\">latest_version</span> <span class=\"ow\">is</span> <span class=\"bp\">None</span><span class=\"p\">:</span> <span class=\"n\">better</span> <span class=\"o\">=</span> <span class=\"bp\">True</span> <span class=\"k\">else</span><span class=\"p\">:</span> <span class=\"n\">better</span> <span class=\"o\">=</span> <span class=\"n\">is_better</span><span class=\"p\">(</span><span class=\"n\">latest_version</span><span class=\"p\">,</span> <span class=\"n\">checkpoint</span><span class=\"p\">)</span> <span class=\"k\">if</span> <span class=\"n\">better</span><span class=\"p\">:</span> <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s\">f&apos;Registering new version: </span><span class=\"si\">{</span><span class=\"n\">model_name</span><span class=\"si\">}</span><span class=\"s\">&apos;</span><span class=\"p\">)</span> <span class=\"n\">model</span><span class=\"p\">.</span><span class=\"n\">register_version</span><span class=\"p\">(</span><span class=\"n\">checkpoint</span><span class=\"p\">.</span><span class=\"n\">uuid</span><span class=\"p\">)</span> <span class=\"k\">return</span> <span class=\"n\">better</span> <span class=\"n\">decide_op</span> <span class=\"o\">=</span> <span class=\"n\">func_to_container_op</span><span class=\"p\">(</span> <span class=\"n\">decide</span><span class=\"p\">,</span> <span class=\"n\">base_image</span><span class=\"o\">=</span><span class=\"s\">&quot;davidhershey/detcli:1.9&quot;</span>\n<span class=\"p\">)</span>\n</code></pre></div></div>\n<p>Next we\u0019ll inspect the results of training and compare it to the current version of the model in the <a href=\"https://docs.determined.ai/latest/tutorials/model-registry.html?highlight=registry\">Determined model registry</a>. If the newly trained model is performing better than the version in the registry, we will register a new version of the model, and the model will be deployed in the next step. Otherwise, we\u0019ll print an alert that the model is not performing as well.</p>\n<h3 id=\"deploy-your-model-with-seldon-core\">Deploy Your Model with Seldon Core</h3>\n<p>The final step of the pipeline will be deploying your model with Seldon Core. This requires a bit of work \u0014 you\u0019ll need to create a wrapper container for your model with a <a href=\"https://docs.seldon.io/projects/seldon-core/en/v1.1.0/python/index.html\">Seldon Core language wrapper</a>. Luckily the Determined model registry makes this a lot easier, as you can instantiate a model with just the model\u0019s name. For an example of how to do this, <a href=\"https://github.com/determined-ai/works-with-determined/tree/master/kubeflow_pipelines/seldon/model\">check out this folder</a> which wraps an MNIST model trained with Determined. For the actual pipeline, we\u0019ll create a container operation with the Kubeflow Pipeline DSL:</p>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">def</span> <span class=\"nf\">create_seldon_op</span><span class=\"p\">(</span> <span class=\"n\">detmaster</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">deployment_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">deployment_namespace</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">model_name</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span>\n<span class=\"p\">):</span> <span class=\"n\">command</span> <span class=\"o\">=</span> <span class=\"p\">[</span> <span class=\"s\">&quot;python&quot;</span><span class=\"p\">,</span> <span class=\"s\">&quot;create_seldon_deployment.py&quot;</span><span class=\"p\">,</span> <span class=\"s\">f&apos;</span><span class=\"si\">{</span><span class=\"n\">deployment_name</span><span class=\"si\">}</span><span class=\"s\">&apos;</span><span class=\"p\">,</span> <span class=\"s\">f&apos;</span><span class=\"si\">{</span><span class=\"n\">deployment_namespace</span><span class=\"si\">}</span><span class=\"s\">&apos;</span><span class=\"p\">,</span> <span class=\"s\">f&apos;</span><span class=\"si\">{</span><span class=\"n\">detmaster</span><span class=\"si\">}</span><span class=\"s\">&apos;</span><span class=\"p\">,</span> <span class=\"s\">f&apos;</span><span class=\"si\">{</span><span class=\"n\">model_name</span><span class=\"si\">}</span><span class=\"s\">&apos;</span><span class=\"p\">,</span> <span class=\"s\">&apos;--image&apos;</span><span class=\"p\">,</span> <span class=\"s\">f&apos;</span><span class=\"si\">{</span><span class=\"n\">image</span><span class=\"si\">}</span><span class=\"s\">&apos;</span><span class=\"p\">,</span> <span class=\"p\">]</span> <span class=\"k\">return</span> <span class=\"n\">dsl</span><span class=\"p\">.</span><span class=\"n\">ContainerOp</span><span class=\"p\">(</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">&apos;Create Seldon Deployment&apos;</span><span class=\"p\">,</span> <span class=\"n\">image</span><span class=\"o\">=</span><span class=\"s\">&apos;davidhershey/seldon-create:1.2&apos;</span><span class=\"p\">,</span> <span class=\"n\">command</span><span class=\"o\">=</span><span class=\"n\">command</span><span class=\"p\">,</span> <span class=\"n\">file_outputs</span><span class=\"o\">=</span><span class=\"p\">{</span> <span class=\"s\">&apos;endpoint&apos;</span><span class=\"p\">:</span> <span class=\"s\">&apos;/tmp/endpoint.txt&apos;</span><span class=\"p\">,</span> <span class=\"p\">}</span> <span class=\"p\">)</span>\n</code></pre></div></div>\n<p>This operation invokes <a href=\"https://github.com/determined-ai/works-with-determined/blob/master/kubeflow_pipelines/seldon/pipeline/create_seldon_deployment.py\">a script we wrote</a> to create a Seldon endpoint from a specific Seldon image and Determined model version. It then writes out the URL of the endpoint that can be used to make predictions.</p>\n<h3 id=\"putting-it-all-together\">Putting it All Together</h3>\n<p>Finally, we\u0019ll compile our pipeline so that we can upload it to Kubeflow:</p>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"o\">@</span><span class=\"n\">dsl</span><span class=\"p\">.</span><span class=\"n\">pipeline</span><span class=\"p\">(</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">&quot;Determined Train and Deploy&quot;</span><span class=\"p\">,</span> <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s\">&quot;Train a model with Determined, deploy the result to Seldon&quot;</span>\n<span class=\"p\">)</span>\n<span class=\"k\">def</span> <span class=\"nf\">det_train_pipeline</span><span class=\"p\">(</span> <span class=\"n\">detmaster</span><span class=\"p\">,</span> <span class=\"n\">mlrepo</span><span class=\"o\">=</span><span class=\"s\">&quot;https://github.com/determined-ai/determined.git&quot;</span><span class=\"p\">,</span> <span class=\"n\">branch</span><span class=\"o\">=</span><span class=\"s\">&quot;0.13.0&quot;</span><span class=\"p\">,</span> <span class=\"n\">config</span><span class=\"o\">=</span><span class=\"s\">&quot;examples/official/trial/mnist_pytorch/const.yaml&quot;</span><span class=\"p\">,</span> <span class=\"n\">context</span><span class=\"o\">=</span><span class=\"s\">&quot;examples/official/trial/mnist_pytorch/&quot;</span><span class=\"p\">,</span> <span class=\"n\">model_name</span><span class=\"o\">=</span><span class=\"s\">&quot;mnist-prod&quot;</span><span class=\"p\">,</span> <span class=\"n\">deployment_name</span><span class=\"o\">=</span><span class=\"s\">&quot;mnist-prod-kf&quot;</span><span class=\"p\">,</span> <span class=\"n\">deployment_namespace</span><span class=\"o\">=</span><span class=\"s\">&quot;david&quot;</span><span class=\"p\">,</span> <span class=\"n\">image</span><span class=\"o\">=</span><span class=\"s\">&quot;davidhershey/seldon-mnist:1.6&quot;</span>\n<span class=\"p\">):</span> <span class=\"n\">volume_op</span> <span class=\"o\">=</span> <span class=\"n\">dsl</span><span class=\"p\">.</span><span class=\"n\">VolumeOp</span><span class=\"p\">(</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">&quot;create pipeline volume&quot;</span><span class=\"p\">,</span> <span class=\"n\">resource_name</span><span class=\"o\">=</span><span class=\"s\">&quot;mlrepo-pvc&quot;</span><span class=\"p\">,</span> <span class=\"n\">modes</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s\">&quot;ReadWriteOnce&quot;</span><span class=\"p\">],</span> <span class=\"n\">size</span><span class=\"o\">=</span><span class=\"s\">&quot;3Gi&quot;</span><span class=\"p\">,</span> <span class=\"p\">)</span> <span class=\"n\">clone</span> <span class=\"o\">=</span> <span class=\"n\">clone_mlrepo</span><span class=\"p\">(</span><span class=\"n\">mlrepo</span><span class=\"p\">,</span> <span class=\"n\">branch</span><span class=\"p\">,</span> <span class=\"n\">volume_op</span><span class=\"p\">.</span><span class=\"n\">volume</span><span class=\"p\">)</span> <span class=\"n\">train</span> <span class=\"o\">=</span> <span class=\"p\">(</span> <span class=\"n\">run_det_and_wait_op</span><span class=\"p\">(</span><span class=\"n\">detmaster</span><span class=\"p\">,</span> <span class=\"n\">config</span><span class=\"p\">,</span> <span class=\"n\">context</span><span class=\"p\">)</span> <span class=\"p\">.</span><span class=\"n\">add_pvolumes</span><span class=\"p\">({</span><span class=\"s\">&quot;/src/&quot;</span><span class=\"p\">:</span> <span class=\"n\">clone</span><span class=\"p\">.</span><span class=\"n\">pvolume</span><span class=\"p\">})</span> <span class=\"p\">.</span><span class=\"n\">after</span><span class=\"p\">(</span><span class=\"n\">clone</span><span class=\"p\">)</span> <span class=\"p\">)</span> <span class=\"n\">decide</span> <span class=\"o\">=</span> <span class=\"n\">decide_op</span><span class=\"p\">(</span><span class=\"n\">detmaster</span><span class=\"p\">,</span> <span class=\"n\">train</span><span class=\"p\">.</span><span class=\"n\">output</span><span class=\"p\">,</span> <span class=\"n\">model_name</span><span class=\"p\">)</span> <span class=\"k\">with</span> <span class=\"n\">dsl</span><span class=\"p\">.</span><span class=\"n\">Condition</span><span class=\"p\">(</span><span class=\"n\">decide</span><span class=\"p\">.</span><span class=\"n\">output</span> <span class=\"o\">==</span> <span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">&quot;Deploy&quot;</span><span class=\"p\">):</span> <span class=\"n\">deploy</span> <span class=\"o\">=</span> <span class=\"n\">create_seldon_op</span><span class=\"p\">(</span> <span class=\"n\">detmaster</span><span class=\"p\">,</span> <span class=\"n\">deployment_name</span><span class=\"p\">,</span> <span class=\"n\">deployment_namespace</span><span class=\"p\">,</span> <span class=\"n\">model_name</span><span class=\"p\">,</span> <span class=\"n\">image</span><span class=\"p\">,</span> <span class=\"p\">)</span> <span class=\"k\">with</span> <span class=\"n\">dsl</span><span class=\"p\">.</span><span class=\"n\">Condition</span><span class=\"p\">(</span><span class=\"n\">decide</span><span class=\"p\">.</span><span class=\"n\">output</span> <span class=\"o\">==</span> <span class=\"bp\">False</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s\">&quot;No-Deploy&quot;</span><span class=\"p\">):</span> <span class=\"n\">print_op</span><span class=\"p\">(</span><span class=\"s\">&apos;Model Not Deployed&apos;</span><span class=\"p\">)</span> <span class=\"k\">if</span> <span class=\"n\">__name__</span> <span class=\"o\">==</span> <span class=\"s\">&quot;__main__&quot;</span><span class=\"p\">:</span> <span class=\"n\">kfp</span><span class=\"p\">.</span><span class=\"n\">compiler</span><span class=\"p\">.</span><span class=\"n\">Compiler</span><span class=\"p\">().</span><span class=\"nb\">compile</span><span class=\"p\">(</span><span class=\"n\">det_train_pipeline</span><span class=\"p\">,</span> <span class=\"s\">&apos;train_and_deploy.yaml&apos;</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<p>You can invoke this script with Python, which will create a pipeline file called <code class=\"language-plaintext highlighter-rouge\">train_and_deploy.yaml</code>:</p>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>python create_pipeline.py\n</code></pre></div></div>\n<p>Upload that file to Kubeflow by clicking \u001cUpload pipeline\u001d:</p>\n<p><img src=\"/assets/images/blogs/kubeflow-integration/kf-upload.png\" alt=\"Upload your pipeline to Kubeflow\"></p>\n<p>And then create a run with your own inputs:</p>\n<p><img src=\"/assets/images/blogs/kubeflow-integration/kf-inputs.png\" alt=\"Enter your inputs to create a pipeline run\"></p>\n<p>And you have yourself a reusable pipeline that trains a model, tracks and versions the results, and deploys that model to a named endpoint!</p> <p>If you want to learn more about how Determined can help productionize your training pipelines, <a href=\"https://github.com/determined-ai/determined\">check out Determined here</a> and <a href=\"https://join.slack.com/t/determined-community/shared_invite/zt-cnj7802v-KcVbaUrIzQOwmkmY7gP0Ew\">join our community Slack</a> if you have any questions! If you\u0019re curious about more examples of how Determined integrates seamlessly with popular ML ecosystem tools like Pachyderm, DVC, Spark, and Argo, check out <a href=\"https://github.com/determined-ai/works-with-determined\">works-with-determined</a> on GitHub.</p>\n</div>",
      "contentAsText": "\n\nAs machine learning teams grow, we commonly see the need for \u001cMLOps\u001d emerge: teams need sophisticated automation of their common ML workflows. By the time your team has developed 10+ models, you\u0019ll quickly run out of time to babysit them; manually monitoring, retraining, and redeploying models will use up all of your energy, leaving you no time to iterate on new models or improve existing models.\nIn this blog post, we\u0019ll show you how to build an automated model training and deployment pipeline by combining three leading open source tools: Kubeflow Pipelines to the Rescue\n\nKubeflow Pipelines are designed to make it easier to build production machine learning pipelines. Kubeflow is Kubernetes-native, meaning you can take advantage of the scaling that comes with using Kubernetes. Kubeflow Pipelines are defined using the Kubeflow Pipeline DSL \u0014 making it easy to declare pipelines using the same Python code you\u0019re using to build your ML models.\nYou\u0019ll still need a tool to manage the actual training process, as well as to keep track of the artifacts of training. Kubeflow has tools for training (like Katib, TFJob, PyTorchJob, MPIJob), but all of them have significant shortcomings when you\u0019re doing ML at scale. None of them provide integrated experiment or artifact tracking, meaning you\u0019ll need to build a solution to keep track of the metrics and artifacts of training for every model you write. Further they\u0019ll require you to write extensive Kubernetes manifests \u0014 a dive into systems engineering that most data scientists would rather avoid.\nDetermined for Production-Grade Training\nDetermined provides a scalable and production-ready model training environment. You can use Determined to configure distributed training by simply changing one line of a configuration file, and every time you train a model all of the artifacts, metrics, and hyperparameters associated with that training job are automatically tracked and programmatically accessible.\nFurther, Determined includes a Model Registry, allowing you to version production-ready models and access those models via a clean API endpoint. The model registry is built to meet the need of production workflows, where having clean APIs to access the newest versions of your models will make the deployment process seamless.\nUsing Determined with Kubeflow Pipelines\nLet\u0019s check out what a production-grade pipeline looks like using Determined and Kubeflow Pipelines:\n\nIn this example we will:\n\nClone a GitHub repository that defines an ML model and Determined experiment\nTrain the model using Determined\nCompare the performance of the newly trained model with the previous version of the model in the Determined model registry\nIf the newly trained model is an improvement, update the model registry with the new version\nDeploy the best model to a REST endpoint using Seldon Core\n\nThis workflow can be easily expanded and customized \u0014 for instance, you can add whatever checks or tests you need at the end of training to ensure a model is ready for production. Some examples are fairness testing, testing on true holdout data, or setting up an A/B deployment for real world testing.\nClone a Model from GitHub\ndef clone_mlrepo(repo_url: str, branch: str, volume: dsl.PipelineVolume): image = \"alpine/git:latest\" commands = [ f\"git clone --single-branch --branch {branch} {repo_url} /src/mlrepo/\", f\"cd /src/mlrepo/\", f\"ls\", ] op = dsl.ContainerOp( name=\"git clone\", image=image, command=[\"sh\"], arguments=[\"-c\", \" && \".join(commands)], pvolumes={\"/src/\": volume}, ) return op\n\nThe first operation clones a Git repository. This repository should define a Determined experiment (both a model and an experiment configuration). You can use any model and experiment configuration you like \u0014 for example, you could do distributed training of an object detection model with this example.\nTrain a Model with Determined\ndef run_det_and_wait(detmaster: str, config: str, context: str) -> int: # Submit Determined experiment via CLI\n import logging import os import re import subprocess logging.basicConfig(level=logging.INFO) os.environ[\"DET_MASTER\"] = detmaster repo_dir = \"/src/mlrepo/\" config = os.path.join(repo_dir, config) context = os.path.join(repo_dir, context) cmd = [\"det\", \"e\", \"create\", config, context] submit = subprocess.run(cmd, capture_output=True) output = str(submit.stdout) experiment_id = int(re.search(\"Created experiment (\\d+)\", output)[1]) logging.info(f\"Created experiment {experiment_id}\") # Wait for experiment to complete via CLI\n wait = subprocess.run([\"det\", \"e\", \"wait\", str(experiment_id)]) logging.info(f\"Experiment {experiment_id} completed!\") return experiment_id run_det_and_wait_op = func_to_container_op( run_det_and_wait, base_image=\"davidhershey/detcli:1.9\"\n)\n\nNext we\u0019ll submit that experiment to Determined using the Determined CLI. Here we use the Kubeflow DSL to provide a Python function that submits the experiment, waits for that experiment to finish, and returns the unique ID of the experiment for use in our next step. The Kubeflow DSL then converts that function into a pipeline component.\nCompare the Model to Previous Versions\ndef decide(detmaster: str, experiment_id: int, model_name: str) -> bool: \"\"\" Compare new model to previous best; if better, save that version and deploy \"\"\" from determined.experimental import Determined import os os.environ['DET_MASTER'] = detmaster def get_validation_metric(checkpoint): config = checkpoint.experiment_config searcher = config['searcher'] smaller_is_better = bool(searcher['smaller_is_better']) metric_name = searcher['metric'] metrics = checkpoint.validation['metrics'] metric = metrics['validationMetrics'][metric_name] return (metric, smaller_is_better) def is_better(c1, c2): m1, smaller_is_better = get_validation_metric(c1) m2, _ = get_validation_metric(c2) if smaller_is_better and m1 < m2: return True return False d = Determined() checkpoint = d.get_experiment(experiment_id).top_checkpoint() try: model = d.get_model(model_name) except: # Model not yet in registry\n print(f'Registering new Model: {model_name}') model = d.create_model(model_name) latest_version = model.get_version() if latest_version is None: better = True else: better = is_better(latest_version, checkpoint) if better: print(f'Registering new version: {model_name}') model.register_version(checkpoint.uuid) return better decide_op = func_to_container_op( decide, base_image=\"davidhershey/detcli:1.9\"\n)\n\nNext we\u0019ll inspect the results of training and compare it to the current version of the model in the Determined model registry. If the newly trained model is performing better than the version in the registry, we will register a new version of the model, and the model will be deployed in the next step. Otherwise, we\u0019ll print an alert that the model is not performing as well.\nDeploy Your Model with Seldon Core\nThe final step of the pipeline will be deploying your model with Seldon Core. This requires a bit of work \u0014 you\u0019ll need to create a wrapper container for your model with a Seldon Core language wrapper. Luckily the Determined model registry makes this a lot easier, as you can instantiate a model with just the model\u0019s name. For an example of how to do this, check out this folder which wraps an MNIST model trained with Determined. For the actual pipeline, we\u0019ll create a container operation with the Kubeflow Pipeline DSL:\ndef create_seldon_op( detmaster: str, deployment_name: str, deployment_namespace: str, model_name: str, image: str,\n): command = [ \"python\", \"create_seldon_deployment.py\", f'{deployment_name}', f'{deployment_namespace}', f'{detmaster}', f'{model_name}', '--image', f'{image}', ] return dsl.ContainerOp( name='Create Seldon Deployment', image='davidhershey/seldon-create:1.2', command=command, file_outputs={ 'endpoint': '/tmp/endpoint.txt', } )\n\nThis operation invokes a script we wrote to create a Seldon endpoint from a specific Seldon image and Determined model version. It then writes out the URL of the endpoint that can be used to make predictions.\nPutting it All Together\nFinally, we\u0019ll compile our pipeline so that we can upload it to Kubeflow:\n@dsl.pipeline( name=\"Determined Train and Deploy\", description=\"Train a model with Determined, deploy the result to Seldon\"\n)\ndef det_train_pipeline( detmaster, mlrepo=\"https://github.com/determined-ai/determined.git\", branch=\"0.13.0\", config=\"examples/official/trial/mnist_pytorch/const.yaml\", context=\"examples/official/trial/mnist_pytorch/\", model_name=\"mnist-prod\", deployment_name=\"mnist-prod-kf\", deployment_namespace=\"david\", image=\"davidhershey/seldon-mnist:1.6\"\n): volume_op = dsl.VolumeOp( name=\"create pipeline volume\", resource_name=\"mlrepo-pvc\", modes=[\"ReadWriteOnce\"], size=\"3Gi\", ) clone = clone_mlrepo(mlrepo, branch, volume_op.volume) train = ( run_det_and_wait_op(detmaster, config, context) .add_pvolumes({\"/src/\": clone.pvolume}) .after(clone) ) decide = decide_op(detmaster, train.output, model_name) with dsl.Condition(decide.output == True, name=\"Deploy\"): deploy = create_seldon_op( detmaster, deployment_name, deployment_namespace, model_name, image, ) with dsl.Condition(decide.output == False, name=\"No-Deploy\"): print_op('Model Not Deployed') if __name__ == \"__main__\": kfp.compiler.Compiler().compile(det_train_pipeline, 'train_and_deploy.yaml')\n\nYou can invoke this script with Python, which will create a pipeline file called train_and_deploy.yaml:\npython create_pipeline.py\n\nUpload that file to Kubeflow by clicking \u001cUpload pipeline\u001d:\n\nAnd then create a run with your own inputs:\n\nAnd you have yourself a reusable pipeline that trains a model, tracks and versions the results, and deploys that model to a named endpoint! If you want to learn more about how Determined can help productionize your training pipelines, check out Determined here and join our community Slack if you have any questions! If you\u0019re curious about more examples of how Determined integrates seamlessly with popular ML ecosystem tools like Pachyderm, DVC, Spark, and Argo, check out works-with-determined on GitHub.\n",
      "publishedDate": "2020-09-10T00:00:00.000Z",
      "description": "Learn how to do production-grade MLOps with scalable, automated machine learning training and deployment using Determined, Kubeflow Pipelines, and Seldon Core.",
      "ogDescription": "Learn how to do production-grade MLOps with scalable, automated machine learning training and deployment using Determined, Kubeflow Pipelines, and Seldon Core."
    },
    {
      "url": "https://k8syaml.com/",
      "title": "Kubernetes YAML Generator",
      "content": "<div class=\"fatal-error\"> <strong>JavaScript must be enabled</strong> <p> You must enable JavaScript in your browser for Octopus Deploy to work. Here are the <a href=\"https://www.enable-javascript.com/\"> instructions how to enable JavaScript in your web browser</a>. </p> </div>",
      "contentAsText": " JavaScript must be enabled  You must enable JavaScript in your browser for Octopus Deploy to work. Here are the  instructions how to enable JavaScript in your web browser.  "
    },
    {
      "url": "https://com2kube.io/",
      "title": "No title"
    },
    {
      "url": "https://com2kube.io",
      "title": "No title"
    },
    {
      "url": "https://william-yeh.net/post/2019/06/autoreload-from-configmap/",
      "title": "Auto-Reload from ConfigMap",
      "content": "<div class=\"post-content\"> <p>My previous article &#x201C;<a href=\"//william-yeh.net/post/2019/06/inotify-in-containers/\">Inotify in Containers</a>&#x201D; has demonstrated that when ConfigMap is mounted as directories, any changes in the ConfigMap will propagate to related pods, and can be detected with <code>inotify</code>-like APIs.</p>\n<p>A follow-up question might be: what should a well-behaved application react to this trigger accordingly? What if it&#x2019;s a ill-designed application?</p>\n<p>To clarify this I&#x2019;ve conducted a series of experiments for 3 possible configmap-reloading strategies:</p>\n<ul>\n<li>Built-in auto-reloading apps</li>\n<li>External signals</li>\n<li>Pod rollout</li>\n</ul>\n<p>In this article I&#x2019;m going to explain the experiments and preliminary findings. All experiment materials are available in the <a href=\"https://github.com/William-Yeh/configmap-auto-reload\">configmap-auto-reload</a> repo.</p>\n<h2 id=\"built-in-auto-reloading-apps\">Built-in auto-reloading apps</h2>\n<p>Some applications (e.g., <a href=\"https://traefik.io/\">Traefik</a>) are smart enough to gracefully reload themselves whenever they detect any configuration changes without downtime. Will this work with Kubernetes ConfigMap?</p>\n<p>See the <a href=\"https://github.com/William-Yeh/configmap-auto-reload/tree/master/traefik-example\">traefik-example</a> demo:</p>\n<p><a href=\"https://asciinema.org/a/251179\"><img src=\"https://asciinema.org/a/251179.svg\" alt=\"asciicast\"></a></p>\n<p>Perfect! Traefik auto-reloads itself as long as you correctly mount the <code>traefik-config</code> ConfigMap as <code>/etc/traefik/</code> directory for the pod. Here&#x2019;s the related <a href=\"https://github.com/William-Yeh/configmap-auto-reload/blob/master/traefik-example/traefik-config.yml#L11-L13\">code snippet</a>:</p>\n<div class=\"highlight\"><pre class=\"chroma\"><code class=\"language-plaintext\">[file]\nwatch = true\ndirectory = &quot;/etc/traefik/&quot;</code></pre></div>\n<h2 id=\"external-signals\">External signals</h2>\n<p>Some applications can <em>reload</em> configurations; but not <em>auto-reload</em>. Instead, they reload their configurations when they are <em>told</em> to do so. For example, when Nginx receives a <code>HUP</code> signal (<code>nginx -s reload</code>) <sup id=\"fnref:1\"><a href=\"#fn:1\" class=\"footnote-ref\">1</a></sup>, and when Apache HTTP Server receives a <code>HUP</code> signal (<code>apache -k restart</code>) or <code>USR1</code> signal (<code>apache -k graceful</code>) <sup id=\"fnref:2\"><a href=\"#fn:2\" class=\"footnote-ref\">2</a></sup>, they will reload new configurations without downtime.</p>\n<p>Who should be the <code>HUP</code> signal sender in Kubernetes?</p>\n<p>Before Docker and Kubernetes rule the world, there were plenty of such tools, e.g., <a href=\"https://github.com/rvoicilas/inotify-tools\">inotify-tools</a> and <a href=\"https://github.com/kimmobrunfeldt/chokidar-cli\">Chokidar cli</a>. People used them to watch for changes in specified directories and to invoke dedicated actions accordingly (including sending signals, of course).</p> <p class=\"box\">\n<figure> <div class=\"img\"> <img src=\"/img/2019/06/inotifywait.png\" alt=\"Combo trick: Inotifywait + Nginx\"> </div> <a href=\"/img/2019/06/inotifywait.png\"></a> <figcaption> <p>Combo trick: Inotifywait + Nginx</p> </figcaption> </figure>\n</p> <p>Will this combo trick work with Kubernetes ConfigMap?</p>\n<p>See the <a href=\"https://github.com/William-Yeh/configmap-auto-reload/tree/master/inotifywait-example\">inotifywait-example</a> demo:</p>\n<p><a href=\"https://asciinema.org/a/251666\"><img src=\"https://asciinema.org/a/251666.svg\" alt=\"asciicast\"></a></p>\n<p>Good! Inotifywait detects the changes and sends <code>HUP</code> signals to Nginx as long as you correctly mount the <code>nginx-config</code> ConfigMap as <code>/etc/nginx/</code> directory for the pod. Here&#x2019;s the related <a href=\"https://github.com/William-Yeh/configmap-auto-reload/blob/master/inotifywait-example/watch-nginx.sh#L13-L22\">code snippet</a>:</p>\n<div class=\"highlight\"><pre class=\"chroma\"><code class=\"language-plaintext\">if [[ &quot;$(inotifywatch -e modify,create,delete,move -t 15 /etc/nginx/ 2&gt;&amp;1)&quot; =~ filename ]]; then\n    echo &quot;Try to verify updated nginx config...&quot;\n    nginx -t\n    if [ $? -ne 0 ]; then\n        echo &quot;ERROR: New configuration is invalid!!&quot;\n    else\n        echo &quot;Reloading nginx with new config...&quot;\n        nginx -s reload\n    fi\nfi;</code></pre></div>\n<p>DISCLAIMER: it&#x2019;s just for demo; not a robust implementation. For more examples, see <sup id=\"fnref:3\"><a href=\"#fn:3\" class=\"footnote-ref\">3</a></sup><sup id=\"fnref:4\"><a href=\"#fn:4\" class=\"footnote-ref\">4</a></sup><sup id=\"fnref:5\"><a href=\"#fn:5\" class=\"footnote-ref\">5</a></sup>.</p>\n<p>CAUTION: it is against the best practice of &#x201C;one process per container&#x201D; policy.<sup id=\"fnref:6\"><a href=\"#fn:6\" class=\"footnote-ref\">6</a></sup> If you really want to use this combo trick, try to model it as &#x201C;multiple containers within a single pod.&#x201D;</p>\n<h2 id=\"pod-rollout\">Pod rollout</h2>\n<p>Some applications do not have any configuration reloading mechanism. What should we do? Maybe the only reasonable way is to rollout their running instances, and just spawn new ones with the new configurations.</p>\n<p><a href=\"https://github.com/stakater/Reloader\">Reloader</a> is a generic solution for Kubernetes. With the help of it, pods can be restarted whenever related ConfigMap has changed.</p>\n<p>See the <a href=\"https://github.com/William-Yeh/configmap-auto-reload/tree/master/reloader-example\">reloader-example</a>:</p>\n<p><a href=\"https://asciinema.org/a/251670\"><img src=\"https://asciinema.org/a/251670.svg\" alt=\"asciicast\"></a></p>\n<p>Perfect! Nginx pods get rolling updated by Reloader as long as you annotate the Nginx deployment with <code>configmap.reloader.stakater.com/reload</code>. Here&#x2019;s the related <a href=\"https://github.com/William-Yeh/configmap-auto-reload/blob/master/reloader-example/nginx-service.yml#L25-L26\">code snippet</a>:</p>\n<div class=\"highlight\"><pre class=\"chroma\"><code class=\"language-plaintext\">apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  annotations:\n    configmap.reloader.stakater.com/reload: &quot;nginx-config&quot;\n  ...</code></pre></div>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>If your application is smart enough to gracefully reload itself whenever it detects any configuration changes, it continues working well with ConfigMap in Kubernetes.</p>\n<p>If not so smart, an easier approach is to use automatic tools (e.g., <a href=\"https://github.com/stakater/Reloader\">Reloader</a>) to rolling update related pods.</p>\n<p>I will not recommend the watch+signal approach (e.g., <a href=\"https://github.com/rvoicilas/inotify-tools\">inotify-tools</a>). It is prone to error and zombie processes.</p> </div>",
      "contentAsText": " My previous article “Inotify in Containers” has demonstrated that when ConfigMap is mounted as directories, any changes in the ConfigMap will propagate to related pods, and can be detected with inotify-like APIs.\nA follow-up question might be: what should a well-behaved application react to this trigger accordingly? What if it’s a ill-designed application?\nTo clarify this I’ve conducted a series of experiments for 3 possible configmap-reloading strategies:\n\nBuilt-in auto-reloading apps\nExternal signals\nPod rollout\n\nIn this article I’m going to explain the experiments and preliminary findings. All experiment materials are available in the configmap-auto-reload repo.\nBuilt-in auto-reloading apps\nSome applications (e.g., Traefik) are smart enough to gracefully reload themselves whenever they detect any configuration changes without downtime. Will this work with Kubernetes ConfigMap?\nSee the traefik-example demo:\n\nPerfect! Traefik auto-reloads itself as long as you correctly mount the traefik-config ConfigMap as /etc/traefik/ directory for the pod. Here’s the related code snippet:\n[file]\nwatch = true\ndirectory = \"/etc/traefik/\"\nExternal signals\nSome applications can reload configurations; but not auto-reload. Instead, they reload their configurations when they are told to do so. For example, when Nginx receives a HUP signal (nginx -s reload) 1, and when Apache HTTP Server receives a HUP signal (apache -k restart) or USR1 signal (apache -k graceful) 2, they will reload new configurations without downtime.\nWho should be the HUP signal sender in Kubernetes?\nBefore Docker and Kubernetes rule the world, there were plenty of such tools, e.g., inotify-tools and Chokidar cli. People used them to watch for changes in specified directories and to invoke dedicated actions accordingly (including sending signals, of course). \n      Combo trick: Inotifywait + Nginx  \n Will this combo trick work with Kubernetes ConfigMap?\nSee the inotifywait-example demo:\n\nGood! Inotifywait detects the changes and sends HUP signals to Nginx as long as you correctly mount the nginx-config ConfigMap as /etc/nginx/ directory for the pod. Here’s the related code snippet:\nif [[ \"$(inotifywatch -e modify,create,delete,move -t 15 /etc/nginx/ 2>&1)\" =~ filename ]]; then\n    echo \"Try to verify updated nginx config...\"\n    nginx -t\n    if [ $? -ne 0 ]; then\n        echo \"ERROR: New configuration is invalid!!\"\n    else\n        echo \"Reloading nginx with new config...\"\n        nginx -s reload\n    fi\nfi;\nDISCLAIMER: it’s just for demo; not a robust implementation. For more examples, see 345.\nCAUTION: it is against the best practice of “one process per container” policy.6 If you really want to use this combo trick, try to model it as “multiple containers within a single pod.”\nPod rollout\nSome applications do not have any configuration reloading mechanism. What should we do? Maybe the only reasonable way is to rollout their running instances, and just spawn new ones with the new configurations.\nReloader is a generic solution for Kubernetes. With the help of it, pods can be restarted whenever related ConfigMap has changed.\nSee the reloader-example:\n\nPerfect! Nginx pods get rolling updated by Reloader as long as you annotate the Nginx deployment with configmap.reloader.stakater.com/reload. Here’s the related code snippet:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  annotations:\n    configmap.reloader.stakater.com/reload: \"nginx-config\"\n  ...\nConclusion\nIf your application is smart enough to gracefully reload itself whenever it detects any configuration changes, it continues working well with ConfigMap in Kubernetes.\nIf not so smart, an easier approach is to use automatic tools (e.g., Reloader) to rolling update related pods.\nI will not recommend the watch+signal approach (e.g., inotify-tools). It is prone to error and zombie processes. ",
      "publishedDate": "2019-06-17T09:00:00.000Z",
      "description": "My previous article &ldquo;Inotify in Containers&rdquo; has demonstrated that when ConfigMap is mounted as directories, any changes in the ConfigMap will propagate to related pods, and can be detected with inotify-like APIs.\nA follow-up question might be: what should a well-behaved application react to this trigger accordingly? What if it&rsquo;s a ill-designed application?\nTo clarify this I&rsquo;ve conducted a series of experiments for 3 possible configmap-reloading strategies:\n Built-in auto-reloading apps External signals Pod rollout  In this article I&rsquo;m going to explain the experiments and preliminary findings.",
      "ogDescription": "My previous article “Inotify in Containers” has demonstrated that when ConfigMap is mounted as directories, any changes in the ConfigMap will propagate to related pods, and can be detected with inotify-like APIs.\nA follow-up question might be: what should a well-behaved application react to this trigger accordingly? What if it’s a ill-designed application?\nTo clarify this I’ve conducted a series of experiments for 3 possible configmap-reloading strategies:\n Built-in auto-reloading apps External signals Pod rollout  In this article I’m going to explain the experiments and preliminary findings."
    },
    {
      "url": "https://www.callumpember.com/Kubernetes-A-Single-OAuth2-Proxy-For-Multiple-Ingresses/",
      "title": "Kubernetes: A single OAuth2 proxy for multiple ingresses",
      "content": "<div><section class=\"post\"> <p class=\"time\"> <time>15 Mar 2019</time> </p> <article id=\"scroll\" class=\"post-content\"> <p>One of the problems most Kubernetes administrators will eventually face is protecting an Ingress from public access. There are a number of ways to do this, including IP whitelisting, TLS authentication, use an internal only service for the ingress controller, and many more.</p> <p>One of my favorite ways is to use <a href=\"https://github.com/pusher/oauth2_proxy\">oauth2_proxy</a>. I\u0019ve used it a number of times over the years, but there was always a drawback that bothered me - with the <a href=\"https://github.com/kubernetes/ingress-nginx/tree/master/docs/examples/auth/oauth-external-auth\">documented setup</a> and other countless examples online, they use a deployment of the oauth2_proxy container per deployment/service/ingress that the user is wanting to protect. Although the resource footprint of oauth2_proxy is small, that is needless waste.</p> <p>Now, onto some oauth2_proxy details. You can use various different providers, like GitHub, Google, GitLab, LinkedIn, Azure and Facebook. What\u0019s one thing nearly every developer in the world has in common? They almost certainly have a GitHub account, so that\u0019s what I use as a provider normally. There are some strict rules though around GitHub OAuth2 and redirection:</p> <p><img src=\"/assets/attachments/github-oauth2/github-oauth-redirection-rules.png\" alt=\"GitHub OAuth2 Redirection rules\"></p> <p>What this means is that if you are using oauth2_proxy as-is, you need a separate deployment for each domain you want to secure.</p> <blockquote> <p>But I want to secure https://prometheus.mydomain.com, https://grafana.mydomain.com and https://alertmanager.mydomain.com. So I need separate deployments of oauth2_proxy for that?</p>\n</blockquote> <p>Out of the box, sadly yes. But with a slight modification of the deployment, we can use a single oauth2_proxy instance for any domain we want. To do this we do the following:</p> <ul> <li>Attach an nginx sidecar container to the oauth2_proxy deployment. This container will redirect to anything after <code class=\"highlighter-rouge\">/redirect/</code> in the request URI.</li> <li>Make the oauth2_proxy have it\u0019s own domain</li> <li>Add an upstream to oauth2_proxy for the /redirect path</li> <li>Set the cookie domain in oauth2_proxy to include all subdomains</li> <li>Setup a GitHub OAuth2 app and point it at the oauth2_proxy domain</li> <li>In the ingresses that we want to protect, use the following annotations (replace $DNS_ZONE_INTERNAL with your own domain):</li>\n</ul> <figure class=\"highlight\"><pre><code class=\"language-yaml\"><p class=\"nn\">---</p>\n<p class=\"s\">nginx.ingress.kubernetes.io/auth-url</p><p class=\"pi\">:</p> <p class=\"s2\">&quot;</p><p class=\"s\">https://oauth2.$DNS_ZONE_INTERNAL/oauth2/auth&quot;</p>\n<p class=\"s\">nginx.ingress.kubernetes.io/auth-signin</p><p class=\"pi\">:</p> <p class=\"s2\">&quot;</p><p class=\"s\">https://oauth2.$DNS_ZONE_INTERNAL/oauth2/start?rd=/redirect/$http_host$request_uri&quot;</p></code></pre></figure> <h3 id=\"oauth2_proxy-deployment\">oauth2_proxy deployment:</h3> <p>Here is a full, working (at least in my cluster) deployment spec for oauth2_proxy with the nginx sidecar.</p> <p>If you want to use it, you\u0019d need to replace all the variables. I personally use envsubst in my deployment pipelines for this. The variables that need replacing are <code class=\"highlighter-rouge\">$DNS_ZONE_INTERNAL</code> <code class=\"highlighter-rouge\">$OAUTH2_CLIENT_ID</code> <code class=\"highlighter-rouge\">$OAUTH2_CLIENT_SECRET</code> and you would want to set your GitHub org.</p> <p><a href=\"/assets/attachments/github-oauth2/full-deployment.yml.txt\">Full Deployment Spec</a></p> <h3 id=\"ingress-example\">Ingress example</h3> <p>Once you\u0019ve got the deployment above working, you can protect an ingress like so (key takeaways are the annotations):</p> <figure class=\"highlight\"><pre><code class=\"language-yaml\"><p class=\"nn\">---</p> <p class=\"na\">apiVersion</p><p class=\"pi\">:</p> <p class=\"s\">extensions/v1beta1</p>\n<p class=\"na\">kind</p><p class=\"pi\">:</p> <p class=\"s\">Ingress</p>\n<p class=\"na\">metadata</p><p class=\"pi\">:</p> <p class=\"na\">name</p><p class=\"pi\">:</p> <p class=\"s\">prometheus</p> <p class=\"na\">namespace</p><p class=\"pi\">:</p> <p class=\"s\">istio-system</p> <p class=\"na\">labels</p><p class=\"pi\">:</p> <p class=\"na\">app</p><p class=\"pi\">:</p> <p class=\"s\">prometheus</p> <p class=\"na\">annotations</p><p class=\"pi\">:</p> <p class=\"s\">kubernetes.io/ingress.class</p><p class=\"pi\">:</p> <p class=\"s2\">&quot;</p><p class=\"s\">nginx-public&quot;</p> <p class=\"s\">nginx.ingress.kubernetes.io/auth-url</p><p class=\"pi\">:</p> <p class=\"s2\">&quot;</p><p class=\"s\">https://oauth2.$DNS_ZONE_INTERNAL/oauth2/auth&quot;</p> <p class=\"s\">nginx.ingress.kubernetes.io/auth-signin</p><p class=\"pi\">:</p> <p class=\"s2\">&quot;</p><p class=\"s\">https://oauth2.$DNS_ZONE_INTERNAL/oauth2/start?rd=/redirect/$http_host$escaped_request_uri&quot;</p>\n<p class=\"na\">spec</p><p class=\"pi\">:</p> <p class=\"na\">rules</p><p class=\"pi\">:</p> <p class=\"pi\">-</p> <p class=\"na\">host</p><p class=\"pi\">:</p> <p class=\"s2\">&quot;</p><p class=\"s\">prometheus.$DNS_ZONE_INTERNAL&quot;</p> <p class=\"na\">http</p><p class=\"pi\">:</p> <p class=\"na\">paths</p><p class=\"pi\">:</p> <p class=\"pi\">-</p> <p class=\"na\">path</p><p class=\"pi\">:</p> <p class=\"s\">/</p> <p class=\"na\">backend</p><p class=\"pi\">:</p> <p class=\"na\">serviceName</p><p class=\"pi\">:</p> <p class=\"s\">prometheus</p> <p class=\"na\">servicePort</p><p class=\"pi\">:</p> <p class=\"s\">http</p></code></pre></figure> <p>Hopefully this saves you some resources in your cluster and some time creating multiple oauth2_proxy deployments!</p> </article> <footer> <p>Made with <span id=\"heart\">=&#xFFFD;</span></p> <p>As an Amazon Associate I earn from qualifying purchases.</p> </footer> </section></div>",
      "contentAsText": "  15 Mar 2019   One of the problems most Kubernetes administrators will eventually face is protecting an Ingress from public access. There are a number of ways to do this, including IP whitelisting, TLS authentication, use an internal only service for the ingress controller, and many more. One of my favorite ways is to use oauth2_proxy. I\u0019ve used it a number of times over the years, but there was always a drawback that bothered me - with the documented setup and other countless examples online, they use a deployment of the oauth2_proxy container per deployment/service/ingress that the user is wanting to protect. Although the resource footprint of oauth2_proxy is small, that is needless waste. Now, onto some oauth2_proxy details. You can use various different providers, like GitHub, Google, GitLab, LinkedIn, Azure and Facebook. What\u0019s one thing nearly every developer in the world has in common? They almost certainly have a GitHub account, so that\u0019s what I use as a provider normally. There are some strict rules though around GitHub OAuth2 and redirection:  What this means is that if you are using oauth2_proxy as-is, you need a separate deployment for each domain you want to secure.  But I want to secure https://prometheus.mydomain.com, https://grafana.mydomain.com and https://alertmanager.mydomain.com. So I need separate deployments of oauth2_proxy for that?\n Out of the box, sadly yes. But with a slight modification of the deployment, we can use a single oauth2_proxy instance for any domain we want. To do this we do the following:  Attach an nginx sidecar container to the oauth2_proxy deployment. This container will redirect to anything after /redirect/ in the request URI. Make the oauth2_proxy have it\u0019s own domain Add an upstream to oauth2_proxy for the /redirect path Set the cookie domain in oauth2_proxy to include all subdomains Setup a GitHub OAuth2 app and point it at the oauth2_proxy domain In the ingresses that we want to protect, use the following annotations (replace $DNS_ZONE_INTERNAL with your own domain):\n ---\nnginx.ingress.kubernetes.io/auth-url: \"https://oauth2.$DNS_ZONE_INTERNAL/oauth2/auth\"\nnginx.ingress.kubernetes.io/auth-signin: \"https://oauth2.$DNS_ZONE_INTERNAL/oauth2/start?rd=/redirect/$http_host$request_uri\" oauth2_proxy deployment: Here is a full, working (at least in my cluster) deployment spec for oauth2_proxy with the nginx sidecar. If you want to use it, you\u0019d need to replace all the variables. I personally use envsubst in my deployment pipelines for this. The variables that need replacing are $DNS_ZONE_INTERNAL $OAUTH2_CLIENT_ID $OAUTH2_CLIENT_SECRET and you would want to set your GitHub org. Full Deployment Spec Ingress example Once you\u0019ve got the deployment above working, you can protect an ingress like so (key takeaways are the annotations): --- apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata: name: prometheus namespace: istio-system labels: app: prometheus annotations: kubernetes.io/ingress.class: \"nginx-public\" nginx.ingress.kubernetes.io/auth-url: \"https://oauth2.$DNS_ZONE_INTERNAL/oauth2/auth\" nginx.ingress.kubernetes.io/auth-signin: \"https://oauth2.$DNS_ZONE_INTERNAL/oauth2/start?rd=/redirect/$http_host$escaped_request_uri\"\nspec: rules: - host: \"prometheus.$DNS_ZONE_INTERNAL\" http: paths: - path: / backend: serviceName: prometheus servicePort: http Hopefully this saves you some resources in your cluster and some time creating multiple oauth2_proxy deployments!   Made with =� As an Amazon Associate I earn from qualifying purchases.  ",
      "description": "Use a single OAuth2 proxy (via GitHub) to protect multiple ingresses",
      "ogDescription": "Use a single OAuth2 proxy (via GitHub) to protect multiple ingresses"
    },
    {
      "url": "https://mbbaig.blog/deploying-applications-to-kubernetes-the-proper-way/",
      "title": "Deploying applications to kubernetes the proper way",
      "content": "<div class=\"post-content\"> <p>In a <a href=\"https://mbbaig.blog/how-to-create-custom-helm-charts/\">previous post</a> I mentioned that would share the proper method of deploying Helm applications with an ingress. This post will detail the process of creating the ingress and using it for some common operations that would be required of a web server.</p><ol><li><code>kubectl</code> CLI v1.15+</li><li><code>helm</code> CLI v3.0+</li></ol><h2 id=\"two-general-practices\">Two general practices</h2><p>I follow some best practices that generally make a cluster more robust. I&apos;m not sure if these industry standards, but they seem to make sense to I implement them in the clusters I maintain.</p><p>First is create <em>namespaces</em> to logically separate the applications being deployed. I generally use namepsaces for the dev, staging, and live environments within my clusters. Certain objects in Kubernetes can be scoped to a namespace which is the next reason I use namespaces in this way.</p><p>Second, create an ingress per publicly accessible namespace. This helps make the whole cluster more robust. One global ingress has the disadvantage of negatively affecting every public facing application on the cluster if it malfunctions. A namespaced ingress means only the applications in that namespace are affected if that ingress malfunctions. The disadvantage here is that, if you have many namespaces, you&apos;ll have to maintain many ingresses. </p><h2 id=\"creating-namespaces\">Creating namespaces</h2><p>Creating namespaces is super easy. We only need to the <code>kubectl</code> CLI and one simple command to create one namespace. We use the following command to create the namespace</p><pre><code class=\"language-sh\">kubectl create namespace dev</code></pre><p>If you like the content I create, please <a href=\"https://mbbaig.blog/signup/\">subscribe</a> to my blog. I&apos;m working other highly specialized content like this and some general content like reviews and opinions.</p><h2 id=\"creating-namespaced-ingress\">Creating namespaced ingress</h2><p>Now let&apos;s use <code>helm</code> to create an ingress that is specific to the namespace that we created earlier. I will using the Kubernetes maintained <strong><a href=\"https://hub.helm.sh/charts/ingress-nginx/ingress-nginx\">ingress-nginx</a>.</strong> It is generally reliable, performant, and provides/exposes the general functionality that is most commonly required. Let&apos;s install it</p><pre><code class=\"language-sh\">helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update\nhelm --namespace=dev install ingress-nginx/ingress-nginx --set=&quot;controller.scope.enabled=true,controller.scope.namespace=dev&quot;</code></pre><p>The above command will install the ingress object on the namespace specified in the <code>--set</code> parameter. This ingress will only be responsible for applications deployed to the <strong>dev</strong> namepsace. It will create a load balancer on whichever cloud platform you&apos;re hosting your cluster on.</p><h2 id=\"using-some-common-functionalities\">Using some common functionalities</h2><p>Now I will go through some common usage of the ingress and modifying it&apos;s behaviour using annotations.</p><p>First you can use the following annotation in your application&apos;s ingress resource to register it on the deployed ingress.</p><pre><code class=\"language-yaml\">kubernetes.io/ingress.class: nginx</code></pre><p>Second, if you want to increase the size of the upload body size.</p><pre><code class=\"language-yaml\">nginx.ingress.kubernetes.io/proxy-body-size: 50m</code></pre><p>Last, if you want to redirect from or to <em>www</em> subdomain of you site. You must create <em>host</em> of you main domain in the your application&apos;s ingress recourse with the base and <em>www</em> subdomain. Then you can enable the redirect behaviour with the following.</p><pre><code class=\"language-yaml\">nginx.ingress.kubernetes.io/from-to-www-redirect: &quot;true&quot;</code></pre><p>There are many more customisations that you can make using <a href=\"https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/\">annotations</a> or even a Kubernetes <strong><a href=\"https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/\">ConfigMap</a></strong>. </p><h2 id=\"support\">Support</h2><p>I will making this content like this and more so please consider supporting me with by <a href=\"https://mbbaig.blog/signup/\">subscribing</a>.</p><p>Happy Hacking :)</p> </div>",
      "contentAsText": " In a previous post I mentioned that would share the proper method of deploying Helm applications with an ingress. This post will detail the process of creating the ingress and using it for some common operations that would be required of a web server.kubectl CLI v1.15+helm CLI v3.0+Two general practicesI follow some best practices that generally make a cluster more robust. I'm not sure if these industry standards, but they seem to make sense to I implement them in the clusters I maintain.First is create namespaces to logically separate the applications being deployed. I generally use namepsaces for the dev, staging, and live environments within my clusters. Certain objects in Kubernetes can be scoped to a namespace which is the next reason I use namespaces in this way.Second, create an ingress per publicly accessible namespace. This helps make the whole cluster more robust. One global ingress has the disadvantage of negatively affecting every public facing application on the cluster if it malfunctions. A namespaced ingress means only the applications in that namespace are affected if that ingress malfunctions. The disadvantage here is that, if you have many namespaces, you'll have to maintain many ingresses. Creating namespacesCreating namespaces is super easy. We only need to the kubectl CLI and one simple command to create one namespace. We use the following command to create the namespacekubectl create namespace devIf you like the content I create, please subscribe to my blog. I'm working other highly specialized content like this and some general content like reviews and opinions.Creating namespaced ingressNow let's use helm to create an ingress that is specific to the namespace that we created earlier. I will using the Kubernetes maintained ingress-nginx. It is generally reliable, performant, and provides/exposes the general functionality that is most commonly required. Let's install ithelm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm repo update\nhelm --namespace=dev install ingress-nginx/ingress-nginx --set=\"controller.scope.enabled=true,controller.scope.namespace=dev\"The above command will install the ingress object on the namespace specified in the --set parameter. This ingress will only be responsible for applications deployed to the dev namepsace. It will create a load balancer on whichever cloud platform you're hosting your cluster on.Using some common functionalitiesNow I will go through some common usage of the ingress and modifying it's behaviour using annotations.First you can use the following annotation in your application's ingress resource to register it on the deployed ingress.kubernetes.io/ingress.class: nginxSecond, if you want to increase the size of the upload body size.nginx.ingress.kubernetes.io/proxy-body-size: 50mLast, if you want to redirect from or to www subdomain of you site. You must create host of you main domain in the your application's ingress recourse with the base and www subdomain. Then you can enable the redirect behaviour with the following.nginx.ingress.kubernetes.io/from-to-www-redirect: \"true\"There are many more customisations that you can make using annotations or even a Kubernetes ConfigMap. SupportI will making this content like this and more so please consider supporting me with by subscribing.Happy Hacking :) ",
      "publishedDate": "2020-09-07T15:05:51.000Z",
      "description": "Deploying apps to Kubernetes with a reverse proxy ingress resource.",
      "ogDescription": "Deploying apps to Kubernetes with a reverse proxy ingress resource."
    },
    {
      "url": "https://iximiuz.com/en/posts/service-proxy-pod-sidecar-oh-my/?utm_medium=reddit&utm_source=r_kubernetes",
      "title": "Service proxy, pod, sidecar, oh my!",
      "content": "<div class=\"article-entry markdown-body\">\n<p><a></a>\nImagine you&apos;re developing a service... For certainty, let&apos;s call it <em>A</em>. It&apos;s going to provide some public HTTP API to its clients. However, to serve requests it needs to call another service. Let&apos;s call this upstream service - <em>B</em>.</p>\n<div> <img src=\"/service-proxy-pod-sidecar-oh-my/10-service-a-service-b.png\" width=\"100%\">\n</div> <p>Obviously, neither network nor service <em>B</em> is ideal. If service <em>A</em> wants to decrease the impact of the failing upstream requests on its public API success rate, it has to do something about errors. For instance, it could start retrying failed requests.\n<a></a></p>\n<div> <img src=\"/service-proxy-pod-sidecar-oh-my/20-service-a-service-b-with-retries.png\" width=\"100%\">\n</div> <p>Implementation of the retry mechanism requires some code changes in the service <em>A</em>, but the codebase is fresh, there are tons of advanced HTTP libraries, so you just need to grab one... Easy-peasy, right?</p>\n<p>Unfortunately, this simplicity is not always the case. Replace service <em>A</em> with service <em>Z</em> that was written 10 years ago in some esoteric language by a developer that already retired. Or add to the equitation services <em>Q</em>, <em>U</em>, and <em>X</em> written by different teams in three different languages. As a result, the cumulative cost of the company-wide retry mechanism implementation in the code gets really high...</p>\n<div> <img src=\"/service-proxy-pod-sidecar-oh-my/30-service-qux-service-b.png\" width=\"100%\">\n</div> <p>But what if retries are not the only thing you need? Proper <em>request timeouts</em> have to be ensured as well. And how about <em>distributed tracing</em>? It&apos;d be nice to correlate the whole request tree with the original customer transaction by propagating some additional HTTP headers. However, every such capability would make the HTTP libraries even more bloated...</p>\n<h2 id=\"a-namemeet-the-solutionameet-the-solution\"><a></a>Meet the solution</h2>\n<p>Let&apos;s try to go one level higher... or lower? &gt;\u0014</p>\n<p>In our original setup, service <em>A</em> has been communicating with service <em>B</em> directly. But what if we put an intermediary infrastructure component in between those services? Thanks to containerization, orchestration, devops, <del>add a buzz word of your choice here</del>, nowadays, it became so simple to configure infrastructure, that the cost of adding another infra component is often lower than the cost of writing application code...</p>\n<div> <img src=\"/service-proxy-pod-sidecar-oh-my/40-service-a-sidecar-service-b.png\" width=\"100%\">\n</div> <p>For the sake of simplicity, let&apos;s call the box enclosing the service <em>A</em> and the secret intermediary component <em>a server</em> (bare metal or virtual, doesn&apos;t really matter). And now it&apos;s about time to introduce one of the fancy words from the article&apos;s title. Any piece of software running on the server <em>alongside</em> the primary service and helping it do its job is called <em>a sidecar</em>. I hope, the idea behind the name is more or less straightforward here.</p>\n<p>But getting back to the service-to-service communication problem, what sidecar should we use to keep the service code free of the low-level details such as retries or request tracing? Well, the needed piece of software is called a <em>service proxy</em>. Probably, the most widely used implementation of the service proxy in the real world is <a href=\"https://www.envoyproxy.io/\">envoy</a>.</p>\n<p>The idea of the service proxy is the following: instead of accessing the service <em>B</em> directly, code in the service <em>A</em> now will be sending requests to the service proxy sidecar. Since both of the processes run on the same server, the loopback network interface (i.e. <code>127.0.0.1</code> <em>aka</em> <code>localhost</code>) is perfectly suitable for this part of the communication. On every received HTTP request, the service proxy sidecar will make a request to the upstream service using the external network interface of the server. The response from the upstream will be eventually forwarded back by the sidecar to the service <em>A</em>.</p>\n<p>I think, at this time, it&apos;s already obvious where the retry, timeouts, tracing, etc. logic should reside. Having this kind of functionality provided by a separate sidecar process makes enhancing any service written in any language with such capabilities rather trivial.</p>\n<p>Interestingly enough, that service proxy could be used not only for outgoing traffic (egress) but also for the incoming traffic (ingress) of the service <em>A</em>. Usually, there is plenty of cross-cutting things that can be tackled on the ingress stage. For instance, proxy sidecars can do <em>SSL</em> termination, request authentication, and more. A detailed diagram of a single server setup could look something like that:</p>\n<div> <img src=\"/service-proxy-pod-sidecar-oh-my/50-single-host-sidecar.png\" width=\"100%\">\n</div> <p>Probably, the last fancy term we are going to cover here is <em>a pod</em>. People have been deploying code using virtual machines or bare metal servers for a long time... A server itself is already a good abstraction and a unit of encapsulation. For instance, every server has at least one external network interface, a network loopback interface for the internal <a href=\"https://en.wikipedia.org/wiki/Inter-process_communication\">IPC</a> needs, and it can run a bunch of processes sharing access to these communication means. Servers are usually addressable within the private network of the company by their IPs. Last but not least, it&apos;s pretty common to use a whole server for a single purpose (otherwise, maintenance quickly becomes a nightmare). I.e. you may have a group of identical servers running instances of service <em>A</em>, another group of servers each running an instance of service <em>B</em>, etc. So, why on earth would anybody want something better than a server?</p>\n<p>Despite being a good abstraction, the orchestration overhead servers introduce is often too high. So people started thinking about how to package applications more efficiently and that&apos;s how we got containers. Well, probably you know that <em>Docker</em> and <em>container</em> had been kind of a synonym for a long time and folks from Docker have been actively advocating for <em>&quot;a one process per container&quot;</em> model. Obviously, this model is pretty different from the widely used <em>server</em> abstraction where multiple processes are allowed to work side by side. And that&apos;s how we got the concept of <em>pods</em>. A pod is just a group of containers sharing a bunch of namespaces. If we now run a single process per container all of the processes in the pod will still share the common execution environment. In particular, the network namespace. Thus, all the containers in the pod will have a shared loopback interface and a shared external interface with an IP address assigned to it. Then it&apos;s up to the orchestration layer (say hi to Kubernetes) how to make all the pods reachable within the network by their IPs. And that&apos;s how people reinvented servers...</p>\n<p>So, getting back to all those blue boxes enclosing the service process and the sidecar on the diagrams above - we can think of them as being either a virtual machine, a bare metal server, or a pod. All three of them are more or less interchangeable abstractions.</p>\n<p>To summarize, let&apos;s try to visualize how the service to service communication could look like with the proxy sidecars:</p>\n<div> <img src=\"/service-proxy-pod-sidecar-oh-my/60-service-to-service-topology.png\" width=\"100%\"> <p><i>Example of service to service communication topology, a.k.a. service mesh.</i></p>\n</div> <h2 id=\"a-nameget-hands-dirtyaget-hands-dirty-practical-part\"><a></a>Get hands dirty (practical part)</h2>\n<p>Since the only way to really understand something is to <del>write a blog post about it</del> implement it yourself, let&apos;s quickly hack a <a href=\"https://github.com/iximiuz/envoy-playground\">demo environment</a>.</p>\n<h4 id=\"a-nameservice-a-talks-to-service-b-directlyaservice-a-talks-to-service-b-directly\"><a></a>Service A talks to service B directly</h4>\n<p>We will start from the simple setup where service <em>A</em> will be accessing service <em>B</em> directly:</p>\n<div> <img src=\"/service-proxy-pod-sidecar-oh-my/70-demo-direct.png\" width=\"100%\">\n</div> <p>The code of the <a href=\"https://github.com/iximiuz/envoy-playground/tree/master/basics/service-a\">service <em>A</em></a> is relatively straightforward. It&apos;s just a simple HTTP server that makes a call to its upstream service <em>B</em> on every client request. Depending on the response from the upstream, <em>A</em> returns either an HTTP 200 or HTTP 500 to the client.</p>\n<pre><code class=\"language-go\">package main\n\n// ...\n\nvar requestCounter = prometheus.NewCounterVec(\n    prometheus.CounterOpts{\n        Name: &quot;service_a_requests_total&quot;,\n        Help: &quot;The total number of requests received by Service A.&quot;,\n    },\n    []string{&quot;status&quot;},\n)\n\nfunc handler(w http.ResponseWriter, r *http.Request) {\n    resp, err := httpGet(os.Getenv(&quot;UPSTREAM_SERVICE&quot;))\n    if err == nil {\n        fmt.Fprintln(w, &quot;Service A: upstream responded with:&quot;, resp)\n        requestCounter.WithLabelValues(&quot;2xx&quot;).Inc()\n    } else {\n        http.Error(w, fmt.Sprintf(&quot;Service A: upstream failed with: %v&quot;, err.Error()),\n            http.StatusInternalServerError)\n        requestCounter.WithLabelValues(&quot;5xx&quot;).Inc()\n    }\n}\n\nfunc main() {\n    // init prometheus /metrics endpoint\n\n    http.HandleFunc(&quot;/&quot;, handler)\n    log.Fatal(http.ListenAndServe(\n        os.Getenv(&quot;SERVICE_HOST&quot;)+&quot;:&quot;+os.Getenv(&quot;SERVICE_PORT&quot;), nil))\n}</code></pre>\n<div> <p><i>(see full version on <a href=\"https://github.com/iximiuz/envoy-playground/tree/master/basics/service-a/main.go\">GitHub</a>)</i></p>\n</div> <p>Notice that instead of hard-coding, we use <code>SERVICE_HOST</code> and <code>SERVICE_PORT</code> env variables to specify the host and port of the HTTP API endpoint. It&apos;ll come in handy soon. Additionally, the code of the service relies on the <code>UPSTREAM_SERVICE</code> env variable when accessing the upstream service <em>B</em>.</p>\n<p>To get some visibility, the code is instrumented with the primitive counter metric <code>service_a_requests_total</code> that gets incremented on every incoming request. We will use an instance of <a href=\"https://github.com/iximiuz/envoy-playground/tree/master/basics/prometheus\">prometheus</a> service to scrape the metrics exposed by the service <em>A</em>.</p>\n<p>The implementation of the upstream <a href=\"https://github.com/iximiuz/envoy-playground/tree/master/basics/service-b\">service <em>B</em></a> is trivial as well. It&apos;s yet another HTTP server. However its behavior is rather close to a static endpoint.</p>\n<pre><code class=\"language-go\">package main\n\n// ...\n\nvar ERROR_RATE int\n\nvar (\n    requestCounter = prometheus.NewCounterVec(\n        prometheus.CounterOpts{\n            Name: &quot;service_b_requests_total&quot;,\n            Help: &quot;The total number of requests received by Service B.&quot;,\n        },\n        []string{&quot;status&quot;},\n    )\n)\n\nfunc handler(w http.ResponseWriter, r *http.Request) {\n    if rand.Intn(100) &gt;= ERROR_RATE {\n        fmt.Fprintln(w, &quot;Service B: Yay! nounce&quot;, rand.Uint32())\n        requestCounter.WithLabelValues(&quot;2xx&quot;).Inc()\n    } else {\n        http.Error(w, fmt.Sprintf(&quot;Service B: Ooops... nounce %v&quot;, rand.Uint32()),\n            http.StatusInternalServerError)\n        requestCounter.WithLabelValues(&quot;5xx&quot;).Inc()\n    }\n}\n\nfunc main() {\n    // set ERROR_RATE\n    // init prometheus /metrics endpoint\n\n    http.HandleFunc(&quot;/&quot;, handler)\n\n    // Listen on all interfaces\n    log.Fatal(http.ListenAndServe(&quot;:&quot;+os.Getenv(&quot;SERVICE_PORT&quot;), nil))\n}</code></pre>\n<div> <p><i>(see full version on <a href=\"https://github.com/iximiuz/envoy-playground/tree/master/basics/service-b/main.go\">GitHub</a>)</i></p>\n</div> <p>Probably the only interesting part here is <code>ERROR_RATE</code>. The service is designed to fail requests with some constant rate, i.e. if <code>ERROR_RATE</code> is <em>20</em>, approximately 20% of requests will fail with HTTP 500 status code. As with the service <em>A</em>, we will use prometheus to scrape basic usage statistics, see the counter <code>service_b_requests_total</code>.</p>\n<p>Now it&apos;s time to launch the services and wire them up together. We are going to use <a href=\"https://github.com/containers/podman\">podman</a> to build and run services. Mostly because unlike Docker, podman <a href=\"https://developers.redhat.com/blog/2019/01/15/podman-managing-containers-pods/\">supports the concept of pods out of the box</a>. Heck, look at its name, it&apos;s <strong>POD</strong>man =5</p>\n<p>We will start from creating the service <em>B</em> since it&apos;s a dependency of the service <em>A</em>. Clone the <a href=\"https://github.com/iximiuz/envoy-playground\">demo repository</a> and run the following commands from its root (a Linux host with installed podman is assumed):</p>\n<details><summary><i>Click here to see service B Dockerfile.</i></summary>\n<p> <pre><code class=\"language-dockerfile\">FROM golang:1.15\n\n# Build\nENV GO111MODULE=on\n\nWORKDIR /app\n\nCOPY go.mod .\nCOPY main.go .\n\nRUN go mod download\nRUN go build -o service-b\n\n# Run\nENV ERROR_RATE=30\n\nENV SERVICE_PORT=80\n\nENV METRICS_PORT=8081\n\nCMD [&quot;/app/service-b&quot;]</code></pre>\n</p><p><a href=\"https://github.com/iximiuz/envoy-playground/tree/master/basics/service-b/Dockerfile\">source on GitHub</a></p> </details> <pre><code class=\"language-bash\"># Build service B image\n$ sudo podman build -t service-b -f service-b/Dockerfile\n\n# Create a pod (read &quot;server&quot;) for service B\n$ sudo podman pod create --name service-b-pod\n\n# Start service B container in the pod\n$ sudo podman run --name service-b -d --rm --pod service-b-pod \\\n    -e ERROR_RATE=20 service-b\n\n# Keep pod&apos;s IP address for future use\n$ POD_B_IP=$(sudo podman inspect -f &quot;{{.NetworkSettings.IPAddress}}&quot; \\\n    $(sudo podman pod inspect -f &quot;{{.InfraContainerID}}&quot; service-b-pod))\n\n$ echo $POD_B_IP\n&gt; 10.88.0.164  # output on my machine</code></pre>\n<p>Notice that the server is listening on pod&apos;s external network interface, port <em>80</em>:</p>\n<pre><code class=\"language-bash\">$ curl $POD_B_IP\n&gt; Service B: Yay! nounce 3494557023\n$ curl $POD_B_IP\n&gt; Service B: Yay! nounce 1634910179\n$ curl $POD_B_IP\n&gt; Service B: Yay! nounce 2013866549\n$ curl $POD_B_IP\n&gt; Service B: Ooops... nounce 1258862891</code></pre>\n<p>Now we are ready to proceed with the service <em>A</em>. First, let&apos;s create a pod:</p>\n<pre><code class=\"language-bash\"># Create a pod (read &quot;server&quot;) for service A\n$ sudo podman pod create --name service-a-pod \\\n    --add-host b.service:$POD_B_IP --publish 8080:80</code></pre>\n<p>Notice how we injected a DNS record like <code>b.service 10.88.0.164</code>. Since both pods reside in the same podman network, they can reach each other using assigned IP addresses. However, as of the time of writing this, podman doesn&apos;t provide DNS support for pods (yet). So, we have to maintain the mappings manually. Of course, we could use the plain IP address of the <em>B</em>&apos;s pod while accessing the upstream from the service <em>A</em> code. However, it&apos;s always nice to have human-readable hostnames instead of raw IP addresses. We will also see how this technique comes in handy with the envoy proxy sidecar below.</p>\n<p>Let&apos;s continue with the service itself. We need to build it and run inside the pod we&apos;ve just created.</p>\n<details><summary><i>Click here to see service A Dockerfile.</i></summary>\n<p> <pre><code class=\"language-dockerfile\">FROM golang:1.15\n\n# Build\nENV GO111MODULE=on\n\nWORKDIR /app\n\nCOPY go.mod .\nCOPY main.go .\n\nRUN go mod download\nRUN go build -o service-a\n\n# Run\nENV SERVICE_HOST=&quot;0.0.0.0&quot;\nENV SERVICE_PORT=80\n\nENV METRICS_HOST=&quot;0.0.0.0&quot;\nENV METRICS_PORT=8081\n\nENV UPSTREAM_SERVICE=&quot;http://b.service/&quot;\n\nCMD [&quot;/app/service-a&quot;]</code></pre>\n</p><p><a href=\"https://github.com/iximiuz/envoy-playground/tree/master/basics/service-a/Dockerfile-service\">source on GitHub</a></p> </details> <pre><code class=\"language-bash\"># Build service A image\n$ sudo podman build -t service-a -f service-a/Dockerfile-service\n\n# Start service A container in the pod\n$ sudo podman run --name service-a -d --rm --pod service-a-pod \\\n   -e SERVICE_HOST=0.0.0.0 -e SERVICE_PORT=80 \\\n   -e UPSTREAM_SERVICE=http://b.service:80 \\\n   service-a\n\n# Keep pod&apos;s IP address for future use\n$ POD_A_IP=$(sudo podman inspect -f &quot;{{.NetworkSettings.IPAddress}}&quot; \\\n    $(sudo podman pod inspect -f &quot;{{.InfraContainerID}}&quot; service-a-pod))\n\n$ echo $POD_A_IP\n&gt; 10.88.0.165  # output on my machine</code></pre>\n<p>Remember the diagram from the beginning of this section. At this part of the exercise service <em>A</em> has to be directly exposed to the outside world (i.e. the host machine) and it has to communicate with the service <em>B</em> directly as well. That&apos;s why we made service <em>A</em> listening on the pod&apos;s external network interface using <code>-e SERVICE_HOST=0.0.0.0 -e SERVICE_PORT=80</code> and provided it with the knowledge how to reach the service <em>B</em> <code>-e UPSTREAM_SERVICE=http://b.service:80</code>.</p>\n<p>The last preparation before pouring some traffic - starting a <a href=\"https://github.com/iximiuz/envoy-playground/tree/master/basics/prometheus\">prometheus</a> node:</p>\n<pre><code class=\"language-bash\"># Scrape configs\n$ cat prometheus/prometheus.yml\n&gt; scrape_configs:\n&gt;   - job_name: service-a\n&gt;     scrape_interval: 1s\n&gt;     static_configs:\n&gt;       - targets: [&apos;a.service:8081&apos;]\n&gt;\n&gt;   - job_name: service-b\n&gt;     scrape_interval: 1s\n&gt;     static_configs:\n&gt;       - targets: [&apos;b.service:8081&apos;]\n\n# Dockerfile\n$ cat prometheus/Dockerfile\n&gt; FROM prom/prometheus:v2.20.1\n&gt; COPY prometheus.yml /etc/prometheus/prometheus.yml\n\n# Build &amp; run\n$ sudo podman build -t envoy-prom -f prometheus/Dockerfile\n\n$ sudo podman run --name envoy-prom -d --rm \\\n   --publish 9090:9090 \\\n   --add-host a.service:$POD_A_IP \\\n   --add-host b.service:$POD_B_IP \\\n   envoy-prom</code></pre>\n<p>At this time, we have two services within a shared network. They can talk to each other using their IP addresses. Additionally, port <em>80</em> of the service <em>A</em> is mapped to the port <em>8080</em> of the host machine and prometheus is exposed on the port <em>9090</em>. I intentionally made these two ports mapped on <code>0.0.0.0</code> &apos;cause I run the demo inside a VirtualBox machine. This way, I can access prometheus graphical interface from the laptop&apos;s host operating system via using <code>&lt;vm_ip_address&gt;:9090/graph</code>.</p>\n<p>Finally, we can send some traffic to the service <em>A</em> and see what happens:</p>\n<pre><code class=\"language-bash\">$ for _ in {1..1000}; do curl --silent localhost:8080; done | sort | uniq -w 24 -c\n&gt;    1000\n&gt;     208 Service A: upstream failed with: HTTP 500 - Service B: Ooops... nounce 1007409508\n&gt;     792 Service A: upstream responded with: Service B: Yay! nounce 1008262846</code></pre>\n<p>Yay! <�>\n<div> <img src=\"/service-proxy-pod-sidecar-oh-my/prom-service-a-direct.png\" width=\"100%\"> <p><i><code>service_a_requests_total</code></i></p>\n</div> <div> <img src=\"/service-proxy-pod-sidecar-oh-my/prom-service-b-direct.png\" width=\"100%\"> <p><i><code>service_b_requests_total</code></i></p>\n</div> <p>Well, I believe it&apos;s not a surprise that both services handled each 1000 of requests and the service <em>A</em> failed as many requests as the service <em>B</em>.</p> <p>Let&apos;s enhance our setup by adding a service proxy sidecar to the service <em>A</em>. For the sake of simplicity of this demo, the only thing the sidecar will be doing is making up to 2 retries of the failed HTTP requests. Hopefully, it&apos;ll improve the overall service <em>A</em> success rate. The desired setup will look as follows:</p>\n<div> <img src=\"/service-proxy-pod-sidecar-oh-my/80-demo-sidecar.png\" width=\"100%\">\n</div> <p>The main difference is that with the sidecar all incoming and outgoing requests will be passing through envoy. In contrast with the previous section setup, service <em>A</em> will neither be exposed publicly nor will be allowed to contact service <em>B</em> directly.</p>\n<p>Let&apos;s review two scenarios:</p>\n<ul>\n<li><p><strong>Ingress</strong>: a client sends an HTTP request to <code>$POD_A_IP</code>. It hits the envoy sidecar listening on <code>$POD_A_IP:80</code>. Envoy, in turn, makes a request to the service <em>A</em> container listening on the pod&apos;s <code>localhost:8000</code>. Once the envoy process gets the response from the service <em>A</em> container, it forwards it back to the client.</p>\n</li>\n<li><p><strong>Egress</strong>: service <em>A</em> gets a request from envoy. To handle it, the service process needs to access the upstream service <em>B</em>. The service <em>A</em> container sends a request to another envoy listener sitting on pod&apos;s <code>localhost:9001</code>. Additionally, service <em>A</em> specifies the HTTP Host header <code>b.local.service</code> allowing envoy to route this request appropriately. When envoy receives a request on <code>localhost:9001</code> it knows that it&apos;s egress traffic. It checks the Host header and if it looks like the service <em>B</em>, it makes a request to <code>$POD_B_IP</code>.</p>\n</li>\n</ul>\n<p>Configuring envoy could quickly become tricky due to its huge set of capabilities. However, the <a href=\"https://www.envoyproxy.io/docs/envoy/latest/\">official documentation</a> is a great place to start. It not only describes the configuration format but also highlights some best practices and explains some concepts. In particular, I suggest these two articles <a href=\"https://www.envoyproxy.io/docs/envoy/v1.15.0/intro/life_of_A_request\">&quot;Life of a Request&quot;</a> and <a href=\"https://www.envoyproxy.io/docs/envoy/v1.15.0/intro/deployment_types/service_to_service\">&quot;Service to service only&quot;</a> for a better understanding of the material.</p>\n<p>From a very high-level overview, Envoy could be seen as a bunch of pipelines. A pipeline starts from the listener and then connected through a set of filters to some number of clusters, where a cluster is just a logical group of network endpoints. Trying to be less abstract:</p>\n<pre><code># Ingress\nlistener 0.0.0.0:80\n       |\nhttp_connection_manager (filter)\n       |\nhttp_router (filter)\n       |\nlocal_service (cluster) [127.0.0.1:8000]\n\n# Egress\nlistener 127.0.0.1:9001\n       |\nhttp_connection_manager (filter)\n       |\nhttp_router (filter)\n       |\nremote_service_b (cluster) [b.service:80]</code></pre><details><summary><i>Click here to see envoy.yaml file.</i></summary>\n<p> <pre><code class=\"language-yaml\">static_resources:\n  listeners:\n  # Ingress\n  - address:\n      socket_address:\n        address: 0.0.0.0\n        port_value: 80\n    filter_chains:\n    - filters:\n      - name: envoy.filters.network.http_connection_manager\n        typed_config:\n          &quot;@type&quot;: type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\n          codec_type: auto\n          stat_prefix: ingress_http\n          access_log:\n          - name: envoy.access_loggers.file\n            typed_config:\n              &quot;@type&quot;: type.googleapis.com/envoy.config.accesslog.v2.FileAccessLog\n              path: /dev/stdout\n          route_config:\n            name: ingress_route\n            virtual_hosts:\n            - name: local_service\n              domains:\n              - &quot;*&quot;\n              routes:\n              - match:\n                  prefix: &quot;/&quot;\n                route:\n                  cluster: local_service\n          http_filters:\n          - name: envoy.filters.http.router\n            typed_config: {}\n  # Egress\n  - address:\n      socket_address:\n        address: 127.0.0.1\n        port_value: 9001\n    filter_chains:\n    - filters:\n      - name: envoy.filters.network.http_connection_manager\n        typed_config:\n          &quot;@type&quot;: type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\n          codec_type: auto\n          stat_prefix: egress_http\n          access_log:\n          - name: envoy.access_loggers.file\n            typed_config:\n              &quot;@type&quot;: type.googleapis.com/envoy.config.accesslog.v2.FileAccessLog\n              path: /dev/stdout\n          route_config:\n            name: egress_route\n            virtual_hosts:\n            - name: remote_service_b\n              domains:\n              - &quot;b.local.service&quot;\n              - &quot;b.local.service:*&quot;\n              retry_policy:\n                retry_on: 5xx\n                num_retries: 2\n              routes:\n              - match:\n                  prefix: &quot;/&quot;\n                route:\n                  cluster: remote_service_b\n          http_filters:\n          - name: envoy.filters.http.router\n            typed_config: {}\n\n  clusters:\n  - name: local_service\n    connect_timeout: 0.25s\n    type: strict_dns\n    lb_policy: round_robin\n    load_assignment:\n      cluster_name: local_service\n      endpoints:\n      - lb_endpoints:\n        - endpoint:\n            address:\n              socket_address:\n                address: 127.0.0.1\n                port_value: 8000\n  - name: remote_service_b\n    connect_timeout: 0.25s\n    type: strict_dns\n    lb_policy: round_robin\n    load_assignment:\n      cluster_name: remote_service_b\n      endpoints:\n      - lb_endpoints:\n        - endpoint:\n            address:\n              socket_address:\n                address: b.service\n                port_value: 80\n\nadmin:\n  access_log_path: &quot;/dev/stdout&quot;\n  address:\n    socket_address:\n      # Beware: it&apos;s insecure to expose admin interface publicly\n      address: 0.0.0.0\n      port_value: 9901</code></pre>\n</p><p><a href=\"https://github.com/iximiuz/envoy-playground/blob/master/basics/service-a/envoy.yaml\">source on GitHub</a></p>\n</details></�></p> <p>Envoy is famous for its observability capabilities. It exposes various statistic information and luckily for us, it supports the prometheus metrics format out of the box. We can extend the prometheus scrape configs adding the following section:</p>\n<pre><code class=\"language-bash\"># prometheus/prometheus.yml\n\n  - job_name: service-a-envoy\n    scrape_interval: 1s\n    metrics_path: /stats/prometheus\n    static_configs:\n      - targets: [&apos;a.service:9901&apos;]</code></pre>\n<p>To build the envoy sidecar image we can run:</p>\n<pre><code class=\"language-bash\">$ sudo podman build -t service-a-envoy -f service-a/Dockerfile-envoy</code></pre>\n<p>We don&apos;t need to rebuild service images since they&apos;ve been made configurable via environment variables. However, we need to recreate the service <em>A</em> to make it listening on the pod&apos;s <code>localhost:8000</code>.</p>\n<pre><code class=\"language-bash\"># Stop existing container and remove pod\n$ sudo podman kill service-a\n$ sudo podman pod rm service-a-pod\n\n$ sudo podman pod create --name service-a-pod \\\n   --add-host b.local.service:127.0.0.1 \\\n   --add-host b.service:$POD_B_IP \\\n   --publish 8080:80\n\n$ sudo podman run --name service-a -d --rm --pod service-a-pod \\\n   -e SERVICE_HOST=127.0.0.1 \\\n   -e SERVICE_PORT=8000 \\\n   -e UPSTREAM_SERVICE=http://b.local.service:9001 \\\n   service-a\n\n$ sudo podman run --name service-a-envoy -d --rm --pod service-a-pod \\\n   -e ENVOY_UID=0 -e ENVOY_GID=0 service-a-envoy\n\n$ POD_A_IP=$(sudo podman inspect -f &quot;{{.NetworkSettings.IPAddress}}&quot; \\\n    $(sudo podman pod inspect -f &quot;{{.InfraContainerID}}&quot; service-a-pod))</code></pre>\n<p>Let&apos;s see what happens if we pour some more traffic:</p>\n<pre><code class=\"language-bash\">$ for _ in {1..1000}; do curl --silent localhost:8080; done | sort | uniq -w 24 -c\n&gt;    1000\n&gt;       9 Service A: upstream failed with: HTTP 500 - Service B: Ooops... nounce 1263663296\n&gt;     991 Service A: upstream responded with: Service B: Yay! nounce 1003014939</code></pre>\n<p>Hooray! <�>A jumped from 80% to 99%! Well, that&apos;s great, but also as expected. The original probability to get HTTP 500 from the service <em>A</em> was equal to the probability of the service <em>B</em> to fail a request since the network conditions are kind of ideal here. But since the introduction of the envoy sidecar, service <em>A</em> got a superpower of retries. The probability to fail 3 consequent requests with a 20% chance of a single attempt failure is <code>0.2 * 0.2 * 0.2 = 0.008</code>, i.e very close to 1%. Thus, we theoretically confirmed the observed 99% success rate.</�></p>\n<p>Last but not least, let&apos;s check out the metrics. We will start from the familiar <code>service_a_requests_total</code> counter:</p>\n<div> <img src=\"/service-proxy-pod-sidecar-oh-my/prom-service-a-envoy.png\" width=\"100%\"> <p><i><code>service_a_requests_total</code></i></p>\n</div> <p>Well, seems like service <em>A</em> again got 1000 requests, but this time it failed only a tiny fraction of it. What&apos;s up with service <em>B</em>?</p>\n<div> <img src=\"/service-proxy-pod-sidecar-oh-my/prom-service-b-envoy.png\" width=\"100%\"> <p><i><code>service_b_requests_total</code></i></p>\n</div> <p>Here we definitely can see the change. Instead of the original 1000, this time service <em>B</em> got about 1250 requests in total. However, only about 1000 have been served successfully.</p>\n<p>What can the envoy sidecar tell us?</p>\n<div> <img src=\"/service-proxy-pod-sidecar-oh-my/prom-envoy-local-service.png\" width=\"100%\"> <p><i><code>envoy_cluster_upstream_rq{envoy_cluster_name=&quot;local_service&quot;}</code></i></p>\n</div> <div> <img src=\"/service-proxy-pod-sidecar-oh-my/prom-envoy-remote-service.png\" width=\"100%\"> <p><i><code>envoy_cluster_upstream_rq{envoy_cluster_name=&quot;remote_service_b&quot;}</code></i></p>\n</div> <p>While both <code>local_service</code> and <code>remote_service_b</code> clusters don&apos;t shed much light on the actual number of retries that were made, there is another metric we can check:</p>\n<div> <img src=\"/service-proxy-pod-sidecar-oh-my/prom-envoy-retries.png\" width=\"100%\"> <p><i><code>envoy_cluster_retry_upstream_rq{envoy_cluster_name=&quot;remote_service_b&quot;}</code></i></p>\n</div> <p>Perfect, we managed to confirm that all those ~250 extra requests the service <em>B</em> received are actually retries originated by the envoy sidecar!</p>\n<h2 id=\"instead-of-conclusion\">Instead of conclusion</h2>\n<p>I hope you enjoyed playing with all these pods and sidecars as much as I did. It&apos;s always beneficial to build such demos from time to time because often the amount of insights you&apos;re going to get while working on it is underestimated. So, I encourage everyone to get the hands dirty and share your findings! See you next time!</p>\n<p>Make code, not war!</p>\n</div>",
      "contentAsText": "\n\nImagine you're developing a service... For certainty, let's call it A. It's going to provide some public HTTP API to its clients. However, to serve requests it needs to call another service. Let's call this upstream service - B.\n \n Obviously, neither network nor service B is ideal. If service A wants to decrease the impact of the failing upstream requests on its public API success rate, it has to do something about errors. For instance, it could start retrying failed requests.\n\n \n Implementation of the retry mechanism requires some code changes in the service A, but the codebase is fresh, there are tons of advanced HTTP libraries, so you just need to grab one... Easy-peasy, right?\nUnfortunately, this simplicity is not always the case. Replace service A with service Z that was written 10 years ago in some esoteric language by a developer that already retired. Or add to the equitation services Q, U, and X written by different teams in three different languages. As a result, the cumulative cost of the company-wide retry mechanism implementation in the code gets really high...\n \n But what if retries are not the only thing you need? Proper request timeouts have to be ensured as well. And how about distributed tracing? It'd be nice to correlate the whole request tree with the original customer transaction by propagating some additional HTTP headers. However, every such capability would make the HTTP libraries even more bloated...\nMeet the solution\nLet's try to go one level higher... or lower? >\u0014\nIn our original setup, service A has been communicating with service B directly. But what if we put an intermediary infrastructure component in between those services? Thanks to containerization, orchestration, devops, add a buzz word of your choice here, nowadays, it became so simple to configure infrastructure, that the cost of adding another infra component is often lower than the cost of writing application code...\n \n For the sake of simplicity, let's call the box enclosing the service A and the secret intermediary component a server (bare metal or virtual, doesn't really matter). And now it's about time to introduce one of the fancy words from the article's title. Any piece of software running on the server alongside the primary service and helping it do its job is called a sidecar. I hope, the idea behind the name is more or less straightforward here.\nBut getting back to the service-to-service communication problem, what sidecar should we use to keep the service code free of the low-level details such as retries or request tracing? Well, the needed piece of software is called a service proxy. Probably, the most widely used implementation of the service proxy in the real world is envoy.\nThe idea of the service proxy is the following: instead of accessing the service B directly, code in the service A now will be sending requests to the service proxy sidecar. Since both of the processes run on the same server, the loopback network interface (i.e. 127.0.0.1 aka localhost) is perfectly suitable for this part of the communication. On every received HTTP request, the service proxy sidecar will make a request to the upstream service using the external network interface of the server. The response from the upstream will be eventually forwarded back by the sidecar to the service A.\nI think, at this time, it's already obvious where the retry, timeouts, tracing, etc. logic should reside. Having this kind of functionality provided by a separate sidecar process makes enhancing any service written in any language with such capabilities rather trivial.\nInterestingly enough, that service proxy could be used not only for outgoing traffic (egress) but also for the incoming traffic (ingress) of the service A. Usually, there is plenty of cross-cutting things that can be tackled on the ingress stage. For instance, proxy sidecars can do SSL termination, request authentication, and more. A detailed diagram of a single server setup could look something like that:\n \n Probably, the last fancy term we are going to cover here is a pod. People have been deploying code using virtual machines or bare metal servers for a long time... A server itself is already a good abstraction and a unit of encapsulation. For instance, every server has at least one external network interface, a network loopback interface for the internal IPC needs, and it can run a bunch of processes sharing access to these communication means. Servers are usually addressable within the private network of the company by their IPs. Last but not least, it's pretty common to use a whole server for a single purpose (otherwise, maintenance quickly becomes a nightmare). I.e. you may have a group of identical servers running instances of service A, another group of servers each running an instance of service B, etc. So, why on earth would anybody want something better than a server?\nDespite being a good abstraction, the orchestration overhead servers introduce is often too high. So people started thinking about how to package applications more efficiently and that's how we got containers. Well, probably you know that Docker and container had been kind of a synonym for a long time and folks from Docker have been actively advocating for \"a one process per container\" model. Obviously, this model is pretty different from the widely used server abstraction where multiple processes are allowed to work side by side. And that's how we got the concept of pods. A pod is just a group of containers sharing a bunch of namespaces. If we now run a single process per container all of the processes in the pod will still share the common execution environment. In particular, the network namespace. Thus, all the containers in the pod will have a shared loopback interface and a shared external interface with an IP address assigned to it. Then it's up to the orchestration layer (say hi to Kubernetes) how to make all the pods reachable within the network by their IPs. And that's how people reinvented servers...\nSo, getting back to all those blue boxes enclosing the service process and the sidecar on the diagrams above - we can think of them as being either a virtual machine, a bare metal server, or a pod. All three of them are more or less interchangeable abstractions.\nTo summarize, let's try to visualize how the service to service communication could look like with the proxy sidecars:\n  Example of service to service communication topology, a.k.a. service mesh.\n Get hands dirty (practical part)\nSince the only way to really understand something is to write a blog post about it implement it yourself, let's quickly hack a demo environment.\nService A talks to service B directly\nWe will start from the simple setup where service A will be accessing service B directly:\n \n The code of the service A is relatively straightforward. It's just a simple HTTP server that makes a call to its upstream service B on every client request. Depending on the response from the upstream, A returns either an HTTP 200 or HTTP 500 to the client.\npackage main\n\n// ...\n\nvar requestCounter = prometheus.NewCounterVec(\n    prometheus.CounterOpts{\n        Name: \"service_a_requests_total\",\n        Help: \"The total number of requests received by Service A.\",\n    },\n    []string{\"status\"},\n)\n\nfunc handler(w http.ResponseWriter, r *http.Request) {\n    resp, err := httpGet(os.Getenv(\"UPSTREAM_SERVICE\"))\n    if err == nil {\n        fmt.Fprintln(w, \"Service A: upstream responded with:\", resp)\n        requestCounter.WithLabelValues(\"2xx\").Inc()\n    } else {\n        http.Error(w, fmt.Sprintf(\"Service A: upstream failed with: %v\", err.Error()),\n            http.StatusInternalServerError)\n        requestCounter.WithLabelValues(\"5xx\").Inc()\n    }\n}\n\nfunc main() {\n    // init prometheus /metrics endpoint\n\n    http.HandleFunc(\"/\", handler)\n    log.Fatal(http.ListenAndServe(\n        os.Getenv(\"SERVICE_HOST\")+\":\"+os.Getenv(\"SERVICE_PORT\"), nil))\n}\n (see full version on GitHub)\n Notice that instead of hard-coding, we use SERVICE_HOST and SERVICE_PORT env variables to specify the host and port of the HTTP API endpoint. It'll come in handy soon. Additionally, the code of the service relies on the UPSTREAM_SERVICE env variable when accessing the upstream service B.\nTo get some visibility, the code is instrumented with the primitive counter metric service_a_requests_total that gets incremented on every incoming request. We will use an instance of prometheus service to scrape the metrics exposed by the service A.\nThe implementation of the upstream service B is trivial as well. It's yet another HTTP server. However its behavior is rather close to a static endpoint.\npackage main\n\n// ...\n\nvar ERROR_RATE int\n\nvar (\n    requestCounter = prometheus.NewCounterVec(\n        prometheus.CounterOpts{\n            Name: \"service_b_requests_total\",\n            Help: \"The total number of requests received by Service B.\",\n        },\n        []string{\"status\"},\n    )\n)\n\nfunc handler(w http.ResponseWriter, r *http.Request) {\n    if rand.Intn(100) >= ERROR_RATE {\n        fmt.Fprintln(w, \"Service B: Yay! nounce\", rand.Uint32())\n        requestCounter.WithLabelValues(\"2xx\").Inc()\n    } else {\n        http.Error(w, fmt.Sprintf(\"Service B: Ooops... nounce %v\", rand.Uint32()),\n            http.StatusInternalServerError)\n        requestCounter.WithLabelValues(\"5xx\").Inc()\n    }\n}\n\nfunc main() {\n    // set ERROR_RATE\n    // init prometheus /metrics endpoint\n\n    http.HandleFunc(\"/\", handler)\n\n    // Listen on all interfaces\n    log.Fatal(http.ListenAndServe(\":\"+os.Getenv(\"SERVICE_PORT\"), nil))\n}\n (see full version on GitHub)\n Probably the only interesting part here is ERROR_RATE. The service is designed to fail requests with some constant rate, i.e. if ERROR_RATE is 20, approximately 20% of requests will fail with HTTP 500 status code. As with the service A, we will use prometheus to scrape basic usage statistics, see the counter service_b_requests_total.\nNow it's time to launch the services and wire them up together. We are going to use podman to build and run services. Mostly because unlike Docker, podman supports the concept of pods out of the box. Heck, look at its name, it's PODman =5\nWe will start from creating the service B since it's a dependency of the service A. Clone the demo repository and run the following commands from its root (a Linux host with installed podman is assumed):\nClick here to see service B Dockerfile.\n FROM golang:1.15\n\n# Build\nENV GO111MODULE=on\n\nWORKDIR /app\n\nCOPY go.mod .\nCOPY main.go .\n\nRUN go mod download\nRUN go build -o service-b\n\n# Run\nENV ERROR_RATE=30\n\nENV SERVICE_PORT=80\n\nENV METRICS_PORT=8081\n\nCMD [\"/app/service-b\"]\nsource on GitHub  # Build service B image\n$ sudo podman build -t service-b -f service-b/Dockerfile\n\n# Create a pod (read \"server\") for service B\n$ sudo podman pod create --name service-b-pod\n\n# Start service B container in the pod\n$ sudo podman run --name service-b -d --rm --pod service-b-pod \\\n    -e ERROR_RATE=20 service-b\n\n# Keep pod's IP address for future use\n$ POD_B_IP=$(sudo podman inspect -f \"{{.NetworkSettings.IPAddress}}\" \\\n    $(sudo podman pod inspect -f \"{{.InfraContainerID}}\" service-b-pod))\n\n$ echo $POD_B_IP\n> 10.88.0.164  # output on my machine\nNotice that the server is listening on pod's external network interface, port 80:\n$ curl $POD_B_IP\n> Service B: Yay! nounce 3494557023\n$ curl $POD_B_IP\n> Service B: Yay! nounce 1634910179\n$ curl $POD_B_IP\n> Service B: Yay! nounce 2013866549\n$ curl $POD_B_IP\n> Service B: Ooops... nounce 1258862891\nNow we are ready to proceed with the service A. First, let's create a pod:\n# Create a pod (read \"server\") for service A\n$ sudo podman pod create --name service-a-pod \\\n    --add-host b.service:$POD_B_IP --publish 8080:80\nNotice how we injected a DNS record like b.service 10.88.0.164. Since both pods reside in the same podman network, they can reach each other using assigned IP addresses. However, as of the time of writing this, podman doesn't provide DNS support for pods (yet). So, we have to maintain the mappings manually. Of course, we could use the plain IP address of the B's pod while accessing the upstream from the service A code. However, it's always nice to have human-readable hostnames instead of raw IP addresses. We will also see how this technique comes in handy with the envoy proxy sidecar below.\nLet's continue with the service itself. We need to build it and run inside the pod we've just created.\nClick here to see service A Dockerfile.\n FROM golang:1.15\n\n# Build\nENV GO111MODULE=on\n\nWORKDIR /app\n\nCOPY go.mod .\nCOPY main.go .\n\nRUN go mod download\nRUN go build -o service-a\n\n# Run\nENV SERVICE_HOST=\"0.0.0.0\"\nENV SERVICE_PORT=80\n\nENV METRICS_HOST=\"0.0.0.0\"\nENV METRICS_PORT=8081\n\nENV UPSTREAM_SERVICE=\"http://b.service/\"\n\nCMD [\"/app/service-a\"]\nsource on GitHub  # Build service A image\n$ sudo podman build -t service-a -f service-a/Dockerfile-service\n\n# Start service A container in the pod\n$ sudo podman run --name service-a -d --rm --pod service-a-pod \\\n   -e SERVICE_HOST=0.0.0.0 -e SERVICE_PORT=80 \\\n   -e UPSTREAM_SERVICE=http://b.service:80 \\\n   service-a\n\n# Keep pod's IP address for future use\n$ POD_A_IP=$(sudo podman inspect -f \"{{.NetworkSettings.IPAddress}}\" \\\n    $(sudo podman pod inspect -f \"{{.InfraContainerID}}\" service-a-pod))\n\n$ echo $POD_A_IP\n> 10.88.0.165  # output on my machine\nRemember the diagram from the beginning of this section. At this part of the exercise service A has to be directly exposed to the outside world (i.e. the host machine) and it has to communicate with the service B directly as well. That's why we made service A listening on the pod's external network interface using -e SERVICE_HOST=0.0.0.0 -e SERVICE_PORT=80 and provided it with the knowledge how to reach the service B -e UPSTREAM_SERVICE=http://b.service:80.\nThe last preparation before pouring some traffic - starting a prometheus node:\n# Scrape configs\n$ cat prometheus/prometheus.yml\n> scrape_configs:\n>   - job_name: service-a\n>     scrape_interval: 1s\n>     static_configs:\n>       - targets: ['a.service:8081']\n>\n>   - job_name: service-b\n>     scrape_interval: 1s\n>     static_configs:\n>       - targets: ['b.service:8081']\n\n# Dockerfile\n$ cat prometheus/Dockerfile\n> FROM prom/prometheus:v2.20.1\n> COPY prometheus.yml /etc/prometheus/prometheus.yml\n\n# Build & run\n$ sudo podman build -t envoy-prom -f prometheus/Dockerfile\n\n$ sudo podman run --name envoy-prom -d --rm \\\n   --publish 9090:9090 \\\n   --add-host a.service:$POD_A_IP \\\n   --add-host b.service:$POD_B_IP \\\n   envoy-prom\nAt this time, we have two services within a shared network. They can talk to each other using their IP addresses. Additionally, port 80 of the service A is mapped to the port 8080 of the host machine and prometheus is exposed on the port 9090. I intentionally made these two ports mapped on 0.0.0.0 'cause I run the demo inside a VirtualBox machine. This way, I can access prometheus graphical interface from the laptop's host operating system via using <vm_ip_address>:9090/graph.\nFinally, we can send some traffic to the service A and see what happens:\n$ for _ in {1..1000}; do curl --silent localhost:8080; done | sort | uniq -w 24 -c\n>    1000\n>     208 Service A: upstream failed with: HTTP 500 - Service B: Ooops... nounce 1007409508\n>     792 Service A: upstream responded with: Service B: Yay! nounce 1008262846\nYay! <�>\n  service_a_requests_total\n   service_b_requests_total\n Well, I believe it's not a surprise that both services handled each 1000 of requests and the service A failed as many requests as the service B. Let's enhance our setup by adding a service proxy sidecar to the service A. For the sake of simplicity of this demo, the only thing the sidecar will be doing is making up to 2 retries of the failed HTTP requests. Hopefully, it'll improve the overall service A success rate. The desired setup will look as follows:\n \n The main difference is that with the sidecar all incoming and outgoing requests will be passing through envoy. In contrast with the previous section setup, service A will neither be exposed publicly nor will be allowed to contact service B directly.\nLet's review two scenarios:\n\nIngress: a client sends an HTTP request to $POD_A_IP. It hits the envoy sidecar listening on $POD_A_IP:80. Envoy, in turn, makes a request to the service A container listening on the pod's localhost:8000. Once the envoy process gets the response from the service A container, it forwards it back to the client.\n\nEgress: service A gets a request from envoy. To handle it, the service process needs to access the upstream service B. The service A container sends a request to another envoy listener sitting on pod's localhost:9001. Additionally, service A specifies the HTTP Host header b.local.service allowing envoy to route this request appropriately. When envoy receives a request on localhost:9001 it knows that it's egress traffic. It checks the Host header and if it looks like the service B, it makes a request to $POD_B_IP.\n\n\nConfiguring envoy could quickly become tricky due to its huge set of capabilities. However, the official documentation is a great place to start. It not only describes the configuration format but also highlights some best practices and explains some concepts. In particular, I suggest these two articles \"Life of a Request\" and \"Service to service only\" for a better understanding of the material.\nFrom a very high-level overview, Envoy could be seen as a bunch of pipelines. A pipeline starts from the listener and then connected through a set of filters to some number of clusters, where a cluster is just a logical group of network endpoints. Trying to be less abstract:\n# Ingress\nlistener 0.0.0.0:80\n       |\nhttp_connection_manager (filter)\n       |\nhttp_router (filter)\n       |\nlocal_service (cluster) [127.0.0.1:8000]\n\n# Egress\nlistener 127.0.0.1:9001\n       |\nhttp_connection_manager (filter)\n       |\nhttp_router (filter)\n       |\nremote_service_b (cluster) [b.service:80]Click here to see envoy.yaml file.\n static_resources:\n  listeners:\n  # Ingress\n  - address:\n      socket_address:\n        address: 0.0.0.0\n        port_value: 80\n    filter_chains:\n    - filters:\n      - name: envoy.filters.network.http_connection_manager\n        typed_config:\n          \"@type\": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\n          codec_type: auto\n          stat_prefix: ingress_http\n          access_log:\n          - name: envoy.access_loggers.file\n            typed_config:\n              \"@type\": type.googleapis.com/envoy.config.accesslog.v2.FileAccessLog\n              path: /dev/stdout\n          route_config:\n            name: ingress_route\n            virtual_hosts:\n            - name: local_service\n              domains:\n              - \"*\"\n              routes:\n              - match:\n                  prefix: \"/\"\n                route:\n                  cluster: local_service\n          http_filters:\n          - name: envoy.filters.http.router\n            typed_config: {}\n  # Egress\n  - address:\n      socket_address:\n        address: 127.0.0.1\n        port_value: 9001\n    filter_chains:\n    - filters:\n      - name: envoy.filters.network.http_connection_manager\n        typed_config:\n          \"@type\": type.googleapis.com/envoy.config.filter.network.http_connection_manager.v2.HttpConnectionManager\n          codec_type: auto\n          stat_prefix: egress_http\n          access_log:\n          - name: envoy.access_loggers.file\n            typed_config:\n              \"@type\": type.googleapis.com/envoy.config.accesslog.v2.FileAccessLog\n              path: /dev/stdout\n          route_config:\n            name: egress_route\n            virtual_hosts:\n            - name: remote_service_b\n              domains:\n              - \"b.local.service\"\n              - \"b.local.service:*\"\n              retry_policy:\n                retry_on: 5xx\n                num_retries: 2\n              routes:\n              - match:\n                  prefix: \"/\"\n                route:\n                  cluster: remote_service_b\n          http_filters:\n          - name: envoy.filters.http.router\n            typed_config: {}\n\n  clusters:\n  - name: local_service\n    connect_timeout: 0.25s\n    type: strict_dns\n    lb_policy: round_robin\n    load_assignment:\n      cluster_name: local_service\n      endpoints:\n      - lb_endpoints:\n        - endpoint:\n            address:\n              socket_address:\n                address: 127.0.0.1\n                port_value: 8000\n  - name: remote_service_b\n    connect_timeout: 0.25s\n    type: strict_dns\n    lb_policy: round_robin\n    load_assignment:\n      cluster_name: remote_service_b\n      endpoints:\n      - lb_endpoints:\n        - endpoint:\n            address:\n              socket_address:\n                address: b.service\n                port_value: 80\n\nadmin:\n  access_log_path: \"/dev/stdout\"\n  address:\n    socket_address:\n      # Beware: it's insecure to expose admin interface publicly\n      address: 0.0.0.0\n      port_value: 9901\nsource on GitHub\n Envoy is famous for its observability capabilities. It exposes various statistic information and luckily for us, it supports the prometheus metrics format out of the box. We can extend the prometheus scrape configs adding the following section:\n# prometheus/prometheus.yml\n\n  - job_name: service-a-envoy\n    scrape_interval: 1s\n    metrics_path: /stats/prometheus\n    static_configs:\n      - targets: ['a.service:9901']\nTo build the envoy sidecar image we can run:\n$ sudo podman build -t service-a-envoy -f service-a/Dockerfile-envoy\nWe don't need to rebuild service images since they've been made configurable via environment variables. However, we need to recreate the service A to make it listening on the pod's localhost:8000.\n# Stop existing container and remove pod\n$ sudo podman kill service-a\n$ sudo podman pod rm service-a-pod\n\n$ sudo podman pod create --name service-a-pod \\\n   --add-host b.local.service:127.0.0.1 \\\n   --add-host b.service:$POD_B_IP \\\n   --publish 8080:80\n\n$ sudo podman run --name service-a -d --rm --pod service-a-pod \\\n   -e SERVICE_HOST=127.0.0.1 \\\n   -e SERVICE_PORT=8000 \\\n   -e UPSTREAM_SERVICE=http://b.local.service:9001 \\\n   service-a\n\n$ sudo podman run --name service-a-envoy -d --rm --pod service-a-pod \\\n   -e ENVOY_UID=0 -e ENVOY_GID=0 service-a-envoy\n\n$ POD_A_IP=$(sudo podman inspect -f \"{{.NetworkSettings.IPAddress}}\" \\\n    $(sudo podman pod inspect -f \"{{.InfraContainerID}}\" service-a-pod))\nLet's see what happens if we pour some more traffic:\n$ for _ in {1..1000}; do curl --silent localhost:8080; done | sort | uniq -w 24 -c\n>    1000\n>       9 Service A: upstream failed with: HTTP 500 - Service B: Ooops... nounce 1263663296\n>     991 Service A: upstream responded with: Service B: Yay! nounce 1003014939\nHooray! <�>A jumped from 80% to 99%! Well, that's great, but also as expected. The original probability to get HTTP 500 from the service A was equal to the probability of the service B to fail a request since the network conditions are kind of ideal here. But since the introduction of the envoy sidecar, service A got a superpower of retries. The probability to fail 3 consequent requests with a 20% chance of a single attempt failure is 0.2 * 0.2 * 0.2 = 0.008, i.e very close to 1%. Thus, we theoretically confirmed the observed 99% success rate.\nLast but not least, let's check out the metrics. We will start from the familiar service_a_requests_total counter:\n  service_a_requests_total\n Well, seems like service A again got 1000 requests, but this time it failed only a tiny fraction of it. What's up with service B?\n  service_b_requests_total\n Here we definitely can see the change. Instead of the original 1000, this time service B got about 1250 requests in total. However, only about 1000 have been served successfully.\nWhat can the envoy sidecar tell us?\n  envoy_cluster_upstream_rq{envoy_cluster_name=\"local_service\"}\n   envoy_cluster_upstream_rq{envoy_cluster_name=\"remote_service_b\"}\n While both local_service and remote_service_b clusters don't shed much light on the actual number of retries that were made, there is another metric we can check:\n  envoy_cluster_retry_upstream_rq{envoy_cluster_name=\"remote_service_b\"}\n Perfect, we managed to confirm that all those ~250 extra requests the service B received are actually retries originated by the envoy sidecar!\nInstead of conclusion\nI hope you enjoyed playing with all these pods and sidecars as much as I did. It's always beneficial to build such demos from time to time because often the amount of insights you're going to get while working on it is underestimated. So, I encourage everyone to get the hands dirty and share your findings! See you next time!\nMake code, not war!\n",
      "description": "Explaining the ideas of pod, sidecar and service proxy with envoy demo playground.",
      "ogDescription": "Explaining the ideas of pod, sidecar and service proxy with envoy demo playground."
    },
    {
      "url": "https://www.nrmitchi.com/2020/09/working-around-kubernetes-sidecar-shutdowns/",
      "title": "Working Around Kubernetes Sidecar Shutdowns",
      "content": "<section id=\"post-body\"> <p>Over 2 years ago I wrote a <a href=\"https://github.com/nrmitchi/k8s-controller-sidecars\">quick Kubernetes controller</a> in order to ensuring that &#x201C;sidecar&#x201D; containers were shut down after the &#x201C;main&#x201D; container in a pod exited. The issue we were seeing was fairly straight forward: we had a container running in a pod to accomplish some application logic, as well as a number of &#x201C;sidecar&#x201D; containers to provide some functionality <em>to the application</em>. Examples of this are log forwarding, sql proxies, networking proxies, or in our case, metric collectors. We had a metric collection sidecar running along side a CronJob pod, which prevented the CronJob from ever &#x201C;succeeding&#x201D;.</p> <p>But why is this necessary in the first place? Why does it make sense for a sidecar container to stay alive after the main container has exited? In short, <strong>it doesn&#x2019;t</strong>, but this is a <a href=\"https://github.com/kubernetes/kubernetes/issues/25908\">long-outstanding issue</a> in Kubernetes that we have to deal with. There has been an <a href=\"https://github.com/kubernetes/enhancements/issues/753\">open proposal</a> to address this issue for years, however it stumbled into the much larger issue of &#x201C;container lifecycle management&#x201D;, and is currently on-hold while #sig-node addresses some underlying node shutdown issues.</p> <p>There is now <em>a lot</em> of lifecycle logic encapsulated in that KEP, however my controller solution focuses only on the issue of <em>terminating sidecar containers after a container exit, in order for the pod to fully complete</em>.</p> <p>It behaves with fairly simple logic:</p> <ul>\n<li>You define a list of &#x201C;sidecar containers&#x201D; as an annotation in your deployable PodSpec</li>\n<li>When a container within a monitored pod exits, the controller checks if all still-running containers are in the sidecar list</li>\n<li>If they are, we send a SIGTERM to each of those containers</li>\n</ul> <p>By following this simple logic, we were able to move past the immediate issue we were seeing without much concern.</p> <p>I will be the first to admit that the code in that repository is far from top-notch. In fact, I am embarassed by it. The controller was written as a stop-gap solution while the reference KEP was finalized and implemented. I never expected that we were be here 2 years later (4 years after the original issue was filed) still having to work around it. The initial project has been forked multiple times, improvements have been made, and I have done a terrible job in pulling improvements upstream and making the tool easy to use (again, based on the belief that it would not be necessary long-term). In particular, <a href=\"https://github.com/lemonade-hq/k8s-controller-sidecars\">Lemonade&#x2019;s fork</a> (work by <a href=\"https://github.com/igorshapiro\">@igorshapiro</a>) and <a href=\"https://github.com/Riskified/k8s-controller-sidecars\">Riskified&#x2019;s fork</a> (work by <a href=\"https://github.com/nisan270390\">@nisan270390</a>) look particularly promising. Now that I&#x2019;m seeing how long this tool is likely to be useful, there are many improvements that I want to pull back into the initial project.</p> <p>Much larger projects in the Kubernetes ecosystem, such as Istio, Linkerd, and even the google-sql proxy, have used sidecars as their implementation method, and have all had to build in work-arounds to this core issue.</p> <p>Other members of the community have attempted to build tools to solve this in similar ways. One example is <a href=\"https://karl.isenberg.us/\">Karl Isenberg</a> (<a href=\"https://github.com/karlkfi\">@karlkfi</a>), who attempted to <a href=\"https://github.com/kubernetes/kubernetes/issues/25908#issuecomment-619638636\">standardize</a> a <a href=\"https://github.com/karlkfi/kubexit\">tombstone approach</a> that can be built into their application images. While I am personally not a fan of this approach (I would rather the deployable images not have to worry about maintaining their own lifecycle), it is an option that has worked for them.</p> <blockquote>\n<p>If you&#x2019;re interested in other alternatives that have been considered, I recommend taking a look at the <a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/0753-sidecarcontainers.md#boolean-flag-on-container-sidecar-true\">Design Doc</a> for the Sidecar Containers KEP.</p>\n</blockquote> <p>The fact is that application and tool developers having to consider this situation at all breaks the premise of Kubernetes as a container orchestrator. When you have to start building your applications specifically to run within Kubernetes, Kubernetes can no longer be considered a &#x201C;container orchestrator&#x201D;; you have to start treating it as a unique deployment target. For some system level tools (such as Istio, Envoy, and other network proxies) that might be okay, as well as for applications specifically wanting to use more advanced Kubernetes features, but it does not feel like an acceptable state of the world for user-facing, platform agnostic web applications or APIs.</p> <p>Hopefully either the controller approach, or tombstone approach, can help bridge the gap until Kubernetes is able to address these issues natively.</p> </section>",
      "contentAsText": " Over 2 years ago I wrote a quick Kubernetes controller in order to ensuring that “sidecar” containers were shut down after the “main” container in a pod exited. The issue we were seeing was fairly straight forward: we had a container running in a pod to accomplish some application logic, as well as a number of “sidecar” containers to provide some functionality to the application. Examples of this are log forwarding, sql proxies, networking proxies, or in our case, metric collectors. We had a metric collection sidecar running along side a CronJob pod, which prevented the CronJob from ever “succeeding”. But why is this necessary in the first place? Why does it make sense for a sidecar container to stay alive after the main container has exited? In short, it doesn’t, but this is a long-outstanding issue in Kubernetes that we have to deal with. There has been an open proposal to address this issue for years, however it stumbled into the much larger issue of “container lifecycle management”, and is currently on-hold while #sig-node addresses some underlying node shutdown issues. There is now a lot of lifecycle logic encapsulated in that KEP, however my controller solution focuses only on the issue of terminating sidecar containers after a container exit, in order for the pod to fully complete. It behaves with fairly simple logic: \nYou define a list of “sidecar containers” as an annotation in your deployable PodSpec\nWhen a container within a monitored pod exits, the controller checks if all still-running containers are in the sidecar list\nIf they are, we send a SIGTERM to each of those containers\n By following this simple logic, we were able to move past the immediate issue we were seeing without much concern. I will be the first to admit that the code in that repository is far from top-notch. In fact, I am embarassed by it. The controller was written as a stop-gap solution while the reference KEP was finalized and implemented. I never expected that we were be here 2 years later (4 years after the original issue was filed) still having to work around it. The initial project has been forked multiple times, improvements have been made, and I have done a terrible job in pulling improvements upstream and making the tool easy to use (again, based on the belief that it would not be necessary long-term). In particular, Lemonade’s fork (work by @igorshapiro) and Riskified’s fork (work by @nisan270390) look particularly promising. Now that I’m seeing how long this tool is likely to be useful, there are many improvements that I want to pull back into the initial project. Much larger projects in the Kubernetes ecosystem, such as Istio, Linkerd, and even the google-sql proxy, have used sidecars as their implementation method, and have all had to build in work-arounds to this core issue. Other members of the community have attempted to build tools to solve this in similar ways. One example is Karl Isenberg (@karlkfi), who attempted to standardize a tombstone approach that can be built into their application images. While I am personally not a fan of this approach (I would rather the deployable images not have to worry about maintaining their own lifecycle), it is an option that has worked for them. \nIf you’re interested in other alternatives that have been considered, I recommend taking a look at the Design Doc for the Sidecar Containers KEP.\n The fact is that application and tool developers having to consider this situation at all breaks the premise of Kubernetes as a container orchestrator. When you have to start building your applications specifically to run within Kubernetes, Kubernetes can no longer be considered a “container orchestrator”; you have to start treating it as a unique deployment target. For some system level tools (such as Istio, Envoy, and other network proxies) that might be okay, as well as for applications specifically wanting to use more advanced Kubernetes features, but it does not feel like an acceptable state of the world for user-facing, platform agnostic web applications or APIs. Hopefully either the controller approach, or tombstone approach, can help bridge the gap until Kubernetes is able to address these issues natively. "
    },
    {
      "url": "https://kube-web-view.readthedocs.io/en/latest/alternatives.html",
      "title": "Alternative UIs�",
      "content": "<body class=\"wy-body-for-nav\"> <p class=\"wy-grid-for-nav\"> <nav class=\"wy-nav-side\"> </nav> <section class=\"wy-nav-content-wrap\"> <nav class=\"wy-nav-top\"> <i class=\"fa fa-bars\"></i> <a href=\"index.html\">Kubernetes Web View</a> </nav> <div class=\"wy-nav-content\"> <div class=\"rst-content\"> <div> </div> <div class=\"document\"> <div> <div class=\"section\" id=\"alternative-uis\"> <p>This page lists a number of alternative, open source UIs for Kubernetes.</p> <div class=\"section\" id=\"kubernator\"> <p><a class=\"reference external\" href=\"https://github.com/smpio/kubernator\">https://github.com/smpio/kubernator</a>, web, node.js</p>\n<blockquote>\n<div>&#x201C;Kubernator is an alternative Kubernetes UI. In contrast to high-level Kubernetes Dashboard, it provides low-level control and clean view on all objects in a cluster with the ability to create new ones, edit and resolve conflicts. As an entirely client-side app (like kubectl), it doesn&#x2019;t require any backend except Kubernetes API server itself, and also respects cluster&#x2019;s access control.&#x201D;</div></blockquote>\n</div>\n<div class=\"section\" id=\"kubernetes-dashboard\"> <p><a class=\"reference external\" href=\"https://github.com/kubernetes/dashboard\">https://github.com/kubernetes/dashboard</a>, web</p>\n<blockquote>\n<div>&#x201C;Kubernetes Dashboard is a general purpose, web-based UI for Kubernetes clusters. It allows users to manage applications running in the cluster and troubleshoot them, as well as manage the cluster itself.&#x201D;</div></blockquote>\n</div> <div class=\"section\" id=\"kubevious\"> <p><a class=\"reference external\" href=\"https://kubevious.io/\">https://kubevious.io/</a>, web</p>\n<blockquote>\n<div>&#x201C;Application-centric Kubernetes viewer and validator. Correlates labels, metadata, and state. Renders configuration in a way easy to understand and debug. TimeMachine enables travel back in time to identify why things broke. Extensible. Lets users define their own validation rules in the UI.&#x201D;</div></blockquote>\n</div> </div> </div> </div> <footer> <div> <p> &#xA9; Copyright 2019, Henning Jacobs <span class=\"commit\"> Revision <code>f27bc7af</code>. </span> </p> </div> Built with <a href=\"http://sphinx-doc.org/\">Sphinx</a> using a <a href=\"https://github.com/rtfd/sphinx_rtd_theme\">theme</a> provided by <a href=\"https://readthedocs.org\">Read the Docs</a>. </footer> </div> </div> </section> </p> </body>",
      "contentAsText": "       Kubernetes Web View         This page lists a number of alternative, open source UIs for Kubernetes.  https://github.com/smpio/kubernator, web, node.js\n\n“Kubernator is an alternative Kubernetes UI. In contrast to high-level Kubernetes Dashboard, it provides low-level control and clean view on all objects in a cluster with the ability to create new ones, edit and resolve conflicts. As an entirely client-side app (like kubectl), it doesn’t require any backend except Kubernetes API server itself, and also respects cluster’s access control.”\n\n https://github.com/kubernetes/dashboard, web\n\n“Kubernetes Dashboard is a general purpose, web-based UI for Kubernetes clusters. It allows users to manage applications running in the cluster and troubleshoot them, as well as manage the cluster itself.”\n  https://kubevious.io/, web\n\n“Application-centric Kubernetes viewer and validator. Correlates labels, metadata, and state. Renders configuration in a way easy to understand and debug. TimeMachine enables travel back in time to identify why things broke. Extensible. Lets users define their own validation rules in the UI.”\n       © Copyright 2019, Henning Jacobs  Revision f27bc7af.    Built with Sphinx using a theme provided by Read the Docs.      "
    },
    {
      "url": "https://github.com/xandout/asg-node-refresh",
      "title": "xandout/asg-node-refresh",
      "content": "<div><div><article class=\"markdown-body entry-content container-lg\">\n<p><a href=\"https://goreportcard.com/badge/github.com/xandout/asg-node-refresh\"><img src=\"https://camo.githubusercontent.com/dd7c63bb6c6560c3728b762b42db3cccedbb91d7a47bcf48bf59a2355032f057/68747470733a2f2f676f7265706f7274636172642e636f6d2f62616467652f6769746875622e636f6d2f78616e646f75742f6173672d6e6f64652d72656672657368\" alt=\"Go Report Card\"></a>\n<a href=\"https://raw.githubusercontent.com/xandout/asg-node-refresh/master/LICENSE\"><img src=\"https://camo.githubusercontent.com/61042288455899b035c88a29e6ca12bc0060e55da24989a9a29c1f020808ed4f/687474703a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d6d69742d626c75652e7376673f7374796c653d666c61742d737175617265\" alt=\"License\"></a></p>\n<p>An AWS Auto Scaling Group refresh tool.</p>\n<h2><a id=\"user-content-why\" class=\"anchor\" href=\"https://github.com/xandout/asg-node-refresh#why\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Why?</h2>\n<p>AWS offers only one way to schedule node refreshes in an ASG: <code>max_instance_lifetime</code>.  This method will refresh an instance after the TTL expires but what happens if the node came up during peak load on a Tuesday afternoon? Well your instance will go away at the same time next Tuesday.  Not ideal.</p>\n<h2><a id=\"user-content-how-does-it-work\" class=\"anchor\" href=\"https://github.com/xandout/asg-node-refresh#how-does-it-work\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>How does it work?</h2>\n<p>This is a Kubernetes native application and uses a <code>ConfigMap</code> to store state.</p>\n<p>The basic workflow is</p>\n<ol>\n<li>Create <code>CronJob</code> to run the refresh when it makes sense for your business</li>\n<li>The <code>Job</code> starts on one of your nodes and triggers an ASG refresh, storing the refresh-id in the <code>ConfigMap</code></li>\n<li>When the node the <code>Job</code> started on is terminated as part of the refresh, it is restarted on another node and picks up where it left off</li>\n<li>Eventually all of the nodes are refreshed and the app cleans up after itself.</li>\n</ol>\n<h2><a id=\"user-content-how-do-i-deploy-it\" class=\"anchor\" href=\"https://github.com/xandout/asg-node-refresh#how-do-i-deploy-it\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>How do I deploy it?</h2>\n<p>Create an IAM user with the following permissions.  Below are two examples of the IAM policy.  The first is restricted to refreshing a single ASG, the second all ASGs(not recommended).</p>\n<h3><a id=\"user-content-iam-policy---read-one-asg\" class=\"anchor\" href=\"https://github.com/xandout/asg-node-refresh#iam-policy---read-one-asg\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>IAM Policy - Read one ASG</h3>\n<div class=\"highlight highlight-source-json\"><pre>{\n    <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>Version<span class=\"pl-pds\">&quot;</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>2012-10-17<span class=\"pl-pds\">&quot;</span></span>,\n    <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>Statement<span class=\"pl-pds\">&quot;</span></span>: [\n        {\n            <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>Sid<span class=\"pl-pds\">&quot;</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>AllowStartRefresh<span class=\"pl-pds\">&quot;</span></span>,\n            <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>Effect<span class=\"pl-pds\">&quot;</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>Allow<span class=\"pl-pds\">&quot;</span></span>,\n            <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>Action<span class=\"pl-pds\">&quot;</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>autoscaling:StartInstanceRefresh<span class=\"pl-pds\">&quot;</span></span>,\n            <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>Resource<span class=\"pl-pds\">&quot;</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>ASG_ARN<span class=\"pl-pds\">&quot;</span></span>\n        },\n        {\n            <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>Sid<span class=\"pl-pds\">&quot;</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>AllowDescribeRefreshes<span class=\"pl-pds\">&quot;</span></span>,\n            <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>Effect<span class=\"pl-pds\">&quot;</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>Allow<span class=\"pl-pds\">&quot;</span></span>,\n            <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>Action<span class=\"pl-pds\">&quot;</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>autoscaling:DescribeInstanceRefreshes<span class=\"pl-pds\">&quot;</span></span>,\n            <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>Resource<span class=\"pl-pds\">&quot;</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>*<span class=\"pl-pds\">&quot;</span></span>\n        }\n    ]\n}</pre></div>\n<h3><a id=\"user-content-iam-policy---read-all-asgs\" class=\"anchor\" href=\"https://github.com/xandout/asg-node-refresh#iam-policy---read-all-asgs\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>IAM Policy - Read All ASGs</h3>\n<div class=\"highlight highlight-source-json\"><pre>{\n    <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>Version<span class=\"pl-pds\">&quot;</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>2012-10-17<span class=\"pl-pds\">&quot;</span></span>,\n    <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>Statement<span class=\"pl-pds\">&quot;</span></span>: [\n        {\n            <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>Sid<span class=\"pl-pds\">&quot;</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>AllowStartRefresh<span class=\"pl-pds\">&quot;</span></span>,\n            <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>Effect<span class=\"pl-pds\">&quot;</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>Allow<span class=\"pl-pds\">&quot;</span></span>,\n            <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>Action<span class=\"pl-pds\">&quot;</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>autoscaling:StartInstanceRefresh<span class=\"pl-pds\">&quot;</span></span>,\n            <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>Resource<span class=\"pl-pds\">&quot;</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>arn:aws:autoscaling:*:*:autoScalingGroup:*:autoScalingGroupName/*<span class=\"pl-pds\">&quot;</span></span>\n        },\n        {\n            <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>Sid<span class=\"pl-pds\">&quot;</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>AllowDescribeRefreshes<span class=\"pl-pds\">&quot;</span></span>,\n            <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>Effect<span class=\"pl-pds\">&quot;</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>Allow<span class=\"pl-pds\">&quot;</span></span>,\n            <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>Action<span class=\"pl-pds\">&quot;</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>autoscaling:DescribeInstanceRefreshes<span class=\"pl-pds\">&quot;</span></span>,\n            <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>Resource<span class=\"pl-pds\">&quot;</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">&quot;</span>*<span class=\"pl-pds\">&quot;</span></span>\n        }\n    ]\n}</pre></div>\n<h2><a id=\"user-content-update-yaml\" class=\"anchor\" href=\"https://github.com/xandout/asg-node-refresh#update-yaml\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Update YAML</h2>\n<h3><a id=\"user-content-set-aws-credentials-in-aws_creds_secretyml\" class=\"anchor\" href=\"https://github.com/xandout/asg-node-refresh#set-aws-credentials-in-aws_creds_secretyml\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Set AWS credentials in <a href=\"https://github.com/xandout/asg-node-refresh/blob/master/_manifests/aws_creds_secret.yml.example\">aws_creds_secret.yml</a></h3>\n<p>You will need to base64 encode the values of <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code> and the name(not ARN) of your target ASG.</p>\n<pre><code>echo -n &quot;YOUR_IAM_ID&quot; | base64\necho -n &quot;YOUR_IAM_SECRET&quot; | base64\necho -n &quot;your-asg-name&quot; | base64\n</code></pre>\n<h3><a id=\"user-content-update-the-env-in-cronjobyml\" class=\"anchor\" href=\"https://github.com/xandout/asg-node-refresh#update-the-env-in-cronjobyml\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Update the <code>env</code> in <a href=\"https://github.com/xandout/asg-node-refresh/blob/master/_manifests/cronjob.yml\">cronjob.yml</a></h3>\n<ul>\n<li>AWS_REGION\n\n</li>\n<li>AWS_ASG_NAME\n\n</li>\n<li>INSTANCE_WARMUP\n\n</li>\n</ul>\n<h3><a id=\"user-content-apply-the-cronjob\" class=\"anchor\" href=\"https://github.com/xandout/asg-node-refresh#apply-the-cronjob\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Apply the CronJob</h3>\n<div class=\"highlight highlight-source-shell\"><pre>kubectl apply -f _manifests/cronjob.yml</pre></div>\n</article></div></div>",
      "contentAsText": "\n\n\nAn AWS Auto Scaling Group refresh tool.\nWhy?\nAWS offers only one way to schedule node refreshes in an ASG: max_instance_lifetime.  This method will refresh an instance after the TTL expires but what happens if the node came up during peak load on a Tuesday afternoon? Well your instance will go away at the same time next Tuesday.  Not ideal.\nHow does it work?\nThis is a Kubernetes native application and uses a ConfigMap to store state.\nThe basic workflow is\n\nCreate CronJob to run the refresh when it makes sense for your business\nThe Job starts on one of your nodes and triggers an ASG refresh, storing the refresh-id in the ConfigMap\nWhen the node the Job started on is terminated as part of the refresh, it is restarted on another node and picks up where it left off\nEventually all of the nodes are refreshed and the app cleans up after itself.\n\nHow do I deploy it?\nCreate an IAM user with the following permissions.  Below are two examples of the IAM policy.  The first is restricted to refreshing a single ASG, the second all ASGs(not recommended).\nIAM Policy - Read one ASG\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowStartRefresh\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"autoscaling:StartInstanceRefresh\",\n            \"Resource\": \"ASG_ARN\"\n        },\n        {\n            \"Sid\": \"AllowDescribeRefreshes\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"autoscaling:DescribeInstanceRefreshes\",\n            \"Resource\": \"*\"\n        }\n    ]\n}\nIAM Policy - Read All ASGs\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowStartRefresh\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"autoscaling:StartInstanceRefresh\",\n            \"Resource\": \"arn:aws:autoscaling:*:*:autoScalingGroup:*:autoScalingGroupName/*\"\n        },\n        {\n            \"Sid\": \"AllowDescribeRefreshes\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"autoscaling:DescribeInstanceRefreshes\",\n            \"Resource\": \"*\"\n        }\n    ]\n}\nUpdate YAML\nSet AWS credentials in aws_creds_secret.yml\nYou will need to base64 encode the values of AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and the name(not ARN) of your target ASG.\necho -n \"YOUR_IAM_ID\" | base64\necho -n \"YOUR_IAM_SECRET\" | base64\necho -n \"your-asg-name\" | base64\n\nUpdate the env in cronjob.yml\n\nAWS_REGION\n\n\nAWS_ASG_NAME\n\n\nINSTANCE_WARMUP\n\n\n\nApply the CronJob\nkubectl apply -f _manifests/cronjob.yml\n",
      "description": "K8S CronJob to schedule AWS ASG refreshes. Contribute to xandout/asg-node-refresh development by creating an account on GitHub.",
      "ogDescription": "K8S CronJob to schedule AWS ASG refreshes. Contribute to xandout/asg-node-refresh development by creating an account on GitHub."
    },
    {
      "url": "https://erickhun.com/posts/kubernetes-faster-services-no-cpu-limits/?reddit",
      "title": "Kubernetes: Make your services faster by removing CPU limits",
      "content": "<div> <p>At Buffer, we&#x2019;ve been using <a href=\"https://kubernetes.io/case-studies/buffer/\">Kubernetes since 2016</a>. We&#x2019;ve been managing our k8s (kubernetes) cluster with <a href=\"https://kops.sigs.k8s.io\">kops</a>, it has about 60 nodes (on AWS), and runs about 1500 containers. Our transition to a micro-service architecture has been full of trial and errors. Even after a few years running k8s, we are still learning its secrets. This post will talk about how something we thought was a good thing, but ended up to be not as great as we thought: <strong>CPU limits</strong>.</p>\n<h2 id=\"cpu-limits-and-throttling\">CPU limits and Throttling</h2>\n<p>It is s general recommendation to set CPU limits. <a href=\"https://cloud.google.com/blog/products/gcp/kubernetes-best-practices-resource-requests-and-limits\">Google, among others, highly recommends it</a>. The danger of not setting a CPU limit is that containers running in the node could exhaust all CPU available. This can trigger a cascade of unwanted events such as having key Kubernetes processes (such as <code>kubelet</code>) to become unresponsive. So it is in theory a great thing to set CPU limit in order to protect your nodes.</p>\n<p>CPU limits is the maximum CPU time a container can uses at a given period (100ms by default). The CPU usage for a container will never go above that limit you specified. Kubernetes use a mechanism called <a href=\"https://en.wikipedia.org/wiki/Completely_Fair_Scheduler\">CFS Quota</a> to <strong>throttle</strong> the container to prevent the CPU usage from going above the limit. That means CPU will be artificially restricted, making the performance of your containers lower (and slower when it comes to latency).</p>\n<h2 id=\"what-can-happen-if-we-dont-set-cpu-limits\">What can happen if we don&#x2019;t set CPU limits?</h2>\n<p>We unfortunately experienced the issue. The <code>kubelet</code> , a process running on every node, and in charge of managing the containers (pods) in the nodes became unresponsive. The node will turn into a <code>NotReady</code> state, and containers (pods) that were present will be rescheduled somewhere else, and create the issue in the new nodes. Definitely not ideal isn&#x2019;t it?</p>\n<h2 id=\"discovering-the-throttling-and-latency-issue\">Discovering the throttling and latency issue</h2>\n<p>A key metric to check when you are running container is the <code>throttling</code> . This indicate the number of time your container has been throttled. Interestingly, we&#x2019;ve discovered a lot of containers having throttling no matter if the CPU usage was near the limits or not. Here the example of one of our main API:</p>\n<p><img src=\"https://erickhun.com/img/kubernetes-cpu-limits/cpu-usage-limits.png\" alt=\"Kubernetes pods CPU usage and limits\"></p>\n<p>You can see in the animation that the CPU limits is set at <code>800m</code> (0.8 core, 80% of a core), and the peak usage is at most <code>200m</code> (20% of a core). After seeing, we might think we have plenty of CPU to let the service running before it throttle right? . Now check this one out:</p>\n<p><img src=\"https://erickhun.com/img/kubernetes-cpu-limits/cpu-throttling-low-usage.gif\" alt=\"Kubernetes pods Low CPU usage, High limit, lot of throttling\"></p>\n<p>You can notice the CPU throttling occurs, even though the CPU usage is below the CPU Limits. The maximum CPU usage isn&#x2019;t even near the CPU limits.</p>\n<p>We&#x2019;ve then found a few resources(<a href=\"https://github.com/kubernetes/kubernetes/issues/67577\">github issue</a>, <a href=\"https://www.youtube.com/watch?v=LpFApeaGv7A&amp;feature=youtu.be&amp;t=1204\">Zalando talk</a>, <a href=\"https://medium.com/omio-engineering/cpu-limits-and-aggressive-throttling-in-kubernetes-c5b20bd8a718\">omio post</a>) talking about how throttling lead to poorer performances and latency for your services.</p>\n<p><strong>Why do we see CPU throttling while CPU usage is low?</strong>\nThe tldr is basically a bug in the Linux kernel throttling unecessarly containers with CPU limit. If you&#x2019;re curious about the nature of it, I invite you to check Dave Chiluk&#x2019;s <a href=\"https://www.youtube.com/watch?v=UE7QX98-kO0\">great talk</a>, a <a href=\"https://engineering.indeedblog.com/blog/2019/12/unthrottled-fixing-cpu-limits-in-the-cloud/\">written version</a> also exists with more details.</p> <p>After many long discussions, we&#x2019;ve decided to remove the CPU limits for all services that were directly or indirectly on the critical path of our users.</p>\n<p>This wasn&#x2019;t an easy decision since we value the stability of our cluster. We&#x2019;ve experimented in the past some instability in our cluster with services using too much resources and disrupting all other services present in the same node. That time was a bit different, we understood more about how our services needed to be and had a good strategy to roll this out.</p>\n<p><img src=\"https://erickhun.com/img/kubernetes-cpu-limits/unleash-k8s.jpg\" alt=\"Buffer-remove-cpu-limits-announcement\"></p>\n<h2 id=\"how-to-keep-your-nodes-safe-when-removing-limits-\">How to keep your nodes safe when removing limits ?</h2>\n<p><strong>Isolating &#x201C;No CPU Limits&#x201D; services:</strong></p>\n<p>In the past we&#x2019;ve seen some nodes going to a <code>notReady</code> state, mainly because some services were using too much resources in a node.</p>\n<p>We&#x2019;ve decided to put those services on some specific nodes (tainted nodes), so those services will not disrupt all the &#x201C;bounded&#x201D; ones. We have better control and could identify easier if any issue occurs with a node. We did this by tainting some nodes and adding toleration to services that were &#x201C;unbounded&#x201D;. Check <a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/\">the documentation</a> on how you can do that.</p>\n<p><img src=\"https://erickhun.com/img/kubernetes-cpu-limits/buffer-k8s-infrastructure-nodes.jpg\" alt=\"Buffer k8s nodes infrastructure\"></p>\n<p><strong>Assigning the correct CPU and memory request:</strong></p>\n<p>The main worry we had was service using too much resources and leading to nodes becoming unresponsive. Because we now had great observability of all services running in our cluster (with Datadog), I\u0019ve analyzed a few months of usage of each service we wanted to &#x201C;unbound&#x201D;. I\u0019ve assigned the maximum CPU usage as the CPU request with a &gt; 20% margin. This will make sure to have allocated space in a node. If k8s won\u0019t try to schedule any other service in that node.</p>\n<p><img src=\"https://erickhun.com/img/kubernetes-cpu-limits/choose-cpu-request-based-on-max.png\" alt=\"Chose CPU request based on max\"></p>\n<p>You can see in the graph that the peak CPU usage was <code>242m</code> CPU core (0.242 CPU core). Simply take this number and make it a bit higher to become the CPU request. You can notice that since the service is user facing, the peak CPU usage matches peak traffic time.</p>\n<p>Do the same with your memory usage and requests, and you will be all set!\nTo add more safety, you can use the horizontal pod autoscaler to create new pods if the resource usage is high, so kubernetes will schedule it in nodes that have room for it. Set an alert if your cluster do not have any room, or use the node austoscaler to add it automatically.</p>\n<p>The downsides are that we lose in &#x201C;<a href=\"https://wiki.openvz.org/WP/Containers_density\">container density</a>&#x201D;, the number of containers that can run in a single node. We could also end up with a lot of \u001cslack\u001d during a low traffic time.\nYou could also hit some high CPU usage, but nodes autoscaling should help you with it.</p>\n<h2 id=\"results\">Results</h2>\n<p>I&#x2019;m happy to publish really great results after few weeks of experimentation, we&#x2019;ve already seen really great latency improvements across all the services we&#x2019;ve modified:</p>\n<p><img src=\"https://erickhun.com/img/kubernetes-cpu-limits/speedup-no-cpu-limits2.png\" alt=\"faster-kubernetes-containers\"></p>\n<p>The best result happened on our main landing page (<a href=\"https://buffer.com\">buffer.com</a>) where we speed the service up to <strong>22x</strong> faster!</p>\n<p><img src=\"https://erickhun.com/img/kubernetes-cpu-limits/no-cpu-limit-speedup-buffer-com.jpg\" alt=\"buffer.com speedup without cpu limits\"></p>\n<h2 id=\"is-the-linux-kernel-bug-fixed\">Is the Linux kernel bug fixed?</h2>\n<p>The bug <a href=\"https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=763a9ec06c4\">has been fixed and merged into the kernel</a> for Linux distribution running 4.19 or higher (kudo again to <a href=\"https://twitter.com/dchiluk\">Dave Chiluk</a> for finding and fixing that).</p>\n<p>However, as for <em>September 2nd 2020</em>, when reading <a href=\"https://github.com/kubernetes/kubernetes/issues/67577\">the kubernetes issue</a>, we can see various Linux projects that keep referencing the issue, so I guess some Linux distribution still have the bug and working into integrating the fix.</p>\n<p>If you are below a Linux distribution that has a kernel version below 4.19, I&#x2019;d recommend you to upgrade to the latest Linux distribution for your nodes, but in any case, you should try removing the CPU limits and see if you have any throttling. Here a non exhausting list of various managed Kubernetes services or Linux distribution:</p>\n<ul>\n<li>Debian: The latest version <a href=\"%5Bhttps://www.debian.org/releases/buster/\">buster</a> has the fix, it looks <a href=\"https://tracker.debian.org/news/1167353/accepted-linux-latest-419-105deb10u5deb9u1-source-amd64-into-oldstable-oldstable/\">quite recent (august 2020)</a>. Some previous version might have get patched</li>\n<li>Ubuntu: The latest version <a href=\"https://releases.ubuntu.com/20.04/\">Ubuntu Focal Fosa 20.04</a> has the fix.</li>\n<li>EKS has the fix since <a href=\"https://github.com/aws/containers-roadmap/issues/175\">December 2019</a>. Upgrade your AMI if you have a version below than that</li>\n<li>kops: Since <a href=\"https://github.com/kubernetes/kops/pull/9283\">June 2020</a>, <code>kops 1.18+</code> will start using <code>Ubuntu 20.04</code> as the default host image. If you&#x2019;re using a lower version of kops, you&#x2019;ll have to probably to wait the fix. We are currently in this situation.</li>\n<li>GKE (Goggle Cloud) : The kernel fix was merged in <a href=\"https://cloud.google.com/container-optimized-os/docs/release-notes#cos-stable-77-12371-141-0\">January 2020</a>. But it does looks like throttling are <a href=\"https://news.ycombinator.com/item?id=24356903\">still hapenning</a></li>\n</ul>\n<p>ps: Feel free to <a href=\"https://news.ycombinator.com/item?id=24351566\">comment</a> if you have more precise information, I&#x2019;ll update the post accordingly</p>\n<p><strong>If the fix solved the throttling issue?</strong></p>\n<p>I&#x2019;m unsure if totally solved the issue. I will give it a try once we hit a kernel version where the fix has been implemented and will update this post accordingly. If anyone have upgrade I&#x2019;m keen to hear their results.</p>\n<h2 id=\"takeaways\">Takeaways</h2>\n<ul>\n<li>If you run Docker containers under Linux (no matter Kubernetes/Mesos/Swarm) you might have your containers underperforming because of throttling</li>\n<li>Upgrade to the latest version of your distribution hoping the bug is fixed</li>\n<li>Removing CPU limit is a solution to solve this issue, but this is dangerous and should be made with extra-care (prefer upgrading your kernel first and monitor throttling first)</li>\n<li>If you remove CPU limits, carefully monitor CPU and memory usage in your nodes, and make sure your CPU requests are</li>\n<li>A safe way to is to use the Horizontal pod autoscaler to create new pods if the resource usage is high, so kubernetes will schedule it in nodes that have space.</li>\n</ul>\n<p>=I<strong>Hacker news update: lot of insighful <a href=\"https://news.ycombinator.com/item?id=24351566\">comments</a>. I&#x2019;ve updated the post to have better recommendations. You should prefer upgrading your kernel version over removing the CPU limits. But throttling will still be present. If your goal is low latency, remove CPU limits, but be really mindful when doing this: set the proper CPU requests, add the necessary monitoring when you do this. Read the <a href=\"https://news.ycombinator.com/item?id=24381813\">comment written by Tim Hockin</a> from Google (and one of <a href=\"https://en.wikipedia.org/wiki/Kubernetes#History\">Kubernetes creator</a>)</strong></p>\n<p>I hope this post helps you get performance gains on the containers you are running. If so, don&#x2019;t hesitate to share or <a href=\"https://news.ycombinator.com/item?id=24351566\">comment</a> with always some insighful comments</p>\n<p>Special thanks to <a href=\"https://www.linkedin.com/in/dilyevsky/\">Dmitry</a>, <a href=\"https://coderanger.net/\">Noah</a> and <a href=\"https://mydev.org/\">Andre</a> that adviced me on this.</p>\n<h4 id=\"next-reads\">Next reads:</h4>\n<p>=I <a href=\"https://erickhun.com/posts/why-you-should-have-a-side-project/\">Why you should have a side project</a></p>\n<p>=I <a href=\"https://erickhun.com/posts/sharing-knowledge-in-a-remote-team/\">How we share technical knowledge in a remote team, across timezones</a></p> <center> <a href=\"https://twitter.com/eric_khun\" class=\"twitter-follow-button\">Follow @eric_khun</a> <br> <a class=\"resp-sharing-button__link\" href=\"https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ferickhun.com%2fposts%2fkubernetes-faster-services-no-cpu-limits%2f&amp;s=fb\"> </a> <a class=\"resp-sharing-button__link\" href=\"https://twitter.com/intent/tweet/?text=Kubernetes%3a%20Make%20your%20services%20faster%20by%20removing%20CPU%20limits by @eric_khun &amp;url=https%3a%2f%2ferickhun.com%2fposts%2fkubernetes-faster-services-no-cpu-limits%2f&amp;s=tw\"> </a> <a class=\"resp-sharing-button__link\" href=\"/cdn-cgi/l/email-protection#8db2fef8efe7e8eef9b0c6f8efe8ffe3e8f9e8fea8beeca8bfbdc0ece6e8a8bfbdf4e2f8ffa8bfbdfee8fffbe4eee8fea8bfbdebecfef9e8ffa8bfbdeff4a8bfbdffe8e0e2fbe4e3eaa8bfbdceddd8a8bfbde1e4e0e4f9feadabece0fdb6efe2e9f4b0c6f8efe8ffe3e8f9e8fea8beeca8bfbdc0ece6e8a8bfbdf4e2f8ffa8bfbdfee8fffbe4eee8fea8bfbdebecfef9e8ffa8bfbdeff4a8bfbdffe8e0e2fbe4e3eaa8bfbdceddd8a8bfbde1e4e0e4f9feada0ade5f9f9fdfea8beeca8bfeba8bfebe8ffe4eee6e5f8e3a3eee2e0a8bfebfde2fef9fea8bfebe6f8efe8ffe3e8f9e8fea0ebecfef9e8ffa0fee8fffbe4eee8fea0e3e2a0eefdf8a0e1e4e0e4f9fea8bfebabfeb0e8e0ece4e1\"> </a> <a class=\"resp-sharing-button__link\" href=\"https://reddit.com/submit/?url=https%3a%2f%2ferickhun.com%2fposts%2fkubernetes-faster-services-no-cpu-limits%2f&amp;resubmit=true&amp;title=Kubernetes%3a%20Make%20your%20services%20faster%20by%20removing%20CPU%20limits&amp;s=red\"> </a> <a class=\"resp-sharing-button__link\" href=\"whatsapp://send?text=Kubernetes%3a%20Make%20your%20services%20faster%20by%20removing%20CPU%20limits - https%3a%2f%2ferickhun.com%2fposts%2fkubernetes-faster-services-no-cpu-limits%2f&amp;s=whatsapp\"> </a> </center> </div>",
      "contentAsText": " At Buffer, we’ve been using Kubernetes since 2016. We’ve been managing our k8s (kubernetes) cluster with kops, it has about 60 nodes (on AWS), and runs about 1500 containers. Our transition to a micro-service architecture has been full of trial and errors. Even after a few years running k8s, we are still learning its secrets. This post will talk about how something we thought was a good thing, but ended up to be not as great as we thought: CPU limits.\nCPU limits and Throttling\nIt is s general recommendation to set CPU limits. Google, among others, highly recommends it. The danger of not setting a CPU limit is that containers running in the node could exhaust all CPU available. This can trigger a cascade of unwanted events such as having key Kubernetes processes (such as kubelet) to become unresponsive. So it is in theory a great thing to set CPU limit in order to protect your nodes.\nCPU limits is the maximum CPU time a container can uses at a given period (100ms by default). The CPU usage for a container will never go above that limit you specified. Kubernetes use a mechanism called CFS Quota to throttle the container to prevent the CPU usage from going above the limit. That means CPU will be artificially restricted, making the performance of your containers lower (and slower when it comes to latency).\nWhat can happen if we don’t set CPU limits?\nWe unfortunately experienced the issue. The kubelet , a process running on every node, and in charge of managing the containers (pods) in the nodes became unresponsive. The node will turn into a NotReady state, and containers (pods) that were present will be rescheduled somewhere else, and create the issue in the new nodes. Definitely not ideal isn’t it?\nDiscovering the throttling and latency issue\nA key metric to check when you are running container is the throttling . This indicate the number of time your container has been throttled. Interestingly, we’ve discovered a lot of containers having throttling no matter if the CPU usage was near the limits or not. Here the example of one of our main API:\n\nYou can see in the animation that the CPU limits is set at 800m (0.8 core, 80% of a core), and the peak usage is at most 200m (20% of a core). After seeing, we might think we have plenty of CPU to let the service running before it throttle right? . Now check this one out:\n\nYou can notice the CPU throttling occurs, even though the CPU usage is below the CPU Limits. The maximum CPU usage isn’t even near the CPU limits.\nWe’ve then found a few resources(github issue, Zalando talk, omio post) talking about how throttling lead to poorer performances and latency for your services.\nWhy do we see CPU throttling while CPU usage is low?\nThe tldr is basically a bug in the Linux kernel throttling unecessarly containers with CPU limit. If you’re curious about the nature of it, I invite you to check Dave Chiluk’s great talk, a written version also exists with more details. After many long discussions, we’ve decided to remove the CPU limits for all services that were directly or indirectly on the critical path of our users.\nThis wasn’t an easy decision since we value the stability of our cluster. We’ve experimented in the past some instability in our cluster with services using too much resources and disrupting all other services present in the same node. That time was a bit different, we understood more about how our services needed to be and had a good strategy to roll this out.\n\nHow to keep your nodes safe when removing limits ?\nIsolating “No CPU Limits” services:\nIn the past we’ve seen some nodes going to a notReady state, mainly because some services were using too much resources in a node.\nWe’ve decided to put those services on some specific nodes (tainted nodes), so those services will not disrupt all the “bounded” ones. We have better control and could identify easier if any issue occurs with a node. We did this by tainting some nodes and adding toleration to services that were “unbounded”. Check the documentation on how you can do that.\n\nAssigning the correct CPU and memory request:\nThe main worry we had was service using too much resources and leading to nodes becoming unresponsive. Because we now had great observability of all services running in our cluster (with Datadog), I\u0019ve analyzed a few months of usage of each service we wanted to “unbound”. I\u0019ve assigned the maximum CPU usage as the CPU request with a > 20% margin. This will make sure to have allocated space in a node. If k8s won\u0019t try to schedule any other service in that node.\n\nYou can see in the graph that the peak CPU usage was 242m CPU core (0.242 CPU core). Simply take this number and make it a bit higher to become the CPU request. You can notice that since the service is user facing, the peak CPU usage matches peak traffic time.\nDo the same with your memory usage and requests, and you will be all set!\nTo add more safety, you can use the horizontal pod autoscaler to create new pods if the resource usage is high, so kubernetes will schedule it in nodes that have room for it. Set an alert if your cluster do not have any room, or use the node austoscaler to add it automatically.\nThe downsides are that we lose in “container density”, the number of containers that can run in a single node. We could also end up with a lot of \u001cslack\u001d during a low traffic time.\nYou could also hit some high CPU usage, but nodes autoscaling should help you with it.\nResults\nI’m happy to publish really great results after few weeks of experimentation, we’ve already seen really great latency improvements across all the services we’ve modified:\n\nThe best result happened on our main landing page (buffer.com) where we speed the service up to 22x faster!\n\nIs the Linux kernel bug fixed?\nThe bug has been fixed and merged into the kernel for Linux distribution running 4.19 or higher (kudo again to Dave Chiluk for finding and fixing that).\nHowever, as for September 2nd 2020, when reading the kubernetes issue, we can see various Linux projects that keep referencing the issue, so I guess some Linux distribution still have the bug and working into integrating the fix.\nIf you are below a Linux distribution that has a kernel version below 4.19, I’d recommend you to upgrade to the latest Linux distribution for your nodes, but in any case, you should try removing the CPU limits and see if you have any throttling. Here a non exhausting list of various managed Kubernetes services or Linux distribution:\n\nDebian: The latest version buster has the fix, it looks quite recent (august 2020). Some previous version might have get patched\nUbuntu: The latest version Ubuntu Focal Fosa 20.04 has the fix.\nEKS has the fix since December 2019. Upgrade your AMI if you have a version below than that\nkops: Since June 2020, kops 1.18+ will start using Ubuntu 20.04 as the default host image. If you’re using a lower version of kops, you’ll have to probably to wait the fix. We are currently in this situation.\nGKE (Goggle Cloud) : The kernel fix was merged in January 2020. But it does looks like throttling are still hapenning\n\nps: Feel free to comment if you have more precise information, I’ll update the post accordingly\nIf the fix solved the throttling issue?\nI’m unsure if totally solved the issue. I will give it a try once we hit a kernel version where the fix has been implemented and will update this post accordingly. If anyone have upgrade I’m keen to hear their results.\nTakeaways\n\nIf you run Docker containers under Linux (no matter Kubernetes/Mesos/Swarm) you might have your containers underperforming because of throttling\nUpgrade to the latest version of your distribution hoping the bug is fixed\nRemoving CPU limit is a solution to solve this issue, but this is dangerous and should be made with extra-care (prefer upgrading your kernel first and monitor throttling first)\nIf you remove CPU limits, carefully monitor CPU and memory usage in your nodes, and make sure your CPU requests are\nA safe way to is to use the Horizontal pod autoscaler to create new pods if the resource usage is high, so kubernetes will schedule it in nodes that have space.\n\n=IHacker news update: lot of insighful comments. I’ve updated the post to have better recommendations. You should prefer upgrading your kernel version over removing the CPU limits. But throttling will still be present. If your goal is low latency, remove CPU limits, but be really mindful when doing this: set the proper CPU requests, add the necessary monitoring when you do this. Read the comment written by Tim Hockin from Google (and one of Kubernetes creator)\nI hope this post helps you get performance gains on the containers you are running. If so, don’t hesitate to share or comment with always some insighful comments\nSpecial thanks to Dmitry, Noah and Andre that adviced me on this.\nNext reads:\n=I Why you should have a side project\n=I How we share technical knowledge in a remote team, across timezones  Follow @eric_khun             ",
      "publishedDate": "2020-08-31T15:56:00.000Z",
      "description": "Eric Khun's personal website",
      "ogDescription": "At Buffer, we’ve been using Kubernetes since 2016. We’ve been managing our k8s (kubernetes) cluster with kops, it has about 60 nodes (on AWS), and runs about 1500 containers. Our transition to a micro-service architecture has been full of trial and errors. Even after a few years running k8s, we are still learning its secrets. This post will talk about how something we thought was a good thing, but ended up to be not as great as we thought: CPU limits."
    },
    {
      "url": "https://github.com/geritol/secret-backup-operator",
      "title": "geritol/secret-backup-operator",
      "content": "<div><div><article class=\"markdown-body entry-content container-lg\">\n<p>An operator to backup secrets on a Kubernetes cluster.<br>\nBackup happens when secrets are modified.<br>\nBackup data is stored in an other secret <code>&lt;secret-name&gt;-backup</code>, that has a single key <code>BACKUP</code> containing the secrets versions in a JSON encoded list.</p>\n<h2><a id=\"user-content-setup\" class=\"anchor\" href=\"https://github.com/geritol/secret-backup-operator#setup\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Setup</h2>\n<p>To be able to run this on a cluster you need to deploy a Deployment that runs an image built using the provided <code>Dockerfile</code>, the pods need to run with a <code>ServiceAccount</code> that is authorized to create, read, watch and edit secrets. Eg.:</p>\n<div class=\"highlight highlight-source-yaml\"><pre><span class=\"pl-ent\">apiVersion</span>: <span class=\"pl-c1\">v1</span>\n<span class=\"pl-ent\">kind</span>: <span class=\"pl-s\">ServiceAccount</span>\n<span class=\"pl-ent\">metadata</span>:\n  <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">secret-operator-service-account</span>\n  <span class=\"pl-ent\">namespace</span>: <span class=\"pl-s\">some-namepace</span>\n---\n<span class=\"pl-ent\">apiVersion</span>: <span class=\"pl-s\">rbac.authorization.k8s.io/v1</span>\n<span class=\"pl-ent\">kind</span>: <span class=\"pl-s\">ClusterRole</span>\n<span class=\"pl-ent\">metadata</span>:\n  <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">secret-reader</span>\n<span class=\"pl-ent\">rules</span>:\n  - <span class=\"pl-ent\">apiGroups</span>: <span class=\"pl-s\">[&quot;&quot;]</span>\n    <span class=\"pl-ent\">resources</span>: <span class=\"pl-s\">[&quot;secrets&quot;]</span>\n    <span class=\"pl-ent\">verbs</span>: <span class=\"pl-s\">[&quot;get&quot;, &quot;watch&quot;, create&quot;, &quot;update&quot;, &quot;patch&quot;]</span>\n---\n<span class=\"pl-ent\">apiVersion</span>: <span class=\"pl-s\">rbac.authorization.k8s.io/v1</span>\n<span class=\"pl-ent\">kind</span>: <span class=\"pl-s\">ClusterRoleBinding</span>\n<span class=\"pl-ent\">metadata</span>:\n  <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">read-secrets</span>\n  <span class=\"pl-ent\">namespace</span>: <span class=\"pl-s\">default</span>\n<span class=\"pl-ent\">subjects</span>:\n  - <span class=\"pl-ent\">kind</span>: <span class=\"pl-s\">ServiceAccount</span>\n    <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">secret-operator-service-account</span>\n    <span class=\"pl-ent\">namespace</span>: <span class=\"pl-s\">some-namepace</span>\n<span class=\"pl-ent\">roleRef</span>:\n  <span class=\"pl-ent\">kind</span>: <span class=\"pl-s\">ClusterRole</span>\n  <span class=\"pl-ent\">name</span>: <span class=\"pl-s\">secret-reader</span>\n  <span class=\"pl-ent\">apiGroup</span>: <span class=\"pl-s\">rbac.authorization.k8s.io</span></pre></div>\n</article></div></div>",
      "contentAsText": "\nAn operator to backup secrets on a Kubernetes cluster.\nBackup happens when secrets are modified.\nBackup data is stored in an other secret <secret-name>-backup, that has a single key BACKUP containing the secrets versions in a JSON encoded list.\nSetup\nTo be able to run this on a cluster you need to deploy a Deployment that runs an image built using the provided Dockerfile, the pods need to run with a ServiceAccount that is authorized to create, read, watch and edit secrets. Eg.:\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: secret-operator-service-account\n  namespace: some-namepace\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: secret-reader\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"secrets\"]\n    verbs: [\"get\", \"watch\", create\", \"update\", \"patch\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: read-secrets\n  namespace: default\nsubjects:\n  - kind: ServiceAccount\n    name: secret-operator-service-account\n    namespace: some-namepace\nroleRef:\n  kind: ClusterRole\n  name: secret-reader\n  apiGroup: rbac.authorization.k8s.io\n",
      "description": "Kubernetes operator for backing up secrets. Contribute to geritol/secret-backup-operator development by creating an account on GitHub.",
      "ogDescription": "Kubernetes operator for backing up secrets. Contribute to geritol/secret-backup-operator development by creating an account on GitHub."
    },
    {
      "url": "https://github.com/roubles/kubeshell",
      "title": "roubles/kubeshell",
      "content": "<div><div><article class=\"markdown-body entry-content container-lg\">\n<p>kubeshell is a command line tool to interactively shell in to (and out of) kubernetes pods.</p>\n<pre><code>~$ kubeshell\n Pick your pod:\n    NAME                                                              READY   STATUS             RESTARTS   AGE\n\n =&gt; some-pod-7f9b5f475wdcmv                                           1/1     Running            0          26h\n    some-other-pod-556p4cfw                                           0/1     CrashLoopBackOff   83         7h10m\n    yet-another-pod-589cf77568-rjzkv                                  0/1     Running            82         7h4m\n    exit\n</code></pre>\n<h2><a id=\"user-content-install\" class=\"anchor\" href=\"https://github.com/roubles/kubeshell#install\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Install</h2>\n<p>You can install from pypi as follows</p>\n<pre><code>$ pip install kubeshell\n</code></pre>\n<p>OR, clone the repo</p>\n<pre><code>$ git clone https://github.com/roubles/kubeshell\n$ cd kubeshell\n$ python setup.py install\n</code></pre>\n<h2><a id=\"user-content-usage\" class=\"anchor\" href=\"https://github.com/roubles/kubeshell#usage\"><svg class=\"octicon octicon-link\" width=\"16\" height=\"16\"><path/></svg></a>Usage</h2>\n<pre><code>usage: kubeshell [-h] [-s SHELL] [-n NAMESPACE] [substring]\n\ninteractively shell into k8s pods\n\npositional arguments:\n  substring             substring to filter pods\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -s SHELL, --shell SHELL\n                        Which shell to use\n  -n NAMESPACE, --namespace NAMESPACE\n                        Which namespace to use\n</code></pre>\n</article></div></div>",
      "contentAsText": "\nkubeshell is a command line tool to interactively shell in to (and out of) kubernetes pods.\n~$ kubeshell\n Pick your pod:\n    NAME                                                              READY   STATUS             RESTARTS   AGE\n\n => some-pod-7f9b5f475wdcmv                                           1/1     Running            0          26h\n    some-other-pod-556p4cfw                                           0/1     CrashLoopBackOff   83         7h10m\n    yet-another-pod-589cf77568-rjzkv                                  0/1     Running            82         7h4m\n    exit\n\nInstall\nYou can install from pypi as follows\n$ pip install kubeshell\n\nOR, clone the repo\n$ git clone https://github.com/roubles/kubeshell\n$ cd kubeshell\n$ python setup.py install\n\nUsage\nusage: kubeshell [-h] [-s SHELL] [-n NAMESPACE] [substring]\n\ninteractively shell into k8s pods\n\npositional arguments:\n  substring             substring to filter pods\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -s SHELL, --shell SHELL\n                        Which shell to use\n  -n NAMESPACE, --namespace NAMESPACE\n                        Which namespace to use\n\n",
      "description": "interactively shell into k8s pods. Contribute to roubles/kubeshell development by creating an account on GitHub.",
      "ogDescription": "interactively shell into k8s pods. Contribute to roubles/kubeshell development by creating an account on GitHub."
    },
    {
      "url": "https://thenewstack.io/learning-kubernetes-the-need-for-a-realistic-playground/",
      "title": "Learning Kubernetes: The Need for a Realistic Playground",
      "content": "<div class=\"post-content\"> <p>Depending on a team\u0019s experience, Kubernetes can either be a steep learning curve or refreshingly simple. Regardless of a team\u0019s background, being able to rapidly and safely experiment within a Kubernetes playground is the key to becoming productive quickly.</p>\n<h2>From PaaS to K8s</h2>\n<a href=\"https://www.datawire.io\" class=\"clearfix infoBlock \"> <div class=\"infoBlockTextBlock\"> <div class=\"infoBlockText\">Daniel Bryant works as a Product Architect at Datawire. His technical expertise focuses on DevOps tooling, cloud/container platforms, and microservice implementations. Daniel is a Java Champion, a TechBeacon DevOps 100 Influencer, and contributes to several open source projects. Additionally, Daniel is a regular contributor to industry publications and is a frequent speaker at international conferences such as KubeCon, QCon and JavaOne.</div> </div> </a>\n<p>If a development team is used to building and releasing applications via a platform-as-a-service (PaaS) such as <a href=\"https://www.heroku.com/\" class=\"ext-link\">Heroku</a> or <a href=\"https://www.cloudfoundry.org/\" class=\"ext-link\">Cloud Foundry</a>, the additional complexity that comes with Kubernetes can be troublesome. Gone are the simple abstractions, and deploying code is no longer an easy \u001cgit push heroku master.\u001d I\u0019ve heard some engineers use an analogy that moving from a PaaS to Kubernetes was like moving from traveling via train to driving yourself in a kit car that you have to assemble yourself from parts.</p>\n<p>Teams with this type of experience need to be able to experiment with an <a href=\"https://itnext.io/building-a-kubernetes-based-platform-focus-on-progressive-delivery-the-edge-and-observability-3a702e0c19a7\" class=\"ext-link\">application-ready Kubernetes cluster</a> that they can quickly and repeatedly deploy services to and test and observe how user traffic will be handled. A key early goal here will be to establish the build pipeline deployment mechanisms and understand how the local developer experience maps to the remote deployment experience.</p>\n<h2>From VMs (and Duct Tape) to K8s</h2>\n<p>If an organization\u0019s developers are used to building and deploying applications to infrastructure via a series of scripts (often with manual intervention) that configure VMs, networking, and other hardware, then Kubernetes can be a big win. Kubernetes has clear abstractions, such as Ingress, Pods and Services, and all of the configurations are driven from declarative configuration files. The integral control loop within Kubernetes (which can be extended via custom utilities that implement the \u001c<a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/operator/\" class=\"ext-link\">operator</a>\u001d pattern) also provides the continual \u001ccheck and set\u001d mechanism that prevents configuration drift.</p>\n<p>Teams with this background often need to be able to deploy an experimental application-ready Kubernetes cluster that can be configured and customized to run alongside their existing infrastructure. A core goal here will be to be able to create clusters with varying configurations and rapidly deploy and test services to see if existing applications (and related developer workflows) can be easily shifted across.</p>\n<h2>A K8s Playground for Everyone</h2>\n<p>Regardless of the team\u0019s experience, one thing that is clearly beneficial during the learning process is the need for a playground. Websites such as <a href=\"https://www.katacoda.com/\" class=\"ext-link\">Katacoda</a>, the <a href=\"https://play.golang.org/\" class=\"ext-link\">Go playground</a>, and the <a href=\"https://play.openpolicyagent.org/\" class=\"ext-link\">Open Policy Agent Rego playground</a> have successfully pioneered this model of interactive learning in the cloud age.</p>\n<p>However, a Kubernetes playground needs to fit into how an organization will deploy and operate this framework. Running Kubernetes on AWS vs GCP is a different experience, particularly in relation to getting user traffic into your cluster and security configuration. A playground must also be easy for engineers to create and destroy via self-service mechanisms.</p>\n<h2>Building and Maintaining a K8s Platform</h2>\n<p>Creating a Kubernetes playground is not as simple as pointing developers to the <a href=\"https://kubernetes.io/docs/tasks/tools/install-minikube/\" class=\"ext-link\">Minikube installation page</a>, handing them a walkthrough script, and saying \u001chave at it.\u001d This unstructured approach can quickly lead to anarchy, with highly motivated engineers integrating all kinds of tooling into the (now snowflake) cluster, and engineers new to the space wondering where to begin.</p>\n<p>The top three approaches to building and maintaining a Kubernetes playground include: leveraging existing K8s playground products, creating a bespoke playground using <a href=\"https://helm.sh/docs/topics/charts/\" class=\"ext-link\">Helm Charts</a>, and using Kubernetes initializers or environment quickstart services.</p>\n<h2>Using an Existing K8s Playground</h2>\n<p>There are several popular Kubernetes playground products, such as <a href=\"https://www.katacoda.com/courses/kubernetes/playground\" class=\"ext-link\">Katacoda</a> and <a href=\"https://labs.play-with-k8s.com/\" class=\"ext-link\">Play with Kubernetes</a>. These typically offer the least amount of friction to get started and provide the most structured approach to training. This is often a great starting point for large-scale or enterprise use cases, where the goal is to quickly build the development and delivery \u001cmental model\u001d of the Kubernetes ecosystem.</p>\n<p>The drawbacks with this approach include that these types of playgrounds are often the most restrictive in terms of ad hoc experimentation (and don\u0019t allow engineers to learn via performing \u001coff script\u001d actions), and the underlying environments created are often not very configurable or production-like. Typically this type of playground is seen as a good first step that is later augmented with a more flexible and comprehensive platform.</p>\n<h2>Building a Bespoke Playground using Helm Charts</h2>\n<p>Defining and building a bespoke playground using <a href=\"https://helm.sh/docs/topics/charts/\" class=\"ext-link\">Helm Charts</a> is often combined with bootstrapping a cluster and integration with cloud services via infrastructure as code tooling such as <a href=\"https://www.terraform.io/\" class=\"ext-link\">Terraform</a> and <a href=\"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/\" class=\"ext-link\">kudeadm</a>. This provides engineers with much more scope in regards to the range of experimentation that can be conducted. It also provides flexible cluster configuration and repeatable builds \u0014 perfect for those moments when you accidentally destroy a cluster!</p>\n<p>The challenge with this type of approach is the learning curve associated with creating the playground. Often this requires a team of platform \u001cpioneers\u001d to begin the Kubernetes learning journey months before the rest of the organization. These platform engineers also typically build some kind of UI- or CLI-driven platform configuration facade to enable self-service by engineers, otherwise everyone has to learn Helm at the same time as Kubernetes.</p>\n<h2>Using Kubernetes Initializers or Environment Quickstarts</h2>\n<p>Using Kubernetes initializers or environment quickstart services, such as the <a href=\"https://app.getambassador.io/\" class=\"ext-link\">K8s Initializer</a>, <a href=\"https://docs.openshift.com/container-platform/3.6/install_config/imagestreams_templates.html#creating-instantapp-templates\" class=\"ext-link\">OpenShift Quickstart Templates</a>, or <a href=\"https://rio.io/\" class=\"ext-link\">RancherLabs Rio</a>, can provide engineers with a production-like cluster quickly. Typically you answer a few questions via a UI or specify configuration properties in a simple file, crank the handle, and out pops a fully-formed K8s cluster ready for experimentation. This approach is popular as a \u001cbridging\u001d playground for engineers that have acquired the basic mental model of Kubernetes and want to experiment with a more application-ready or production-like experience, but without having to go all-in on committing to something like Helm.</p>\n<p>The limitations of this approach include a potential bounded choice as to what tooling can be installed as part of the initialization process. And also the \u001cday 2\u001d support options or platform component upgrade paths can be somewhat limited.</p>\n<h2>Where to Begin?</h2>\n<p>Bringing in the experience of working with 1000s of engineers learning Kubernetes and related technologies, and also many super useful community discussions, I generally recommend folks create a playground by using a Kubernetes initializer or environment quickstart.</p>\n<p>If you are using a \u001cvanilla\u001d upstream Kubernetes distribution, or a cloud-hosted offering, the initializer approach strikes a nice balance between the complete experience of learning to work with actual Kubernetes primitives (as opposed to virtualized in-browser playgrounds) without having to learn additional tooling to bootstrap the platform (e.g. Helm).</p>\n<p>If you\u0019ve already selected a platform that adds additional abstractions on top of the Kubernetes API (e.g. OpenShift), then this approach is also a no-brainer, as you get access to a production-like experience from day 0.</p>\n<h2>Wrapping Up</h2>\n<p>Providing a Kubernetes playground is essential for the learning journey associated with this framework. Kubernetes is a fantastic foundation for modern platforms, but as is the case when learning anything complicated, you need a safe playground that rewards experimentation and structured learning while minimizing the potential for any negative consequences.</p>\n<p class=\"attribution\">Feature image <a href=\"https://pixabay.com/photos/playground-swing-kindergarten-2543311/\" class=\"ext-link\">via</a> Pixabay.</p> </div>",
      "contentAsText": " Depending on a team\u0019s experience, Kubernetes can either be a steep learning curve or refreshingly simple. Regardless of a team\u0019s background, being able to rapidly and safely experiment within a Kubernetes playground is the key to becoming productive quickly.\nFrom PaaS to K8s\n  Daniel Bryant works as a Product Architect at Datawire. His technical expertise focuses on DevOps tooling, cloud/container platforms, and microservice implementations. Daniel is a Java Champion, a TechBeacon DevOps 100 Influencer, and contributes to several open source projects. Additionally, Daniel is a regular contributor to industry publications and is a frequent speaker at international conferences such as KubeCon, QCon and JavaOne.  \nIf a development team is used to building and releasing applications via a platform-as-a-service (PaaS) such as Heroku or Cloud Foundry, the additional complexity that comes with Kubernetes can be troublesome. Gone are the simple abstractions, and deploying code is no longer an easy \u001cgit push heroku master.\u001d I\u0019ve heard some engineers use an analogy that moving from a PaaS to Kubernetes was like moving from traveling via train to driving yourself in a kit car that you have to assemble yourself from parts.\nTeams with this type of experience need to be able to experiment with an application-ready Kubernetes cluster that they can quickly and repeatedly deploy services to and test and observe how user traffic will be handled. A key early goal here will be to establish the build pipeline deployment mechanisms and understand how the local developer experience maps to the remote deployment experience.\nFrom VMs (and Duct Tape) to K8s\nIf an organization\u0019s developers are used to building and deploying applications to infrastructure via a series of scripts (often with manual intervention) that configure VMs, networking, and other hardware, then Kubernetes can be a big win. Kubernetes has clear abstractions, such as Ingress, Pods and Services, and all of the configurations are driven from declarative configuration files. The integral control loop within Kubernetes (which can be extended via custom utilities that implement the \u001coperator\u001d pattern) also provides the continual \u001ccheck and set\u001d mechanism that prevents configuration drift.\nTeams with this background often need to be able to deploy an experimental application-ready Kubernetes cluster that can be configured and customized to run alongside their existing infrastructure. A core goal here will be to be able to create clusters with varying configurations and rapidly deploy and test services to see if existing applications (and related developer workflows) can be easily shifted across.\nA K8s Playground for Everyone\nRegardless of the team\u0019s experience, one thing that is clearly beneficial during the learning process is the need for a playground. Websites such as Katacoda, the Go playground, and the Open Policy Agent Rego playground have successfully pioneered this model of interactive learning in the cloud age.\nHowever, a Kubernetes playground needs to fit into how an organization will deploy and operate this framework. Running Kubernetes on AWS vs GCP is a different experience, particularly in relation to getting user traffic into your cluster and security configuration. A playground must also be easy for engineers to create and destroy via self-service mechanisms.\nBuilding and Maintaining a K8s Platform\nCreating a Kubernetes playground is not as simple as pointing developers to the Minikube installation page, handing them a walkthrough script, and saying \u001chave at it.\u001d This unstructured approach can quickly lead to anarchy, with highly motivated engineers integrating all kinds of tooling into the (now snowflake) cluster, and engineers new to the space wondering where to begin.\nThe top three approaches to building and maintaining a Kubernetes playground include: leveraging existing K8s playground products, creating a bespoke playground using Helm Charts, and using Kubernetes initializers or environment quickstart services.\nUsing an Existing K8s Playground\nThere are several popular Kubernetes playground products, such as Katacoda and Play with Kubernetes. These typically offer the least amount of friction to get started and provide the most structured approach to training. This is often a great starting point for large-scale or enterprise use cases, where the goal is to quickly build the development and delivery \u001cmental model\u001d of the Kubernetes ecosystem.\nThe drawbacks with this approach include that these types of playgrounds are often the most restrictive in terms of ad hoc experimentation (and don\u0019t allow engineers to learn via performing \u001coff script\u001d actions), and the underlying environments created are often not very configurable or production-like. Typically this type of playground is seen as a good first step that is later augmented with a more flexible and comprehensive platform.\nBuilding a Bespoke Playground using Helm Charts\nDefining and building a bespoke playground using Helm Charts is often combined with bootstrapping a cluster and integration with cloud services via infrastructure as code tooling such as Terraform and kudeadm. This provides engineers with much more scope in regards to the range of experimentation that can be conducted. It also provides flexible cluster configuration and repeatable builds \u0014 perfect for those moments when you accidentally destroy a cluster!\nThe challenge with this type of approach is the learning curve associated with creating the playground. Often this requires a team of platform \u001cpioneers\u001d to begin the Kubernetes learning journey months before the rest of the organization. These platform engineers also typically build some kind of UI- or CLI-driven platform configuration facade to enable self-service by engineers, otherwise everyone has to learn Helm at the same time as Kubernetes.\nUsing Kubernetes Initializers or Environment Quickstarts\nUsing Kubernetes initializers or environment quickstart services, such as the K8s Initializer, OpenShift Quickstart Templates, or RancherLabs Rio, can provide engineers with a production-like cluster quickly. Typically you answer a few questions via a UI or specify configuration properties in a simple file, crank the handle, and out pops a fully-formed K8s cluster ready for experimentation. This approach is popular as a \u001cbridging\u001d playground for engineers that have acquired the basic mental model of Kubernetes and want to experiment with a more application-ready or production-like experience, but without having to go all-in on committing to something like Helm.\nThe limitations of this approach include a potential bounded choice as to what tooling can be installed as part of the initialization process. And also the \u001cday 2\u001d support options or platform component upgrade paths can be somewhat limited.\nWhere to Begin?\nBringing in the experience of working with 1000s of engineers learning Kubernetes and related technologies, and also many super useful community discussions, I generally recommend folks create a playground by using a Kubernetes initializer or environment quickstart.\nIf you are using a \u001cvanilla\u001d upstream Kubernetes distribution, or a cloud-hosted offering, the initializer approach strikes a nice balance between the complete experience of learning to work with actual Kubernetes primitives (as opposed to virtualized in-browser playgrounds) without having to learn additional tooling to bootstrap the platform (e.g. Helm).\nIf you\u0019ve already selected a platform that adds additional abstractions on top of the Kubernetes API (e.g. OpenShift), then this approach is also a no-brainer, as you get access to a production-like experience from day 0.\nWrapping Up\nProviding a Kubernetes playground is essential for the learning journey associated with this framework. Kubernetes is a fantastic foundation for modern platforms, but as is the case when learning anything complicated, you need a safe playground that rewards experimentation and structured learning while minimizing the potential for any negative consequences.\nFeature image via Pixabay. ",
      "publishedDate": "2020-08-27T10:00:08.000Z",
      "description": "Kubernetes is a fantastic foundation for modern platforms, but as is the case when learning anything complicated, you need a safe playground that rewards experimentation and structured learning while minimizing the potential for any negative consequences.",
      "ogDescription": "Kubernetes is a fantastic foundation for modern platforms, but as is the case when learning anything complicated, you need a safe playground that rewards experimentation and structured learning while minimizing the potential for any negative consequences."
    },
    {
      "url": "https://techcommunity.microsoft.com/t5/itops-talk-blog/journey-of-an-app-how-to-create-an-aks-cluster-to-deploy-a/ba-p/1620398?WT.mc_id=modinfra-8689-abartolo",
      "title": "Journey of an App: How to create an AKS cluster to deploy a Windows Container application",
      "content": "<div class=\"lia-message-body-content\"> <p>Welcome to the third and final video/blog post in this series where we are modernizing a web application from Windows Server 2012 R2 running on-premises to Azure Kubernetes Services using Windows Containers. In case you missed it, here are <a href=\"https://techcommunity.microsoft.com/t5/itops-talk-blog/journey-of-an-app-how-to-extract-a-web-app-from-iis-to-create-a/ba-p/1590189?WT.mc_id=modinfra-8689-abartolo\">part one</a> and <a href=\"https://techcommunity.microsoft.com/t5/itops-talk-blog/journey-of-an-app-how-to-push-a-container-image-from-on-premises/ba-p/1602721?WT.mc_id=modinfra-8689-abartolo\">part two</a> of the video series.<br>&#xA0;</p> <p>In this third part of our series we cover how to create an AKS cluster and how to deploy our containerized application on top of it. We start by using the Azure portal to create a new AKS cluster with most of the default options, but with key changes to include Windows Server worker nodes, and to ensure we have authentication set up against our Azure Container Registry, so the image we uploaded can be pulled into the nodes running our application. I highly recommend you check the <a href=\"https://docs.microsoft.com/en-us/azure/aks/windows-container-cli?WT.mc_id=modinfra-8689-abartolo\">AKS documentation</a> to see more details on the deployment and operation options of AKS.<br>&#xA0;</p>\n<p>After creating the AKS cluster, we used this <a href=\"https://github.com/vrapolinario/AKSDemo/blob/master/vinibeer.yaml\">sample YAML</a> file to describe how the application should be deployed. Finally, instead of connecting remotely using Kubectl, we used the recently announced feature (under preview) <a href=\"https://docs.microsoft.com/en-us/azure/aks/kubernetes-portal?WT.mc_id=modinfra-8689-abartolo\">resource management from the Azure Portal</a>. This new feature allows you to manage Kubernetes resources directly from the portal, so we were able to paste our YAML file directly from there.<br>&#xA0;</p>\n<p>To validate everything, we looked at the deployment and opened the application running on AKS and the application worked the same way it was working before.</p> <p>Hopefully, this gave you an idea on how the end to end process of modernizing your application works. We&apos;re looking forward to seeing what you think of this video series and what you want to see next!</p>\n<p>Vinicius!</p> <p>Twitter:&#xA0;<a href=\"https://twitter.com/vrapolinario\">@vrapolinario</a></p> </div>",
      "contentAsText": " Welcome to the third and final video/blog post in this series where we are modernizing a web application from Windows Server 2012 R2 running on-premises to Azure Kubernetes Services using Windows Containers. In case you missed it, here are part one and part two of the video series.  In this third part of our series we cover how to create an AKS cluster and how to deploy our containerized application on top of it. We start by using the Azure portal to create a new AKS cluster with most of the default options, but with key changes to include Windows Server worker nodes, and to ensure we have authentication set up against our Azure Container Registry, so the image we uploaded can be pulled into the nodes running our application. I highly recommend you check the AKS documentation to see more details on the deployment and operation options of AKS. \nAfter creating the AKS cluster, we used this sample YAML file to describe how the application should be deployed. Finally, instead of connecting remotely using Kubectl, we used the recently announced feature (under preview) resource management from the Azure Portal. This new feature allows you to manage Kubernetes resources directly from the portal, so we were able to paste our YAML file directly from there. \nTo validate everything, we looked at the deployment and opened the application running on AKS and the application worked the same way it was working before. Hopefully, this gave you an idea on how the end to end process of modernizing your application works. We're looking forward to seeing what you think of this video series and what you want to see next!\nVinicius! Twitter: @vrapolinario ",
      "publishedDate": "2020-08-31T07:01:00.031Z",
      "description": "Welcome to the third and final video/blog post in this series where we are modernizing a web application from Windows Server 2012 R2 running on-premises to Azure Kubernetes Services using Windows Containers.",
      "ogDescription": "Welcome to the third and final video/blog post in this series where we are modernizing a web application from Windows Server 2012 R2 running on-premises to Azure Kubernetes Services using Windows Containers. In case you missed it, here are part one and part two of the video series.      In this thir..."
    },
    {
      "url": "https://ymmt2005.hatenablog.com/entry/2020/08/31/Use_go-grpc-middleware_with_kubebuilder",
      "title": "Use go-grpc-middleware with kubebuilder",
      "content": "<div class=\"entry-content\"> <p>Kubernetes and gRPC are technologies to build distributed microservices.</p> <p>Writing <a href=\"https://grpc.io/docs/languages/go/\">gRPC servers in Go</a> is fun and easy.\nWith <a href=\"https://github.com/grpc-ecosystem/go-grpc-middleware\">go-grpc-middleware</a>, you can add various features to gRPC servers including access logs and Prometheus metrics.</p> <p><a href=\"https://kubebuilder.io/\">Kubebuilder</a> is a toolkit to develop custom Kubernetes controllers.\nThe main component of kubebuilder is a library called <a href=\"https://github.com/kubernetes-sigs/controller-runtime\">controller-runtime</a> that provides logging and Prometheus metrics as well as Kubernetes client libraries.</p> <p>When developing a microservice for Kubernetes, these two technologies are often used together.\nThis article describes how. The readers need to have basic knowledge about Go, kubebuilder/controller-runtime, and gRPC.</p> <h2 id=\"Introducing-go-grpc-middleware\">Introducing go-grpc-middleware</h2> <p><a href=\"https://github.com/grpc-ecosystem/go-grpc-middleware\">go-grpc-middleware</a> is an umbrella project for useful gRPC interceptors for Go.\nYou can easily add access logging, Prometheus metrics, and OpenTracing tags to your gRPC server with go-grpc-middleware.</p> <p>The following code illustrates how to extract fields from request parameters and log requests using <a href=\"https://github.com/uber-go/zap\">zap</a>.</p> <pre class=\"code lang-go\"><span class=\"synStatement\">import</span> ( grpc_middleware <span class=\"synConstant\">&quot;github.com/grpc-ecosystem/go-grpc-middleware&quot;</span> grpc_zap <span class=\"synConstant\">&quot;github.com/grpc-ecosystem/go-grpc-middleware/logging/zap&quot;</span> grpc_ctxtags <span class=\"synConstant\">&quot;github.com/grpc-ecosystem/go-grpc-middleware/tags&quot;</span> <span class=\"synConstant\">&quot;go.uber.org/zap&quot;</span> <span class=\"synConstant\">&quot;google.golang.org/grpc&quot;</span>\n) <span class=\"synStatement\">func</span> extractFields(fullMethod <span class=\"synType\">string</span>, req <span class=\"synStatement\">interface</span>{}) <span class=\"synType\">map</span>[<span class=\"synType\">string</span>]<span class=\"synStatement\">interface</span>{} { args, ok := req.(*SomeArgumentType) <span class=\"synStatement\">if</span> !ok { <span class=\"synStatement\">return</span> <span class=\"synStatement\">nil</span> } ret := <span class=\"synStatement\">make</span>(<span class=\"synType\">map</span>[<span class=\"synType\">string</span>]<span class=\"synStatement\">interface</span>{}) <span class=\"synStatement\">if</span> args.Foo != <span class=\"synConstant\">&quot;&quot;</span> { ret[<span class=\"synConstant\">&quot;foo&quot;</span>] = args.Foo } ret[<span class=\"synConstant\">&quot;bar&quot;</span>] = args.Bar <span class=\"synStatement\">return</span> ret\n} <span class=\"synStatement\">func</span> makeServer(log *zap.Logger) *grpc.Server { <span class=\"synStatement\">return</span> grpc.NewServer(grpc.UnaryInterceptor(\n        grpc_middleware.ChainUnaryServer(\n            grpc_ctxtags.UnaryServerInterceptor(grpc_ctxtags.WithFieldExtractor(extractFields)),\n            grpc_zap.UnaryServerInterceptor(log),\n        ),\n    ))\n}\n</pre> <p>This server will log requests with <code>grpc.request.foo</code> and <code>grpc.request.bar</code> fields.\nIf zap is configured to output JSON, it should look like:</p> <pre class=\"code lang-json\"><span class=\"synSpecial\">{</span> &quot;<span class=\"synStatement\">grpc.code</span>&quot;: &quot;<span class=\"synConstant\">OK</span>&quot;, &quot;<span class=\"synStatement\">grpc.method</span>&quot;: &quot;<span class=\"synConstant\">Add</span>&quot;, &quot;<span class=\"synStatement\">grpc.request.foo</span>&quot;: &quot;<span class=\"synConstant\">some message</span>&quot;, &quot;<span class=\"synStatement\">grpc.request.bar</span>&quot;: <span class=\"synConstant\">10</span>, &quot;<span class=\"synStatement\">grpc.service</span>&quot;: &quot;<span class=\"synConstant\">your.service</span>&quot;, &quot;<span class=\"synStatement\">grpc.start_time</span>&quot;: &quot;<span class=\"synConstant\">2020-08-30T10:18:49Z</span>&quot;, &quot;<span class=\"synStatement\">grpc.time_ms</span>&quot;: <span class=\"synConstant\">102.34100341796875</span>, &quot;<span class=\"synStatement\">level</span>&quot;: &quot;<span class=\"synConstant\">info</span>&quot;, &quot;<span class=\"synStatement\">msg</span>&quot;: &quot;<span class=\"synConstant\">finished unary call with code OK</span>&quot;, &quot;<span class=\"synStatement\">peer.address</span>&quot;: &quot;<span class=\"synConstant\">@</span>&quot;, &quot;<span class=\"synStatement\">span.kind</span>&quot;: &quot;<span class=\"synConstant\">server</span>&quot;, &quot;<span class=\"synStatement\">system</span>&quot;: &quot;<span class=\"synConstant\">grpc</span>&quot;, &quot;<span class=\"synStatement\">ts</span>&quot;: <span class=\"synConstant\">1598782729.4605289</span>\n<span class=\"synSpecial\">}</span>\n</pre> <h2 id=\"Problems\">Problems</h2> <p>Kubebuilder generates code that assumes the main function runs only <a href=\"https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/manager?tab=doc#Manager\"><code>manager.Manager</code></a>.\nCustom controllers, admission webhooks, and the metrics exporter for Prometheus are all run under the manager. The first problem is: <strong>how can we do the same for gRPC server?</strong></p> <p>The second and third problems are the duplication of features between kubebuilder and go-grpc-middleware.</p> <p>Both uses <a href=\"https://github.com/uber-go/zap\">zap</a> as a logging library, but kubebuilder abstracts it in <a href=\"https://github.com/go-logr/logr\">logr</a> interface and does not expose raw zap structs. The second problem is: <strong>how can we share the same zap logger?</strong></p> <p>To export metrics for Prometheus, kubebuilder defines a custom <a href=\"https://pkg.go.dev/github.com/prometheus/client_golang/prometheus?tab=doc#Registry\"><code>Registry</code></a> whereas go-grpc-middleware&apos;s example uses <a href=\"https://pkg.go.dev/github.com/prometheus/client_golang/prometheus?tab=doc#pkg-variables\">the builtin Registry</a> as follows:</p> <pre class=\"code lang-go\">myServer := grpc.NewServer(\n    \n    grpc.UnaryInterceptor(grpc_middleware.ChainUnaryServer(\n        grpc_ctxtags.UnaryServerInterceptor(),\n        grpc_prometheus.UnaryServerInterceptor,  \n</pre> <p>So the last problem is: <strong>how can we register gRPC metrics with kubebuilder&apos;s Registry?</strong></p> <h2 id=\"How-to-run-gRPC-server-with-managerManager\">How to run gRPC server with <code>manager.Manager</code></h2> <p><code>manager.Manager</code> can run arbitrary processes that implements <a href=\"https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/manager?tab=doc#Runnable\"><code>Runnable</code></a> interface.\nThe following example runs gRPC server under the manager.</p> <pre class=\"code lang-go\"><span class=\"synStatement\">import</span> ( <span class=\"synConstant\">&quot;google.golang.org/grpc&quot;</span> ctrl <span class=\"synConstant\">&quot;sigs.k8s.io/controller-runtime&quot;</span>\n) <span class=\"synStatement\">type</span> GRPCRunner { server *grpc.Server listener net.Listener\n} <span class=\"synStatement\">func</span> (r GRPCRunner) Start(ch &lt;-<span class=\"synType\">chan</span> <span class=\"synStatement\">struct</span>{}) <span class=\"synType\">error</span> { <span class=\"synStatement\">go</span> <span class=\"synType\">func</span>() { &lt;-ch r.server.GracefulStop() }() <span class=\"synStatement\">return</span> r.server.Serve(r.listener)\n} <span class=\"synStatement\">func</span> main() { mgr, _ := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{...} l, _ := net.Listen(<span class=\"synConstant\">&quot;unix&quot;</span>, <span class=\"synConstant\">&quot;/...&quot;</span>)\n\n    grpcServer := grpc.NewServer()\n    r := GRPCRunner{server: gRPCServer, listener: l}\n    mgr.Add(r)\n\n    mgr.Start(ctrl.SetupSignalHandler())\n}\n</pre> <p>Normally, kubebuilder generates code like this:</p> <pre class=\"code lang-go\"><span class=\"synStatement\">import</span> <span class=\"synConstant\">&quot;sigs.k8s.io/controller-runtime/pkg/log/zap&quot;</span> <span class=\"synStatement\">func</span> main() { ctrl.SetLogger(zap.New(zap.UseDevMode(<span class=\"synStatement\">true</span>)))\n}\n</pre> <p>The problem here is that <a href=\"https://pkg.go.dev/sigs.k8s.io/controller-runtime@v0.6.2/pkg/log/zap?tab=doc#New\"><code>zap.New</code></a> from controller-runtime does <em>not</em> return the raw <code>*zap.Logger</code>. Instead, it returns an object wrapped in <a href=\"https://pkg.go.dev/github.com/go-logr/logr?tab=doc#Logger\"><code>logr.Logger</code></a> interface.</p> <p>We can rewrite the code so that it creates and shares the same <code>*zap.Logger</code> as follows:</p> <pre class=\"code lang-go\"><span class=\"synStatement\">import</span> ( <span class=\"synConstant\">&quot;github.com/go-logr/zapr&quot;</span> grpc_middleware <span class=\"synConstant\">&quot;github.com/grpc-ecosystem/go-grpc-middleware&quot;</span> grpc_zap <span class=\"synConstant\">&quot;github.com/grpc-ecosystem/go-grpc-middleware/logging/zap&quot;</span> grpc_ctxtags <span class=\"synConstant\">&quot;github.com/grpc-ecosystem/go-grpc-middleware/tags&quot;</span> <span class=\"synConstant\">&quot;google.golang.org/grpc&quot;</span> <span class=\"synConstant\">&quot;sigs.k8s.io/controller-runtime/pkg/log/zap&quot;</span>\n) <span class=\"synStatement\">func</span> main() { zapLogger := zap.NewRaw() <span class=\"synStatement\">defer</span> zapLogger.Sync() ctrl.SetLogger(zapr.NewLogger(zapLogger)) grpcLogger := zapLogger.Named(<span class=\"synConstant\">&quot;grpc&quot;</span>)\n    grpcServer := grpc.NewServer(grpc.UnaryInterceptor(\n        grpc_middleware.ChainUnaryServer(\n            grpc_ctxtags.UnaryServerInterceptor(),\n            grpc_zap.UnaryServerInterceptor(gRPCLogger),\n        ),\n    ))\n}\n</pre> <h2 id=\"How-to-register-gRPC-metrics-with-kubebuilders-Registry\">How to register gRPC metrics with kubebuilder&apos;s Registry</h2> <p>Kubebuilder exports various metrics registered with <a href=\"https://pkg.go.dev/sigs.k8s.io/controller-runtime@v0.6.2/pkg/metrics?tab=doc#RegistererGatherer\"><code>metrics.Registry</code></a>.\nTo register gRPC metrics with <code>metrics.Registry</code>, write code like this:</p> <pre class=\"code lang-go\"><span class=\"synStatement\">import</span> ( grpc_middleware <span class=\"synConstant\">&quot;github.com/grpc-ecosystem/go-grpc-middleware&quot;</span> grpc_ctxtags <span class=\"synConstant\">&quot;github.com/grpc-ecosystem/go-grpc-middleware/tags&quot;</span> grpc_prometheus <span class=\"synConstant\">&quot;github.com/grpc-ecosystem/go-grpc-prometheus&quot;</span> <span class=\"synConstant\">&quot;google.golang.org/grpc&quot;</span> <span class=\"synConstant\">&quot;sigs.k8s.io/controller-runtime/pkg/metrics&quot;</span>\n) <span class=\"synStatement\">func</span> main() {\n    grpcMetrics := grpc_prometheus.NewServerMetrics()\n    metrics.Registry.MustRegister(grpcMetrics)\n\n    grpcServer := grpc.NewServer(grpc.UnaryInterceptor(\n        grpc_middleware.ChainUnaryServer(\n            grpc_ctxtags.UnaryServerInterceptor(grpc_ctxtags.WithFieldExtractor(fieldExtractor)),\n            grpcMetrics.UnaryServerInterceptor(),\n        ),\n    ))\n\n    \n\n    \n    grpcMetrics.InitializeMetrics(grpcServer)\n}\n</pre> <p>This combined with <code>manager.Manager</code> exports metrics including gRPC&apos;s.</p> <h2 id=\"Conclusion\">Conclusion</h2> <p>Now we can use go-grpc-middleware together with kubebuilder.\nYou can find a real world example in <a href=\"https://github.com/cybozu-go/coil/blob/c56bd0b5559505c04df5ba030d7f347caa91e438/v2/runners/coild_server.go\">github.com/cybozu-go/coil/v2/runners/coild_server.go</a>.</p> </div>",
      "contentAsText": " Kubernetes and gRPC are technologies to build distributed microservices. Writing gRPC servers in Go is fun and easy.\nWith go-grpc-middleware, you can add various features to gRPC servers including access logs and Prometheus metrics. Kubebuilder is a toolkit to develop custom Kubernetes controllers.\nThe main component of kubebuilder is a library called controller-runtime that provides logging and Prometheus metrics as well as Kubernetes client libraries. When developing a microservice for Kubernetes, these two technologies are often used together.\nThis article describes how. The readers need to have basic knowledge about Go, kubebuilder/controller-runtime, and gRPC. Introducing go-grpc-middleware go-grpc-middleware is an umbrella project for useful gRPC interceptors for Go.\nYou can easily add access logging, Prometheus metrics, and OpenTracing tags to your gRPC server with go-grpc-middleware. The following code illustrates how to extract fields from request parameters and log requests using zap. import ( grpc_middleware \"github.com/grpc-ecosystem/go-grpc-middleware\" grpc_zap \"github.com/grpc-ecosystem/go-grpc-middleware/logging/zap\" grpc_ctxtags \"github.com/grpc-ecosystem/go-grpc-middleware/tags\" \"go.uber.org/zap\" \"google.golang.org/grpc\"\n) func extractFields(fullMethod string, req interface{}) map[string]interface{} { args, ok := req.(*SomeArgumentType) if !ok { return nil } ret := make(map[string]interface{}) if args.Foo != \"\" { ret[\"foo\"] = args.Foo } ret[\"bar\"] = args.Bar return ret\n} func makeServer(log *zap.Logger) *grpc.Server { return grpc.NewServer(grpc.UnaryInterceptor(\n        grpc_middleware.ChainUnaryServer(\n            grpc_ctxtags.UnaryServerInterceptor(grpc_ctxtags.WithFieldExtractor(extractFields)),\n            grpc_zap.UnaryServerInterceptor(log),\n        ),\n    ))\n}\n This server will log requests with grpc.request.foo and grpc.request.bar fields.\nIf zap is configured to output JSON, it should look like: { \"grpc.code\": \"OK\", \"grpc.method\": \"Add\", \"grpc.request.foo\": \"some message\", \"grpc.request.bar\": 10, \"grpc.service\": \"your.service\", \"grpc.start_time\": \"2020-08-30T10:18:49Z\", \"grpc.time_ms\": 102.34100341796875, \"level\": \"info\", \"msg\": \"finished unary call with code OK\", \"peer.address\": \"@\", \"span.kind\": \"server\", \"system\": \"grpc\", \"ts\": 1598782729.4605289\n}\n Problems Kubebuilder generates code that assumes the main function runs only manager.Manager.\nCustom controllers, admission webhooks, and the metrics exporter for Prometheus are all run under the manager. The first problem is: how can we do the same for gRPC server? The second and third problems are the duplication of features between kubebuilder and go-grpc-middleware. Both uses zap as a logging library, but kubebuilder abstracts it in logr interface and does not expose raw zap structs. The second problem is: how can we share the same zap logger? To export metrics for Prometheus, kubebuilder defines a custom Registry whereas go-grpc-middleware's example uses the builtin Registry as follows: myServer := grpc.NewServer(\n    \n    grpc.UnaryInterceptor(grpc_middleware.ChainUnaryServer(\n        grpc_ctxtags.UnaryServerInterceptor(),\n        grpc_prometheus.UnaryServerInterceptor,  \n So the last problem is: how can we register gRPC metrics with kubebuilder's Registry? How to run gRPC server with manager.Manager manager.Manager can run arbitrary processes that implements Runnable interface.\nThe following example runs gRPC server under the manager. import ( \"google.golang.org/grpc\" ctrl \"sigs.k8s.io/controller-runtime\"\n) type GRPCRunner { server *grpc.Server listener net.Listener\n} func (r GRPCRunner) Start(ch <-chan struct{}) error { go func() { <-ch r.server.GracefulStop() }() return r.server.Serve(r.listener)\n} func main() { mgr, _ := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{...} l, _ := net.Listen(\"unix\", \"/...\")\n\n    grpcServer := grpc.NewServer()\n    r := GRPCRunner{server: gRPCServer, listener: l}\n    mgr.Add(r)\n\n    mgr.Start(ctrl.SetupSignalHandler())\n}\n Normally, kubebuilder generates code like this: import \"sigs.k8s.io/controller-runtime/pkg/log/zap\" func main() { ctrl.SetLogger(zap.New(zap.UseDevMode(true)))\n}\n The problem here is that zap.New from controller-runtime does not return the raw *zap.Logger. Instead, it returns an object wrapped in logr.Logger interface. We can rewrite the code so that it creates and shares the same *zap.Logger as follows: import ( \"github.com/go-logr/zapr\" grpc_middleware \"github.com/grpc-ecosystem/go-grpc-middleware\" grpc_zap \"github.com/grpc-ecosystem/go-grpc-middleware/logging/zap\" grpc_ctxtags \"github.com/grpc-ecosystem/go-grpc-middleware/tags\" \"google.golang.org/grpc\" \"sigs.k8s.io/controller-runtime/pkg/log/zap\"\n) func main() { zapLogger := zap.NewRaw() defer zapLogger.Sync() ctrl.SetLogger(zapr.NewLogger(zapLogger)) grpcLogger := zapLogger.Named(\"grpc\")\n    grpcServer := grpc.NewServer(grpc.UnaryInterceptor(\n        grpc_middleware.ChainUnaryServer(\n            grpc_ctxtags.UnaryServerInterceptor(),\n            grpc_zap.UnaryServerInterceptor(gRPCLogger),\n        ),\n    ))\n}\n How to register gRPC metrics with kubebuilder's Registry Kubebuilder exports various metrics registered with metrics.Registry.\nTo register gRPC metrics with metrics.Registry, write code like this: import ( grpc_middleware \"github.com/grpc-ecosystem/go-grpc-middleware\" grpc_ctxtags \"github.com/grpc-ecosystem/go-grpc-middleware/tags\" grpc_prometheus \"github.com/grpc-ecosystem/go-grpc-prometheus\" \"google.golang.org/grpc\" \"sigs.k8s.io/controller-runtime/pkg/metrics\"\n) func main() {\n    grpcMetrics := grpc_prometheus.NewServerMetrics()\n    metrics.Registry.MustRegister(grpcMetrics)\n\n    grpcServer := grpc.NewServer(grpc.UnaryInterceptor(\n        grpc_middleware.ChainUnaryServer(\n            grpc_ctxtags.UnaryServerInterceptor(grpc_ctxtags.WithFieldExtractor(fieldExtractor)),\n            grpcMetrics.UnaryServerInterceptor(),\n        ),\n    ))\n\n    \n\n    \n    grpcMetrics.InitializeMetrics(grpcServer)\n}\n This combined with manager.Manager exports metrics including gRPC's. Conclusion Now we can use go-grpc-middleware together with kubebuilder.\nYou can find a real world example in github.com/cybozu-go/coil/v2/runners/coild_server.go. ",
      "publishedDate": "1970-01-19T12:07:39.396Z",
      "description": "Kubernetes and gRPC are technologies to build distributed microservices. Writing gRPC servers in Go is fun and easy. With go-grpc-middleware, you can add various features to gRPC servers including access logs and Prometheus metrics. Kubebuilder is a toolkit to develop custom Kubernetes controllers. …",
      "ogDescription": "Kubernetes and gRPC are technologies to build distributed microservices. Writing gRPC servers in Go is fun and easy. With go-grpc-middleware, you can add various features to gRPC servers including access logs and Prometheus metrics. Kubebuilder is a toolkit to develop custom Kubernetes controllers. …"
    },
    {
      "url": "https://medium.com/epiphani/automate-kubernetes-alerts-response-using-epiphani-playbook-engine-4714bf0c3f26",
      "title": "Automate Kubernetes Alerts Response using Epiphani Playbook Engine",
      "content": "<div><article><section class=\"cj ck cl cm aj cn co s\"></section><div><section class=\"ct cu cv cw cx\"><div class=\"n p\"><div class=\"ab ac ae af ag cy ai aj\"><figure class=\"gg gh gi gj gk gl cl cm paragraph-image\"><img alt=\"Image for post\" class=\"t u v gt aj\" src=\"https://miro.medium.com/max/1660/1*e-N3RPYoj6Ku195_Awrnvw.jpeg\" width=\"830\" srcset=\"https://miro.medium.com/max/552/1*e-N3RPYoj6Ku195_Awrnvw.jpeg 276w, https://miro.medium.com/max/1104/1*e-N3RPYoj6Ku195_Awrnvw.jpeg 552w, https://miro.medium.com/max/1280/1*e-N3RPYoj6Ku195_Awrnvw.jpeg 640w, https://miro.medium.com/max/1400/1*e-N3RPYoj6Ku195_Awrnvw.jpeg 700w\" sizes=\"700px\"></figure><p id=\"9fb9\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\"><strong class=\"hf ib\">Introduction</strong></p><figure class=\"gg gh gi gj gk gl\"></figure><p id=\"533f\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">Operationalizing Kubernetes is hard and you can use all the help you can get. You want to be able to monitor your Kubernetes system closely, trigger alerts based on defined rules and finally you need the capability to automate response to these alerts</p><p id=\"989c\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\"><strong class=\"hf ib\">Alerting Framework</strong></p><p id=\"739a\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">Prometheus provides the means to monitor Kubernetes and trigger alerts based on defined rules. Alertmanager, on the other hand, consumes these alerts from Prometheus and provides the capability to group the alerts, inhibit and silence the alerts as well as route the alerts to configured receivers.</p><p id=\"799a\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\"><strong class=\"hf ib\">Automated Response</strong></p><p id=\"dc15\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">One can configure Slack, Pagertduty etc to be the receivers for these alerts, in which case the alert notification is delivered as a message to the recipients, but what you truly want is the ability to automate the response to these alerts. Instead of manual intervention to resolve these alerts, you ideally want automation to auto-remediate if possible. At the minimum, you want to enrich the message with diagnostic information related to the alert before sending it out to the on-call users so that much of the leg work required to collect this information is performed upfront and the user can focus on what\u0019s important, i.e. resolve the issue expeditiously.</p></div></div></section><section class=\"ct cu cv cw cx\"><div class=\"n p\"><div class=\"ab ac ae af ag cy ai aj\"><p id=\"09c1\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\"><strong class=\"hf ib\">Closing out the Kubernetes monitoring loop using Epiphani Playbooks</strong></p><p id=\"b72a\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">Configuring Epiphani as the receiver for alerts in Alertmanager enables automation of alerts response. You can now easily and simply auto remediate the alerts, essentially closing out the monitoring loop.</p><p id=\"a888\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">Let\u0019s walkthrough an example that illustrates how this can be achieved.</p></div></div></section><section class=\"ct cu cv cw cx\"><div class=\"n p\"><div class=\"ab ac ae af ag cy ai aj\"><figure class=\"gg gh gi il im gl in io fn ip iq ir is it bh if iu iv iw ix iy paragraph-image\"><img alt=\"Image for post\" class=\"t u v gt aj\" src=\"https://miro.medium.com/max/3292/1*3maGSpONuCRwNyM1MqIM_w.png\" width=\"1646\" srcset=\"https://miro.medium.com/max/552/1*3maGSpONuCRwNyM1MqIM_w.png 276w, https://miro.medium.com/max/1000/1*3maGSpONuCRwNyM1MqIM_w.png 500w\" sizes=\"500px\"></figure><p id=\"d1b8\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\"><strong class=\"hf ib\"><em class=\"ja\">Kubernetes Setup:</em></strong></p><ul class><li id=\"4800\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia jb jc jd dy\">We have deployed Kubernetes on Minikube</li><li id=\"a60d\" class=\"hd he db hf b hg je hi hj hk jf hm hn ho jg hq hr hs jh hu hv hw ji hy hz ia jb jc jd dy\">nginx application</li><li id=\"6dcb\" class=\"hd he db hf b hg je hi hj hk jf hm hn ho jg hq hr hs jh hu hv hw ji hy hz ia jb jc jd dy\">Prometheus &amp; Alertmanager deployed in monitoring namespace</li></ul><p id=\"e706\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">Please refer to the <a class=\"et jj\" href=\"https://medium.com/faun/trying-prometheus-operator-with-helm-minikube-b617a2dccfa3\">following link</a> for more information regarding installing Prometheus and Alertmanager and setting up rules and alert configuration</p></div></div></section><section class=\"ct cu cv cw cx\"><div class=\"n p\"><div class=\"ab ac ae af ag cy ai aj\"><figure class=\"gg gh gi il im gl in io fn ip iq ir is it bh if iu iv iw ix iy paragraph-image\"><img alt=\"Image for post\" class=\"t u v gt aj\" src=\"https://miro.medium.com/max/2560/1*H73G_qxy7iV83yVDufQ3Mw.png\" width=\"1280\" srcset=\"https://miro.medium.com/max/552/1*H73G_qxy7iV83yVDufQ3Mw.png 276w, https://miro.medium.com/max/1000/1*H73G_qxy7iV83yVDufQ3Mw.png 500w\" sizes=\"500px\"></figure><p id=\"f5b1\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\"><strong class=\"hf ib\"><em class=\"ja\">Prometheus Rule:</em></strong></p><p id=\"c293\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">A simple rule to generate an alert if the nginx cpu usage breaches a certain threshold has been configured</p></div></div></section><section class=\"ct cu cv cw cx\"><div class=\"n p\"><div class=\"ab ac ae af ag cy ai aj\"><figure class=\"gg gh gi il im gl in io fn ip iq ir is it bh if iu iv iw ix iy paragraph-image\"><img alt=\"Image for post\" class=\"t u v gt aj\" src=\"https://miro.medium.com/max/800/1*VqsDSuwE1CFoll6C5lz0sQ.jpeg\" width=\"400\" srcset=\"https://miro.medium.com/max/552/1*VqsDSuwE1CFoll6C5lz0sQ.jpeg 276w, https://miro.medium.com/max/800/1*VqsDSuwE1CFoll6C5lz0sQ.jpeg 400w\" sizes=\"400px\"></figure><p id=\"7256\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\"><strong class=\"hf ib\"><em class=\"ja\">Alertmanager Configuration:</em></strong></p><p id=\"5c04\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">Using the webhook configuration, we specify Epiphani webhook endpoint. We are in the process of integrating Epiphani configuration natively in to Alertmanager</p></div></div></section><section class=\"ct cu cv cw cx\"><div class=\"n p\"><div class=\"ab ac ae af ag cy ai aj\"><figure class=\"gg gh gi il im gl in io fn ip iq ir is it bh if iu iv iw ix iy paragraph-image\"><img alt=\"Image for post\" class=\"t u v gt aj\" src=\"https://miro.medium.com/max/1076/1*Ll0djeUbuWDDlri8bjKJUw.png\" width=\"538\" srcset=\"https://miro.medium.com/max/552/1*Ll0djeUbuWDDlri8bjKJUw.png 276w, https://miro.medium.com/max/1000/1*Ll0djeUbuWDDlri8bjKJUw.png 500w\" sizes=\"500px\"></figure><p id=\"ea7e\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\"><strong class=\"hf ib\"><em class=\"ja\">Epiphani Alert Routing:</em></strong></p><ul class><li id=\"1b0d\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia jb jc jd dy\">Create an alert routing playbook that routes incoming alerts from Alertmanager to a corresponding handler playbook</li><li id=\"1823\" class=\"hd he db hf b hg je hi hj hk jf hm hn ho jg hq hr hs jh hu hv hw ji hy hz ia jb jc jd dy\">Playbook consists of connectors, each performing a specific action. Connectors can be 3rd party connectors such as Slack, AWS EC2, Pagerduty, etc or they can be in house scripts. Epiphani provides a rich set of connectors.</li></ul><figure class=\"gg gh gi gj gk gl in io fn ip iq ir is it bh if iu iv iw ix iy paragraph-image\"><img alt=\"Image for post\" class=\"t u v gt aj\" src=\"https://miro.medium.com/max/1418/1*Ia01H4MNO9ycdRWXmX0wsw.jpeg\" width=\"709\" srcset=\"https://miro.medium.com/max/552/1*Ia01H4MNO9ycdRWXmX0wsw.jpeg 276w, https://miro.medium.com/max/1000/1*Ia01H4MNO9ycdRWXmX0wsw.jpeg 500w\" sizes=\"500px\"></figure><ul class><li id=\"16b5\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia jb jc jd dy\">In our example, PodHighCpuLoad alert will cause execution along the red path out the Alert Parser node (first node), versus a ServiceDown alert will take the green path. Blue is the default path. This is achieved by configuring match/action rules inside the nodes</li><li id=\"4131\" class=\"hd he db hf b hg je hi hj hk jf hm hn ho jg hq hr hs jh hu hv hw ji hy hz ia jb jc jd dy\">Add an alert handler playbook as a node in the Alert routing playbook. In our alert routing playbook shown above, \u001cRestart Pod Playbook\u001d and \u001cService Down Playbook\u001d nodes invoke respective handler playbooks for PodHighCpuAlert and ServiceDown alerts. We are using the \u001cnested playbooks\u001d capability here to invoke a child playbook from within a parent playbook.</li></ul></div></div></section><section class=\"ct cu cv cw cx\"><div class=\"n p\"><div class=\"ab ac ae af ag cy ai aj\"><figure class=\"gg gh gi il im gl in io fn ip iq ir is it bh if iu iv iw ix iy paragraph-image\"><img alt=\"Image for post\" class=\"t u v gt aj\" src=\"https://miro.medium.com/max/862/1*73LfAAcDFdzZ7569uw4UZQ.png\" width=\"431\" srcset=\"https://miro.medium.com/max/552/1*73LfAAcDFdzZ7569uw4UZQ.png 276w, https://miro.medium.com/max/862/1*73LfAAcDFdzZ7569uw4UZQ.png 431w\" sizes=\"431px\"></figure><p id=\"c606\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\"><strong class=\"hf ib\"><em class=\"ja\">Alert Handler Playbook:</em></strong></p><ul class><li id=\"5367\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia jb jc jd dy\">Create an alert handler playbook to auto remediate a specific alert, PodHighCpuLoad in our example. These playbooks are triggered by the alert routing playbook as explained in the previous step</li><li id=\"33e4\" class=\"hd he db hf b hg je hi hj hk jf hm hn ho jg hq hr hs jh hu hv hw ji hy hz ia jb jc jd dy\">We have used shell scripts that invoke kubectl commands, Slack and Pagerduty connectors for this example</li><li id=\"500d\" class=\"hd he db hf b hg je hi hj hk jf hm hn ho jg hq hr hs jh hu hv hw ji hy hz ia jb jc jd dy\">As an auto remediation example, we check to see if the pod exists in the cluster. If yes, we follow the green execution path, Send a slack message, restart the pod and finally send an \u001cenriched\u001d page to the on call person with the current state of the cluster</li></ul></div></div></section><section class=\"ct cu cv cw cx\"><div class=\"n p\"><div class=\"ab ac ae af ag cy ai aj\"><p id=\"fdfc\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">At this point we are ready with the setup to handle alerts for our example use case, PodHighCpuLoad. As simple as that!!</p><p id=\"3cdc\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">Let\u0019s follow along the execution&amp;</p><figure class=\"gg gh gi gj gk gl in io fn ip iq ir is it bh if iu iv iw ix iy paragraph-image\"><img alt=\"Image for post\" class=\"t u v gt aj\" src=\"https://miro.medium.com/max/620/1*Xqi8MqH5bcpELZ3OhuhZjg.png\" width=\"310\" srcset=\"https://miro.medium.com/max/552/1*Xqi8MqH5bcpELZ3OhuhZjg.png 276w, https://miro.medium.com/max/620/1*Xqi8MqH5bcpELZ3OhuhZjg.png 310w\" sizes=\"310px\"></figure><p id=\"ea35\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">We login to the nginx container and run an inline script to cause cpu to spike, resulting in triggering of the alert</p><figure class=\"gg gh gi gj gk gl in io fn ip iq ir is it bh if iu iv iw ix iy paragraph-image\"><img alt=\"Image for post\" class=\"t u v gt aj\" src=\"https://miro.medium.com/max/1428/1*8H07NE3fHwfI3M0IA27q7A.jpeg\" width=\"714\" srcset=\"https://miro.medium.com/max/552/1*8H07NE3fHwfI3M0IA27q7A.jpeg 276w, https://miro.medium.com/max/1000/1*8H07NE3fHwfI3M0IA27q7A.jpeg 500w\" sizes=\"500px\"></figure><p id=\"f2f9\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">Which in turn triggers execution of the alert routing playbook</p><figure class=\"gg gh gi gj gk gl in io fn ip iq ir is it bh if iu iv iw ix iy paragraph-image\"><img alt=\"Image for post\" class=\"t u v gt aj\" src=\"https://miro.medium.com/max/1082/1*dGw5xuHJ9bnboPTSA10p2Q.png\" width=\"541\" srcset=\"https://miro.medium.com/max/552/1*dGw5xuHJ9bnboPTSA10p2Q.png 276w, https://miro.medium.com/max/1000/1*dGw5xuHJ9bnboPTSA10p2Q.png 500w\" sizes=\"500px\"></figure><p id=\"9182\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">You can follow along live execution of the playbook. The color of the triangle on top left corner of each connector indicates state of execution of that connector. Blue indicates the action completed successfully and orange indicates \u001cin process\u001d. As can be seen, execution follows the red path as expected and triggers execution of the alert handler playbook for PodHighCpuLoad</p><figure class=\"gg gh gi gj gk gl in io fn ip iq ir is it bh if iu iv iw ix iy paragraph-image\"><img alt=\"Image for post\" class=\"t u v gt aj\" src=\"https://miro.medium.com/max/704/1*KXiiigBkk468xb4WCK2Npw.jpeg\" width=\"352\" srcset=\"https://miro.medium.com/max/552/1*KXiiigBkk468xb4WCK2Npw.jpeg 276w, https://miro.medium.com/max/704/1*KXiiigBkk468xb4WCK2Npw.jpeg 352w\" sizes=\"352px\"></figure><p id=\"89f8\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">Alert handler playbook execution proceeds along the green path as expected, triggering slack message and restarting of the pod (i.e deletion of the pod. K8s re-spawns the pod). Finally an enriched page is sent to the user</p><figure class=\"gg gh gi gj gk gl in io fn ip iq ir is it bh if iu iv iw ix iy paragraph-image\"><img alt=\"Image for post\" class=\"t u v gt aj\" src=\"https://miro.medium.com/max/1796/1*kQKJPyi7lO50KAyG0zIlBw.jpeg\" width=\"898\" srcset=\"https://miro.medium.com/max/552/1*kQKJPyi7lO50KAyG0zIlBw.jpeg 276w, https://miro.medium.com/max/1000/1*kQKJPyi7lO50KAyG0zIlBw.jpeg 500w\" sizes=\"500px\"></figure><p id=\"7fcb\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">Slack messages sent as part of the playbook execution</p><figure class=\"gg gh gi gj gk gl in io fn ip iq ir is it bh if iu iv iw ix iy paragraph-image\"><img alt=\"Image for post\" class=\"t u v gt aj\" src=\"https://miro.medium.com/max/1360/1*W0MwYu5jum6u_5rdOI-3GQ.png\" width=\"680\" srcset=\"https://miro.medium.com/max/552/1*W0MwYu5jum6u_5rdOI-3GQ.png 276w, https://miro.medium.com/max/1000/1*W0MwYu5jum6u_5rdOI-3GQ.png 500w\" sizes=\"500px\"></figure><p id=\"868d\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">\u001cEnriched\u001d Pagerduty notification with information regarding the incident. Corrective action taken and the current state of the Pods.</p></div></div></section><section class=\"ct cu cv cw cx\"><div class=\"n p\"><div class=\"ab ac ae af ag cy ai aj\"><p id=\"4f48\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">As you can see, in a few simple steps, you can create a robust, customized and automated response to Kubernetes alerts.</p><p id=\"16ff\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">I truly hope you found this useful . If you would like to know more about creating playbooks, please visit <a href=\"https://epiphani.ai/\" class=\"et jj\">https://epiphani.ai</a></p><p id=\"8644\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">If you are interested in alert response automation, you can sign up to for alpha trial at <a href=\"https://epiphani.ai/#signup\" class=\"et jj\">https://epiphani.ai/#signup</a></p><p id=\"a319\" class=\"hd he db hf b hg hh hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ct dy\">Finally, we would love to hear your thoughts on this article. Kindly provide feedback at <a href=\"mailto:feedback@epiphani.ai\" class=\"et jj\">feedback@epiphani.ai</a></p></div></div></section></div></article></div>",
      "contentAsText": "IntroductionOperationalizing Kubernetes is hard and you can use all the help you can get. You want to be able to monitor your Kubernetes system closely, trigger alerts based on defined rules and finally you need the capability to automate response to these alertsAlerting FrameworkPrometheus provides the means to monitor Kubernetes and trigger alerts based on defined rules. Alertmanager, on the other hand, consumes these alerts from Prometheus and provides the capability to group the alerts, inhibit and silence the alerts as well as route the alerts to configured receivers.Automated ResponseOne can configure Slack, Pagertduty etc to be the receivers for these alerts, in which case the alert notification is delivered as a message to the recipients, but what you truly want is the ability to automate the response to these alerts. Instead of manual intervention to resolve these alerts, you ideally want automation to auto-remediate if possible. At the minimum, you want to enrich the message with diagnostic information related to the alert before sending it out to the on-call users so that much of the leg work required to collect this information is performed upfront and the user can focus on what\u0019s important, i.e. resolve the issue expeditiously.Closing out the Kubernetes monitoring loop using Epiphani PlaybooksConfiguring Epiphani as the receiver for alerts in Alertmanager enables automation of alerts response. You can now easily and simply auto remediate the alerts, essentially closing out the monitoring loop.Let\u0019s walkthrough an example that illustrates how this can be achieved.Kubernetes Setup:We have deployed Kubernetes on Minikubenginx applicationPrometheus & Alertmanager deployed in monitoring namespacePlease refer to the following link for more information regarding installing Prometheus and Alertmanager and setting up rules and alert configurationPrometheus Rule:A simple rule to generate an alert if the nginx cpu usage breaches a certain threshold has been configuredAlertmanager Configuration:Using the webhook configuration, we specify Epiphani webhook endpoint. We are in the process of integrating Epiphani configuration natively in to AlertmanagerEpiphani Alert Routing:Create an alert routing playbook that routes incoming alerts from Alertmanager to a corresponding handler playbookPlaybook consists of connectors, each performing a specific action. Connectors can be 3rd party connectors such as Slack, AWS EC2, Pagerduty, etc or they can be in house scripts. Epiphani provides a rich set of connectors.In our example, PodHighCpuLoad alert will cause execution along the red path out the Alert Parser node (first node), versus a ServiceDown alert will take the green path. Blue is the default path. This is achieved by configuring match/action rules inside the nodesAdd an alert handler playbook as a node in the Alert routing playbook. In our alert routing playbook shown above, \u001cRestart Pod Playbook\u001d and \u001cService Down Playbook\u001d nodes invoke respective handler playbooks for PodHighCpuAlert and ServiceDown alerts. We are using the \u001cnested playbooks\u001d capability here to invoke a child playbook from within a parent playbook.Alert Handler Playbook:Create an alert handler playbook to auto remediate a specific alert, PodHighCpuLoad in our example. These playbooks are triggered by the alert routing playbook as explained in the previous stepWe have used shell scripts that invoke kubectl commands, Slack and Pagerduty connectors for this exampleAs an auto remediation example, we check to see if the pod exists in the cluster. If yes, we follow the green execution path, Send a slack message, restart the pod and finally send an \u001cenriched\u001d page to the on call person with the current state of the clusterAt this point we are ready with the setup to handle alerts for our example use case, PodHighCpuLoad. As simple as that!!Let\u0019s follow along the execution&We login to the nginx container and run an inline script to cause cpu to spike, resulting in triggering of the alertWhich in turn triggers execution of the alert routing playbookYou can follow along live execution of the playbook. The color of the triangle on top left corner of each connector indicates state of execution of that connector. Blue indicates the action completed successfully and orange indicates \u001cin process\u001d. As can be seen, execution follows the red path as expected and triggers execution of the alert handler playbook for PodHighCpuLoadAlert handler playbook execution proceeds along the green path as expected, triggering slack message and restarting of the pod (i.e deletion of the pod. K8s re-spawns the pod). Finally an enriched page is sent to the userSlack messages sent as part of the playbook execution\u001cEnriched\u001d Pagerduty notification with information regarding the incident. Corrective action taken and the current state of the Pods.As you can see, in a few simple steps, you can create a robust, customized and automated response to Kubernetes alerts.I truly hope you found this useful . If you would like to know more about creating playbooks, please visit https://epiphani.aiIf you are interested in alert response automation, you can sign up to for alpha trial at https://epiphani.ai/#signupFinally, we would love to hear your thoughts on this article. Kindly provide feedback at feedback@epiphani.ai",
      "publishedDate": "2020-08-31T03:01:19.801Z",
      "description": "Operationalizing Kubernetes is hard and you can use all the help you can get. You want to be able to monitor your Kubernetes system closely, trigger alerts based on defined rules and finally you need…",
      "ogDescription": "Introduction"
    },
    {
      "url": "https://filippobuletto.github.io/kubectl-java-test/",
      "title": "I\u0019ve developed a kubectl plugin in Java, and I\u0019m proud of it!",
      "content": "<p class=\"page__inner-wrap\"> <section class=\"page__content\"> <p>Come up and have a look at it here:</p> <p><a href=\"https://github.com/filippobuletto/kubectl-java-test\">https://github.com/filippobuletto/kubectl-java-test</a></p> <p><strong>Please note that this isn\u0019t a true kubectl plugin, this is just sample code!</strong></p> <p>java-test plugin gets pod names and other information, you can limit the namespace scope or use a regex pattern to search for pod names.</p> <div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nv\">$ </span>kubectl java-test <span class=\"nt\">-h</span>\nUsage: Simple kubectl plugin <span class=\"o\">[</span><span class=\"nt\">-hV</span><span class=\"o\">]</span> <span class=\"o\">[</span><span class=\"nt\">-n</span><span class=\"o\">=</span>NAMESPACE] <span class=\"o\">[</span>NAMEPATTERN]\nGets Pods names and other stuff <span class=\"o\">[</span>NAMEPATTERN] Regex pattern <span class=\"nt\">-h</span>, <span class=\"nt\">--help</span> Show this <span class=\"nb\">help </span>message and exit. <span class=\"nt\">-n</span>, <span class=\"nt\">--namespace</span><span class=\"o\">=</span>NAMESPACE The namespace <span class=\"nt\">-V</span>, <span class=\"nt\">--version</span>       Print version information and exit.\n</code></pre></div></div> <h2 id=\"install\">Install</h2> <p>Install <a href=\"https://adoptopenjdk.net/installation.html\">Java 11+</a> and <a href=\"https://jbang.dev/download\">jbang</a> and then:</p> <div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\"># Note: the .java extension must be omitted in order to make kubectl able to recognize the plugin</span>\n<span class=\"c\"># See https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/#naming-a-plugin</span>\ncurl <span class=\"nt\">-Lo</span> kubectl-java_test https://github.com/filippobuletto/kubectl-java-test/releases/latest/download/kubectl-java_test.java\n<span class=\"nb\">sudo install</span> <span class=\"nt\">-m755</span> kubectl-java_test /usr/local/bin\n<span class=\"nb\">rm </span>kubectl-java_test\n</code></pre></div></div> <h2 id=\"usage\">Usage</h2> <div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>kubectl java-test -n &lt;namespace&gt; &lt;pod_name_regex&gt;\n</code></pre></div></div> <p>More on how to do it yourself later!</p> </section> <footer class=\"page__meta\"> <p class=\"page__taxonomy\"> <strong><i class=\"fas fa-fw fa-tags\"></i> Tags: </strong> <span> <a href=\"/tags/#java\" class=\"page__taxonomy-item\">java</a><span class=\"sep\">, </span> <a href=\"/tags/#jbang\" class=\"page__taxonomy-item\">jbang</a><span class=\"sep\">, </span> <a href=\"/tags/#kubernetes\" class=\"page__taxonomy-item\">kubernetes</a><span class=\"sep\">, </span> <a href=\"/tags/#tools\" class=\"page__taxonomy-item\">tools</a> </span> </p> <p class=\"page__taxonomy\"> <strong><i class=\"fas fa-fw fa-folder-open\"></i> Categories: </strong> <span> <a href=\"/categories/#devops\" class=\"page__taxonomy-item\">DevOps</a><span class=\"sep\">, </span> <a href=\"/categories/#programming\" class=\"page__taxonomy-item\">Programming</a> </span> </p> <p class=\"page__date\"><strong><i class=\"fas fa-fw fa-calendar-alt\"></i> Updated:</strong> <time>August 30, 2020</time></p> </footer> <section class=\"page__share\"> <a href=\"https://twitter.com/intent/tweet?via=filippomito&amp;text=I%27ve+developed+a+kubectl+plugin+in+Java%2C+and+I%27m+proud+of+it%21%20https%3A%2F%2Ffilippobuletto.github.io%2Fkubectl-java-test%2F\" class=\"btn btn--twitter\"><i class=\"fab fa-fw fa-twitter\"></i><span> Twitter</span></a> <a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Ffilippobuletto.github.io%2Fkubectl-java-test%2F\" class=\"btn btn--facebook\"><i class=\"fab fa-fw fa-facebook\"></i><span> Facebook</span></a> <a href=\"https://plus.google.com/share?url=https%3A%2F%2Ffilippobuletto.github.io%2Fkubectl-java-test%2F\" class=\"btn btn--google-plus\"><i class=\"fab fa-fw fa-google-plus\"></i><span> Google+</span></a> <a href=\"https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3A%2F%2Ffilippobuletto.github.io%2Fkubectl-java-test%2F\" class=\"btn btn--linkedin\"><i class=\"fab fa-fw fa-linkedin\"></i><span> LinkedIn</span></a>\n</section> </p>",
      "contentAsText": "  Come up and have a look at it here: https://github.com/filippobuletto/kubectl-java-test Please note that this isn\u0019t a true kubectl plugin, this is just sample code! java-test plugin gets pod names and other information, you can limit the namespace scope or use a regex pattern to search for pod names. $ kubectl java-test -h\nUsage: Simple kubectl plugin [-hV] [-n=NAMESPACE] [NAMEPATTERN]\nGets Pods names and other stuff [NAMEPATTERN] Regex pattern -h, --help Show this help message and exit. -n, --namespace=NAMESPACE The namespace -V, --version       Print version information and exit.\n Install Install Java 11+ and jbang and then: # Note: the .java extension must be omitted in order to make kubectl able to recognize the plugin\n# See https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/#naming-a-plugin\ncurl -Lo kubectl-java_test https://github.com/filippobuletto/kubectl-java-test/releases/latest/download/kubectl-java_test.java\nsudo install -m755 kubectl-java_test /usr/local/bin\nrm kubectl-java_test\n Usage kubectl java-test -n <namespace> <pod_name_regex>\n More on how to do it yourself later!     Tags:   java,  jbang,  kubernetes,  tools     Categories:   DevOps,  Programming    Updated: August 30, 2020    Twitter  Facebook  Google+  LinkedIn\n ",
      "publishedDate": "2020-08-29T22:00:00.000Z",
      "description": "Here it is a simple kubectl plugin written in Java language and runned using jbang. Detailed guide later.",
      "ogDescription": "Here it is a simple kubectl plugin written in Java language and runned using jbang. Detailed guide later."
    }
  ]
}